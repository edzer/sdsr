# Proximity and Areal Data  {#area}

Areal units of observation are very often used when simultaneous observations are aggregated within non-overlapping boundaries. The boundaries may be those of administrative entities, and may be related to underlying spatial processes, such as commuting flows, but are usually arbitrary. If they do not match the underlying and unobserved spatial processes in one or more variables of interest, proximate areal units will contain parts of the underlying processes, engendering spatial autocorrelation. By proximity, we mean _closeness_ in ways that make sense for the data generation processes thought to be involved. In cross-sectional geostatistical analysis with point support, measured distance makes sense for typical data generation processes. In similar analysis of areal data, sharing a border may make more sense, because that is what we do know, but we cannot measure the distance between the areas in as adequate a way.  

With support of data we mean the physical size (length, area, volume) associated with an individual observational unit (measurement).  It is possible to represent the support of areal data by a point, despite the fact that the data have polygonal support. The centroid of the polygon may be taken as a representative point, or the centroid of the largest polygon in a multi-polygon object. When data with intrinsic point support are treated as areal data, the change of support goes the other way, from the known point to a non-overlapping tessellation such as a Voronoi diagram or Dirichlet tessellation or Thiessen polygons often through a Delaunay triangulation using projected coordinates. Here, different metrics may also be chosen, or distances measured on a network rather than on the plane. There is also a literature using weighted Voronoi diagrams in local spatial analysis [see for example @doi:10.1080/13658810601034267; @doi:10.1080/13658810701587891; @SHE201570].

When the intrinsic support of the data is represented as points, but the underlying process is between proximate observations rather than driven chiefly by distance however measured between observations, the data may be aggregate counts or totals (polling stations, retail turnover) or represent a directly observed characteristic of the observation (opening hours of the polling station). Obviously, the risk of mis-representing the footprint of the underlying spatial processes remains in all of these cases, not least because the observations are taken as encompassing the entirety of the underlying process in the case of tessellation of the whole area of interest. This is distinct from the geostatistical setting in which observations are rather samples taken using some scheme within the area of interest. It is also partly distinct from the practice of taking areal sample plots within the area of interest but covering only a small proportion of the area, typically used in ecological and environmental research.

In order to explore and analyse areal data of these kinds in chapters \@ref(spatautocorr), \@ref(spatglmm) and \@ref(spatecon), methods are needed to represent the proximity of observations. This chapter then considers a subset of the such methods, where the spatial processes are considered as working through proximity understood in the first instance as contiguity, as a graph linking observations taken as neighbours. This graph is typically undirected and unweighted, but may be directed and/or weighted in certain settings, which then leads to further issues with regard to symmetry. In principle, proximity would be expected to operate symmetrically in space, that is that the influence of $i$ on $j$ and of $j$ on $i$ based on their relative positions should be equivalent. Edge effects are not considered in standard treatments.


## Representing proximity in **spdep**

Handling spatial autocorrelation using relationships to neighbours on a graph takes the graph as given, chosen by the analyst. This differs from the geostatistical approach in which the analyst chooses the binning of the empirical variogram and function used, and then the way the variogram is fitted. Both involve a priori choices, but represent the underlying correlation in different ways [@wall:04]. In Bavaud [-@bavaud:98] and work citing his contribution, attempts have been made to place graph-based neighbours in a broader context.

One issue arising in the creation of objects representing neighbourhood relationships is that of no-neighbour areal units [@bivand+portnov:04]. Islands or units separated by rivers may not be recognised as neighbours when the units have areal support and when using topological relationships such as shared boundaries. In some settings, for example `mrf` (Markov Random Field) terms in `mgcv::gam()` and similar model fitting functions that require undirected connected graphs, a requirement which is violated when there are disconnected subgraphs. 

No-neighbour observations can also occur when a distance threshold is used between points, where the threshold is smaller than the maximum nearest neighbour distance. Shared boundary contiguities are not affected by using geographical, unprojected coordinates, but all point-based approaches use distance in one way or another, and need to calculate distances in an appropriate way.

The **spdep** package provides an `nb` class for neighbours, a list of length equal to the number of observations, with integer vector components. No-neighbours are encoded as an integer vector with a single element `0L`, and observations with neighbours as sorted integer vectors containing values in `1L:n` pointing to the neighbouring observations. This is a typical row-oriented sparse representation of neighbours. **spdep** provides many ways of constructing `nb` objects, and the representation and construction functions are widely used in other packages. 

**spdep** builds on the `nb` representation (undirected or directed graphs) with the `listw` object, a list with three components, an `nb` object, a matching list of numerical weights, and a single element character vector containing the single letter name of the way in which the weights were calculated. The most frequently used approach in the social sciences is calculating weights by row standardization, so that all the non-zero weights for one observation will be the inverse of the cardinality of its set of neighbours (`1/card(nb[[i]])`).

We will be using election data from the 2015 Polish Presidential election in this chapter, with 2495 municipalities and Warsaw boroughs (see figure \@ref(fig:plotpolpres15) for a **tmap** map (section \@ref(tmap)) of the municipality types), and complete count data from polling stations aggregated to these areal units. The data are an **sf** `sf` object:

```{r setup_sa, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, paged.print=FALSE)
```

```{r}
library(sf)
```

```{r}
data(pol_pres15, package = "spDataLarge")
pol_pres15 %>% 
    subset(select = c(TERYT, name, types)) %>% 
    head()
```
(ref:plotpolpres15fc) Polish municipality types 2015
```{r plotpolpres15, out.width='100%', fig.cap='(ref:plotpolpres15fc)', fig=TRUE, echo=TRUE}
library(tmap)
tm_shape(pol_pres15) + tm_fill("types")
```

For safety's sake, we impose topological validity:

```{r}
if (!all(st_is_valid(pol_pres15))) pol_pres15 <- st_make_valid(pol_pres15)
```

Between early 2002 and April 2019, **spdep** contained functions for constructing and handling neighbour and spatial weights objects, tests for spatial autocorrelation, and model fitting functions. The latter have been split out into **spatialreg**, and will be discussed in the next chapter. **spdep** [@R-spdep] now accommodates objects represented using **sf** classes and **sp** classes directly.

```{r, message=FALSE}
library(spdep)
```

## Contiguous neighbours

The `poly2nb()` function in **spdep** takes the boundary points making up the polygon boundaries in the object passed as the `pl=` argument, typically an `"sf"` or `"sfc"` object with `"POLYGON"` or `"MULTIPOLYGON"` geometries. For each observation, the function checks whether at least one (`queen=TRUE`, default), or at least two (rook, `queen=FALSE`) points are within `snap=` distance units of each other. The distances are planar in the raw coordinate units, ignoring geographical projections. Once the required number of sufficiently close points is found, the search is stopped.

```{r}
args(poly2nb)
```
From **spdep** 1.1-7, the GEOS interface of the **sf** package is used within `poly2nb()` if `foundInBox=NULL` to find the candidate neighbours and populate `foundInBox` internally. In this case, this use of spatial indexing (STRtree queries) in GEOS through **sf** is the default:

```{r}
system.time(pol_pres15 %>% poly2nb(queen = TRUE) -> nb_q)
```
Earlier, `foundInBox=` accepted the output of the  **rgeos** `gUnarySTRtreeQuery()` function to list candidate neighbours, that is polygons whose bounding boxes intersect the bounding boxes of other polygons. The print method shows the summary structure of the neighbour object:

```{r}
nb_q
```

From **sf** version 1.0-0, the **s2** package [@R-s2] is used by default for spherical geometries, as `st_intersects()` used in `poly2nb()` passes calculation to `s2::s2_intersects_matrix()` (see chapter \@ref(spherical)). From **spdep** version 1.1-9, if `sf_use_s2()`is `TRUE`, spherical intersection is used to find candidate neighbours; as with GEOS, the underlying `s2` library uses fast spatial indexing.

```{r, echo=FALSE}
old_use_s2 <- sf_use_s2()
```

```{r}
sf_use_s2(TRUE)
```

```{r}
(pol_pres15 %>% st_transform("OGC:CRS84") -> pol_pres15_ll) %>% 
    poly2nb(queen = TRUE) -> nb_q_s2
```

Spherical and planar intersection of the input polygons yield the same contiguity neighbours in this case; in both cases valid input geometries are desirable:

```{r}
all.equal(nb_q, nb_q_s2, check.attributes=FALSE)
```

Note that `nb` objects record both symmetric neighbour relationships, because these objects admit asymmetric relationships as well, but these duplications are not needed for object construction.

Most of the **spdep** functions for constructing neighbour objects take a `row.names=` argument, the value of which is stored as a `region.id` attribute. If not given, the values are taken from `row.names()` of the first argument. These can be used to check that the neighbours object is in the same order as data. If `nb` objects are subsetted, the indices change to continue to be within `1:length(subsetted_nb)`, but the `region.id` attribute values point back to the object from which it was constructed. This is used in out-of-sample prediction from spatial regression models discussed briefly in section \@ref(spatecon-pred).

We can also check that this undirected graph is connected using the `n.comp.nb()` function; while some model estimation techniques do not support graphs that are not connected, it is helpful to be aware of possible problems [@FRENISTERRANTINO201825]:
```{r}
(nb_q %>% n.comp.nb())$nc
```

This approach is equivalent to treating the neighbour object as a graph and using graph analysis on that graph [@csardi+nepusz:06; @R-igraph], by first coercing to a binary sparse matrix [@R-Matrix]:

```{r}
library(Matrix, warn.conflicts = FALSE)
nb_q %>% 
    nb2listw(style = "B") %>% 
    as("CsparseMatrix") -> smat
library(igraph, warn.conflicts = FALSE)
(smat %>% 
        graph.adjacency() -> g1) %>% 
    count_components()
```

Neighbour objects may be exported and imported in GAL format for exchange with other software, using `write.nb.gal()` and `read.gal()`:

```{r}
tf <- tempfile(fileext = ".gal")
write.nb.gal(nb_q, tf)
```

## Graph-based neighbours

If areal units are an appropriate representation, but only points on the plane have been observed, contiguity relationships may be approximated using graph-based neighbours. In this case, the imputed boundaries tessellate the plane such that points closer to one observation than any other fall within its polygon. The simplest form is by using triangulation, here using the `deldir()` function in the **deldir** package. Because the function returns from $i$ and to $j$ identifiers, it is easy to construct a long representation of a `listw` object, as used in the S-Plus SpatialStats module and the `sn2listw()` function internally to construct an `nb` object (ragged wide representation). Alternatives such as GEOS often fail to return sufficient information to permit the neighbours to be identified.

The output of these functions is then converted to the `nb` representation using `graph2nb()`, with the possible use of the `sym=` argument to coerce to symmetry. We take the centroids of the largest component polygon for each observation as the point representation; population-weighted centroids might have been a better choice if they were available:

```{r}
pol_pres15 %>% 
    st_geometry() %>% 
    st_centroid(of_largest_polygon = TRUE) -> coords 
(coords %>% tri2nb() -> nb_tri)
```


The average number of neighbours is similar to the Queen boundary contiguity case, but if we look at the distribution of edge lengths using `nbdists()`, we can see that although the upper quartile is about 15 km, the maximum is almost 300 km, an edge along much of one side of the convex hull. The short minimum distance is also of interest, as many centroids of urban municipalities are very close to the centroids of their surrounding rural counterparts.

```{r}
nb_tri %>% 
    nbdists(coords) %>% 
    unlist() %>% 
    summary()
```
Triangulated neighbours also yield a connected graph:
```{r}
(nb_tri %>% n.comp.nb())$nc
```

Graph-based approaches include `soi.graph()` - discussed here, `relativeneigh()` and `gabrielneigh()`.

The Sphere of Influence `soi.graph()` function takes triangulated neighbours and prunes off neighbour relationships represented by edges that are unusually long for each point, especially around the convex hull [@avis+horton:1985].
```{r}
(nb_tri %>% 
        soi.graph(coords) %>% 
        graph2nb() -> nb_soi)
```
Unpicking the triangulated neighbours does however remove the connected character of the underlying graph:

```{r}
(nb_soi %>% n.comp.nb() -> n_comp)$nc
```
The algorithm has stripped out longer edges leading to urban and rural municipality pairs where their centroids are very close to each other because the rural ones completely surround the urban, giving 15 pairs of neighbours unconnected to the main graph:

```{r}
table(n_comp$comp.id)
```

The largest length edges along the convex hull have been removed, but "holes" have appeared where the unconnected pairs of neighbours have appeared. The differences between `nb_tri` and `nb_soi` are shown in orange in Figure \@ref(fig:plotnbdiff).

(ref:plotnbdifffc) Triangulated (orange + black) and sphere of influence neighbours (black)
```{r plotnbdiff, out.width='100%', fig.cap='(ref:plotnbdifffc)', fig=TRUE}
opar <- par(mar = c(0,0,0,0)+0.5)
pol_pres15 %>% 
    st_geometry() %>% 
    plot(border = "grey", lwd = 0.5)
nb_soi %>% 
    plot(coords = coords, add = TRUE, points = FALSE, lwd = 0.5)
nb_tri %>% 
    diffnb(nb_soi) %>% 
    plot(coords = coords, col = "orange", add = TRUE, points = FALSE, lwd = 0.5)
par(opar)
```

## Distance-based neighbours

Distance-based neighbours can be constructed using `dnearneigh()`, with a distance band with lower `d1=` and upper `d2=` bounds controlled by the `bounds=` argument. If spherical coordinates are used and either specified in the coordinates object `x` or with `x` as a two column matrix and `longlat=TRUE`, great circle distances in km will be calculated assuming the WGS84 reference ellipsoid, or if `use_s2=TRUE`, a 200-cell `s2` buffer is constructed around each point, points falling within the buffer chosen, and then chosen points beyond `d2=` dropped. From tests, it appears that spherical spatial indexing is not used, and so is slower than the legacy brute-force approach (see chapter \@ref(spherical)). 

From **spdep** 1.1-7, two arguments have been added, to use functionality in the **dbscan** package [@R-dbscan] for finding neighbours using planar spatial indexing in two or three dimensions by default, and not to test the symmetry of the output neighbour object. 

The `knearneigh()` function for $k$-nearest neighbours returns a `knn` object, converted to an `nb` object using `knn2nb()`. It can also use great circle distances, not least because nearest neighbours may differ when uprojected coordinates are treated as planar. `k=` should be a small number. For projected coordinates, the **dbscan** package is used to compute nearest neighbours more efficiently. Note that `nb` objects constructed in this way are most unlikely to be symmetric, hence `knn2nb()` has a `sym=` argument to permit the imposition of symmetry, which will mean that all units have at least `k=` neighbours, not that all units will have exactly `k=` neighbours. From **spdep** version 1.1-9 and when `sf_use_s2()` is `TRUE`, `knearneigh()` will use fast spherical spatial indexing when the input object is of class `"sf"` or `"sfc"`.

The `nbdists()` function returns the length of neighbour relationship edges in the units of the coordinates if the coordinates are projected, in km otherwise. In order to set the upper limit for distance bands, one may first find the maximum first nearest neighbour distance, using `unlist()` to remove the list structure of the returned object. From **spdep** version 1.1-9 and when `sf_use_s2()` is `TRUE`, `nbdists()` will use fast spherical distance calculations when the input object is of class `"sf"` or `"sfc"`.

```{r}
coords %>% 
    knearneigh(k = 1) %>% 
    knn2nb() %>% 
    nbdists(coords) %>% 
    unlist() %>% 
    summary()
```
Here the largest first nearest neighbour distance is just under 18 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour:

```{r}
system.time(coords %>% dnearneigh(0, 18000) -> nb_d18)
```

For this moderate number of observations, use of spatial indexing does not yield advantages in run times:

```{r}
system.time(coords %>% dnearneigh(0, 18000, use_kd_tree = FALSE) -> nb_d18a)
```

and the output objects are the same:

```{r}
all.equal(nb_d18, nb_d18a, check.attributes = FALSE)
```
```{r}
nb_d18
```
However, even though there are no no-neighbour observations (their presence is reported by the print method for `nb` objects), the graph is not connected, as a pair of observations are each others' only neighbours.
```{r}
(nb_d18 %>% n.comp.nb() -> n_comp)$nc
```
```{r}
table(n_comp$comp.id)
```
Adding 300 m to the threshold gives us a neighbour object with no no-neighbour units, and all units can be reached from all others across the graph.
```{r}
(coords %>% dnearneigh(0, 18300) -> nb_d183)
```
```{r}
(nb_d183 %>% n.comp.nb())$nc
```

One characteristic of distance-based neighbours is that more densely settled areas, with units which are smaller in terms of area (Warsaw boroughs are much smaller on average, but have almost 30 neighbours). Having many neighbours smooths the neighbour relationship across more neighbours. 

For use later, we also construct a neighbour object with no-neighbour units, using a threshold of 16 km:
```{r}
(coords %>% dnearneigh(0, 16000) -> nb_d16)
```

It is possible to control the numbers of neighbours directly using $k$-nearest neighbours, either accepting asymmetric neighbours:
```{r}
((coords %>% knearneigh(k = 6) -> knn_k6) %>% knn2nb() -> nb_k6)
```

or imposing symmetry:
```{r}
(knn_k6 %>% knn2nb(sym = TRUE) -> nb_k6s)
```

Here the size of `k=` is sufficient to ensure connectedness, although the graph is not planar as edges cross at locations other than nodes, which is not the case for contiguous or graph-based neighbours.
```{r}
(nb_k6s %>% n.comp.nb())$nc
```

In the case of points on the sphere (see chapter \@ref(spherical)), the output of `st_centroid()` will differ, so rather than inverse projecting the points, we extract points as geographical coordinates from the inverse projected polygon geometries:

```{r, echo=FALSE}
old_use_s2 <- sf_use_s2()
```

```{r}
sf_use_s2(TRUE)
```

```{r}
pol_pres15_ll %>% 
    st_geometry() %>% 
    st_centroid(of_largest_polygon = TRUE) -> coords_ll
```

From **spdep** version 1.1-9, fast spatial indexing in **s2** is used to find the nearest neighbours:

```{r}
(coords_ll %>% knearneigh(k = 6) %>% knn2nb() -> nb_k6_ll)
```
These neighbours differ from the planar `k=6` nearest neighbours as would be expected:

```{r}
isTRUE(all.equal(nb_k6, nb_k6_ll, check.attributes = FALSE))
```
The `nbdists()` function also uses **s2** to find distances on the sphere when the `"sf"` or `"sfc"`input object is in geographical coordinates (distances returned in kilometres):

```{r}
nb_q %>% nbdists(coords_ll) %>% unlist() %>% summary()
```
These differ a little for the same weights object when planar coordinates are used (distances returned in the metric of the points):

```{r}
nb_q %>% nbdists(coords) %>% unlist() %>% summary()
```
```{r, echo=FALSE, results='hide'}
sf_use_s2(old_use_s2)
```


## Weights specification

Once neighbour objects are available, further choices need to made in specifying the weights objects. The `nb2listw()` function is used to create a `listw` weights object with an `nb` object, a matching list of weights vectors, and a style specification. Because handling no-neighbour observations now begins to matter, the `zero.policy=` argument is introduced. By default, this is `FALSE`, indicating that no-neighbour observations will cause an error, as the spatially lagged value for an observation with no neighbours is not available. By convention, zero is substituted for the lagged value, as the cross product of a vector of zero-valued weights and a data vector, hence the name of `zero.policy`.

```{r}
args(nb2listw)
```
We will be using the helper function `spweights.constants()` below to show some consequences of varying style choices. It returns constants for a `listw` object, $n$ is the number of observations, `n1` to `n3` are $n-1, \ldots$, `nn` is $n^2$ and $S_0$, $S_1$ and $S_2$ are constants, $S_0$ being the sum of the weights. There is a full discussion of the constants in Bivand and Wong [-@Bivand2018].

```{r}
args(spweights.constants)
```
The `"B"` binary style gives a weight of unity to each neighbour relationship, and typically upweights units with no boundaries on the edge of the study area.

```{r}
(nb_q %>% 
        nb2listw(style = "B") -> lw_q_B) %>% 
    spweights.constants() %>% 
    data.frame() %>% 
    subset(select = c(n, S0, S1, S2))
```

The `"W"` row-standardized style upweights units around the edge of the study area that necessarily have fewer neighbours. This style first gives a weight of unity to each neighbour relationship, then divides these weights by the per unit sums of weights. Naturally this leads to division by zero where there are no neighbours, a not-a-number result, unless the chosen policy is to permit no-neighbour observations. We can see that $S_0$ is now equal to $n$.

```{r}
(nb_q %>% 
        nb2listw(style = "W") -> lw_q_W) %>% 
    spweights.constants() %>% 
    data.frame() %>% 
    subset(select = c(n, S0, S1, S2))
```
Inverse distance weights are used in a number of scientific fields. Some use dense inverse distance matrices, but many of the inverse distances are close to zero, so have little practical contribution, especially as the spatial process matrix is itself dense. Inverse distance weights may be constructed by taking the lengths of edges, changing units to avoid most weights being too large or small (here from m to km), taking the inverse, and passing through the `glist=` argument to `nb2listw()`:

```{r}
nb_d183 %>% 
    nbdists(coords) %>% 
    lapply(function(x) 1/(x/1000)) -> gwts
(nb_d183 %>% nb2listw(glist=gwts, style="B") -> lw_d183_idw_B) %>% 
    spweights.constants() %>% 
    data.frame() %>% 
    subset(select=c(n, S0, S1, S2))
```
No-neighbour handling is by default to prevent the construction of a weights object, making the analyst take a position on how to proceed.

```{r}
try(nb_d16 %>% nb2listw(style="B") -> lw_d16_B)
```
Use can be made of the `zero.policy=` argument to many functions used with `nb` and `listw` objects.

```{r}
nb_d16 %>% 
    nb2listw(style="B", zero.policy=TRUE) %>% 
    spweights.constants(zero.policy=TRUE) %>% 
    data.frame() %>% 
    subset(select=c(n, S0, S1, S2))
```
Note that by default the `adjust.n=` argument to `spweights.constants()` is set by default to `TRUE`, subtracting the count of no-neighbour observations from the observation count, so $n$ is smaller with possible consequences for inference. The complete count can be retrieved by changing the argument.

## Higher order neighbours

We recall the characteristics of the neighbour object based on Queen contiguities:

```{r}
nb_q
```
If we wish to create an object showing $i$ to $k$ neighbours, where $i$ is a neighbour of $j$, and $j$ in turn is a neighbour of $k$, so taking two steps on the neighbour graph, we can use `nblag()`, which automatically removes $i$ to $i$ self-neighbours:

```{r}
(nb_q %>% nblag(2) -> nb_q2)[[2]]
```
The `nblag_cumul()` function cumulates the list of neighbours for the whole list of lags:

```{r}
nblag_cumul(nb_q2)
```
while the set operation `union.nb()` takes two objects, giving here the same outcome:

```{r}
union.nb(nb_q2[[2]], nb_q2[[1]])
```
Returning to the graph representation of the same neighbour object, we can ask how many steps might be needed to traverse the graph:

```{r}
diameter(g1)
```
We step out from each observation across the graph to establish the number of steps needed to reach each other observation by the shortest path, once again finding the same count, and that the municipality is called Lutowiska, close to the Ukrainian border in the far south east of the country.

```{r}
g1 %>% shortest.paths() -> sps
(sps %>% apply(2, max) -> spmax) %>% max()
```

```{r}
mr <- which.max(spmax)
pol_pres15$name0[mr]
```

```{r, echo=FALSE}
pol_pres15$sps1 <- sps[,mr]
tm1 <- tm_shape(pol_pres15) + tm_fill("sps1", title = "Shortest path\ncount")
```


```{r, echo=FALSE}
coords[mr] %>% 
    st_distance(coords) %>% 
    c() %>% 
    (function(x) x/1000)() %>% 
    units::set_units(NULL) -> pol_pres15$dist_52
library(ggplot2)
g1 <- ggplot(pol_pres15, aes(x = sps1, y = dist_52)) + geom_point() + xlab("Shortest path count") +  ylab("km distance")
```

Figure \@ref(fig:shortestpath) shows that contiguity neighbours represent the same kinds of relationships with other observations as distance. Some approaches prefer distance neighbours on the basis that, for example, inverse distance neighbours show clearly how all observations are related to each other. However, the development of tests for spatial autocorrelation and spatial regression models has involved the inverse of a spatial process model, which in turn can be represented as the sum of a power series of the product of a coefficient and a spatial weights matrix, so intrinsically acknowledging the relationships of all observations with all other. Sparse contiguity neighbour objects accommodate rich dependency structures without the need to make the structures explicit.

(ref:shortestpathfc) Relationship of shortest paths to distance for Lutowiska; left panel: shortest path counts from Lutowiska; right panel: plot of shortest paths from Lutowiska to other observations and distances (km) from Lutowiska to other observations
```{r shortestpath, out.width='100%', fig.cap='(ref:shortestpathfc)', fig=TRUE, echo=TRUE, message=FALSE}
gridExtra::grid.arrange(tmap_grob(tm1), g1, nrow=1)
```

## Exercises

1. Which kinds of geometry support are appropriate for which functions creating neighbour objects?
2. Which functions creating neighbour objects are only appropriate for planar representations?
3. What difference might the choice of `rook` rather than `queen` contiguities make on a chessboard?
4. What are the relationships between neighbour set cardinalities (neighbour counts) and row-standardized weights, and how do they open analyses up to edge effects? Use the chessboard you constructed in exercise 3 for both `rook` and `queen` neighbours.



# Measures of spatial autocorrelation {#spatautocorr}

When analysing areal data, it has long been recognised that, if present, spatial autocorrelation changes how we may infer, relative to the default position of independent observations. In the presence of spatial autocorrelation, we can predict the values of observation $i$ from the values observed at $j \in N_i$, the set of its proximate neighbours. Early results [@moran48; @geary:54], entered into research practice gradually, for example the social sciences [@duncanetal61]. These results were then collated and extended to yield a set of basic tools of analysis [@cliff+ord:73; @cliff+ord:81]. 

Cliff and Ord [-@cliff+ord:73] generalised and extended the expression of the spatial weights matrix representation as part of the framework for establishing the distribution theory for join count, Moran's $I$ and Geary's $C$ statistics. This development of what have become known as global measures, returning a single value of autocorrelation for the total study area, has been supplemented by local measures returning values for each areal unit [@getis+ord:92; @anselin:95].

## Measures and process mis-specification

It is not and has never been the case that Tobler's first law of geography: "Everything is related to everything else, but near things are more related than distant things" always holds absolutely. This is and has always been an oversimplification, disguising the underlying entitation, support and other mis-specification problems. Are the units of observation appropriate for the scale of the underlying spatial process?  Could the spatial patterning of the variable of interest for the chosen entitation be accounted for by another variable? 

@10.2307/143141 was published in the same special issue of *Economic Geography* as @10.2307/143140, but Olsson does grasp the important point that spatial autocorrelation is not inherent in spatial phenomena, but often is engendered by inappropriate entitation, by omitted variables and/or inappropriate functional form. The key quote from Olsson is on p. 228:

> The existence of such autocorrelations makes it tempting to agree with Tobler (1970, 236 [the original refers to the pagination of a conference paper]) that 'everything is related to everything else, but near things are more related than distant things.' On the other hand, the fact that the autocorrelations seem to hide systematic specification errors suggests that the elevation of this statement to the status of 'the first law of geography' is at best premature. At worst, the statement may represent the spatial variant of the post hoc fallacy, which would mean that coincidence has been mistaken for a causal relation.

The status of the "first law" is very similar to the belief that John Snow induced the cause of cholera as water-borne from a map. It may be a good way of selling GIS, but it is inaccurate; Snow had a strong working hypothesis prior to visiting Soho, and the map was prepared after the Broad street pump was disabled as documentation that the hypothesis held [@BRODY200064].

Measures of spatial autocorrelation unfortunately pick up other mis-specifications in the way that we model data [@schabenberger+gotway:2005; @McMillen:2003]. For reference, Moran's $I$ is given as [@cliff+ord:81, page 17]:

$$
I = \frac{n \sum_{(2)} w_{ij} z_i z_j}{S_0 \sum_{i=1}^{n} z_i^2}
$$
where $x_i, i=1, \ldots, n$ are $n$ observations on the numeric variable of interest, $z_i = x_i - \bar{x}$, $\bar{x} = \sum_{i=1}^{n} x_i / n$, $\sum_{(2)} = \stackrel{\sum_{i=1}^{n} \sum_{j=1}^{n}}{i \neq j}$, $w_{ij}$ are the spatial weights, and $S_0 = \sum_{(2)} w_{ij}$. 
First we test a random variable using the Moran test, here under the normality assumption (argument `randomisation=FALSE`, default `TRUE`). Inference is made on the statistic $Z(I) = \frac{I - E(I)}{\sqrt{\mathrm{Var}(I)}}$, the z-value compared with the Normal distribution for $E(I)$ and $\mathrm{Var}(I)$ for the chosen assumptions; this `x` does not show spatial autocorrelation with these spatial weights:

```{r}
glance_htest <- function(ht) c(ht$estimate, 
    "Std deviate" = unname(ht$statistic), 
    "p.value" = unname(ht$p.value))
set.seed(1)
(pol_pres15 %>% 
        nrow() %>% 
        rnorm() -> x) %>% 
    moran.test(lw_q_B, randomisation = FALSE, alternative = "two.sided") %>% 
    glance_htest()
```
The test however detects quite strong positive spatial autocorrelation when we insert a gentle trend into the data, but omit to include it in the mean model, thus creating a missing variable problem but finding spatial autocorrelation instead:

```{r}
beta <- 0.0015
coords %>% 
    st_coordinates() %>% 
    subset(select = 1, drop = TRUE) %>% 
    (function(x) x/1000)() -> t
(x + beta * t -> x_t) %>% 
    moran.test(lw_q_B, randomisation = FALSE, alternative = "two.sided") %>% 
    glance_htest()
```
If we test the residuals of a linear model including the trend, the apparent spatial autocorrelation disappears:

```{r}
lm(x_t ~ t) %>% 
    lm.morantest(lw_q_B, alternative = "two.sided") %>% 
    glance_htest()
```

A comparison of implementations of measures of spatial autocorrelation shows that a wide range of measures is available in R in a number of packages, chiefly in the **spdep** package [@R-spdep], and that differences from other implementations can be attributed to design decisions [@Bivand2018]. The **spdep** package also includes the only implementations of exact and saddlepoint approximations to global and local Moran's I for regression residuals [@tiefelsdorf:02; @bivandetal:09].

## Global measures

Global measures consider the average level of spatial autocorrelation across all observations; they can of course be biassed (as most spatial statistics) by edge effects where important spatial process components fall outside the study area.

### Join-count tests for categorical data

We will begin by examining join count statistics, where `joincount.test()` takes a `"factor"` vector of values `fx=` and a `listw` object, and returns a list of `htest` (hypothesis test) objects defined in the **stats** package, one `htest` object for each level of the `fx=` argument. The observed counts are of neighbours with the same factor levels, known as same-colour joins.

```{r}
args(joincount.test)
```
The function takes an `alternative=` argument for hypothesis testing, a `sampling=` argument showing the basis for the construction of the variance of the measure, where the default `"nonfree"` choice corresponds to analytical permutation; the `spChk=` argument is retained for backward compatibility. For reference, the counts of factor levels for the type of municipality or Warsaw borough are:

```{r}
(pol_pres15 %>% 
        st_drop_geometry() %>% 
        subset(select = types, drop = TRUE) -> Types) %>% 
    table()
```
Since there are four levels, we re-arrange the list of `htest` objects to give a matrix of estimated results. The observed same-colour join counts are tabulated with their expectations based on the counts of levels of the input factor, so that few joins would be expected between for example Warsaw boroughs, because there are very few of them. The variance calculation uses the underlying constants of the chosen `listw` object and the counts of levels of the input factor. The z-value is obtained in the usual way by dividing the difference between the observed and expected join counts by the square root of the variance.

The join count test was subsequently adapted for multi-colour join counts [@upton+fingleton:85]. The implementation as `joincount.mult()` in **spdep** returns a table based on nonfree sampling, and does not report p-values.

```{r}
Types %>% joincount.multi(listw = lw_q_B)
```

So far, we have used binary weights, so the sum of join counts multiplied by the weight on that join remains integer. If we change to row standardised weights, where the weights almost always fractions of 1, the counts, expectations and variances change, but there are few major changes in the z-values.

Using an inverse distance based `listw` object does, however, change the z-values markedly, because closer centroids are upweighted relatively strongly:

```{r}
Types %>% joincount.multi(listw = lw_d183_idw_B)
```

### Moran's $I$

The implementation of Moran's $I$ in **spdep** in the `moran.test()` function has similar arguments to those of `joincount.test()`, but `sampling=` is replaced by `randomisation=` to indicate the underlying analytical approach used for calculating the variance of the measure. It is also possible to use ranks rather than numerical values [@cliff+ord:81, p. 46]. The `drop.EI2=` argument may be used to reproduce results where the final component of the variance term is omitted as found in some legacy software implementations.

```{r}
args(moran.test)
```

The default for the `randomisation=` argument is `TRUE`, but here we will simply show that the test under normality is the same as a test of least squares residuals with only the intercept used in the mean model; the analysed variable is first round turnout proportion of registered voters in municipalities and Warsaw burroughs in the 2015 Polish presidential election. The spelling of randomisation is that of Cliff and Ord [-@cliff+ord:73].

```{r}
(pol_pres15 %>% 
        st_drop_geometry() %>% 
        subset(select = I_turnout, drop = TRUE) -> z) %>% 
    moran.test(listw = lw_q_B, randomisation = FALSE) %>% 
    glance_htest()
```

The `lm.morantest()` function also takes a `resfun=` argument to set the function used to extract the residuals used for testing, and clearly lets us model other salient features of the response variable [@cliff+ord:81, p. 203]. To compare with the standard test, we are only using the intercept here and, as can be seen, the results are the same.

```{r}
lm(I_turnout ~ 1, pol_pres15) %>% 
    lm.morantest(listw = lw_q_B) %>% 
    glance_htest()
```
The only difference between tests under normality and randomisation is that an extra term is added if the kurtosis of the variable of interest indicates a flatter or more peaked distribution, where the measure used is the classical measure of kurtosis. Under the default randomisation assumption of analytical randomisation, the results are largely unchanged.

```{r}
(z %>% 
    moran.test(listw = lw_q_B) -> mtr) %>% 
    glance_htest()
```

From the very beginning in the early 1970s, interest was shown in Monte Carlo tests, also known as Hope-type tests and as permutation bootstrap. By default, `moran.mc()` returns a `"htest"` object, but may simply use `boot::boot()` internally and return a `"boot"` object when `return_boot=TRUE`. In addition the number of simulations needs to be given as `nsim=`; that is the number of times the values of the observations are shuffled at random.

```{r}
set.seed(1)
z %>% 
    moran.mc(listw = lw_q_B, nsim = 999, return_boot = TRUE) -> mmc
```
The bootstrap permutation retains the outcomes of each of the random permutations, reporting the observed value of the statistic, here Moran's $I$, the difference between this value and the mean of the simulations under randomisation (equivalent to $E(I)$), and the standard deviation of the simulations under randomisation. 

If we compare the Monte Carlo and analytical variances of $I$ under randomisation, we typically see few differences, arguably rendering Monte Carlo testing unnecessary.

```{r}
c("Permutation bootstrap" = var(mmc$t), 
  "Analytical randomisation" = unname(mtr$estimate[3]))
```
Geary's global $C$ is implemented in `geary.test()` largely following the same argument structure as `moran.test()`. The Getis-Ord $G$ test includes extra arguments to accommodate differences between implementations, as Bivand and Wong [-@Bivand2018] found multiple divergences from the original definitions, often to omit no-neighbour observations generated when using distance band neighbours. It is given by [@getis+ord:92, page 194]. For $G^*$, the $\sum_{(2)}$ constraint is relaxed by including $i$ as a neighbour of itself (thereby also removing the no-neighbour problem, because all observations have at least one neighbour).

Finally, the empirical Bayes Moran's $I$ takes account of the denominator in assessing spatial autocorrelation in rates data [@assuncao+reis:99]. Until now, we have considered the proportion of valid votes cast in relation to the numbers entitled to vote by spatial entity, but using `EBImoran.mc()` we can try to accommodate uncertainty in extreme rates in entities with small numbers entitled to vote. There is, however, little impact on the outcome in this case.

Global measures of spatial autocorrelation using spatial weights objects based on graphs of neighbours are, as we have seen, rather blunt tools, which for interpretation depend critically on a reasoned mean model of the variable in question. If the mean model is just the intercept, the global measures will respond to all kinds of mis-specification, not only spatial autocorrelation. The choice of entities for aggregation of data will typically be a key source of mis-specification.


## Local measures

Building on insights from the weaknesses of global measures, local indicators of spatial association began to appear in the first half of the 1990s [@anselin:95; @getis+ord:92; @getis+ord:96]. 

In addition, the Moran plot was introduced, plotting the values of the variable of interest against their spatially lagged values, typically using row-standardised weights to make the axes more directly comparable [@anselin:96]. The `moran.plot()` function also returns an influence measures object used to label observations exerting more than propotional influence on the slope of the line representing global Moran's $I$. In figure \@ref(fig:moranplot), we can see that there are many spatial entities exerting such influence. These pairs of observed and lagged observed values make up in aggregate the global measure, but can also be explored in detail. The quadrants of the Moran plot also show low-low pairs in the lower left quadrant, high-high in the upper right quadrant, and fewer low-high and high-low pairs in the upper left and lower right quadrants. 

(ref:moranplotfc) Moran plot of I round turnout, row standardised weights
```{r moranplot, out.width='100%', fig.cap='(ref:moranplotfc)', fig=TRUE}
z %>% 
    moran.plot(listw = lw_q_W, labels = pol_pres15$TERYT, cex = 1, pch = ".",
        xlab = "I round turnout", ylab = "lagged turnout") -> infl_W
```
If we extract the hat value influence measure from the returned object, Figure \@ref(fig:moranhat) suggests that some edge entities exert more than proportional influence (perhaps because of row standardisation), as do entities in or near larger urban areas.

(ref:moranhat) Moran plot hat values, row standardised neighbours
```{r moranhat, out.width='100%', fig.cap='(ref:moranhat)', fig=TRUE}
pol_pres15$hat_value <- infl_W$hat
tm_shape(pol_pres15) + tm_fill("hat_value")
```

### Local Moran's $I_i$

Bivand and Wong [-@Bivand2018] discuss issues impacting the use of local indicators, such as local Moran's $I_i$ and local Getis-Ord $G_i$. Some issues affect the calculation of the local indicators, others inference from their values. Because $n$ statistics may be being calculated from the same number of observations, there are multiple comparison problems that need to be addressed. Although the apparent detection of hotspots from values of local indicators has been quite widely adopted, it remains fraught with difficulty because adjustment of the inferential basis to accommodate multiple comparisons is not often chosen, and as in the global case, mis-specification also remains a source of confusion. Further, interpreting local spatial autocorrelation in the presence of global spatial autocorrelation is challenging [@ord+getis:01; @tiefelsdorf:02; @bivandetal:09]. 

```{r}
args(localmoran)
```

The `mlvar=` and `adjust.x=` arguments to `localmoran()` are discussed in Bivand and Wong [-@Bivand2018], and permit matching with other implementations. The `p.adjust.method=` argument uses an untested speculation implemented in `p.adjustSP()` that adjustment should only take into account the cardinality of the neighbour set of each observation when adjusting for multiple comparisons; using `stats::p.adjust()` is preferable.

Taking `"two.sided"` p-values, we obtain:

```{r}
z %>% 
    localmoran(listw = lw_q_W, alternative = "two.sided") -> locm
```

The $I_i$ local indicators when summed and divided by the sum of the spatial weights equal global Moran's $I$, showing the possible presence of positive and negative local spatial autocorrelation:
```{r}
all.equal(sum(locm[,1])/Szero(lw_q_W), unname(moran.test(z, lw_q_W)$estimate[1]))
```
Using `stats::p.adjust()` to adjust for multiple comparisons, we see that almost 29\% of the 2495 local measures have p-values < 0.05 if no adjustment is applied, but only 12\% using Bonferroni adjustment to control the familywise error rate, with two other choices shown: `fdr` is the @fdr-BH false discovery rate and `BY` [@10.1214/aos/1013699998], another false discovery rate adjustment:

```{r}
pva <- function(pv) cbind("none" = pv, "bonferroni" = p.adjust(pv, "bonferroni"),
    "fdr" = p.adjust(pv, "fdr"), "BY" = p.adjust(pv, "BY"))
locm %>% 
    subset(select = "Pr(z != 0)", drop = TRUE) %>% 
    pva() -> pvsp
f <- function(x) sum(x < 0.05)
apply(pvsp, 2, f)
```

In the global measure case, bootstrap permutations may be used as an alternative to analytical methods for possible inference, where both the theoretical development of the analytical variance of the measure, and the permutation scheme, shuffle all of the observed values. In the local case, conditional permutation may be used, fixing the value at observation $i$ and randomly sampling from the remaining $n-1$ values to find randomised values at neighbours, and is provided as `localmoran_perm()`, which may use multiple nodes to sample in parallel if provided, and permits the setting of a seed for the random number generator across the compute nodes:

```{r, message=FALSE}
library(parallel)
set.coresOption(ifelse(detectCores() == 1, 1, detectCores()-1L))
system.time(z %>% 
        localmoran_perm(listw = lw_q_W, nsim = 499, alternative = "two.sided",
            iseed = 1) -> locm_p)
```
The outcome is that almost 32\% of observations have two sided p-values < 0.05 without multiple comparison adjustment, and under 3\% with Bonferroni adjustment.

```{r}
locm_p %>% 
    subset(select = "Pr(z != 0)", drop = TRUE) %>% 
    pva() -> pvsp
apply(pvsp, 2, f)
```
We can see what is happening by tabulating counts of the standard deviate of local Moran's $I$, where the two-sided $\alpha=0.05$ bounds would be $0.025$ and $0.975$, but Bonferroni adjustment is close to $0.00001$ and $0.99999$. Without adjustment, almost 800 observations are significant, with Bonferroni adjustment, only almost 70 in the conditional permutation case:

```{r}
brks <- qnorm(c(0, 0.00001, 0.0001, 0.001, 0.01, 0.025, 0.5, 0.975, 0.99, 0.999, 0.9999,
    0.99999, 1))
(locm_p %>% 
        subset(select = Z.Ii, drop = TRUE) %>% 
        cut(brks) %>% 
        table()-> tab)
sum(tab[c(1:5, 8:12)])
sum(tab[c(1, 12)])
```
In an important clarification, @sauer_oshan_rey_wolf_2021 show that the comparison of standard deviates for local Moran's $I_i$ based on analytical formulae and conditional permutation in @Bivand2018 was based on a misunderstanding. @sokaletal:98 provide alternative analytical formulae for standard deviates of local Moran's $I_i$ based either on total or conditional permutation, but the analytical formulae used in @Bivand2018, based on earlier practice, only use total permutation, and consequently do not match the simulation conditional permutations. Thanks to a timely pull request, `localmoran()` now has a `conditional=` argument using alternative formulae from the appendix of @sokaletal:98:

```{r}
z %>% 
    localmoran(listw = lw_q_W, conditional = TRUE, alternative = "two.sided") -> locm_c
```

yielding standard deviates that correspond closely to those from simulation:
```{r}
locm_c %>% 
    subset(select = "Pr(z != 0)", drop = TRUE) %>% 
    pva() -> pvsp
apply(pvsp, 2, f)
```
(ref:localmoranZfc) Analytical total and conditional permutation, and bootstrap conditional permutation standard deviates of local Moran's I for first round turnout, row-standardised neighbours
```{r localmoranZ, out.width='100%', fig.cap='(ref:localmoranZfc)', fig=TRUE}
pol_pres15$locm_Z <- locm[, "Z.Ii"]
pol_pres15$locm_c_Z <- locm_c[, "Z.Ii"]
pol_pres15$locm_p_Z <- locm_p[, "Z.Ii"]
tm_shape(pol_pres15) + tm_fill(c("locm_Z", "locm_c_Z", "locm_p_Z"), breaks = brks,
    midpoint = 0, title = "Standard deviates of\nLocal Moran's I") +
    tm_facets(free.scales = FALSE, ncol = 2) + tm_layout(panel.labels = c("Analytical total",
        "Analytical conditional", "Conditional permutation"))
```
Figure \@ref(fig:localmoranZ) shows that conditional permutation scales back the proportion of standard deviate values taking extreme values, especially positive values. The analytical total standard deviates of local Moran's $I$ should probably not be used since alternatives are available, not least thanks to the clarification by @sauer_oshan_rey_wolf_2021.

In presenting local Moran's $I$, use is often made of "hotspot" maps. Because $I_i$ takes high values both for strong positive autocorrelation of low and high values of the input variable, it is hard to show where "clusters" of similar neighbours with low or high values of the input variable occur. The quadrants of the Moran plot are used, by creating a categorical quadrant variable interacting the input variable and its spatial lag split at their means. The quadrant categories are then set to NA if, for the chosen standard deviate and adjustment, $I_i$ would be considered insignificant. Here, for the Bonferroni adjusted conditional analytical standard deviates, 14 observations belong to "Low-Low clusters", and 55 to "High-High clusters":

```{r}
quadr <- interaction(
    cut(infl_W$x, c(-Inf, mean(infl_W$x), Inf), labels=c("Low X", "High X")),
    cut(infl_W$wx, c(-Inf, mean(infl_W$wx), Inf), labels=c("Low WX", "High WX")),
    sep=" : ")
a <- table(quadr)
pol_pres15$hs_an_q <- quadr
is.na(pol_pres15$hs_an_q) <- !(pol_pres15$locm_Z < brks[6] | 
        pol_pres15$locm_Z > brks[8])
b <- table(pol_pres15$hs_an_q)
pol_pres15$hs_ac_q <- quadr
is.na(pol_pres15$hs_ac_q) <- !(pol_pres15$locm_c_Z < brks[2] | 
        pol_pres15$locm_c_Z > brks[12])
c <- table(pol_pres15$hs_ac_q)
pol_pres15$hs_cp_q <- quadr
is.na(pol_pres15$hs_cp_q) <- !(pol_pres15$locm_p_Z < brks[2] | 
        pol_pres15$locm_p_Z > brks[12])
d <- table(pol_pres15$hs_cp_q)
t(rbind("Moran plot quadrants" = a, "Unadjusted analytical total" = b, 
    "Bonferroni analytical cond." = c, "Bonferroni cond. perm." = d))
```

(ref:morani) Local Moran's I hotspot maps $\alpha = 0.05$: left panel upper: unadjusted analytical standard deviates; right upper panel: Bonferroni adjusted analytical conditional standard deviates; left lower panel: Bonferroni adjusted bootstrap conditional permutation standard deviates, first round turnout, row-standardised neighbours

```{r Iihotspots, out.width='100%', fig.cap='(ref:morani)' }
tm_shape(pol_pres15) + tm_fill(c("hs_an_q", "hs_ac_q", "hs_cp_q"), colorNA="grey95",
    textNA="Not significant", title="Turnout hotspot status\nLocal Moran's I") +
    tm_facets(free.scales=FALSE, ncol=2) + tm_layout(panel.labels=
            c("Unadjusted analytical total", "Bonferroni analytical cond.", 
              "Cond. perm. with Bonferroni"))
```
Figure \@ref(fig:Iihotspots) shows the impact of using analytical or conditional permutation standard deviates, and no or Bonferroni adjustment, reducing the counts of observations in "Low-Low clusters" from 370 to 14, and "High-High clusters" from 342 to 54; the "High-High clusters" are metropolitan areas.

@tiefelsdorf:02 argues that standard approaches to the calculation of the standard deviates of local Moran's $I_i$ should be supplemented by numerical estimates, and shows that Saddlepoint approximations are a computationally efficient way of achieving this goal. The `localmoran.sad()` function takes a fitted linear model as its first argument, so we first fit a null (intercept only) model, but use case weights because the numbers entitled to vote vary greatly between observations:

```{r}
lm(z ~ 1, weights = pol_pres15$I_entitled_to_vote) -> lm_null
```
Saddlepoint approximation is much more computationally intensive than conditional permutation:

```{r, cache=TRUE}
system.time(lm_null %>% localmoran.sad(nb = nb_q, style = "W", alternative = "two.sided") %>%
        summary() -> locm_sad_null)
```
However, standard approaches do not permit richer mean models with covariates or case weights. Next we add the categorical variable distinguishing between rural, urban and other types of observational unit:

```{r}
lm(z ~ Types, weights=pol_pres15$I_entitled_to_vote) -> lm_types
```

```{r, cache=TRUE}
system.time(lm_types %>% localmoran.sad(nb = nb_q, style = "W", alternative = "two.sided") %>%
        summary() -> locm_sad_types)
```
To conclude, we add the spatially lagged categories, although the spatial lag of a categorical variable, represented by dummies in the model matrix, is not well defined:

```{r}
spatialreg::lmSLX(z ~ Types, listw = lw_q_W, 
    weights = pol_pres15$I_entitled_to_vote) -> lm_Dtypes
```

```{r, cache=TRUE}
system.time(lm_Dtypes %>% localmoran.sad(nb = nb_q, style = "W", alternative = "two.sided") %>%
        summary() -> locm_sad_Dtypes)
```

(ref:localmoranZfcsad) Saddlepoint weighted null model, weighted types model, weighted Durbin types model, and analytical conditional standard deviates of local Moran's I for first round turnout, row-standardised neighbours
```{r localmoranZsad, out.width='100%', fig.cap='(ref:localmoranZfcsad)', fig=TRUE}
pol_pres15$locm_sad_null_Z <- locm_sad_null[, "Saddlepoint"]
pol_pres15$locm_sad_types_Z <- locm_sad_types[, "Saddlepoint"]
pol_pres15$locm_sad_Dtypes_Z <- locm_sad_Dtypes[, "Saddlepoint"]
tm_shape(pol_pres15) + tm_fill(c("locm_sad_null_Z", "locm_sad_types_Z",
    "locm_sad_Dtypes_Z", "locm_c_Z"), breaks = brks, midpoint=0,
    title="Standard deviates of\nLocal Moran's I") +
    tm_facets(free.scales = FALSE, ncol = 2) + tm_layout(panel.labels = c(
        "Saddlepoint weighted null",
        "Saddlepoint weighted types",
        "Saddlepoint weighted Durbin types",
        "Analytical conditional"))
```
Figure \@ref(fig:localmoranZsad) includes the analytical conditional standard deviates for comparison (lower right panel), but in general it can be seen that the Saddlepoint approximation standard deviates lie closer to zero, possibly because some of the mis-specification in the mean model has been removed by using richer versions, and possibly because the approximation approach is inherently local, relating regression residual values at $i$ to those of its neighbours. It is also possible to use Saddlepoint approximation where the global spatial process has been incorporated, removing the conflation of global and local spatial autocorrelation in standard approaches. The same can also be accomplished using exact methods [@bivandetal:09].

### Local Getis-Ord $G_i$

The local Getis-Ord $G$ measure is reported as a standard deviate, and may also take the $G^*_i$ form where self-neighbours are inserted into the neighbour object using `include.self()`. The observed and expected values of local $G$ with their analytical variances may also be returned if `return_internals=TRUE`. 

```{r}
system.time(z %>% 
        localG(lw_q_W) -> locG)
system.time(z %>% 
        localG_perm(lw_q_W, nsim = 499, iseed = 1) -> locG_p)
```
Once again we face the problem of multiple comparisons, with the count of areal unit p-values < 0.05 being reduced by an order of magnitude when employing Bonferroni correction:

```{r}
locG %>% 
    c() %>% 
    abs() %>% 
    pnorm(lower.tail = FALSE) %>% 
    (function(x) x*2)() %>% 
    pva() -> pvsp
apply(pvsp, 2, f)
```


(ref:localZvaluesfc) Plots of analytical conditional against bootstrap standard deviates; left: local Moran's I; right: local G; first round turnout, row-standardised neighbours
```{r localZvalues, out.width='100%', fig.cap='(ref:localZvaluesfc)'}
library(ggplot2)
p1 <- ggplot(data.frame(Zi=locm_c[,4], Zi_perm=locm_p[,4])) + geom_point(aes(x=Zi,
    y=Zi_perm), alpha=0.2) + xlab("Analytical conditional") + 
    ylab("Bootstrap conditional") + coord_fixed() + ggtitle("Local Moran's I")
p2 <- ggplot(data.frame(Zi=c(locG), Zi_perm=c(locG_p))) + geom_point(aes(x=Zi, 
    y=Zi_perm), alpha=0.2) + xlab("Analytical conditional") + 
    ylab("Bootstrap conditional") + coord_fixed() + ggtitle("Local G")
gridExtra::grid.arrange(p1, p2, nrow=1)
```

Figure \@ref(fig:localZvalues) shows that, when using analytical conditional standard deviates, the values from analytical and bootstrap estimates coincide for both $I_i$ and $G_i$. In both cases, one may argue that the bootstrap approach is superfluous in exploratory spatial data analysis.

```{r}
pol_pres15$locG_Z <- c(locG)
pol_pres15$hs_G <- cut(c(locG), c(-Inf, brks[2], brks[12], Inf), 
    labels = c("Low", "Not significant", "High"))
table(pol_pres15$hs_G)
```

(ref:localGfc) Left: analytical standard deviates of local G; right: Bonferroni hotspots; first round turnout, row-standardised neighbours
```{r localG, out.width='100%', fig.cap='(ref:localGfc)', fig=TRUE}
m1 <- tm_shape(pol_pres15) + tm_fill(c("locG_Z"), midpoint = 0, title = "Standard\ndeviate")
m2 <- tm_shape(pol_pres15) + tm_fill(c("hs_G"), 
    title="Local G Bonferroni\nadjusted hotspot status")
tmap_arrange(m1, m2, nrow=1)
```

As can be seen from Figure \@ref(fig:localZvalues), we do not need to contrast the two estimation methods, and showing the mapped standard deviate is as informative as the "hotspot" status for the chosen adjustment (Figure \@ref(fig:localG)). In the case of $G_i$, the values taken by the measure reflect the values of the input variable, so a "High cluster" is found for observations with high values of the input variable, here high turnout in metropolitan areas.

Very recently, Geoda has been wrapped for R as **rgeoda** [@R-rgeoda], and will provide very similar functionalities for the exploration of spatial autocorrelation in areal data as **spdep**. The active objects are kept as pointers to a compiled code workspace; using compiled code for all operations (as in Geoda itself) makes **rgeoda** perform fast, but leaves less flexible when modifications or enhancements are desired.

The contiguity neighbours it constructs are the same as those found by `poly2nb()`, as almost are the $I_i$ measures. The difference is as established by @Bivand2018, that `localmoran()` calculates the input variable variance divinding by $n$, but Geoda uses $(n-1)$, as can be reproduced by setting `mlvar=FALSE`:

```{r, message=FALSE}
library(rgeoda)
system.time(Geoda_w <- queen_weights(pol_pres15))
summary(Geoda_w)
system.time(lisa <- local_moran(Geoda_w, pol_pres15["I_turnout"], 
    cpu_threads = ifelse(parallel::detectCores() == 1, 1, parallel::detectCores()-1L),
    permutations = 499, seed = 1))
all.equal(card(nb_q), lisa_num_nbrs(lisa), check.attributes = FALSE)
all.equal(lisa_values(lisa), localmoran(pol_pres15$I_turnout, 
    listw=lw_q_W, mlvar = FALSE)[,1], check.attributes = FALSE)
```


## Exercises

1. Why are join-count measures on a chessboard so different between `rook` and `queen` neighbours?
2. Please repeat the simulation shown in section 15.1 using the chessboard polygons and the row-standardized `queen` contiguity neighbours. Why is it important to understand that spatial autocorrelation usually signals (unavoidable) mis-specification in our data?
3. Do we need to use conditional permutation in inference for local measures of spatial autocorrelation?
4. Why is false discovery rate adjustment recommended for local measures of spatial autocorrelation?
5. Compare the local Moran's $I_i$ standard deviate values for the simulated data from exercise 15.2 for the analytical conditional approach, and Saddlepoint approximation. Consider the advantages and disadvantages of the Saddlepoint approximation approach.

