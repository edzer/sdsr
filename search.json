[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Spatial Data Science",
    "section": "",
    "text": "Preface\nData science is concerned with finding answers to questions on the basis of available data, and communicating that effort. Besides showing the results, this communication involves sharing the data used, but also exposing the path that led to the answers in a comprehensive and reproducible way. It also acknowledges the fact that available data may not be sufficient to answer questions, and that any answers are conditional on the data collection or sampling protocols employed.\nThis book introduces and explains the concepts underlying spatial data: points, lines, polygons, rasters, coverages, geometry attributes, data cubes, reference systems, as well as higher-level concepts including how attributes relate to geometries and how this affects analysis. The relationship of attributes to geometries is known as support, and changing support also changes the characteristics of attributes. Some data generation processes are continuous in space, and may be observed everywhere. Others are discrete, observed in tesselated containers. In modern spatial data analysis, tesellated methods are often used for all data, extending across the legacy partition into point process, geostatistical and lattice models. It is support (and the understanding of support) that underlies the importance of spatial representation. The book aims at data scientists who want to get a grip on using spatial data in their analysis. To exemplify how to do things, it uses R. In future editions we hope to extend this with examples using Python (see, e.g., Bivand 2022a) and Julia.\nIt is often thought that spatial data boils down to having observations’ longitude and latitude in a dataset, and treating these just like any other variable. This carries the risk of missed opportunities and meaningless analyses. For instance,\nWe introduce the concepts behind spatial data, coordinate reference systems, spatial analysis, and introduce a number of packages, including sf (Pebesma 2018, 2022a), stars (Pebesma 2022b), s2 (Dunnington, Pebesma, and Rubak 2023) and lwgeom (Pebesma 2023), as well as a number of spatial tidyverse (Wickham et al. 2019; Wickham 2022) extensions, and a number of spatial analysis and visualisation packages that can be used with these packages, including gstat (Pebesma 2004; Pebesma and Graeler 2022), spdep (Bivand 2022b), spatialreg (Bivand and Piras 2022), spatstat (Baddeley, Rubak, and Turner 2015; Baddeley, Turner, and Rubak 2022), tmap (Tennekes 2018, 2022) and mapview (Appelhans et al. 2022).\nLike data science, spatial data science seems to be a field that arises bottom-up in and from many existing scientific disciplines and industrial activities concerned with application of spatial data, rather than being a sub-discipline of an existing scientific discipline. Although there are various activities trying to scope it through focused conferences, symposia, chairs and study programs, we believe that the versatility of spatial data applications and questions will render such activity hard. Giving this book the title “spatial data science” is not another attempt to define the bounds of this field but rather an attempt to contribute to it from our 3-4 decades of experience working with researchers from various fields willing to publicly share research questions, data, and attempts to solve these questions with software. As a consequence, the selection of topics found in this book has a certain bias towards our own areas of research interest and experience. Platforms that have helped create an open research community include the ai-geostats and r-sig-geo mailing lists, sourceforge, r-forge, GitHub, and the OpenGeoHub summer schools organized yearly since 2007. The current possibility and willingness to cross data science language barriers opens a new and very exciting perspective. Our motivation to contribute to this field is a belief that open science leads to better science, and that better science might contribute to a more sustainable world.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#printed-version-of-this-book",
    "href": "index.html#printed-version-of-this-book",
    "title": "Spatial Data Science",
    "section": "Printed version of this book",
    "text": "Printed version of this book\n\n\n\n\nThe printed version of this book (1st edition) is available from the Routeledge/CRC or Taylor and Francis websites. Cover art was created by Allison Horst.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#citing-this-book",
    "href": "index.html#citing-this-book",
    "title": "Spatial Data Science",
    "section": "Citing this book",
    "text": "Citing this book\nThe full reference is: Pebesma, E.; Bivand, R. (2023). Spatial Data Science: With Applications in R (1st ed.). 314 pages. Chapman and Hall/CRC, Boca Raton. https://doi.org/10.1201/9780429459016\nBibTeX:\n@book{sds,\n  author = {Edzer Pebesma and Roger Bivand},\n  year = 2023,\n  title = {Spatial Data Science: With Applications in {R}},\n  publisher = {Chapman and Hall/CRC},\n  address = {Boca Raton},\n  URL = {https://doi.org/10.1201/9780429459016},\n  doi = {10.1201/9780429459016}\n}",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Spatial Data Science",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe are grateful to the entire r-spatial community, especially those who\n\ndeveloped r-spatial packages or contributed to their development,\ncontributed to discussions on twitter and Mastodon #rspatial, or on GitHub,\nbrought comments or asked questions in courses, summer schools, or conferences.\n\nWe are in particular grateful to Dewey Dunnington for implementing the s2 package, and for active contributions from Sahil Bhandari, Jonathan Bahlmann for preparing the figures in 6  Data Cubes, Claus Wilke, Jakub Nowosad, the “Spatial Data Science with R” classes of 2021 and 2022, and to those who actively contributed with GitHub issues, pull requests, or discussions:\n\nto the book repository (Nowosad, jonathom, JaFro96, singhkpratham, liuyadong, hurielreichel, PPaccioretti, Robinlovelace, Syverpet, jonas-hurst, angela-li, ALanguillaume, florisvdh, ismailsunni, andronaco),\nto the sf repository (aecoleman, agila5, andycraig, angela-li, ateucher, barryrowlingson, bbest, BenGraeler, bhaskarvk, Bisaloo, bkmgit, christophertull, chrisyeh96, cmcaine, cpsievert, daissi, dankelley, DavisVaughan, dbaston, dblodgett-usgs, dcooley, demorenoc, dpprdan, drkrynstrng, etiennebr, famuvie, fdetsch, florisvdh, gregleleu, hadley, hughjonesd, huizezhang-sherry, jeffreyhanson, jeroen, jlacko, joethorley, joheisig, JoshOBrien, jwolfson, kadyb, karldw, kendonB, khondula, KHwong12, krlmlr, lambdamoses, lbusett, lcgodoy, lionel-, loicdtx, marwahaha, MatthieuStigler, mdsumner, MichaelChirico, microly, mpadge, mtennekes, nikolai-b, noerw, Nowosad, oliverbeagley, Pakillo, paleolimbot, pat-s, PPaccioretti, prdm0, ranghetti, rCarto, renejuan, rhijmans, rhurlin, rnuske, Robinlovelace, robitalec, rubak, rundel, statnmap, thomasp85, tim-salabim, tyluRp, uribo, Valexandre, wibeasley, wittja01, yutannihilation, Zedseayou),\nto the stars repository (a-benini, ailich, ateucher, btupper, dblodgett-usgs, djnavarro, ErickChacon, ethanwhite, etiennebr, flahn, floriandeboissieu, gavg712, gdkrmr, jannes-m, jeroen, JoshOBrien, kadyb, kendonB, mdsumner, michaeldorman, mtennekes, Nowosad, pat-s, PPaccioretti, przell, qdread, Rekyt, rhijmans, rubak, rushgeo, statnmap, uribo, yutannihilation),\nto the s2 repository (kylebutts, spiry34, jeroen, eddelbuettel).\n\n\n\n\n\n\n\n\nAppelhans, Tim, Florian Detsch, Christoph Reudenbach, and Stefan Woellauer. 2022. Mapview: Interactive Viewing of Spatial Data in r. https://github.com/r-spatial/mapview.\n\n\nBaddeley, Adrian, Ege Rubak, and Rolf Turner. 2015. Spatial Point Patterns: Methodology and Applications with R. Chapman & Hall/CRC.\n\n\nBaddeley, Adrian, Rolf Turner, and Ege Rubak. 2022. Spatstat: Spatial Point Pattern Analysis, Model-Fitting, Simulation, Tests. http://spatstat.org/.\n\n\nBivand, Roger. 2022a. “R Packages for Analyzing Spatial Data: A Comparative Case Study with Areal Data.” Geographical Analysis 54 (3): 488–518. https://doi.org/10.1111/gean.12319.\n\n\n———. 2022b. Spdep: Spatial Dependence: Weighting Schemes, Statistics.\n\n\nBivand, Roger, and Gianfranco Piras. 2022. Spatialreg: Spatial Regression Analysis. https://CRAN.R-project.org/package=spatialreg.\n\n\nDunnington, Dewey, Edzer Pebesma, and Ege Rubak. 2023. S2: Spherical Geometry Operators Using the S2 Geometry Library. https://CRAN.R-project.org/package=s2.\n\n\nPebesma, Edzer. 2004. “Multivariable Geostatistics in S: The Gstat Package.” Computers & Geosciences 30: 683–91.\n\n\n———. 2018. “Simple Features for R: Standardized Support for Spatial Vector Data.” The R Journal 10 (1): 439–46. https://doi.org/10.32614/RJ-2018-009.\n\n\n———. 2022a. Sf: Simple Features for r. https://CRAN.R-project.org/package=sf.\n\n\n———. 2022b. Stars: Spatiotemporal Arrays, Raster and Vector Data Cubes. https://CRAN.R-project.org/package=stars.\n\n\n———. 2023. Lwgeom: Bindings to Selected Liblwgeom Functions for Simple Features. https://github.com/r-spatial/lwgeom/.\n\n\nPebesma, Edzer, and Benedikt Graeler. 2022. Gstat: Spatial and Spatio-Temporal Geostatistical Modelling, Prediction and Simulation. https://github.com/r-spatial/gstat/.\n\n\nTennekes, Martijn. 2018. “tmap: Thematic Maps in R.” Journal of Statistical Software 84 (6): 1–39. https://doi.org/10.18637/jss.v084.i06.\n\n\n———. 2022. Tmap: Thematic Maps. https://github.com/r-tmap/tmap.\n\n\nWickham, Hadley. 2022. Tidyverse: Easily Install and Load the Tidyverse. https://CRAN.R-project.org/package=tidyverse.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://joss.theoj.org/papers/10.21105/joss.01686.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "part-1.html",
    "href": "part-1.html",
    "title": "Spatial Data",
    "section": "",
    "text": "The first part of this book introduces concepts of spatial data science: maps, projections, vector and raster data structures, software, attributes and support, and data cubes. This part uses R only to generate text output or figures. The R code for this is not shown or explained, as it would distract from the message: Part II focuses on the use of R. The online version of this book, found at https://r-spatial.org/book/ contains the R code at the place where it is used in hidden sections that can be unfolded on demand and copied to the clipboard for execution and experimenting. Output from R code uses code font and has lines starting with a #, as in\n\nCodelibrary(sf)\n\n# Linking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.4.0; sf_use_s2() is TRUE\n\n\nSome of the code sections (e.g., in 6  Data Cubes) contain code written to generate figures with R not relevant to the subject matter of the book. Code sections relevant to data analysis should be easy to follow when understanding R at the level of, say, R for Data Science (Wickham and Grolemund 2017).\nMore detailed explanation of R code to solve spatial data science problems starts in the second part of this book. Appendix B — R Basics contains a short, elementary explanation of R data structures, Wickham (2014) gives a more extensive treatment on this.\n\n\n\n\n\n\nWickham, Hadley. 2014. Advanced R, Second Edition. CRC Press.https://adv-r.hadley.nz/ .\n\n\nWickham, Hadley, and Garret Grolemund. 2017. R for Data Science. O’Reilly. http://r4ds.had.co.nz/.",
    "crumbs": [
      "Spatial Data"
    ]
  },
  {
    "objectID": "01-hello.html",
    "href": "01-hello.html",
    "title": "1  Getting Started",
    "section": "",
    "text": "1.1 A first map\nThis chapter introduces a number of concepts associated with handling spatial and spatiotemporal data, pointing forward to later chapters where these concepts are discussed in more detail. It also introduces a number of open source technologies that form the foundation of all spatial data science language implementations.\nThe typical way to graph spatial data is by creating a map. Let us consider a simple map, shown in Figure 1.1.\nCodelibrary(tidyverse)\nlibrary(sf)\nsystem.file(\"gpkg/nc.gpkg\", package=\"sf\") |&gt;\n    read_sf() -&gt; nc\nnc.32119 &lt;- st_transform(nc, 'EPSG:32119')\nnc.32119 |&gt;\n    select(BIR74) |&gt;\n    plot(graticule = TRUE, axes = TRUE)\n\n\n\n\n\n\nFigure 1.1: A first map: birth counts 1974-78, North Carolina counties\nA number of graphical elements are present here, in this case:\nPolygons are a particular form of geometry; spatial geometries (points, lines, polygons, pixels) are discussed in detail in Chapter 3. Polygons consist of sequences of points, connected by straight lines. How point locations of spatial data are expressed, or measured, is discussed in Chapter 2. As can be seen from Figure 1.1, lines of equal latitude and longitude do not form straight lines, indicating that some form of projection took place before plotting; projections are also discussed in Chapter 2 and Section 8.1.\nThe colour values in Figure 1.1 are derived from numeric values of a variable, BIR74, which has a single value associated with each geometry or feature. Chapter 5 discusses such feature attributes, and how they can relate to feature geometries. In this case, BIR74 refers to birth counts, meaning counts over the region. This implies that the count does not refer to a value associated with every point inside the polygon, which the continuous colour might suggest, but rather measures an integral (sum) over the polygon.\nBefore plotting Figure 1.1 we had to read the data, in this case from a file (Section 7.1). Printing a data summary for the first three records of three attribute variables shows:\nCodenc |&gt; select(AREA, BIR74, SID74) |&gt; print(n = 3)\n\n# Simple feature collection with 100 features and 3 fields\n# Geometry type: MULTIPOLYGON\n# Dimension:     XY\n# Bounding box:  xmin: -84.3 ymin: 33.9 xmax: -75.5 ymax: 36.6\n# Geodetic CRS:  NAD27\n# # A tibble: 100 × 4\n#    AREA BIR74 SID74                                             geom\n#   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;                               &lt;MULTIPOLYGON [°]&gt;\n# 1 0.114  1091     1 (((-81.5 36.2, -81.5 36.3, -81.6 36.3, -81.6 36…\n# 2 0.061   487     0 (((-81.2 36.4, -81.2 36.4, -81.3 36.4, -81.3 36…\n# 3 0.143  3188     5 (((-80.5 36.2, -80.5 36.3, -80.5 36.3, -80.5 36…\n# # ℹ 97 more rows\nThe printed output shows:\nMore complicated plots can involve facet plots with a map in each facet, as shown in Figure 1.2.\nCodeyear_labels &lt;- c(\"SID74\" = \"1974 - 1978\", \"SID79\" = \"1979 - 1984\")\nnc.32119 |&gt; select(SID74, SID79) |&gt;\n    pivot_longer(starts_with(\"SID\")) -&gt; nc_longer\nggplot() + geom_sf(data = nc_longer, aes(fill = value), linewidth = 0.4) + \n  facet_wrap(~ name, ncol = 1, labeller = labeller(name = year_labels)) +\n  scale_y_continuous(breaks = 34:36) +\n  scale_fill_gradientn(colors = sf.colors(20)) +\n  theme(panel.grid.major = element_line(color = \"white\"))\n\n\n\n\n\n\nFigure 1.2: Facet maps of sudden infant death syndrome counts, 1974-78 and 1979-84, North Carolina counties\nAn interactive, leaflet-based map is obtained in Figure 1.3.\nCodelibrary(mapview) |&gt; suppressPackageStartupMessages()\nmapviewOptions(fgb = FALSE)\nnc.32119 |&gt; mapview(zcol = \"BIR74\", legend = TRUE, col.regions = sf.colors)\n\n\n\n\n\n\nFigure 1.3: Interactive map created with mapview: pan and zoom move the map and change scale; clicking a county pops up window with the available county properties.",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "01-hello.html#a-first-map",
    "href": "01-hello.html#a-first-map",
    "title": "1  Getting Started",
    "section": "",
    "text": "polygons are drawn with a black outline and filled with colours chosen according to a variable BIR74, whose name is in the title\na legend key explains the meaning of the colours, and has a certain colour palette and colour breaks, values at which colour changes\nthe background of the map shows curved lines with constant latitude or longitude (graticule)\nthe axis ticks show the latitude and longitude values\n\n\n\n\n\n\n\n\n\nthe (selected) dataset has 100 features (records) and 3 fields (attributes)\nthe geometry type is MULTIPOLYGON (Chapter 3)\nit has dimension XY, indicating that each point will consist of 2 coordinate values\nthe range of \\(x\\) and \\(y\\) values of the geometry\nthe coordinate reference system (CRS) is geodetic, with coordinates in degrees longitude and latitude associated to the NAD27 datum (Chapter 2)\nthe three selected attribute variables are followed by a variable geom of type MULTIPOLYGON with unit degrees that contains the polygon information",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "01-hello.html#coordinate-reference-systems",
    "href": "01-hello.html#coordinate-reference-systems",
    "title": "1  Getting Started",
    "section": "\n1.2 Coordinate reference systems",
    "text": "1.2 Coordinate reference systems\n \nIn Figure 1.1, the grey lines denote the graticule, a grid with lines along constant latitude or longitude. Clearly, these lines are not straight, which indicates that a projection of the data was used for which the \\(x\\) and \\(y\\) axes do not align with longitude and latitude. In Figure 1.3 we see that the north boundary of North Carolina is plotted as a straight line again, indicating that another projection was used.\nThe ellipsoidal coordinates of the graticule of Figure 1.1 are associated with a particular datum (here: NAD27), which implicates a set of rules, what the shape of the Earth is and how it is attached to the Earth (to which point of the Earth is the origin associated, and how is it directed.) If one would measure coordinates with a GPS device (such as a mobile phone) it would typically report coordinates associated with the World Geodetic System 1984 (WGS84) datum, which can be around 30 m different from the identical coordinate values when associated with the North American Datum 1927 (NAD27).\n\nProjections describe how we go back and forth between\n\nellipsoidal coordinates which are expressed as degrees latitude and longitude, pointing to locations on a shape approximating the Earth’s shape (ellipsoid or spheroid), and\nprojected coordinates which are coordinates on a flat, two-dimensional coordinate system, used when plotting maps.\n\n \nDatums transformations are associated with moving from one datum to another. Both topics are covered by spatial reference systems, and are described in more detail in Chapter 2.",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "01-hello.html#sec-rasterize",
    "href": "01-hello.html#sec-rasterize",
    "title": "1  Getting Started",
    "section": "\n1.3 Raster and vector data",
    "text": "1.3 Raster and vector data\nPolygon, point, and line geometries are examples of vector data: point coordinates describe the “exact” locations that can be anywhere. Raster data on the other hand describe data where values are aligned on a raster, meaning on a regularly laid out lattice of usually square pixels. An example is shown in Figure 1.4.\n \n\nCodelibrary(stars)\npar(mfrow = c(2, 2))\npar(mar = rep(1, 4))\ntif &lt;- system.file(\"tif/L7_ETMs.tif\", package = \"stars\")\nx &lt;- read_stars(tif)[,,,1]\nimage(x, main = \"(a)\")\nimage(x[,1:10,1:10], text_values = TRUE, border = 'grey', main = \"(b)\")\nimage(x, main = \"(c)\")\nset.seed(131)\npts &lt;- st_sample(st_as_sfc(st_bbox(x)), 3)\nplot(pts, add = TRUE, pch = 3, col = 'blue')\nimage(x, main = \"(d)\")\nplot(st_buffer(pts, 500), add = TRUE, pch = 3, border = 'blue', col = NA, lwd = 2)\n\n\n\n\n\n\nFigure 1.4: Raster maps (Olinda, Atlantic coast of Brazil): Landsat-7 blue band, with colour values derived from data values (a), the top-left \\(10 \\times 10\\) sub-image from (a) with numeric values shown (b), and overlayed by two different types of vector data: three sample points (c), and a 500 m radius around the points represented as polygons (d)\n\n\n\n\nVector and raster data can be combined in different ways; for instance we can query the raster at the three points of Figure 1.4(c) or compute an aggregate, such as the average, over arbitrary regions such as the circles shown in Figure 1.4(d).\n\nCodest_extract(x, pts) # query at points\naggregate(x, st_buffer(pts, 500), FUN = mean) |&gt; st_as_sf() # aggregate over circles\n\n\n\nOther raster-to-vector conversions are discussed in Section 7.6 and include:\n\nconverting raster pixels into point values\nconverting raster pixels into small polygons, possibly merging polygons with identical values (“polygonize”)\ngenerating lines or polygons that delineate continuous pixel areas with a certain value range (“contour”)\n\n \n\nCodeplot(st_rasterize(nc[\"BIR74\"], dx = 0.1), col = sf.colors(), breaks = \"equal\")\n\n\n\n\n\n\nFigure 1.5: Map obtained by rasterizing county births counts for the period 1974-78 shown in 1.1\n\n\n\n\nVector-to-raster conversions can be as simple as rasterizing polygons, as shown in Figure 1.5. Other, more general vector-to-raster conversions that may involve statistical modelling include:\n\ninterpolation of point values to points on a regular grid (Chapter 12)\nestimating densities of points over a regular grid (Chapter 11)\narea-weighted interpolation of polygon values to grid cells (Section 5.3)\ndirect rasterization of points, lines, or polygons (Section 7.6)",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "01-hello.html#raster-types",
    "href": "01-hello.html#raster-types",
    "title": "1  Getting Started",
    "section": "\n1.4 Raster types",
    "text": "1.4 Raster types\nRaster dimensions describe how the rows and columns relate to spatial coordinates. Figure 1.6 shows a number of different possibilities.\n\nCodex &lt;- 1:5\ny &lt;- 1:4\nd &lt;- st_dimensions(x = x, y = y, .raster = c(\"x\", \"y\"))\nm &lt;- matrix(runif(20),5,4)\nr1 &lt;- st_as_stars(r = m, dimensions = d)\n\nr &lt;- attr(d, \"raster\")\nr$affine &lt;- c(0.2, -0.2)\nattr(d, \"raster\") = r\nr2 &lt;- st_as_stars(r = m, dimensions = d)\n\nr &lt;- attr(d, \"raster\")\nr$affine &lt;- c(0.1, -0.3)\nattr(d, \"raster\") = r\nr3 = st_as_stars(r = m, dimensions = d)\n\nx &lt;- c(1, 2, 3.5, 5, 6)\ny &lt;- c(1, 1.5, 3, 3.5)\nd &lt;- st_dimensions(x = x, y = y, .raster = c(\"x\", \"y\"))\nr4 &lt;- st_as_stars(r = m, dimensions = d)\n\ngrd &lt;- st_make_grid(cellsize = c(10,10), offset = c(-130,10), n = c(8,5), crs = st_crs('OGC:CRS84'))\nr5 &lt;- st_transform(grd, \"+proj=laea +lon_0=-70 +lat_0=35\")\n\npar(mfrow = c(2,3), mar = c(0.1, 1, 1.1, 1))\nr1 &lt;- st_make_grid(cellsize = c(1,1), n = c(5,4), offset = c(0,0))\nplot(r1, main = \"regular\")\nplot(st_geometry(st_as_sf(r2)), main = \"rotated\")\nplot(st_geometry(st_as_sf(r3)), main = \"sheared\")\nplot(st_geometry(st_as_sf(r4, as_points = FALSE)), main = \"rectilinear\")\nplot(st_geometry((r5)), main = \"curvilinear\")\n\n\n\n\n\n\nFigure 1.6: Various raster geometry types\n\n\n\n\n \nRegular rasters like those shown in Figure 1.6 have a constant, not necessarily square cell size and axes aligned with the \\(x\\) and \\(y\\) (Easting and Northing) axes. Other raster types include those where the axes are no longer aligned with \\(x\\) and \\(y\\) (rotated), where axes are no longer perpendicular (sheared), or where cell size varies along a dimension (rectilinear). Finally, curvilinear rasters have cell size and/or direction properties that are no longer independent from the other raster dimension.\nWhen a raster that is regular in a given coordinate reference system is projected to another raster while keeping each raster cell intact, it changes shape and may become rectilinear (for instance when going from ellipsoidal coordinates to Mercator, as in Figure 1.3) or curvilinear (for instance when going from ellipsoidal coordinates to Lambert Conic Conformal, as used in Figure 1.1). When reverting this procedure, one can recover the exact original raster.\nCreating a new, regular grid in the new projection is called raster (or image) reprojection or warping (Section 7.8). Warping is lossy, irreversible, and needs to be informed whether raster cells should be interpolated, averaged or summed, or whether resampling using nearest neighbours should be used. For such choices it matters whether cell values reflect a categorical or continuous variable (see also Section 1.6).",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "01-hello.html#time-series-arrays-data-cubes",
    "href": "01-hello.html#time-series-arrays-data-cubes",
    "title": "1  Getting Started",
    "section": "\n1.5 Time series, arrays, data cubes",
    "text": "1.5 Time series, arrays, data cubes\nA lot of spatial data is not only spatial, but also temporal. Just like any observation is associated with an observation location, it is associated with an observation time or period. The dataset on the North Carolina counties shown above contains disease cases counted over two time periods (shown in Figure 1.2). Although the original dataset has these variables in two different columns, for plotting them these columns had to be stacked first, while repeating the associated geometries - a form called tidy Wickham (2014). When we have longer time series associated with geometries, neither option - distributing time over multiple columns, or stacking columns while repeating geometries - works well, and a more effective way of storing such data would be a matrix or array, where one dimension refers to time, and the other(s) to space. The natural way for image or raster data is already to store them in matrices; time series of rasters then lead to a three-dimensional array. The general term for such data is a (spatiotemporal) data cube, where cube refers to arrays with any number of dimensions. Data cubes can refer to both raster and vector data, examples are given in Chapter 6.",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "01-hello.html#sec-support",
    "href": "01-hello.html#sec-support",
    "title": "1  Getting Started",
    "section": "\n1.6 Support",
    "text": "1.6 Support\n \nWhen we have spatial data with geometries that are not points but collections of points (multi-points, lines, polygons, pixels), then an attribute associated with these geometries has one of several different relationships to them. An attribute can have:\n\na constant value for every point of the geometry\na single value that is an aggregate over all points of the geometry\na value that is unique to only this geometry, describing its identity\n\n\nAn example of a constant is land use or bedrock type of a polygon. An example of an aggregate is the number of births of a county over a given period of time. An example of an identity is a county name.\nThe spatial area an attribute value refers to is called its support: aggregate properties have “block” (or area, or line) support, constant properties have “point” support (they apply to every point). Support matters when we manipulate the data. For instance, Figure 1.5 was derived from a variable that has polygon support: the number of births per county. Rasterizing these values gives pixels with values that remain associated with counties. The result of the rasterization is a meaningless map: the numeric values (“birth totals”) are not associated with the raster cells, and the county boundaries are no longer present. Totals of birth for the whole state or birth densities can no longer be recovered from the pixel values. Ignoring support can easily lead to meaningless results. Chapter 5 discusses this further.\nRaster cell values may have point support or block support. An example of point support is elevation, when cells record the elevation of the point at the cell centre in a digital elevation model. An example of block (or cell) support is a satellite image pixel that gives the colour values averaged over (an area similar to) a pixel. Most file formats do not provide this information, yet it may be important to know when aggregating, regridding or warping rasters (Section 7.8), extracting values at point locations.",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "01-hello.html#spatial-data-science-software",
    "href": "01-hello.html#spatial-data-science-software",
    "title": "1  Getting Started",
    "section": "\n1.7 Spatial data science software",
    "text": "1.7 Spatial data science software\n \nAlthough this book largely uses R and R packages for spatial data science, a number of these packages use software libraries that were not developed for R specifically. As an example, the dependency of R package sf on other R packages and system libraries is shown in Figure 1.7.\n\n\n\n\n\n\n\nFigure 1.7: sf and its dependencies; arrows indicate strong dependency, dashed arrows weak dependency\n\n\n\n\nThe C or C++ libraries used (GDAL, GEOS, PROJ, liblwgeom, s2geometry, NetCDF, udunits2) are all developed, maintained, and used by (spatial) data science communities that are large and mostly different from the R community. By using these libraries, R users share how we understand what we are doing with these other communities. Because R, Python, and Julia provide interactive interfaces to this software, many users get closer to these libraries than do users of other software based on these libraries. The first part of this book describes many of the concepts implemented in these libraries, which is relevant to spatial data science in general.\nGDAL\n\nGDAL (Geospatial Data Abstraction Library) can be seen as the Swiss army knife of spatial data; besides for R it is being used in Python, QGIS, PostGIS, and more than 100 other software projects.\nGDAL is a “library of libraries” – in order to read and write these data, it needs a large number of other libraries. It typically links to over 100 other libraries, each of which may provide access to a particular data file format, a database, a web service, or a particular compression codec.\nBinary R packages distributed by CRAN contain only statically linked code: CRAN does not want to make any assumptions about presence of third-party libraries on the host system. As a consequence, when the sf package is installed in binary form from CRAN, it includes a copy of all the required external libraries as well as their dependencies, which may amount to 100 Mb.\nPROJ\n\nPROJ (or PR\\(\\phi\\)J) is a library for cartographic projections and datum transformations: it converts spatial coordinates from one coordinate reference system to another. It comes with a large database of known projections and access to datum grids (high-precision, pre-calculated values for datum transformations). It aligns with an international standard for coordinate reference systems (Lott 2015). Chapter 2 deals with coordinate systems, and PROJ.\nGEOS and s2geometry\n \nGEOS (Geometry Engine Open Source) and s2geometry are two libraries for geometric operations. They are used to find measures (length, area, distance), and calculate predicates (do two geometries have any points in common?) or new geometries (which points do these two geometries have in common?). GEOS does this for flat, two-dimensional space (indicated by \\(R^2\\)), s2geometry does this for geometries on the sphere (indicated by \\(S^2\\)). Chapter 2 introduces coordinate reference systems, and Chapter 4 discusses more about the differences between working with these two spaces.\nNetCDF, udunits2, liblwgeom\n \nNetCDF (UCAR 2020) refers to a file format as well as a C library for reading and writing NetCDF files. It allows the definition of arrays of any dimensionality, and is widely used for spatial and spatiotemporal information, especially in the (climate) modelling communities. Udunits2 (UCAR 2014; Pebesma, Mailund, and Hiebert 2016; Pebesma et al. 2022) is a database and software library for units of measurement that allows the conversion of units, handles derived units, and supports user-defined units. The liblwgeom “library” is a software component of PostGIS (Obe and Hsu 2015) that contains several routines missing from GDAL or GEOS, including convenient access to GeographicLib routines (Karney 2013) that ship with PROJ.",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "01-hello.html#exercises",
    "href": "01-hello.html#exercises",
    "title": "1  Getting Started",
    "section": "\n1.8 Exercises",
    "text": "1.8 Exercises\n\nList five differences between raster and vector data. \n\nIn addition to those listed below Figure 1.1, list five further graphical components that are often found on a map. \n\nIn your own words, why is the numeric information shown in Figure 1.5 misleading (or meaningless)? \n\nUnder which conditions would you expect strong differences when doing geometrical operations on \\(S^2\\), compared to doing them on \\(R^2\\)? \n\n\n\n\n\n\n\n\nKarney, Charles F. F. 2013. “Algorithms for Geodesics.” Journal of Geodesy 87 (1): 43–55. https://link.springer.com/content/pdf/10.1007/s00190-012-0578-z.pdf.\n\n\nLott, Roger. 2015. “Geographic Information-Well-Known Text Representation of Coordinate Reference Systems.” Open Geospatial Consortium.http://docs.opengeospatial.org/is/12-063r5/12-063r5.html .\n\n\nObe, Regina O., and Leo S. Hsu. 2015. PostGIS in Action. Manning Publications Co.\n\n\nPebesma, Edzer, Thomas Mailund, and James Hiebert. 2016. “Measurement Units in R.” The R Journal 8 (2): 486–94. https://doi.org/10.32614/RJ-2016-061.\n\n\nPebesma, Edzer, Thomas Mailund, Tomasz Kalinowski, and Iñaki Ucar. 2022. Units: Measurement Units for R Vectors. https://github.com/r-quantities/units.\n\n\nUCAR. 2014. UDUNITS 2.2.26 Manual. https://www.unidata.ucar.edu/software/udunits/udunits-current/doc/udunits/udunits2.html.\n\n\n———. 2020. The NetCDF User’s Guide. https://www.unidata.ucar.edu/software/netcdf/docs/user_guide.html.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (1).https://www.jstatsoft.org/article/view/v059i10 .",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "02-Spaces.html",
    "href": "02-Spaces.html",
    "title": "2  Coordinates",
    "section": "",
    "text": "2.1 Quantities, units, datum\n“Data are not just numbers, they are numbers with a context”; “In data analysis, context provides meaning” (Cobb and Moore 1997)\nBefore we can try to understand geometries like points, lines, polygons, coverage and grids, it is useful to review coordinate systems so that we have an idea what exactly coordinates of a point reflect. For spatial data, the location of observations are characterised by coordinates, and coordinates are defined in a coordinate system. Different coordinate systems can be used for this, and the most important difference is whether coordinates are defined over a 2 dimensional or 3 dimensional space referenced to orthogonal axes (Cartesian coordinates), or using distance and directions (polar coordinates, spherical and ellipsoidal coordinates). Besides a location of observation, all observations are associated with time of observation, and so time coordinate systems are also briefly discussed. First we will briefly review quantities, to learn what units and datum are.\nThe VIM (“International Vocabulary of Metrology”, BIPM et al. (2012)) defines a quantity as a “property of a phenomenon, body, or substance, where the property has a magnitude that can be expressed as a number and a reference”, where “[a] reference can be a measurement unit, a measurement procedure, a reference material, or a combination of such.” Although one could argue about whether all data is constituted of quantities, there is no need to argue that proper data handling requires that numbers (or symbols) are accompanied by information on what they mean, in particular what they refer to.\nA measurement system consists of base units for base quantities, and derived units for derived quantities. For instance, the SI system of units (Bureau International des Poids et Mesures 2006) consists of seven base units: length (metre, m), mass (kilogram, kg), time (second, s), electric current (ampere, A), thermodynamic temperature (Kelvin, K), amount of substance (mole, mol), and luminous intensity (candela, cd). Derived units are composed of products of integer powers of base units; examples are speed (\\(\\mbox{m}~\\mbox{s}^{-1}\\)), density (\\(\\mbox{kg}~\\mbox{m}^{-3}\\)) and area (\\(\\mbox{m}^2\\)).\nThe special case of unitless measures can refer to either cases where units cancel out (for instance mass fraction: kg/kg, or angle measured in rad: m/m) or to cases where objects or events were counted (such as “5 apples”). Adding an angle to a count of apples would not make sense; adding 5 apples to 3 oranges may make sense if the result is reinterpreted in terms of a superclass, in this case as pieces of fruit. Many data variables have units that are not expressible as SI base units or derived units. Hand (2004) discusses many such measurement scales, including those used to measure variables like intelligence in social sciences, in the context of measurement units.\nFor many quantities, the natural origin of values is zero. This works for amounts, where differences between amounts result in meaningful negative values. For locations and times, differences have a natural zero interpretation: distance and duration. Absolute location (position) and time need a fixed origin, from which we can meaningfully measure other absolute space time points: we call this a datum. For space, a datum involves more than one dimension. The combination of a datum and a measurement unit (scale) is a reference system.\nWe will now elaborate how spatial locations can be expressed as either ellipsoidal or Cartesian coordinates. The next sections will deal with temporal and spatial reference systems, and how they are handled in R.",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coordinates</span>"
    ]
  },
  {
    "objectID": "02-Spaces.html#ellipsoidal-coordinates",
    "href": "02-Spaces.html#ellipsoidal-coordinates",
    "title": "2  Coordinates",
    "section": "\n2.2 Ellipsoidal coordinates",
    "text": "2.2 Ellipsoidal coordinates\n\nCodepar(mar = rep(0,4))\nplot(3, 4, xlim = c(-6,6), ylim = c(-6,6), asp = 1)\naxis(1, pos = 0, at = 0:6)\naxis(2, pos = 0, at = -6:6)\nxd &lt;- seq(-5, 5, by = .1)\nlines(xd, sqrt(25 - xd^2), col = 'grey')\nlines(xd, -sqrt(25 - xd^2), col = 'grey')\narrows(0, 0, 3, 4, col = 'red', length = .15, angle = 20)\ntext(1.5, 2.7, label = \"r\", col = 'red')\nxd &lt;- seq(3/5, 1, by = .1)\nlines(xd, sqrt(1 - xd^2), col = 'red')\ntext(1.2, 0.5, label = parse(text = \"phi\"), col = 'red')\nlines(c(3,3), c(0,4), lty = 2, col = 'blue')\nlines(c(0,3), c(4,4), lty = 2, col = 'blue')\ntext(3.3, 0.3, label = \"x\", col = 'blue')\ntext(0.3, 4.3, label = \"y\", col = 'blue')\n\n\n\n\n\n\nFigure 2.1: Two-dimensional polar (red) and Cartesian (blue) coordinates\n\n\n\n\nFigure 2.1 shows both polar and Cartesian coordinates for a two-dimensional situation. In Cartesian coordinates, the point shown is \\((x,y) = (3,4)\\), for polar coordinates it is \\((r,\\phi) = (5, \\mbox{arctan}(4/3))\\), where \\(\\mbox{arctan}(4/3)\\) is approximately \\(0.93\\) radians, or \\(53^{\\circ}\\). Note that \\(x\\), \\(y\\) and \\(r\\) all have length units, where \\(\\phi\\) is an angle (a unitless length/length ratio). Converting back and forth between Cartesian and polar coordinates is trivial, as \\[x = r~\\mbox{cos} \\phi,\\ \\ \\ y = r~\\mbox{sin} \\phi, \\ \\mbox{and}\\] \\[r = \\sqrt{x^2 + y^2}, \\ \\ \\ \\phi = \\mbox{atan2}(y, x)\\] where \\(\\mbox{atan2}\\) is used in favour of \\(\\mbox{atan}(y/x)\\) to take care of the right quadrant.\nSpherical or ellipsoidal coordinates\n \nIn three dimensions, where Cartesian coordinates are expressed as \\((x,y,z)\\), spherical coordinates are the three-dimensional equivalent of polar coordinates and can be expressed as \\((r,\\lambda,\\phi)\\), where:\n\n\n\\(r\\) is the radius of the sphere,\n\n\\(\\lambda\\) is the longitude, measured in the \\((x,y)\\) plane counter-clockwise from positive \\(x\\), and\n\n\\(\\phi\\) is the latitude, the angle between the vector and the \\((x,y)\\) plane.\n\nFigure 2.2 illustrates Cartesian geocentric and ellipsoidal coordinates.\n\nCodelibrary(sf) |&gt; suppressPackageStartupMessages()\ne &lt;- cbind(-90:90,0) # equator\nf1 &lt;- rbind(cbind(0, -90:90)) # 0/antimerid.\nf2 &lt;- rbind(cbind(90, -90:90), cbind(270, 90:-90))# +/- 90\neq &lt;- st_sfc(st_linestring(e), st_linestring(f1), st_linestring(f2), crs='OGC:CRS84')\n\ngeoc &lt;- st_transform(eq, \"+proj=geocent\")\ncc &lt;- rbind(geoc[[1]], NA, geoc[[2]], NA, geoc[[3]])\nfrom3d &lt;- function(x, offset, maxz, minz) {\n    x = x[,c(2,3,1)] + offset # move to y right, x up, z backw\n    x[,2] = x[,2] - maxz      # shift y to left\n    d = maxz\n    z = x[,3] - minz + offset\n    x[,1] = x[,1] * (d/z)\n    x[,2] = x[,2] * (d/z)\n    x[,1:2]\n}\nmaxz &lt;- max(cc[,3], na.rm = TRUE)\nminz &lt;- min(cc[,3], na.rm = TRUE)\noffset &lt;- 3e7\ncirc &lt;- from3d(cc, offset, maxz, minz)\nmx &lt;- max(cc, na.rm = TRUE) * 1.1\nx &lt;- rbind(c(0, 0, 0), c(mx, 0, 0))\ny &lt;- rbind(c(0, 0, 0), c(0, mx, 0))\nz &lt;- rbind(c(0, 0, 0), c(0, 0, mx))\nll &lt;- rbind(x, NA, y, NA, z)\nl0 &lt;-  from3d(ll, offset, maxz, minz)\nmx &lt;- max(cc, na.rm = TRUE) * 1.2\nx &lt;- rbind(c(0, 0, 0), c(mx, 0, 0))\ny &lt;- rbind(c(0, 0, 0), c(0, mx, 0))\nz &lt;- rbind(c(0, 0, 0), c(0, 0, mx))\nll &lt;- rbind(x, NA, y, NA, z)\nl &lt;-  from3d(ll, offset, maxz, minz)\n\npar(mfrow = c(1, 2))\npar(mar = rep(0,4))\nplot.new()\nplot.window(xlim = c(min(circ[,1],na.rm = TRUE), 3607103*1.02), \n                        ylim = c(min(circ[,2],na.rm = TRUE), 2873898*1.1), asp = 1)\nlines(circ)\nlines(l0)\ntext(l[c(2,5,8),], labels = c(\"x\", \"y\", \"z\"), col = 'red')\n# add POINT(60 47)\np &lt;- st_as_sfc(\"POINT(60 47)\", crs = 'OGC:CRS84') |&gt; st_transform(\"+proj=geocent\")\np &lt;- p[[1]]\npts &lt;- rbind(c(0,0,0), c(p[1],0,0), c(p[1],p[2],0), c(p[1],p[2],p[2]))\nptsl &lt;- from3d(pts, offset, maxz, minz)\nlines(ptsl, col = 'blue', lty = 2, lwd = 2)\npoints(ptsl[4,1], ptsl[4,2], col = 'blue', cex = 1, pch = 16)\n\nplot.new()\nplot.window(xlim = c(min(circ[,1],na.rm = TRUE), 3607103*1.02), \n                        ylim = c(min(circ[,2],na.rm = TRUE), 2873898*1.1), asp = 1)\nlines(circ)\n\np &lt;- st_as_sfc(\"POINT(60 47)\", crs = 'OGC:CRS84') |&gt; st_transform(\"+proj=geocent\")\np &lt;- p[[1]]\npts &lt;- rbind(c(0,0,0), c(p[1],p[2],p[3]))\npt &lt;-  from3d(pts, offset, maxz, minz)\nlines(pt)\npoints(pt[2,1], pt[2,2], col = 'blue', cex = 1, pch = 16)\n\np0 &lt;- st_as_sfc(\"POINT(60 0)\", crs = 'OGC:CRS84') |&gt; st_transform(\"+proj=geocent\")\np0 &lt;- p0[[1]]\npts &lt;- rbind(c(0,0,0), c(p0[1],p0[2],p0[3]))\npt &lt;-  from3d(pts, offset, maxz, minz)\nlines(pt)\n\np0 &lt;- st_as_sfc(\"POINT(0 0)\", crs = 'OGC:CRS84') |&gt; st_transform(\"+proj=geocent\")\np0 &lt;- p0[[1]]\npts &lt;- rbind(c(0,0,0), c(p0[1],p0[2],p0[3]))\npt &lt;-  from3d(pts, offset, maxz, minz)\nlines(pt)\n\np0 &lt;- st_as_sfc(\"POINT(0 90)\", crs = 'OGC:CRS84') |&gt; st_transform(\"+proj=geocent\")\np0 &lt;- p0[[1]]\npts &lt;- rbind(c(0,0,0), c(p0[1],p0[2],p0[3]))\npt &lt;-  from3d(pts, offset, maxz, minz)\nlines(pt, lty = 2)\n\np0 &lt;- st_as_sfc(\"POINT(90 0)\", crs = 'OGC:CRS84') |&gt; st_transform(\"+proj=geocent\")\np0 &lt;- p0[[1]]\npts &lt;- rbind(c(0,0,0), c(p0[1],p0[2],p0[3]))\npt &lt;-  from3d(pts, offset, maxz, minz)\nlines(pt, lty = 2)\n\nf1 &lt;- rbind(cbind(0:60, 0))\narc &lt;- st_sfc(st_linestring(f1), crs='OGC:CRS84')\ngeoc &lt;- st_transform(arc, \"+proj=geocent\")\ncc &lt;- rbind(geoc[[1]])\ncirc &lt;- from3d(cc, offset, maxz, minz)\nlines(circ, col = 'red', lwd = 2, lty = 2)\n\nf1 &lt;- rbind(cbind(60, 0:47))\narc &lt;- st_sfc(st_linestring(f1), crs='OGC:CRS84')\ngeoc &lt;- st_transform(arc, \"+proj=geocent\")\ncc &lt;- rbind(geoc[[1]])\ncirc &lt;- from3d(cc, offset, maxz, minz)\nlines(circ, col = 'blue', lwd = 2, lty = 2)\n\ntext(pt[1,1]+100000, pt[1,2]+50000, labels = expression(phi), col = 'blue') # lat\ntext(pt[1,1]+20000, pt[1,2]-50000, labels = expression(lambda), col = 'red') # lng\n\n\n\n\n\n\nFigure 2.2: Cartesian geocentric coordinates (left) measure three distances, ellipsoidal coordinates (right) measure two angles, and possibly an ellipsoidal height\n\n\n\n\n\\(\\lambda\\) typically varies between \\(-180^{\\circ}\\) and \\(180^{\\circ}\\) (or alternatively from \\(0^{\\circ}\\) to \\(360^{\\circ}\\)), \\(\\phi\\) from \\(-90^{\\circ}\\) to \\(90^{\\circ}\\). When we are only interested in points on a sphere with given radius, we can drop \\(r\\): \\((\\lambda,\\phi)\\) now suffice to identify any point.\n\nIt should be noted that this is just a definition, one could for instance also choose to measure polar angle, the angle between the vector and \\(z\\), instead of latitude. There is also a long tradition of specifying points as \\((\\phi,\\lambda)\\) but throughout this book we will stick to longitude-latitude, \\((\\lambda,\\phi)\\). The point denoted in Figure 2.2 has \\((\\lambda,\\phi)\\) or ellipsoidal coordinates with angular values\n\nCodep &lt;- st_as_sfc(\"POINT(60 47)\", crs = 'OGC:CRS84')\np[[1]]\n\n# POINT (60 47)\n\n\nmeasured in degrees, and geocentric coordinates\n\nCodep &lt;- st_as_sfc(\"POINT(60 47)\", crs = 'OGC:CRS84') |&gt; st_transform(\"+proj=geocent\")\np[[1]]\n\n# POINT Z (2178844 3773868 4641765)\n\n\nmeasured in metres.\n\nFor points on an ellipse, there are two ways in which angle can be expressed (Figure 2.3): measured from the centre of the ellipse (\\(\\psi\\)), or measured perpendicular to the tangent on the ellipse at the target point (\\(\\phi\\)).\n\nCodepar(mar = rep(0,4))\nx &lt;- 4\ny &lt;- 5/8 * sqrt(48)\nplot(x, y, xlim = c(-6,6), ylim = c(-8,8), asp = 1)\naxis(1, pos = 0, at = 0:9)\naxis(2, pos = 0, at = -5:5)\nxd &lt;- seq(-8, 8, by = .1)\nlines(xd, 5/8 * sqrt(64 - xd^2), col = 'grey')\nlines(xd, 5/8 * -sqrt(64 - xd^2), col = 'grey')\narrows(0, 0, x, y, col = 'red', length = .15, angle = 20)\nb &lt;- (x * 25) / (-y * 64)\na &lt;- y - x * b\nabline(a, b, col = 'grey')\nb &lt;- -1/b\nx0 &lt;- x - y / b\narrows(x0, 0, x, y, col = 'blue', length = .15, angle = 20)\ntext(1.2, 0.5, label = parse(text = \"psi\"), col = 'red')\ntext(3, 0.5, label = parse(text = \"phi\"), col = 'blue')\n\n\n\n\n\n\nFigure 2.3: Angles on an ellipse: geodetic (blue) and geocentric (red) latitude\n\n\n\n\nThe most commonly used parametric model for the Earth is an ellipsoid of revolution, an ellipsoid with two equal semi-axes (Iliffe and Lott 2008). In effect, this is a flattened sphere (or spheroid): the distance between the poles is (slightly: about 0.33%) smaller than the distance between two opposite points on the equator. Under this model, longitude is always measured along a circle (as in Figure 2.2), and latitude along an ellipse (as in Figure 2.3). If we think of Figure 2.3 as a cross section of the Earth passing through the poles, the geodetic latitude measure \\(\\phi\\) is the one used when no further specification is given. The latitude measure \\(\\psi\\) is called the geocentric latitude.\n \nwe can add altitude or elevation to longitude and latitude to define points that are above or below the ellipsoid, and obtain a three-dimensional space again. When defining altitude, we need to choose:\n\nwhere zero altitude is: on the ellipsoid, or relative to the surface approximating mean sea level (the geoid)?\nwhich direction is positive, and\nwhich direction is “straight up”: perpendicular to the ellipsoid surface, or in the direction of gravity, perpendicular to the surface of the geoid?\n\nAll these choices may matter, depending on the application area and required measurement accuracies.\nThe shape of the Earth is not a perfect ellipsoid. As a consequence, several ellipsoids with different shape parameters and bound to the Earth in different ways are being used. Such ellipsoids are called datums, and are briefly discussed in Section 2.3, along with coordinate reference systems.\nProjected coordinates, distances\nBecause paper maps and computer screens are much more abundant and practical than globes, when we look at spatial data we see it projected: drawn on a flat, two-dimensional surface. Computing the locations in a two-dimensional space means that we work with projected coordinates. Projecting ellipsoidal coordinates means that shapes, directions, areas, or even all three, are distorted (Iliffe and Lott 2008).\n \nDistances between two points \\(p_i\\) and \\(p_j\\) in Cartesian coordinates are computed as Euclidean distances, in two dimensions by \\[d_{ij} = \\sqrt{(x_i-x_j)^2+(y_i-y_j)^2}\\] with \\(p_i = (x_i,y_i)\\) and in three dimensions by \\[d_{ij} = \\sqrt{(x_i-x_j)^2+(y_i-y_j)^2+(z_i-z_j)^2}\\] with \\(p_i = (x_i,y_i,z_i).\\) These distances represent the length of a straight line between two points \\(i\\) and \\(j\\).\nFor two points on a circle, the length of the arc of two points \\(c_1 = (r,{\\phi}_1)\\) and \\(c_2 = (r, \\phi_2)\\) is \\[s_{ij}=r~|\\phi_1-\\phi_2| = r ~\\theta\\] with \\(\\theta\\) the angle between \\(\\phi_1\\) and \\(\\phi_2\\) in radians. For very small values of \\(\\theta\\), we will have \\(s_{ij} \\approx d_{ij}\\), because a small arc segment is nearly straight.\n\nFor two points \\(p_1 = (\\lambda_1,\\phi_1)\\) and \\(p_2 =\n(\\lambda_2,\\phi_2)\\) on a sphere with radius \\(r'\\), the great circle distance is the arc length between \\(p_1\\) and \\(p_2\\) on the circle that passes through \\(p_1\\) and \\(p_2\\) and has the centre of the sphere as its centre, and is given by \\(s_{12} = r ~ \\theta_{12}\\) with \\[\\theta_{12} = \\arccos(\\sin \\phi_1 \\cdot \\sin \\phi_2 + \\cos \\phi_1 \\cdot \\cos \\phi_2 \\cdot \\cos(|\\lambda_1-\\lambda_2|))\\] the angle between \\(p_1\\) and \\(p_2\\), in radians.\nArc distances between two points on a spheroid are more complicated to compute; a good discussion on the topic and an explanation of the method implemented in GeographicLib (part of PROJ) is given in Karney (2013).\n\nTo show that these distance measures actually give different values, we computed them for the distance Berlin - Paris. Here, gc_ refers to ellipsoidal and spherical great circle distances, str_ refers to straight line, Euclidean distances between Cartesian geocentric coordinates associated on the WGS84 ellipse and sphere:\n\nCodepts &lt;- st_sfc(st_point(c(13.4050, 52.5200)), st_point(c(2.3522, 48.8566)), crs = 'OGC:CRS84')\ns2_orig &lt;- sf_use_s2(FALSE)\nd1 &lt;- c(gc_ellipse = st_distance(pts)[1,2])\nsf_use_s2(TRUE)\n# or, without using s2, use st_distance(st_transform(pts, \"+proj=cart +ellps=sphere\"))\nd2 &lt;- c(gc_sphere = st_distance(pts)[1,2])\np &lt;- st_transform(pts, \"+proj=cart +ellps=WGS84\")\nd3 &lt;- c(str_ellipse = units::set_units(sqrt(sum(apply(do.call(cbind, p), 1, diff)^2)), m))\np2 &lt;- st_transform(pts, \"+proj=cart +ellps=sphere\")\nd4 &lt;- c(str_sphere = units::set_units(sqrt(sum(apply(do.call(cbind, p2), 1, diff)^2)), m))\nres &lt;- c(d1, d3, d2, d4) # note order\n# print as km, re-add names:\nsf_use_s2(s2_orig) # back to what it was before changing\nres |&gt; units::set_units(km) |&gt; setNames(names(res)) |&gt; print(digits = 5)\n\n# Units: [km]\n#  gc_ellipse str_ellipse   gc_sphere  str_sphere \n#      879.70      879.00      877.46      876.77\n\n\nBounded and unbounded spaces\n \nTwo-dimensional and three-dimensional Euclidean spaces (\\(R^2\\) and \\(R^3\\)) are unbounded. Every line in this space has infinite length, and areas or volumes have no natural upper limit. In contrast, spaces defined on a circle (\\(S^1\\)) or sphere (\\(S^2\\)) define a bounded set: there may be infinitely many points but the length and area of the circle, and the radius, area and volume of a sphere are bounded.\nThis may sound trivial but leads to some interesting challenges when handling spatial data. A polygon on \\(R^2\\) has unambiguously an inside and an outside. On a sphere, \\(S^2\\), any polygon divides the sphere in two parts, and which of these two is to be considered inside and which outside is ambiguous and needs to be defined by the traversal direction. Chapter 4 will further discuss consequences when working with geometries on \\(S^2\\).",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coordinates</span>"
    ]
  },
  {
    "objectID": "02-Spaces.html#sec-crs",
    "href": "02-Spaces.html#sec-crs",
    "title": "2  Coordinates",
    "section": "\n2.3 Coordinate reference systems",
    "text": "2.3 Coordinate reference systems\n\nWe follow Lott (2015) when defining the following concepts (italics indicate literal quoting):\n\na coordinate system is a set of mathematical rules for specifying how coordinates are to be assigned to points,\na datum is a parameter or set of parameters that define the position of the origin, the scale, and the orientation of a coordinate system,\na geodetic datum is a datum describing the relationship of a two- or three-dimensional coordinate system to the Earth, and\na coordinate reference system is a coordinate system that is related to an object by a datum; for geodetic and vertical datums, the object will be the Earth.\n\n\n \nA readable text that further explains these concepts is Iliffe and Lott (2008).\nThe Earth does not follow a regular shape. The topography of the Earth is of course known to vary strongly, but also the surface formed by constant gravity at mean sea level, the geoid, is irregular. A commonly used model that is fit to the geoid is an ellipsoid of revolution, which is an ellipsoid with two identical minor axes. Fitting such an ellipsoid to the Earth gives a datum. However, fitting it to different areas, or based on different sets of reference points gives different fits, and hence different datums: a datum can for instance be fixed to a particular tectonic plate (like the European Terrestrial Reference System 1989 (ETRS89)), others can be globally fit (like WGS84). More local fits lead to smaller approximation errors.\n\nThe definitions above imply that coordinates in degrees longitude and latitude only have a meaning and can only be interpreted unambiguously as Earth coordinates, when the datum they are associated with is given.\nNote that for projected data, the data that were projected are associated with a reference ellipsoid (datum). Going from one projection to another without changing datum is called coordinate conversion, and passes through the ellipsoidal coordinates associated with the datum involved. This process is lossless and invertible: the parameters and equations associated with a conversion are not empirical. Recomputing coordinates in a new datum is called coordinate transformation, and is approximate: because datums are a result of model fitting, transformations between datums are models too that have been fit; the equations involved are empirical, and multiple transformation paths, based on different model fits and associated with different accuracies, are possible.\n \nPlate tectonics imply that within a global datum, fixed objects may have coordinates that change over time, and that transformations from one datum to another may be time-dependent. Earthquakes are a cause of more local and sudden changes in coordinates. Local datums may be fixed to tectonic plates (such as ETRS89), or may be dynamic.",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coordinates</span>"
    ]
  },
  {
    "objectID": "02-Spaces.html#sec-projlib",
    "href": "02-Spaces.html#sec-projlib",
    "title": "2  Coordinates",
    "section": "\n2.4 PROJ and mapping accuracy",
    "text": "2.4 PROJ and mapping accuracy\nVery few living people active in open source geospatial software can remember the time before PROJ. PROJ (Evenden 1990) started in the 1970s as a Fortran project, and was released in 1985 as a C library for cartographic projections. It came with command line tools for direct and inverse projections, and could be linked to software to let it support (re)projection directly. Originally, datums were considered implicit, and no datum transformations were allowed.\n \nIn the early 2000s, PROJ was known as PROJ.4, after its never-changing major version number. Amongst others motivated by the rise of GPS, the need for datum transformations increased and PROJ.4 was extended with rudimentary datum support. PROJ definitions for coordinate reference systems would look like this:\n+proj=utm +zone=33 +datum=WGS84 +units=m +no_defs\nwhere key=value pairs are preceded by a + and separated by a space. This form came to be known as “PROJ.4 string”, since the PROJ project stayed at version 4.x for several decades. Other datums would come with fields like:\n+ellps=bessel +towgs84=565.4,50.3,465.6,-0.399,0.344,-1.877,4.072\nindicating another ellipse, as well as the seven (or three) parameters for transforming from this ellipse to WGS84 (the “World Geodetic System 1984” global datum once popularised by GPS), effectively defining the datum in terms of a transformation to WGS84.\nAlong with PROJ.4 came a set of databases with known (registered) projections, from which the best known is the European Petroleum Survey Group (EPSG) registry. National mapping agencies would provide (and update over time) their best guesses of +towgs84 parameters for national coordinate reference systems, and distribute through the EPSG registry, which was part of PROJ distributions. For some transformations, datum grids were available and distributed as part of PROJ.4: such grids are raster maps that provide for every location pre-computed values for the shift in longitude and latitude, or elevation, for a particular datum transformation.\nIn PROJ.4, every coordinate transformation had to go through a conversion to and from WGS84; even reprojecting data associated with a datum different from WGS84 had to go through a transformation to and from WGS84. The associated errors of up to 100 m were acceptable for mapping purposes for not too small areas, but some applications need higher accuracy transformations. Examples include precision agriculture, planning flights of UAV’s, or object tracking.\nIn 2018, after a successful “GDAL Coordinate System Barn Raising” initiative, a number of companies profiting from the open source geospatial software stack supported the development of a more modern, mature coordinate transformation system in PROJ. Over a few years, PROJ.4 evolved through versions 5, 6, 7, 8 and 9 and was hence renamed into PROJ (or PR\\(\\phi\\)J).\n \nThe most notable changes include:\n\nalthough PROJ.4 strings can still be used to initialise certain coordinate reference systems, they are no longer sufficient to represent all of them; a new format, WKT-2 (described in next section) replaces it\nWGS84 as a hub datum is dropped: coordinate transformations no longer need to go through a particular datum\nmultiple conversion or transformation paths (so-called pipelines) to go from CRS A to CRS B are possible, and can be reported along with the associated accuracy; PROJ will by default use the most accurate one but user control is possible\ntransformation pipelines can chain an arbitrary number of elementary transformation operations, including swapping of axes and unit transformations\ndatum grids, of which there are now many more, are no longer distributed with the library but are accessible from a content delivery network (CDN); PROJ allows enabling and disabling network access to these grids and only downloads the section(s) of the grid actually needed, storing it in a cache on the user’s machine for future use\ncoordinate transformations receive support for epochs, time-dependent transformations (and hence: four-dimensional coordinates, including the source and target time)\nthe set of files with registered coordinate reference systems is handled in an SQLite database\ninstead of always handling axis order (longitude, latitude), when the authority defines differently this is now obeyed (but see Section 2.5 and Section 7.7.6)\n\n \nAll these points sound like massive improvements, and accuracies of transformation can be below 1 metre. An interesting point is the last: Where we could safely assume for many decades that spatial data with ellipsoidal coordinates would have axis order (longitude, latitude), this is no longer the case. We will see in Section 7.7.6 how to deal with this.\n\n\nCodelibrary(stars)\nlibrary(rnaturalearth)\nlibrary(dplyr) |&gt; suppressPackageStartupMessages()\ncountries110 |&gt; \n    st_as_sf() |&gt;\n    filter(ADMIN == \"United Kingdom\") |&gt;\n    st_geometry() -&gt; uk\nfilename = \"data/uk_os_OSTN15_NTv2_OSGBtoETRS.tif\"\nr &lt;- if (file.exists(filename)) {\n    r &lt;- read_stars(filename)\n  } else {\n    read_stars(\"/vsicurl/https://cdn.proj.org/uk_os_OSTN15_NTv2_OSGBtoETRS.tif\")\n  }\nhook &lt;- function() {\n        plot(uk, border = \"orange\", col = NA, add = TRUE)\n}\nplot(r[,,,1:2], axes = TRUE, hook = hook, key.pos = 4)\n\n\n\n\n\n\nFigure 2.4: UK horizontal datum grid, from datum OSGB 1936 (EPSG:4277) to datum ETRS89 (EPSG:4258); units arc-seconds\n\n\n\n\n\nCodefilename = \"data/uk_os_OSGM15_GB.tif\"\nh &lt;- if (file.exists(filename)) {\n    read_stars(filename)\n  } else {\n    read_stars(\"/vsicurl/https://cdn.proj.org/uk_os_OSGM15_GB.tif\")\n  } \nplot(h, axes = TRUE, reset = FALSE)\nplot(uk, border = \"orange\", col = NA, add = TRUE)\n\n\n\n\n\n\nFigure 2.5: UK vertical datum grid, from ETRS89 (EPSG:4937) to ODN height (EPSG:5701), units m\n\n\n\n\n \nExamples of a horizontal datum grids, downloaded from cdn.proj.org, are shown in Figure 2.4 and for a vertical datum grid in Figure 2.5. Datum grids may carry per-pixel accuracy values.",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coordinates</span>"
    ]
  },
  {
    "objectID": "02-Spaces.html#sec-wkt2",
    "href": "02-Spaces.html#sec-wkt2",
    "title": "2  Coordinates",
    "section": "\n2.5 WKT-2",
    "text": "2.5 WKT-2\nLott (2015) describes a standard for encoding coordinate reference systems, as well as transformations between them using well-known text; the standard (and format) is referred to informally as WKT-2. As mentioned above, GDAL and PROJ fully support this encoding. An example of WKT-2 for CRS EPSG:4326 is:\n \nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\nThis shows a coordinate system with the axis order latitude, longitude, although in most practical cases the axis order used is longitude, latitude. The ensemble of WGS84 ellipsoids listed represents its various updates over time. Ambiguity about which of these ensemble members a particular dataset should use leads to an uncertainty of several meters. The coordinate reference system OGC:CRS84 disambiguates the axis order and explicitly states it to be longitude, latitude, and is the recommended alternative to WGS84 datasets using this axis order. It does not disambiguate the datum ensemble problem.\n \nA longer introduction on the history and recent changes in PROJ is given in Bivand (2020), building upon the work of Knudsen and Evers (2017) and Evers and Knudsen (2017).",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coordinates</span>"
    ]
  },
  {
    "objectID": "02-Spaces.html#exercises",
    "href": "02-Spaces.html#exercises",
    "title": "2  Coordinates",
    "section": "\n2.6 Exercises",
    "text": "2.6 Exercises\nTry to solve the following exercises with R (without loading packages); try to use functions where appropriate:\n\nlist three geographic measures that do not have a natural zero origin \n\nconvert the \\((x,y)\\) points \\((10,2)\\), \\((-10,-2)\\), \\((10,-2)\\), and \\((0,10)\\) to polar coordinates\nconvert the polar \\((r,\\phi)\\) points \\((10,45^{\\circ})\\), \\((0,100^{\\circ})\\), and \\((5,359^{\\circ})\\) to Cartesian coordinates\nassuming the Earth is a sphere with a radius of 6371 km, compute for \\((\\lambda,\\phi)\\) points the great circle distance between \\((10,10)\\) and \\((11,10)\\), between \\((10,80)\\) and \\((11,80)\\), between \\((10,10)\\) and \\((10,11)\\), and between \\((10,80)\\) and \\((10,81)\\) (units: degree). What are the distance units?\n\n\n\n\n\n\n\nBIPM, IEC, ILAC IFCC, IUPAP IUPAC, and OIML ISO. 2012. “The International Vocabulary of Metrology–Basic and General Concepts and Associated Terms (VIM), 3rd Edn. JCGM 200: 2012.” JCGM (Joint Committee for Guides in Metrology). https://www.bipm.org/en/publications/guides/.\n\n\nBivand, Roger. 2020. Why Have CRS, Projections and Transformations Changed?https://rgdal.r-forge.r-project.org/articles/CRS_projections_transformations.html .\n\n\nBureau International des Poids et Mesures. 2006. The International System of Units (SI), 8th Edition. Organisation Intergouvernementale de la Convention du Mètre. https://www.bipm.org/en/publications/si-brochure/download.html.\n\n\nCobb, George W., and David S. Moore. 1997. “Mathematics, Statistics and Teaching.” The American Mathematical Monthly 104: 801–23. https://www.jstor.org/stable/2975286.\n\n\nEvenden, Gerald I. 1990. Cartographic Projection Procedures for the UNIX Environment — a User’s Manual. http://download.osgeo.org/proj/OF90-284.pdf.\n\n\nEvers, Kristian, and Thomas Knudsen. 2017. Transformation Pipelines for PROJ.4. https://www.fig.net/resources/proceedings/fig_proceedings/fig2017/papers/iss6b/ISS6B_evers_knudsen_9156.pdf.\n\n\nHand, David J. 2004. Measurement Theory and Practice. Arnold, London.\n\n\nIliffe, Jonathan, and Roger Lott. 2008. Datums and Map Projections for Remote Sensing, GIS, and Surveying. Whittles Pub. CRC Press, Scotland, UK.\n\n\nKarney, Charles F. F. 2013. “Algorithms for Geodesics.” Journal of Geodesy 87 (1): 43–55. https://link.springer.com/content/pdf/10.1007/s00190-012-0578-z.pdf.\n\n\nKnudsen, Thomas, and Kristian Evers. 2017. Transformation Pipelines for PROJ.4. https://meetingorganizer.copernicus.org/EGU2017/EGU2017-8050.pdf.\n\n\nLott, Roger. 2015. “Geographic Information-Well-Known Text Representation of Coordinate Reference Systems.” Open Geospatial Consortium.http://docs.opengeospatial.org/is/12-063r5/12-063r5.html .",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coordinates</span>"
    ]
  },
  {
    "objectID": "03-Geometries.html",
    "href": "03-Geometries.html",
    "title": "3  Geometries",
    "section": "",
    "text": "3.1 Simple feature geometries\nHaving learned how we represent coordinates systems, we can define how geometries can be described using these coordinate systems. This chapter will explain:\nGeometries on the sphere are discussed in Chapter 4, rasters and other rectangular sub-divisions of space or space time are discussed in Chapter 6.\nSimple feature geometries are a way to describe the geometries of features. By features we mean things that have a geometry, potentially implicitly some time properties, and further attributes that could include labels describing the thing and/or values quantitatively measuring it. The main application of simple feature geometries is to describe geometries in two-dimensional space by points, lines, or polygons. The “simple” adjective refers to the fact that the line or polygon geometries are represented by sequences of points connected with straight lines that do not self-intersect.\nSimple features access is a standard (Herring 2011, 2010; ISO 2004) for describing simple feature geometries. It includes:\nWe will first discuss the seven most common simple feature geometry types.",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geometries</span>"
    ]
  },
  {
    "objectID": "03-Geometries.html#sec-simplefeatures",
    "href": "03-Geometries.html#sec-simplefeatures",
    "title": "3  Geometries",
    "section": "",
    "text": "a class hierarchy\na set of operations\nbinary and text encodings\n\n\nThe big seven\n \nThe most common simple feature geometries used to represent a single feature are:\n\n\n\n\n\n\ntype\ndescription\n\n\n\nPOINT\nsingle point geometry\n\n\nMULTIPOINT\nset of points\n\n\nLINESTRING\nsingle linestring (two or more points connected by straight lines)\n\n\nMULTILINESTRING\nset of linestrings\n\n\nPOLYGON\nexterior ring with zero or more inner rings, denoting holes\n\n\nMULTIPOLYGON\nset of polygons\n\n\nGEOMETRYCOLLECTION\nset of the geometries above\n\n\n\n\nCodelibrary(sf) |&gt; suppressPackageStartupMessages()\npar(mfrow = c(2,4))\npar(mar = c(1,1,1.2,1))\n\n# 1\np &lt;- st_point(0:1)\nplot(p, pch = 16)\ntitle(\"point\")\nbox(col = 'grey')\n\n# 2\nmp &lt;- st_multipoint(rbind(c(1,1), c(2, 2), c(4, 1), c(2, 3), c(1,4)))\nplot(mp, pch = 16)\ntitle(\"multipoint\")\nbox(col = 'grey')\n\n# 3\nls &lt;- st_linestring(rbind(c(1,1), c(5,5), c(5, 6), c(4, 6), c(3, 4), c(2, 3)))\nplot(ls, lwd = 2)\ntitle(\"linestring\")\nbox(col = 'grey')\n\n# 4\nmls &lt;- st_multilinestring(list(\n  rbind(c(1,1), c(5,5), c(5, 6), c(4, 6), c(3, 4), c(2, 3)),\n  rbind(c(3,0), c(4,1), c(2,1))))\nplot(mls, lwd = 2)\ntitle(\"multilinestring\")\nbox(col = 'grey')\n\n# 5 polygon\npo &lt;- st_polygon(list(rbind(c(2,1), c(3,1), c(5,2), c(6,3), c(5,3), c(4,4), c(3,4), c(1,3), c(2,1)),\n    rbind(c(2,2), c(3,3), c(4,3), c(4,2), c(2,2))))\nplot(po, border = 'black', col = '#ff8888', lwd = 2)\ntitle(\"polygon\")\nbox(col = 'grey')\n\n# 6 multipolygon\nmpo &lt;- st_multipolygon(list(\n    list(rbind(c(2,1), c(3,1), c(5,2), c(6,3), c(5,3), c(4,4), c(3,4), c(1,3), c(2,1)),\n        rbind(c(2,2), c(3,3), c(4,3), c(4,2), c(2,2))),\n    list(rbind(c(3,7), c(4,7), c(5,8), c(3,9), c(2,8), c(3,7)))))\nplot(mpo, border = 'black', col = '#ff8888', lwd = 2)\ntitle(\"multipolygon\")\nbox(col = 'grey')\n\n# 7 geometrycollection\ngc &lt;- st_geometrycollection(list(po, ls + c(0,5), st_point(c(2,5)), st_point(c(5,4))))\nplot(gc, border = 'black', col = '#ff6666', pch = 16, lwd = 2)\ntitle(\"geometrycollection\")\nbox(col = 'grey')\n\n\n\n\n\n\nFigure 3.1: Sketches of the main simple feature geometry types\n\n\n\n\nFigure 3.1 shows examples of these basic geometry types. The human-readable, “well-known text” (WKT) representation of the geometries plotted are:\n \n\nCodep\nmp\nls\nmls\npo\nmpo\ngc\n\n\nPOINT (0 1)\nMULTIPOINT ((1 1), (2 2), (4 1), (2 3), (1 4))\nLINESTRING (1 1, 5 5, 5 6, 4 6, 3 4, 2 3)\nMULTILINESTRING ((1 1, 5 5, 5 6, 4 6, 3 4, 2 3), (3 0, 4 1, 2 1))\nPOLYGON ((2 1, 3 1, 5 2, 6 3, 5 3, 4 4, 3 4, 1 3, 2 1),\n    (2 2, 3 3, 4 3, 4 2, 2 2))\nMULTIPOLYGON (((2 1, 3 1, 5 2, 6 3, 5 3, 4 4, 3 4, 1 3, 2 1),\n    (2 2, 3 3, 4 3, 4 2, 2 2)), ((3 7, 4 7, 5 8, 3 9, 2 8, 3 7)))\nGEOMETRYCOLLECTION (\n    POLYGON ((2 1, 3 1, 5 2, 6 3, 5 3, 4 4, 3 4, 1 3, 2 1),\n      (2 2 , 3 3, 4 3, 4 2, 2 2)),\n    LINESTRING (1 6, 5 10, 5 11, 4 11, 3 9, 2 8),\n    POINT (2 5),\n    POINT (5 4)\n)\nIn this representation, coordinates are separated by space, and points by commas. Sets are grouped by parentheses, and separated by commas. Polygons consist of an outer ring followed by zero or more inner rings denoting holes.\nIndividual points in a geometry contain at least two coordinates: \\(x\\) and \\(y\\), in that order. If these coordinates refer to ellipsoidal coordinates, \\(x\\) and \\(y\\) usually refer to longitude and latitude, respectively, although sometimes to latitude and longitude (see Section 2.4 and Section 7.7.6).\nSimple and valid geometries, ring direction\n \nLinestrings are called simple when they do not self-intersect:\n\nCode(ls &lt;- st_linestring(rbind(c(0,0), c(1,1), c(2,2), c(0,2), c(1,1), c(2,0))))\n\n# LINESTRING (0 0, 1 1, 2 2, 0 2, 1 1, 2 0)\n\nCodec(is_simple = st_is_simple(ls))\n\n# is_simple \n#     FALSE\n\n\nValid polygons and multi-polygons obey all of the following properties:\n\npolygon rings are closed (the last point equals the first)\npolygon holes (inner rings) are inside their exterior ring\npolygon inner rings maximally touch the exterior ring in single points, not over a line\na polygon ring does not repeat its own path\nin a multi-polygon, an external ring maximally touches another exterior ring in single points, not over a line\n\nIf this is not the case, the geometry concerned is not valid. Invalid geometries typically cause errors when they are processed, but they can usually be repaired to make them valid.\nA further convention is that the outer ring of a polygon is winded counter-clockwise, while the holes are winded clockwise, but polygons for which this is not the case are still considered valid. For polygons on the sphere, the “clockwise” concept is not very useful: if for instance we take the equator as polygon, is the Northern Hemisphere or the Southern Hemisphere “inside”? The convention taken here is to consider the area on the left while traversing the polygon is considered the polygon’s inside (see also Section 7.3).\nZ and M coordinates\n \nIn addition to X and Y coordinates, single points (vertices) of simple feature geometries may have:\n\na Z coordinate, denoting altitude, and/or\nan M value, denoting some “measure”\n\nThe M attribute shall be a property of the vertex. It sounds attractive to encode a time stamp in it for instance to pack movement data (trajectories) in LINESTRINGs. These become however invalid (or “non-simple”) once the trajectory self-intersects, which happens when only X and Y are considered for self-intersections.\nBoth Z and M are not found often, and software support to do something useful with them is (still) rare. Their WKT representations are fairly easily understood:\n\nCodest_point(c(1,3,2))\n\n# POINT Z (1 3 2)\n\nCodest_point(c(1,3,2), dim = \"XYM\")\n\n# POINT M (1 3 2)\n\nCodest_linestring(rbind(c(3,1,2,4), c(4,4,2,2)))\n\n# LINESTRING ZM (3 1 2 4, 4 4 2 2)\n\n\nEmpty geometries\n \nA very important concept in the feature geometry framework is that of the empty geometry. Empty geometries arise naturally when we do geometrical operations (Section 3.2), for instance when we want to know the intersection of POINT (0 0) and POINT (1 1):\n\nCode(e &lt;- st_intersection(st_point(c(0,0)), st_point(c(1,1))))\n# GEOMETRYCOLLECTION EMPTY\n\n\nand it represents essentially the empty set: when combining (unioning) an empty point with other non-empty geometries, it vanishes.\nAll geometry types have a special value representing the empty (typed) geometry, like\n\nCodest_point()\n\n# POINT EMPTY\n\nCodest_linestring(matrix(1,1,3)[0,], dim = \"XYM\")\n\n# LINESTRING M EMPTY\n\n\nand so on, but they all point to the empty set, differing only in their dimension (Section 3.2.2).\nTen further geometry types\nThere are 10 more geometry types which are more rare, but increasingly find implementation:\n\n\n\n\n\n\ntype\ndescription\n\n\n\nCIRCULARSTRING\nThe CircularString is the basic curve type, similar to a LineString in the linear world. A single segment requires three points, the start and end points (first and third) and any other point on the arc. The exception to this is for a closed circle, where the start and end points are the same. In this case the second point MUST be the centre of the arc, i.e., the opposite side of the circle. To chain arcs together, the last point of the previous arc becomes the first point of the next arc, just like in LineString. This means that a valid circular string must have an odd number of points greater than 1.\n\n\nCOMPOUNDCURVE\nA CompoundCurve is a single, continuous curve that has both curved (circular) segments and linear segments. That means that in addition to having well-formed components, the end point of every component (except the last) must be coincident with the start point of the following component.\n\n\nCURVEPOLYGON\nExample compound curve in a curve polygon: CURVEPOLYGON( COMPOUNDCURVE( CIRCULARSTRING(0 0,2 0, 2 1, 2 3, 4 3),(4 3, 4 5, 1 4, 0 0)), CIRCULARSTRING(1.7 1, 1.4 0.4, 1.6 0.4, 1.6 0.5, 1.7 1))\n\n\n\nMULTICURVE\nA MultiCurve is a 1 dimensional GeometryCollection whose elements are Curves. It can include linear strings, circular strings, or compound strings.\n\n\nMULTISURFACE\nA MultiSurface is a 2 dimensional GeometryCollection whose elements are Surfaces, all using coordinates from the same coordinate reference system.\n\n\nCURVE\nA Curve is a 1 dimensional geometric object usually stored as a sequence of Points, with the subtype of Curve specifying the form of the interpolation between Points\n\n\nSURFACE\nA Surface is a 2 dimensional geometric object\n\n\nPOLYHEDRALSURFACE\nA PolyhedralSurface is a contiguous collection of polygons, which share common boundary segments\n\n\nTIN\nA TIN (triangulated irregular network) is a PolyhedralSurface consisting only of Triangle patches.\n\n\nTRIANGLE\nA Triangle is a polygon with three distinct, non-collinear vertices and no interior boundary\n\n\n\nCIRCULARSTRING, COMPOUNDCURVE and CURVEPOLYGON are not described in the SFA standard, but in the SQL-MM part 3 standard. The descriptions above were copied from the PostGIS manual.\n \nText and binary encodings\n \nPart of the simple feature standard are two encodings: a text and a binary encoding. The well-known text encoding, used above, is human-readable. The well-known binary encoding is machine-readable. Well-known binary (WKB) encodings are lossless and typically faster to work with than text encoding (and decoding), and they are used for instance in all communications between R package sf and the GDAL, GEOS, liblwgeom, and s2geometry libraries (Figure 1.7).",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geometries</span>"
    ]
  },
  {
    "objectID": "03-Geometries.html#sec-opgeom",
    "href": "03-Geometries.html#sec-opgeom",
    "title": "3  Geometries",
    "section": "\n3.2 Operations on geometries",
    "text": "3.2 Operations on geometries\n\nSimple feature geometries can be queried for properties, or transformed or combined into new geometries, and combinations of geometries can be queried for further properties. This section gives an overview of the operations entirely focusing on geometrical properties. Chapter 5 focuses on the analysis of non-geometrical feature properties, in relationship to their geometries. Some of the material in this section appeared in Pebesma (2018).\nWe can categorise operations on geometries in terms of what they take as input, and what they return as output. In terms of output we have operations that return:\n\n\npredicates: a logical asserting a certain property is TRUE\n\n\nmeasures: a quantity (a numeric value, possibly with measurement unit)\n\ntransformations: newly generated geometries\n\nand in terms of what they operate on, we distinguish operations that are:\n\n\nunary when they work on a single geometry\n\nbinary when they work on pairs of geometries\n\nn-ary when they work on sets of geometries\n\nUnary predicates\n \nUnary predicates describe a certain property of a geometry. The predicates is_simple, is_valid, and is_empty return respectively whether a geometry is simple, valid, or empty. Given a coordinate reference system, is_longlat returns whether the coordinates are geographic or projected. is(geometry, class) checks whether a geometry belongs to a particular class.\n \nBinary predicates and DE-9IM\n \nThe Dimensionally Extended Nine-Intersection Model (DE-9IM, Clementini, Di Felice, and Oosterom 1993; Egenhofer and Franzosa 1991) is a model that describes the qualitative relation between any two geometries in two-dimensional space (\\(R^2\\)). Any geometry has a dimension value that is:\n\n0 for points,\n1 for linear geometries,\n2 for polygonal geometries, and\nF (false) for empty geometries\n\nAny geometry also has an inside (I), a boundary (B), and an exterior (E); these roles are obvious for polygons, however, for:\n\n\nlines the boundary is formed by the end points, and the inside by all non-end points on the line\n\npoints have a zero-dimensional inside but no boundary\n\n\nCodelibrary(sf)\npolygon &lt;- po &lt;- st_polygon(list(rbind(c(0,0), c(1,0), c(1,1), c(0,1), c(0,0))))\np0 &lt;- st_polygon(list(rbind(c(-1,-1), c(2,-1), c(2,2), c(-1,2), c(-1,-1))))\nline &lt;- li &lt;- st_linestring(rbind(c(.5, -.5), c(.5, 0.5)))\ns &lt;- st_sfc(po, li)\n\npar(mfrow = c(3,3))\npar(mar = c(1,1,1,1))\n\n# \"1020F1102\"\n# 1: 1\nplot(s, col = c(NA, 'darkgreen'), border = 'blue', main = expression(paste(\"I(pol)\",intersect(),\"I(line) = 1\")))\nlines(rbind(c(.5,0), c(.5,.495)), col = 'red', lwd = 2)\npoints(0.5, 0.5, pch = 1)\n\n# 2: 0\nplot(s, col = c(NA, 'darkgreen'), border = 'blue', main = expression(paste(\"I(pol)\",intersect(),\"B(line) = 0\")))\npoints(0.5, 0.5, col = 'red', pch = 16)\n\n# 3: 2\nplot(s, col = c(NA, 'darkgreen'), border = 'blue', main = expression(paste(\"I(pol)\",intersect(),\"E(line) = 2\")))\nplot(po, col = '#ff8888', add = TRUE)\nplot(s, col = c(NA, 'darkgreen'), border = 'blue', add = TRUE)\n\n# 4: 0\nplot(s, col = c(NA, 'darkgreen'), border = 'blue', main = expression(paste(\"B(pol)\",intersect(),\"I(line) = 0\")))\npoints(.5, 0, col = 'red', pch = 16)\n\n# 5: F\nplot(s, col = c(NA, 'darkgreen'), border = 'blue', main = expression(paste(\"B(pol)\",intersect(),\"B(line) = F\")))\n\n# 6: 1\nplot(s, col = c(NA, 'darkgreen'), border = 'blue', main = expression(paste(\"B(pol)\",intersect(),\"E(line) = 1\")))\nplot(po, border = 'red', col = NA, add = TRUE, lwd = 2)\n\n# 7: 1\nplot(s, col = c(NA, 'darkgreen'), border = 'blue', main = expression(paste(\"E(pol)\",intersect(),\"I(line) = 1\")))\nlines(rbind(c(.5, -.5), c(.5, 0)), col = 'red', lwd = 2)\n\n# 8: 0\nplot(s, col = c(NA, 'darkgreen'), border = 'blue', main = expression(paste(\"E(pol)\",intersect(),\"B(line) = 0\")))\npoints(.5, -.5, col = 'red', pch = 16)\n\n# 9: 2\nplot(s, col = c(NA, 'darkgreen'), border = 'blue', main = expression(paste(\"E(pol)\",intersect(),\"E(line) = 2\")))\nplot(p0 / po, col = '#ff8888', add = TRUE)\nplot(s, col = c(NA, 'darkgreen'), border = 'blue', add = TRUE)\n\n\n\n\n\n\nFigure 3.2: DE-9IM: intersections between the interior, boundary, and exterior of a polygon (rows) and of a linestring (columns) indicated by red\n\n\n\n\nFigure 3.2 shows the intersections between the I, B, and E components of a polygon and a linestring indicated by red; the sub-plot title gives the dimension of these intersections (0, 1, 2 or F). The relationship between the polygon and the line geometry is the concatenation of these dimensions:\n\nCodest_relate(polygon, line)\n\n#      [,1]       \n# [1,] \"1020F1102\"\n\n\nwhere the first three characters are associated with the inside of the first geometry (polygon): Figure 3.2 is summarised row-wise. Using this ability to express relationships, we can also query pairs of geometries about particular conditions expressed in a mask string. As an example, the string \"*0*******\" would evaluate TRUE when the second geometry has one or more boundary points in common with the interior of the first geometry; the symbol * standing for “any dimensionality” (0, 1, 2 or F). The mask string \"T********\" matches pairs of geometry with intersecting interiors, where the symbol T stands for any non-empty intersection of dimensionality 0, 1, or 2.\nBinary predicates are further described using normal-language verbs, using DE-9IM definitions. For instance, the predicate equals corresponds to the relationship \"T*F**FFF*\". If any two geometries obey this relationship, they are (topologically) equal, but may have a different ordering of nodes.\nA list of binary predicates, with their meaning for non-empty geometries:\n\n\n\n\n\n\n\npredicate\nmeaning of A predicate B\ninverse of\n\n\n\ncontains\nB has no points in the exterior of A and the insides of A and B have at least one point in common\nwithin\n\n\ncontains_properly\nA contains B and B has no points in common with the boundary of A\n\n\n\ncovers\nB has no points in the exterior of A\ncovered_by\n\n\ncovered_by\nInverse of covers\n\ncovers\n\n\ncrosses\nA and B have some but not all interior points in common\n\n\n\ndisjoint\nA and B have no points in common\nintersects\n\n\nequals\nA and B are topologically equal: node order or number of nodes may differ; identical to A contains B and A within B\n\n\n\nequals_exact\nA equal B and A and B have identical node order\n\n\n\nintersects\nA and B are not disjoint\ndisjoint\n\n\nis_within_distance\nthe shortest distance from A to B is within a given distance\n\n\n\nwithin\nA has no points in the exterior of B, and the insides of A and B have at least one point in common\ncontains\n\n\ntouches\nA and B have at least one boundary point but no interior points in common\n\n\n\noverlaps\nA and B have the same dimension and some but not all points in common; the dimension of the common points is identical to that of A and B\n\n\n\nrelate\nGiven a mask string, return whether A relate B adheres to its pattern\n\n\n\n\n \nThe Wikipedia DE-9IM page provides the relate patterns for each of these verbs. They are important to check out; for instance covers and contains (and their inverses) are often not completely intuitive:\n\nif A contains B, B has no points in common with the exterior or boundary of A\nif A covers B, B has no points in common with the exterior of A\n\nThis implies for instance that a polygon covers its own boundary, but does not contain it.\nUnary measures\n\nUnary measures return a measure or quantity that describes a property of the geometry:\n\n\n\n\n\n\nmeasure\nreturns\n\n\n\ndimension\n0 for points, 1 for linear, 2 for polygons, possibly NA for empty geometries\n\n\narea\nthe area of a geometry\n\n\nlength\nthe length of a linear geometry\n\n\n\n \nBinary measures\n\ndistance returns the distance between pairs of geometries. The qualitative measure relate (without mask) gives the relation pattern. A description of the geometrical relationship between two geometries is given in Section 3.2.2.\n \nUnary transformers\n\nUnary transformations work on a per-geometry basis, and return for each geometry a new geometry.\n\n\n\n\n\n\ntransformer\nreturns a geometry …\n\n\n\ncentroid\nof type POINT with the geometry’s centroid\n\n\nbuffer\nthat is larger (or smaller) than the input geometry, depending on the buffer size\n\n\njitter\nthat was moved in space a certain amount, using a bivariate uniform distribution\n\n\nwrap_dateline\ncut into pieces that no longer cover or cross the dateline\n\n\nboundary\nwith the boundary of the input geometry\n\n\nconvex_hull\nthat forms the convex hull of the input geometry (Figure 3.3)\n\n\nline_merge\nafter merging connecting LINESTRING elements of a MULTILINESTRING into longer LINESTRINGs.\n\n\nmake_valid\nthat is valid\n\n\nnode\nwith added nodes to linear geometries at intersections without a node; only works on individual linear geometries\n\n\npoint_on_surface\nwith a (arbitrary) point on a surface\n\n\npolygonize\nof type polygon, created from lines that form a closed ring\n\n\nsegmentize\na (linear) geometry with nodes at a given density or minimal distance\n\n\nsimplify\nsimplified by removing vertices/nodes (lines or polygons)\n\n\nsplit\nthat has been split with a splitting linestring\n\n\ntransform\ntransformed or convert to a new coordinate reference system (Chapter 2)\n\n\ntriangulate\nwith Delauney triangulated polygon(s) (Figure 3.3)\n\n\nvoronoi\nwith the Voronoi tessellation of an input geometry (Figure 3.3)\n\n\nzm\nwith removed or added Z and/or M coordinates\n\n\ncollection_extract\nwith sub-geometries from a GEOMETRYCOLLECTION of a particular type\n\n\ncast\nthat is converted to another type\n\n\n+\nthat is shifted over a given vector\n\n\n*\nthat is multiplied by a scalar or matrix\n\n\n\n\nCodepar(mar = rep(0,4), mfrow = c(1, 3))\nset.seed(133331)\nmp &lt;- st_multipoint(matrix(runif(20), 10))\nplot(mp, cex = 2)\nplot(st_convex_hull(mp), add = TRUE, col = NA, border = 'red')\nbox()\nplot(mp, cex = 2)\nplot(st_voronoi(mp), add = TRUE, col = NA, border = 'red')\nbox()\nplot(mp, cex = 2)\nplot(st_triangulate(mp), add = TRUE, col = NA, border = 'darkgreen')\nbox()\n\n\n\n\n\n\nFigure 3.3: For a set of points, left: convex hull (red); middle: Voronoi polygons; right: Delauney triangulation\n\n\n\n\n \nBinary transformers\n\nBinary transformers are functions that return a geometry based on operating on a pair of geometries. They include:\n\n\n\n\n\n\n\nfunction\nreturns\ninfix operator\n\n\n\nintersection\nthe overlapping geometries for pair of geometries\n&\n\n\nunion\nthe combination of the geometries; removes internal boundaries and duplicate points, nodes or line pieces\n|\n\n\ndifference\nthe geometries of the first after removing the overlap with the second geometry\n/\n\n\nsym_difference\nthe combinations of the geometries after removing where they intersect; the negation (opposite) of intersection\n\n%/%\n\n\n\n \nN-ary transformers\n\nN-ary transformers operate on sets of geometries. union can be applied to a set of geometries to return its geometrical union. Otherwise, any set of geometries can be combined into a MULTI-type geometry when they have equal dimension, or else into a GEOMETRYCOLLECTION. Without unioning, this may lead to a geometry that is not valid, for instance when two polygon rings have a boundary line in common.\n \nN-ary intersection and difference take a single argument but operate (sequentially) on all pairs, triples, quadruples, etc. Consider the plot in Figure 3.4: how do we identify the area where all three boxes overlap? Using binary intersections gives us intersections for all pairs: 1-1, 1-2, 1-3, 2-1, 2-2, 2-3, 3-1, 3-2, 3-3, but that does not let us identify areas where more than two geometries intersect. Figure 3.4 (right) shows the n-ary intersection: the seven unique, non-overlapping geometries originating from intersection of one, two, or more geometries.\n\nCodepar(mar = rep(.1, 4), mfrow = c(1, 2))\nsq &lt;- function(pt, sz = 1) st_polygon(list(rbind(c(pt - sz), \n  c(pt[1] + sz, pt[2] - sz), c(pt + sz), c(pt[1] - sz, pt[2] + sz), c(pt - sz))))\nx &lt;- st_sf(box = 1:3, st_sfc(sq(c(0, 0)), sq(c(1.7, -0.5)), sq(c(0.5, 1))))\nplot(st_geometry(x), col = NA, border = sf.colors(3, categorical = TRUE), lwd = 3)\nplot(st_intersection(st_geometry(x)), col = sf.colors(7, categorical=TRUE, alpha = .5))\n\n\n\n\n\n\nFigure 3.4: Left: three overlapping squares – how do we identify the small box where all three overlap? Right: unique, non-overlapping n-ary intersections\n\n\n\n\nSimilarly, one can compute an n-ary difference from a set \\(\\{s_1, s_2,\ns_3, ...\\}\\) by creating differences \\(\\{s_1, s_2-s_1, s_3-s_2-s_1,\n...\\}\\). This is shown in Figure 3.5, (left) for the original set, and (right) for the set after reversing its order to make clear that the result here depends on the ordering of the input geometries. Again, resulting geometries do not overlap.\n\nCodepar(mar = rep(.1, 4), mfrow = c(1, 2)) \nxg &lt;- st_geometry(x)\nplot(st_difference(xg), col = sf.colors(3, alpha = .5, categorical=TRUE))\nplot(st_difference(xg[3:1]), col = sf.colors(3, alpha = .5, categorical=TRUE))\n\n\n\n\n\n\nFigure 3.5: Difference between subsequent boxes, left: in original order; right: in reverse order",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geometries</span>"
    ]
  },
  {
    "objectID": "03-Geometries.html#sec-precision",
    "href": "03-Geometries.html#sec-precision",
    "title": "3  Geometries",
    "section": "\n3.3 Precision",
    "text": "3.3 Precision\n \nGeometrical operations, such as finding out whether a certain point is on a line, may fail when coordinates are represented by double precision floating point numbers, such as 8-byte doubles used in R. An often chosen remedy is to limit the precision of the coordinates before the operation. For this, a precision model is adopted; the most common is to choose a factor \\(p\\) and compute rounded coordinates \\(c'\\) from original coordinates \\(c\\) by \\[c' = \\mbox{round}(p \\cdot c) / p\\]\nRounding of this kind brings the coordinates to points on a regular grid with spacing \\(1/p\\), which is beneficial for geometric computations. Of course, it also affects all computations like areas and distances, and may turn valid geometries into invalid ones. Which precision values are best for which application is often a matter of common sense combined with trial and error.",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geometries</span>"
    ]
  },
  {
    "objectID": "03-Geometries.html#sec-coverages",
    "href": "03-Geometries.html#sec-coverages",
    "title": "3  Geometries",
    "section": "\n3.4 Coverages: tessellations and rasters",
    "text": "3.4 Coverages: tessellations and rasters\n\nThe Open Geospatial Consortium defines a coverage as a “feature that acts as a function to return values from its range for any direct position within its spatiotemporal domain” (Baumann, Hirschorn, and Masó 2017). Having a function implies that for every space time “point”, every combination of a spatial point and a moment in time of the spatiotemporal domain, we have a single value for the range. This is a very common situation for spatiotemporal phenomena, a few examples can be given:\n\nboundary disputes aside, at a given time every point in a region (domain) belongs to a single administrative unit (range)\nat any given moment in time, every point in a region (domain) has a certain land cover type (range)\nevery point in an area (domain) has a single surface elevation (range), which could be measured with respect to a given mean sea level surface\nevery spatiotemporal point in a three-dimensional body of air (domain) has single value for temperature (range)\n\nA caveat here is that because observation or measurement always takes time and requires space, measured values are always an average over a spatiotemporal volume, and hence range variables can rarely be measured for true, zero-volume “points”; for many practical cases however the measured volume is small enough to be considered a “point”. For a variable like land cover type the volume needs to be chosen such that the types distinguished make sense with respect to the measured areal units.\nIn the first two of the given examples the range variable is categorical, in the last two the range variable is continuous. For categorical range variables, if large connected areas have a constant range value, an efficient way to represent these data is by storing the boundaries of the areas with constant value, such as country boundaries. Although this can be done (and is often done) by a set of simple feature geometries (polygons or multi-polygons), this brings along some challenges:\n\nit is hard to guarantee for such a set of simple feature polygons that they do not overlap, or that there are no unwanted gaps between them\nsimple features have no way of assigning points on the boundary of two adjacent polygons uniquely to a single polygon, which conflicts with the interpretation as coverage\n\nTopological models\n\nA data model that guarantees no inadvertent gaps or overlaps of polygonal coverages is the topological model, examples of which are found in geographic information systems (GIS) like GRASS GIS or ArcGIS. Topological models store boundaries between polygons only once and register which polygonal area is on either side of a boundary.\nDeriving the set of (multi)polygons for each area with a constant range value from a topological model is straightforward; the other way around, reconstructing topology from a set of polygons typically involves setting thresholds on errors and handling gaps or overlaps.\nRaster tessellations\n \nA tessellation is a sub-division of a space (area, volume) into smaller elements by ways of polygons. A regular tessellation does this with regular polygons: triangles, squares, or hexagons. Tessellations using squares are commonly used for spatial data and are called raster data. Raster data tessellate each spatial dimension \\(d\\) into regular cells, formed by left-closed and right-open intervals \\(d_i\\): \\[\\begin{equation}\nd_i = d_0 + [i \\times \\delta, (i+1) \\times \\delta)\n\\end{equation}\\] with \\(d_0\\) an offset, \\(\\delta\\) the interval (cell or pixel) size, and where the cell index \\(i\\) is an arbitrary but consecutive set of integers. The \\(\\delta\\) value is often taken negative for the \\(y\\)-axis (Northing), indicating that raster row numbers increasing Southwards correspond to \\(y\\)-coordinates increasing Northwards.\nWhereas in arbitrary polygon tessellations the assignment of points to polygons is ambiguous for points falling on a boundary shared by two polygons, using left-closed “[” and right-open “)” intervals in regular tessellations removes this ambiguity. This means that for rasters with negative \\(\\delta\\) values for the \\(y\\)-coordinate and positive for the \\(x\\)-coordinate, only the top-left corner point is part of each raster cell. An artifact resulting from this is shown in Figure 3.6.\n\nCodelibrary(stars) |&gt; suppressPackageStartupMessages()\npar(mar = rep(1, 4))\nls &lt;- st_sf(a = 2, st_sfc(st_linestring(rbind(c(0.1, 0), c(1, .9)))))\ngrd &lt;- st_as_stars(st_bbox(ls), nx = 10, ny = 10, xlim = c(0, 1.0), ylim = c(0, 1),\n   values = -1)\nr &lt;- st_rasterize(ls, grd, options = \"ALL_TOUCHED=TRUE\")\nr[r == -1] &lt;- NA\nplot(st_geometry(st_as_sf(grd)), border = 'orange', col = NA, \n     reset = FALSE, key.pos = NULL)\nplot(r, axes = FALSE, add = TRUE, breaks = \"equal\", main = NA) # ALL_TOUCHED=FALSE;\nplot(ls, add = TRUE, col = \"red\", lwd = 2)\n\n\n\n\n\n\nFigure 3.6: Rasterization artifact: as only top-left corners are part of the raster cell, only cells touching the red line below the diagonal line are rasterized\n\n\n\n\n\nTessellating the time dimension with left-closed right-open intervals is very common, and it reflects the implicit assumption underlying time series software such as the xts package in R, where time stamps indicate the start of time intervals. Different models can be combined: one could use simple feature polygons to tessellate space and combine this with a regular tessellation of time in order to cover a space time vector data cube. Raster and vector data cubes are discussed in Chapter 6.\nAs mentioned above, besides square cells the other two shapes that can lead to regular tessellations of \\(R^2\\) are triangles and hexagons. On the sphere, there are a few more, including cube, octahedron, icosahedron, and dodecahedron. A spatial index that builds on the cube is s2geometry, the H3 library uses the icosahedron and densifies that with (mostly) hexagons. Mosaics that cover the entire Earth are also called discrete global grids.",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geometries</span>"
    ]
  },
  {
    "objectID": "03-Geometries.html#networks",
    "href": "03-Geometries.html#networks",
    "title": "3  Geometries",
    "section": "\n3.5 Networks",
    "text": "3.5 Networks\n\nSpatial networks are typically composed of linear (LINESTRING) elements, but possess further topological properties describing the network coherence:\n\nstart- and end-points of a linestring may be connected to other linestring start or end points, forming a set of nodes and edges\nedges may be directed, to only allow for connection (flow, transport) in one way\n\nR packages including osmar (Schlesinger and Eugster 2013), stplanr (Lovelace, Ellison, and Morgan 2022), and sfnetworks (van der Meer et al. 2022) provide functionality for constructing network objects, and working with them, including computation of shortest or fastest routes through a network. Package spatstat (Baddeley, Turner, and Rubak 2022; Baddeley, Rubak, and Turner 2015) has infrastructure for analysing point patterns on linear networks (Chapter 11). Chapter 12 of Lovelace, Nowosad, and Muenchow (2019) has a transportation application using networks.",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geometries</span>"
    ]
  },
  {
    "objectID": "03-Geometries.html#exercises",
    "href": "03-Geometries.html#exercises",
    "title": "3  Geometries",
    "section": "\n3.6 Exercises",
    "text": "3.6 Exercises\nFor the following exercises, use R where possible.\n\nGive two examples of geometries in 2-D (flat) space that cannot be represented as simple feature geometries, and create a plot of them.\nRecompute the coordinates 10.542, 0.01, 45321.6789 using precision values 1, 1e3, 1e6, and 1e-2.\nDescribe a practical problem for which an n-ary intersection would be needed.\nHow can you create a Voronoi diagram (Figure 3.3) that has one closed polygons for every single point?\nGive the unary measure dimension for geometries POINT Z (0 1 1), LINESTRING Z (0 0 1,1 1 2), and POLYGON Z ((0 0 0,1 0 0,1 1 0,0 0 0))\n\nGive the DE-9IM relation between LINESTRING(0 0,1 0) and LINESTRING(0.5 0,0.5 1); explain the individual characters.\nCan a set of simple feature polygons form a coverage? If so, under which constraints?\nFor the nc counties in the dataset that comes with R package sf, find the points touched by four counties.\nHow would Figure 3.6 look like if \\(\\delta\\) for the \\(y\\)-coordinate was positive?\n\n\n\n\n\n\n\nBaddeley, Adrian, Ege Rubak, and Rolf Turner. 2015. Spatial Point Patterns: Methodology and Applications with R. Chapman & Hall/CRC.\n\n\nBaddeley, Adrian, Rolf Turner, and Ege Rubak. 2022. Spatstat: Spatial Point Pattern Analysis, Model-Fitting, Simulation, Tests. http://spatstat.org/.\n\n\nBaumann, Peter, Eric Hirschorn, and Joan Masó. 2017. “OGC Coverage Implementation Schema.” OGC Implementation Standard. https://docs.opengeospatial.org/is/09-146r6/09-146r6.html.\n\n\nClementini, Eliseo, Paolino Di Felice, and Peter van Oosterom. 1993. “A Small Set of Formal Topological Relationships Suitable for End-User Interaction.” In Advances in Spatial Databases, edited by David Abel and Beng Chin Ooi, 277–95. Berlin, Heidelberg: Springer Berlin Heidelberg.\n\n\nEgenhofer, Max J., and Robert D. Franzosa. 1991. “Point-Set Topological Spatial Relations.” International Journal of Geographical Information Systems 5 (2): 161–74. https://doi.org/10.1080/02693799108927841.\n\n\nHerring, J. R. 2010. “OpenGIS Implementation Standard for Geographic Information-Simple Feature Access-Part 2: SQL Option.” Open Geospatial Consortium Inc. http://portal.opengeospatial.org/files/?artifact_id=25354.\n\n\n———. 2011. “OpenGIS Implementation Standard for Geographic Information-Simple Feature Access-Part 1: Common Architecture.” Open Geospatial Consortium Inc, 111. http://portal.opengeospatial.org/files/?artifact_id=25355.\n\n\nISO. 2004. Geographic Information – Simple Feature Access – Part 1: Common Architecture.https://www.iso.org/standard/40114.html .\n\n\nLovelace, Robin, Richard Ellison, and Malcolm Morgan. 2022. Stplanr: Sustainable Transport Planning. https://CRAN.R-project.org/package=stplanr.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019. Geocomputation with R. Chapman & Hall/CRC.https://geocompr.robinlovelace.net/ .\n\n\nPebesma, Edzer. 2018. “Simple Features for R: Standardized Support for Spatial Vector Data.” The R Journal 10 (1): 439–46. https://doi.org/10.32614/RJ-2018-009.\n\n\nSchlesinger, Thomas, and Manuel J. A. Eugster. 2013. Osmar: OpenStreetMap and r. http://osmar.r-forge.r-project.org/.\n\n\nvan der Meer, Lucas, Lorena Abad, Andrea Gilardi, and Robin Lovelace. 2022. Sfnetworks: Tidy Geospatial Networks. https://CRAN.R-project.org/package=sfnetworks.",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geometries</span>"
    ]
  },
  {
    "objectID": "04-Spherical.html",
    "href": "04-Spherical.html",
    "title": "4  Spherical Geometries",
    "section": "",
    "text": "4.1 Straight lines\n“There are too many false conclusions drawn and stupid measurements made when geographic software, built for projected Cartesian coordinates in a local setting, is applied at the global scale” (Chrisman 2012)\nThe previous chapter discussed geometries defined on the plane, \\(R^2\\). This chapter discusses what changes when we consider geometries not on the plane, but on the sphere (\\(S^2\\)).\nAlthough we learned in Chapter 2 that the shape of the Earth is usually approximated by an ellipsoid, none of the libraries shown in green in Figure 1.7 provide access to a comprehensive set of functions that compute on an ellipsoid. Only the s2geometry (Dunnington, Pebesma, and Rubak 2023; Veach et al. 2020) library does provide it using a sphere rather than an ellipsoid. However, when compared to using a flat (projected) space as we did in the previous chapter, a sphere is a much better approximation to an ellipsoid.\nThe basic premise of simple features of Chapter 3 is that geometries are represented by sequences of points connected by straight lines. On \\(R^2\\) (or any Cartesian space), this is trivial, but on a sphere straight lines do not exist. The shortest line connecting two points is an arc of the circle through both points and the centre of the sphere, also called a great circle segment. A consequence is that “the” shortest distance line connecting two points on opposing sides of the sphere does not exist, as any great circle segment connecting them has equal length. Note that the GeoJSON standard (Butler et al. 2016) has its own definition of straight lines in geodetic coordinates (see Exercise 1 at the end of this chapter).",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spherical Geometries</span>"
    ]
  },
  {
    "objectID": "04-Spherical.html#ring-direction-and-full-polygon",
    "href": "04-Spherical.html#ring-direction-and-full-polygon",
    "title": "4  Spherical Geometries",
    "section": "\n4.2 Ring direction and full polygon",
    "text": "4.2 Ring direction and full polygon\nAny polygon on the sphere divides the sphere surface in two parts with finite area: the inside and the outside. Using the “counter-clockwise rule” as was done for \\(R^2\\) will not work because the direction interpretation depends on what is defined as inside. A convention here is to define the inside as the left (or right) side of the polygon boundary when traversing its points in sequence. Reversal of the node order then switches inside and outside.\n \nIn addition to empty polygons, one can define the full polygon on a sphere, which comprises its entire surface. This is useful, for instance for computing the oceans as the geometric difference between the full polygon and the union of the land mass (see Figure 8.1 and Figure 11.6).",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spherical Geometries</span>"
    ]
  },
  {
    "objectID": "04-Spherical.html#bounding-box-rectangle-and-cap",
    "href": "04-Spherical.html#bounding-box-rectangle-and-cap",
    "title": "4  Spherical Geometries",
    "section": "\n4.3 Bounding box, rectangle, and cap",
    "text": "4.3 Bounding box, rectangle, and cap\n \nWhere in \\(R^2\\) one can easily define bounding boxes as the range of the \\(x\\) and \\(y\\) coordinates, for ellipsoidal coordinates these ranges are not of much use when geometries cross the antimeridian (longitude +/- 180) or one of the poles. The assumption in \\(R^2\\) that lower \\(x\\) values are Westwards of higher ones does not hold when crossing the antimeridian. An alternative to delineating an area on a sphere that is more natural is the bounding cap, defined by its centre coordinates and a radius. For Antarctica, as depicted in Figures 4.1 (a) and (c), the bounding box formed by coordinate ranges is\n\nCodelibrary(sf) |&gt; suppressPackageStartupMessages()\nlibrary(maps) |&gt; suppressPackageStartupMessages()\nlibrary(dplyr) |&gt; suppressPackageStartupMessages()\nmap(fill = TRUE, plot = FALSE) |&gt;\n  st_as_sf() |&gt;\n  filter(ID == \"Antarctica\") -&gt; a\nst_bbox(a)\n\n#   xmin   ymin   xmax   ymax \n# -180.0  -85.2  179.6  -60.5\n\n\nwhich clearly does not contain the region (ymin being -90 and xmax 180). Two geometries that do contain the region are the bounding cap:\n\nCodelibrary(s2)\ns2_bounds_cap(a)\n\n#   lng lat angle\n# 1   0 -90  29.5\n\n\nand the bounding rectangle:\n\nCodes2_bounds_rect(a)\n\n#   lng_lo lat_lo lng_hi lat_hi\n# 1   -180    -90    180  -60.5\n\n\nFor an area spanning the antimeridian, here the Fiji island country, the bounding box:\n\nCodemap(fill = TRUE, plot = FALSE) |&gt;\n  st_as_sf() |&gt;\n  filter(ID == \"Fiji\") -&gt; Fiji\nst_bbox(Fiji)\n\n#   xmin   ymin   xmax   ymax \n# -179.9  -21.7  180.2  -12.5\n\n\nseems to span most of the Earth, as opposed to the bounding rectangle:\n\nCodes2_bounds_rect(Fiji)\n\n#   lng_lo lat_lo lng_hi lat_hi\n# 1    175  -21.7   -178  -12.5\n\n\nwhere a value lng_lo larger than lng_hi indicates that the bounding rectangle spans the antimeridian. This property could not be inferred from the coordinate ranges.",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spherical Geometries</span>"
    ]
  },
  {
    "objectID": "04-Spherical.html#validity-on-the-sphere",
    "href": "04-Spherical.html#validity-on-the-sphere",
    "title": "4  Spherical Geometries",
    "section": "\n4.4 Validity on the sphere",
    "text": "4.4 Validity on the sphere\n \nMany global datasets are given in ellipsoidal coordinates but are prepared in a way that they “work” when interpreted on the \\(R^2\\) space [-180,180] \\(\\times\\) [-90,90]. This means that:\n\ngeometries crossing the antimeridian (longitude +/- 180) are cut in half, such that they no longer cross it (but nearly touch each other)\ngeometries including a pole, like Antarctica, are cut at +/- 180 and make an excursion through -180,-90 and 180,-90 (both representing the Geographic South Pole)\n\nFigure 4.1 shows two different representations of Antarctica, plotted with ellipsoidal coordinates taken as \\(R^2\\) (top) and in a Polar Stereographic projection (bottom), without (left) and with (right) an excursion through the Geographic South Pole. In the projections as plotted, polygons (b) and (c) are valid, polygon (a) is not valid as it self-intersects, and polygon (d) is not valid because it traverses the same edge to the South Pole twice. On the sphere (\\(S^2\\)), polygon (a) is valid but (b) is not, for the same reason as (d) is not valid.\n\nCode# maps:\npar(mfrow = c(2,2))\npar(mar = c(1,1.2,1,1))\nm &lt;- st_as_sf(map(fill=TRUE, plot=FALSE))\nm &lt;- m[m$ID == \"Antarctica\", ]\nplot(st_geometry(m), asp = 2)\ntitle(\"a (not valid)\")\n# ne:\nlibrary(rnaturalearth)\nne &lt;- ne_countries(returnclass = \"sf\")\nne &lt;- ne[ne$region_un == \"Antarctica\", \"region_un\"]\nplot(st_geometry(ne), asp = 2)\ntitle(\"b (valid)\")\n# 3031\nm |&gt;\n  st_geometry() |&gt;\n  st_transform(3031) |&gt;\n  plot()\ntitle(\"c (valid)\")\nne |&gt;\n  st_geometry() |&gt;\n  st_transform(3031) |&gt;\n  plot()\ntitle(\"d (not valid)\")\n\n\n\n\n\n\nFigure 4.1: Antarctica polygon, (a, c): not passing through POINT(-180 -90); (b, d): passing through POINT(-180 -90) and POINT(180 -90)",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spherical Geometries</span>"
    ]
  },
  {
    "objectID": "04-Spherical.html#exercises",
    "href": "04-Spherical.html#exercises",
    "title": "4  Spherical Geometries",
    "section": "\n4.5 Exercises",
    "text": "4.5 Exercises\nFor the following exercises, use R where possible or relevant.\n\nHow does the GeoJSON format (Butler et al. 2016) define “straight” lines between ellipsoidal coordinates (Section 3.1.1)? Using this definition of straight, how does LINESTRING(0 85,180 85) look like in an Arctic polar projection? How could this geometry be modified to have it cross the North Pole?\nFor a typical polygon on \\(S^2\\), how can you find out ring direction?\nAre there advantages of using bounding caps over using bounding boxes? If so, list them.\nWhy is, for small areas, the orthographic projection centred at the area a good approximation of the geometry as handled on \\(S^2\\)?\nFor rnaturalearth::ne_countries(country = \"Fiji\", returnclass = \"sf\"), check whether the geometry is valid on \\(R^2\\), on an orthographic projection centred on the country, and on \\(S^2\\). How can the geometry be made valid on \\(S^2\\)? Plot the resulting geometry back on \\(R^2\\). Compare the centroid of the country, as computed on \\(R^2\\) and on \\(S^2\\), and the distance between the two.\nConsider dataset gisco_countries in R package giscoR, and select the country with NAME_ENGL == \"Fiji\". Does it have a valid geometry on the sphere? If so, how was this accomplished?\n\n\n\n\n\n\n\nButler, H., M. Daly, A. Doyl, S. Gillies, S. Hagen, and T. Schaub. 2016. “The GeoJSON Format.” Vol. Request for Comments: 7946. Internet Engineering Task Force (IETF). https://tools.ietf.org/html/rfc7946.\n\n\nChrisman, Nicholas. 2012. “A Deflationary Approach to Fundamental Principles in GIScience.” In Francis Harvey (Ed.) Are There Fundamental Principles in Geographic Information Science?, 42–64. CreateSpace, United States.\n\n\nDunnington, Dewey, Edzer Pebesma, and Ege Rubak. 2023. S2: Spherical Geometry Operators Using the S2 Geometry Library. https://CRAN.R-project.org/package=s2.\n\n\nVeach, Eric, Jesse Rosenstock, Eric Engle, Robert Snedegar, Julien Basch, and Tom Manshreck. 2020. “S2 Geometry.” Website. https://s2geometry.io/.",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spherical Geometries</span>"
    ]
  },
  {
    "objectID": "05-Attributes.html",
    "href": "05-Attributes.html",
    "title": "5  Attributes and Support",
    "section": "",
    "text": "5.1 Attribute-geometry relationships and support\nFeature attributes refer to the properties of features (“things”) that do not describe the feature’s geometry. Feature attributes can be derived from geometry (such as length of a LINESTRING or area of a POLYGON), but they can also refer to non-derived properties, such as:\nIn some cases, time properties can be seen as attributes of features, for instance the date of birth of a person or the construction year of a road. When an attribute such as for instance air quality is a function of both space and time, time is best handled on equal footing with geometry (often in a data cube, see Chapter 6).\nSpatial data science software implementing simple features typically organises data in tables that contain both geometries and attributes for features; this is true for geopandas in Python, PostGIS tables in PostgreSQL, and sf objects in R. The geometric operations described in Section 3.2 operate on geometries only, and may occasionally yield new attributes (predicates, measures, or transformations), but they do not operate on attributes present.\nWhen, while manipulating geometries, attribute values are retained unmodified, support problems may arise. If we look into a simple case of replacing a county polygon with the centroid of that polygon on a dataset that has attributes, we see that R package sf issues a warning:\nThe reason for this is that the dataset contains variables with values that are associated with entire polygons – in this case population counts – meaning they are not associated with a POINT geometry replacing the polygon.\nIn Section 1.6 we already described that for non-point geometries (lines, polygons), feature attribute values either have point support, meaning that the value applies to every point, or they have block support, meaning that the value summarises all points in the geometry. (Other options, non-point support smaller than the geometry, or support larger than the associated geometry, may also occur.) This chapter will describe different ways in which an attribute may relate to the geometry, its consequences on analysing such data, and ways to derive attribute data for different geometries (up- and downscaling).\nChanging the feature geometry without changing the feature attributes does change the feature, since the feature is characterised by the combination of geometry and attributes. Can we, ahead of time, predict whether the resulting feature will still meaningfully relate to the attribute value when we replace all geometries for instance with their convex hull or centroid? It depends.\nTake the example of a road, represented by a LINESTRING, which has an attribute property road width equal to 10 m. What can we say about the road width of an arbitrary sub-section of this road? That depends on whether the attribute road length describes, for instance the road width everywhere, meaning that road width is constant along the road, or whether it describes an aggregate property, such as minimum or average road width. In case of the minimum, for an arbitrary subsection of the road one could still argue that the minimum road width must be at least as large as the minimum road width for the whole segment, but it may no longer be the minimum for that subsection. This gives us two “types” for the attribute-geometry relationship (AGR):\nFor polygon data, typical examples of constant AGR (point support) variables are:\nA typical property of such variables is that they have geometries that are not man-made and also not associated with a sensor device (such as remote sensing image pixel boundaries). Instead, the geometry follows from mapping the variable observed.\nExamples for the aggregate AGR (block support) variables are:\nA typical property of such variables is that associated geometries come for instance from legislation, observation devices or analysis choices, but not intrinsically from the observed variable.\nA third type of AGR arises when an attribute identifies a feature geometry; we call an attribute an identity variable when the associated geometry uniquely identifies the variable’s value (there are no other geometries with the same value). An example is county name: the name identifies the county, and is still the county for any sub-area (point support), but for arbitrary sub-areas, the attributes loses the identity property to become a constant attribute. An example is:\nThe challenge here is that spatial information (ignoring time for simplicity) belongs to different phenomena types (Scheider et al. 2016), including:\nbut that different spatial geometry types (points, lines, polygons, raster cells) have no simple mapping to these phenomena types:\nProperly specifying attribute-geometry relationships, and warning against their absence or cases when change in geometry (change of support) implies a change of information can help to avoid a large class of common spatial data analysis mistakes (Stasch et al. 2014) associated with the support of spatial data.",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Attributes and Support</span>"
    ]
  },
  {
    "objectID": "05-Attributes.html#sec-agr",
    "href": "05-Attributes.html#sec-agr",
    "title": "5  Attributes and Support",
    "section": "",
    "text": "constant the attribute value is valid everywhere in or over the geometry; we can think of the feature as consisting of an infinite number of points that all have this attribute value; in geostatistical terminology this is known as a variable with point support\n\n\naggregate the attribute is an aggregate, a summary value over the geometry; we can think of the feature as a single observation with a value that is associated with the entire geometry; this is also known as a variable having block support\n\n\n\n\n\nland use for a land use polygon\nrock units or geologic strata in a geological map\nsoil type in a soil map\nelevation class in an elevation map that shows elevation as classes\nclimate zone in a climate zone map\n\n\n\n\npopulation, either as number of persons or as population density\nother socio-economic variables, summarised by area\naverage reflectance over a remote sensing pixel\ntotal emission of pollutants by region\nblock mean NO\\(_2\\) concentrations, such as obtained by block kriging over square blocks (Section 12.5) or by a dispersion model that predicts areal means\n\n\n\n\n\nan arbitrary point (or region) inside a county is still part of the county and must have the same value for county name, but it no longer identifies the (entire) geometry corresponding to that county\n\n\n\n\nfields: where over continuous space, every location corresponds to a single value, examples including elevation, air quality, or land use\n\nobjects: found at a discrete set of locations, such as houses, trees, or persons\n\naggregates: values arising as spatial sums, totals, or averages of fields, counts or densities of objects, associated with lines or regions\n\n\n\n\npoints may refer to sample locations of observations on fields (air quality) or to locations of objects\nlines may be used for objects (roads, rivers), contours of a field, or administrative borders\nraster pixels and polygons may reflect fields of a categorical variable such as land use (coverage), but also aggregates such as population density\nraster or other mesh triangulations may have different variables associated with nodes (points), edges (lines), or faces (areas, cells), for instance when partial differential equations are approximated using staggered grids (Haltiner and Williams 1980; Collins et al. 2013)",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Attributes and Support</span>"
    ]
  },
  {
    "objectID": "05-Attributes.html#aggregating-and-summarising",
    "href": "05-Attributes.html#aggregating-and-summarising",
    "title": "5  Attributes and Support",
    "section": "\n5.2 Aggregating and summarising",
    "text": "5.2 Aggregating and summarising\nAggregating records in a table (or data.frame) involves two steps:\n\ngrouping records based on a grouping predicate, and\napplying an aggregation function to the attribute values of a group to summarise them into a single number.\n\nIn SQL, this looks for instance like\nSELECT GroupID, SUM(population) FROM table GROUP BY GroupID;\nindicating the aggregation function (SUM) and the grouping predicate (GroupID).\nR package dplyr for instance uses two steps to accomplish this: function group_by specifies the group membership of records, summarise computes data summaries (such as sum or mean) for each of the groups. The (base) R method aggregate combines both in a single function call that takes the table, the grouping predicate(s), and the aggregation function(s) as arguments.\nAn example for the North Carolina counties is shown in Figure 5.1. Here, we grouped counties by their position (according to the quadrant in which the county centroid is with respect to ellipsoidal coordinate POINT(-79, 35.5)) and summed the number of disease cases per group. The result shows that the geometries of the resulting groups have been unioned (Section 3.2.6): this is necessary because the MULTIPOLYGON formed by just putting all the county geometries together would have many duplicate boundaries, and hence would not be valid (Section 3.1.2).\n\nCodenc &lt;- read_sf(system.file(\"gpkg/nc.gpkg\", package = \"sf\"))\n# encode quadrant by two logicals:\nnc$lng &lt;- st_coordinates(st_centroid(st_geometry(nc)))[,1] &gt; -79\nnc$lat &lt;- st_coordinates(st_centroid(st_geometry(nc)))[,2] &gt; 35.5\nnc.grp &lt;- aggregate(nc[\"SID74\"], list(nc$lng, nc$lat), sum)\nnc.grp[\"SID74\"] |&gt; st_transform('EPSG:32119') |&gt;\n  plot(graticule = TRUE, axes = TRUE)\n\n\n\n\n\n\nFigure 5.1: SID74 total incidences aggregated to four areas\n\n\n\n\nPlotting collated county polygons is technically not a problem, but for this case would raise the wrong suggestion that the group sums relate to individual counties, rather than to the grouped counties.\nOne particular property of aggregation in this way is that each record is assigned to a single group; this has the advantage that the sum of the group-wise sums equals the sum of the un-grouped data: for variables that reflect amount, nothing gets lost and nothing is added. The newly formed geometry is the result of unioning the geometries of the contributing records.\n\nCodenc &lt;- st_transform(nc, 2264)\ngr &lt;- st_sf(\n   label = apply(expand.grid(1:10, LETTERS[10:1])[,2:1], 1, paste0, collapse = \" \"),\n   geom = st_make_grid(nc))\nplot(st_geometry(nc), reset = FALSE, border = 'grey')\nplot(st_geometry(gr), add = TRUE)\n\n\n\n\n\n\nFigure 5.2: Example target blocks plotted over North Carolina counties\n\n\n\n\nWhen we need an aggregate for a new area that is not a union of the geometries for a group of records, and we use a spatial predicate, then single records may be matched to multiple groups. When taking the rectangles of Figure 5.2 as the target areas, and summing for each rectangle the disease cases of the counties that intersect with the rectangles of Figure 5.2, the sum of these will be much larger:\n\nCodea &lt;- aggregate(nc[\"SID74\"], gr, sum)\nc(sid74_sum_counties = sum(nc$SID74),\n  sid74_sum_rectangles = sum(a$SID74, na.rm = TRUE))\n\n#   sid74_sum_counties sid74_sum_rectangles \n#                  667                 2621\n\n\nChoosing another predicate, such as contains or covers, would on the contrary result in much smaller values, because many counties are not contained by any of the target geometries. However, there are a few cases where this approach might be good or satisfactory:\n\nwhen we want to aggregate POINT geometries by a set of polygons, and all points are contained by a single polygon. If points fall on a shared boundary than they are assigned to both polygons (this is the case for DE-9IM-based GEOS library; the s2geometry library has the option to define polygons as “semi-open”, which implies that points are assigned to maximally one polygon when polygons do not overlap)\nwhen aggregating many very small polygons or raster pixels over larger areas, for instance averaging altitude from a 30 m resolution raster over North Carolina counties, the error made by multiple matches may be insignificant\nwhen the many-to-many match is reduced to the single largest area match, as shown in Figure 7.4\n\n\nA more comprehensive approach to aggregating spatial data associated with areas to larger, arbitrary shaped areas is by using area-weighted interpolation.",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Attributes and Support</span>"
    ]
  },
  {
    "objectID": "05-Attributes.html#sec-area-weighted",
    "href": "05-Attributes.html#sec-area-weighted",
    "title": "5  Attributes and Support",
    "section": "\n5.3 Area-weighted interpolation",
    "text": "5.3 Area-weighted interpolation\n \nWhen we want to combine geometries and attributes of two datasets such that we get attribute values of a source dataset summarised for the geometries of a target, where source and target geometries are unrelated, area-weighted interpolation may be a simple approach. In effect, it considers the area of overlap of the source and target geometries, and uses that to weigh the source attribute values into the target value (Goodchild and Lam 1980; Do, Thomas-Agnan, and Vanhems 2015a, 2015b; Do, Laurent, and Vanhems 2021). This method is also known as conservative region aggregation or regridding (Jones 1999). Here, we follow the notation of Do, Thomas-Agnan, and Vanhems (2015b).\nArea-weighted interpolation computes, for each of \\(q\\) spatial target areas \\(T_j\\), a weighted average from the values \\(Y_i\\) corresponding to the \\(p\\) spatial source areas \\(S_i\\),\n\\[\n\\hat{Y}_j(T_j) = \\sum_{i=1}^p w_{ij} Y_i(S_i)\n\\tag{5.1}\\]\nwhere the \\(w_{ij}\\) depend on the amount of overlap of \\(T_j\\) and \\(S_i\\), and the amount of overlap is \\(A_{ij} = T_j \\cap S_i\\). How \\(w_{ij}\\) depends on \\(A_{ij}\\) is discussed below.\nDifferent options exist for choosing weights, including methods using external variables (including dasymetric mapping, Mennis 2003). Two simple approaches for computing weights that do not use external variables arise, depending on whether the variable \\(Y\\) is intensive or extensive.\nSpatially extensive and intensive variables\n \nExtensive variables correspond to amounts, associated with a physical size (length, area, volume, counts of items). An example of a extensive variable is population count. It is associated with an area, and if that area is cut into smaller areas, the population count needs to be split too. Because population is rarely uniform over space, this does not need to be done proportionally to the smaller areas but the sum of the population count for the smaller areas needs to equal that of the total. Intensive variables are variables that do not have values proportional to support: if the area is split, values may vary but on average remain the same. An example of a related variable that is intensive is population density. If an area is split into smaller areas, population density is not split similarly: the sum of population densities for the smaller areas is a meaningless measure, as opposed to the average of the population densities which will be similar to the density of the total area.\nWhen we assume that the extensive variable \\(Y\\) is uniformly distributed over space, the value \\(Y_{ij}\\), derived from \\(Y_i\\) for a sub-area of \\(S_i\\), \\(A_{ij} = T_j \\cap S_i\\) of \\(S_i\\) is\n\\[\\hat{Y}_{ij}(A_{ij}) = \\frac{|A_{ij}|}{|S_i|} Y_i(S_i)\\]\nwhere \\(|\\cdot|\\) denotes the spatial area. For estimating \\(Y_j(T_j)\\) we sum all the elements over area \\(T_j\\):\n\\[\n\\hat{Y}_j(T_j) = \\sum_{i=1}^p \\frac{|A_{ij}|}{|S_i|} Y_i(S_i)\n\\tag{5.2}\\]\nFor an intensive variable, under the assumption that the variable has a constant value over each area \\(S_i\\), the estimate for a sub-area equals that of the total area,\n\\[\\hat{Y}_{ij} = Y_i(S_i)\\]\nand we can estimate the value of \\(Y\\) for a new spatial unit \\(T_j\\) by an area-weighted average of the source values:\n\\[\n\\hat{Y}_j(T_j) = \\sum_{i=1}^p \\frac{|A_{ij}|}{|T_j|} Y_i(S_i)\n\\tag{5.3}\\]\nDasymetric mapping\n\nDasymetric mapping distributes variables, such as population, known at a coarse spatial aggregation level over finer spatial units by using other variables that are associated with population distribution, such as land use, building density, or road density. The simplest approach to dasymetric mapping is obtained for extensive variables, where the ratio \\(|A_{ij}| / |S_i|\\) in Equation 5.2 is replaced by the ratio of another extensive variable \\(X_{ij}(S_{ij})/X_i(S_i)\\), which has to be known for both the intersecting regions \\(S_{ij}\\) and the source regions \\(S_i\\). Do, Thomas-Agnan, and Vanhems (2015b) discuss several alternatives for intensive \\(Y\\) and/or \\(X\\), and cases where \\(X\\) is known for other areas.\nSupport in file formats\n\nGDAL’s vector API supports reading and writing so-called field domains, which can have a “split policy” and a “merge policy” indicating what should be done with attribute variables when geometries are split or merged. The values of these can be “duplicate” for split and “geometry weighted” for merge, in case of spatially intensive variables, or they can be “geometry ratio” for split and “sum” for merge, in case of spatially extensive variables. At the time of writing this, the file formats supporting this are GeoPackage and FileGDB.",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Attributes and Support</span>"
    ]
  },
  {
    "objectID": "05-Attributes.html#sec-updownscaling",
    "href": "05-Attributes.html#sec-updownscaling",
    "title": "5  Attributes and Support",
    "section": "\n5.4 Up- and Downscaling",
    "text": "5.4 Up- and Downscaling\n \nUp- and downscaling refers in general to obtaining high-resolution information from low-resolution data (downscaling) or obtaining low-resolution information from high-resolution data (upscaling). Both activities involve the attributes’ relation to geometries and both change support. They are synonymous with aggregation (upscaling) and disaggregation (downscaling). The simplest form of downscaling is sampling (or extracting) polygon, line or grid cell values at point locations. This works well for variables with point-support (“constant” AGR), but is at best approximate when the values are aggregates. Challenging applications for downscaling include high-resolution prediction of variables obtained by low-resolution weather prediction models or climate change models, and the high-resolution prediction of satellite image derived variables based on the fusion of sensors with different spatial and temporal resolutions.\nThe application of areal interpolation using (Equation 5.1) with its realisations for extensive (Equation 5.2) and intensive (Equation 5.3) variables allows moving information from any source area \\(S_i\\) to any target area \\(T_j\\) as long as the two areas have some overlap. This means that one can go arbitrarily to much larger units (aggregation) or to much smaller units (disaggregation). Of course this makes only sense to the extent that the assumptions hold: over the source regions extensive variables need to be uniformly distributed and intensive variables need to have a constant value.\nThe ultimate disaggregation involves retrieving (extracting) point values from line or area data. For this, we cannot work with equations 5.2 or 5.3 because \\(|A_{ij}| = 0\\) for points, but under the assumption of having a constant value over the geometry, for intensive variables the value \\(Y_i(S_i)\\) can be assigned to points as long as all points can be uniquely assigned to a single source area \\(S_i\\). For polygon data, this implies that \\(Y\\) needs to be a coverage variable (Section 3.4). For extensive variables, extracting a value at a point is rather meaningless, as it should always give a zero value.\nIn cases where values associated with areas are aggregate values over the area, the assumptions made by area-weighted interpolation or dasymetric mapping – uniformity or constant values over the source areas – are highly unrealistic. In such cases, these simple approaches could still be reasonable approximations, for instance when:\n\nthe source and target area are nearly identical\nthe variability inside source units is very small, and the variable is nearly uniform or constant\n\n\nIn other cases, results obtained using these methods are merely consequences of unjustified assumptions. Statistical aggregation methods that can estimate quantities for larger regions from points or smaller regions include:\n\ndesign-based methods, which require that a probability sample is available from the target region, with known inclusion probabilities (sec-design?), and\nmodel-based methods, which assume a random field model with spatially correlated values (block kriging, Section 12.5)\n\n\nAlternative disaggregation methods include:\n\ndeterministic, smoothing-based approaches such as kernel- or spline-based smoothing methods (Tobler 1979; Martin 1989)\n\nstatistical, model-based approaches such as area-to-area and area-to-point kriging (Kyriakidis 2004; Raim et al. 2021).",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Attributes and Support</span>"
    ]
  },
  {
    "objectID": "05-Attributes.html#exercises",
    "href": "05-Attributes.html#exercises",
    "title": "5  Attributes and Support",
    "section": "\n5.5 Exercises",
    "text": "5.5 Exercises\nWhere relevant, try to make the following exercises with R.\n\nWhen we add a variable to the nc dataset by nc$State = \"North Carolina\" (i.e., all counties get assigned the same state name). Which value would you attach to this variable for the attribute-geometry relationship (agr)?\nCreate a new sf object from the geometry obtained by st_union(nc), and assign \"North Carolina\" to the variable State. Which agr can you now assign to this attribute variable?\nUse st_area to add a variable with name area to nc. Compare the area and AREA variables in the nc dataset. What are the units of AREA? Are the two linearly related? If there are discrepancies, what could be the cause?\nIs the area variable intensive or extensive? Is its agr equal to constant, identity or aggregate?\nConsider Figure 5.3: using the equations in Section 5.3.1, compute the area-weighted interpolations for (a) the dashed cell and (b) for the square enclosing all four solid cells, first for the case where the four cells represent (i) an extensive variable, and (ii) an intensive variable. The red numbers are the data values of the source areas.\n\n\nCodeg &lt;- st_make_grid(st_bbox(st_as_sfc(\"LINESTRING(0 0,1 1)\")), n = c(2,2))\npar(mar = rep(0,4))\nplot(g)\nplot(g[1] * diag(c(3/4, 1)) + c(0.25, 0.125), add = TRUE, lty = 2)\ntext(c(.2, .8, .2, .8), c(.2, .2, .8, .8), c(1,2,4,8), col = 'red')\n\n\n\n\n\n\nFigure 5.3: Example data for area-weighted interpolation\n\n\n\n\n\n\n\n\n\n\n\nBrus, Dick J. 2021. “Statistical Approaches for Spatial Sample Survey: Persistent Misconceptions and New Developments.” European Journal of Soil Science 72 (2): 686–703. https://doi.org/10.1111/ejss.12988.\n\n\nCollins, Sarah N., Robert S. James, Pallav Ray, Katherine Chen, Angie Lassman, and James Brownlee. 2013. “Grids in Numerical Weather and Climate Models.” In Climate Change and Regional/Local Responses, edited by Yuanzhi Zhang and Pallav Ray. Rijeka: IntechOpen. https://doi.org/10.5772/55922.\n\n\nDo, Van Huyen, Thibault Laurent, and Anne Vanhems. 2021. “Guidelines on Areal Interpolation Methods.” In Advances in Contemporary Statistics and Econometrics: Festschrift in Honor of Christine Thomas-Agnan, edited by Abdelaati Daouia and Anne Ruiz-Gazen, 385–407. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-73249-3_20.\n\n\nDo, Van Huyen, Christine Thomas-Agnan, and Anne Vanhems. 2015a. “Accuracy of Areal Interpolation Methods for Count Data.” Spatial Statistics 14: 412–38. https://doi.org/10.1016/j.spasta.2015.07.005.\n\n\n———. 2015b. “Spatial Reallocation of Areal Data: A Review.” Rev. Econ. Rég. Urbaine 1/2: 27–58. https://www.tse-fr.eu/sites/default/files/medias/doc/wp/mad/wp_tse_397_v2.pdf.\n\n\nGoodchild, Michael F, and Nina Siu Ngan Lam. 1980. Areal Interpolation: A Variant of the Traditional Spatial Problem. Department of Geography, University of Western Ontario London, ON, Canada.\n\n\nHaltiner, G. J., and R. T. Williams. 1980. Numerical Prediction and Dynamic Meteorology. New York: John Wiley; Sons.\n\n\nJones, Philip W. 1999. “First- and Second-Order Conservative Remapping Schemes for Grids in Spherical Coordinates.” Mon. Wea. Rev. 127: 2204–10. https://doi.org/ 10.1175/1520-0493(1999) .\n\n\nKyriakidis, P. C. 2004. “A Geostatistical Framework for Areal-to-Point Spatial Interpolation.” Geographical Analysis 36: 259–89.\n\n\nMartin, D. 1989. “Mapping Population Data from Zone Centroid Locations.” Transactions of the Institute of British Geographers, New Series 14: 90–97.\n\n\nMennis, Jeremy. 2003. “Generating Surface Models of Population Using Dasymetric Mapping.” The Professional Geographer 55 (1): 31–42.\n\n\nRaim, A. M., S. H. Holan, J. R. Bradley, and C. K. Wikle. 2021. “Spatio-Temporal Change of Support Modeling with r.” Computational Statistics 36: 749–80. https://doi.org/ 10.1007/s00180-020-01029-4 .\n\n\nScheider, Simon, Benedikt Gräler, Edzer Pebesma, and Christoph Stasch. 2016. “Modeling Spatiotemporal Information Generation.” International Journal of Geographical Information Science 30 (10): 1980–2008. https://doi.org/10.1080/13658816.2016.1151520.\n\n\nStasch, Christoph, Simon Scheider, Edzer Pebesma, and Werner Kuhn. 2014. “Meaningful Spatial Prediction and Aggregation.” Environmental Modelling & Software 51: 149–65. https://doi.org/10.1016/j.envsoft.2013.09.006.\n\n\nTobler, W. R. 1979. “Smooth Pycnophylactic Interpolation for Geographical Regions.” Journal of the American Statistical Association 74: 519–30.",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Attributes and Support</span>"
    ]
  },
  {
    "objectID": "06-Cubes.html",
    "href": "06-Cubes.html",
    "title": "6  Data Cubes",
    "section": "",
    "text": "6.1 A four-dimensional data cube\nData cubes arise naturally when we observe properties of a set of geometries repeatedly over time. Time information may sometimes be considered as an attribute of a feature, such as when we register the year of construction of a building or the date of birth of a person (Chapter 5). In other cases it may refer to the time of observing an attribute, or the time for which a prediction of an attribute has been made. In these cases, time is on equal footing with space, and time and space together describe the physical dimensions over which we observe, model, predict, or make forecasts for.\nOne way of considering our world is that of a four-dimensional space, with three space dimensions and one time dimension. In that view, events become “things” or “objects” that have as duration their size on the time dimension (Galton 2004). Although such a view does not align well with how we experience and describe the world, from a data analytical perspective, four numbers, along with their reference systems, suffice to describe space and time coordinates of an observation associated with a point location and time instance.\nWe define data cubes as array data with one or more array dimensions associated with space and/or time (Lu, Appel, and Pebesma 2018). This implies that raster data, features with attributes, and time series data are all special cases of data cubes. Since we do not restrict to three-dimensional structures, we actually mean hypercubes rather than cubes, and as the cube extent of the different dimensions does not have to be identical, or have comparable units, the better term would be hyper-rectangle. For simplicity, we talk about data cubes instead.\nA canonical form of a data cube is shown in Figure 6.1: it shows in a perspective plot a set of raster layers for the same region that were collected (observed, or modelled) at different time steps. The three cube dimensions longitude, latitude, and time, are thought of as being orthogonal. Arbitrary two-dimensional cube slices are obtained by fixing one of the dimensions at a particular value, one-dimensional slices are obtained by fixing two of the dimensions at a particular value, and a scalar is obtained by fixing three dimensions at a particular value.\nCode# (C) 2021, Jonathan Bahlmann, CC-BY-SA\n# https://github.com/Open-EO/openeo.org/tree/master/documentation/1.0/datacubes/.scripts\n# based on work by Edzer Pebesma, 2019, here: https://gist.github.com/edzer/5f1b0faa3e93073784e01d5a4bb60eca\n\n# plotting runs via a dummy stars object with x, y dimensions (no bands)\n# to not be overly dependent on an input image, time steps and bands\n# are displayed by replacing the matrix contained in the dummy stars object\n# every time something is plotted\n\n# packages, read input ----\nset.seed(1331)\nlibrary(stars)\nlibrary(colorspace) |&gt; suppressPackageStartupMessages()\nlibrary(scales) |&gt; suppressPackageStartupMessages()\n\n# make color palettes ----\nblues &lt;- sequential_hcl(n = 20, h1 = 211, c1 = 80, l1 = 40, l2 = 100, p1 = 2)\ngreens &lt;- sequential_hcl(n = 20, h1 = 134, c1 = 80, l1 = 40, l2 = 100, p1 = 2)\nreds &lt;- sequential_hcl(n = 20, h1 = 360, c1 = 80, l1 = 40, l2 = 100, p1 = 2)\npurples &lt;- sequential_hcl(n = 20, h1 = 299, c1 = 80, l1 = 40, l2 = 100, p1 = 2)\ngreys &lt;- sequential_hcl(n = 20, h1 = 0, c1 = 0, l1 = 40, l2 = 100, p1 = 2)\n\n# matrices from raster ----\n# make input matrices from an actual raster image\ninput &lt;- read_stars(\"data/iceland_delta_cutout_2.tif\") # this raster needs approx 6x7 format\n# if the input raster is changed, every image where a pixel value is written as text needs to be checked and corrected accordingly\ninput &lt;- input[,,,1:4]\nwarped &lt;- st_warp(input, crs = st_crs(input), cellsize = 200) # warp to approx. 6x7 pixel\n\n# these are only needed for resampling\nwarped_highres &lt;- st_warp(warped, crs = st_crs(warped), cellsize = 100) # with different input, cellsize must be adapted\n# this is a bit of a trick, because 3:4 is different format than 6:7\n# when downsampling, the raster of origin isn't so important anyway\nwarped_lowres &lt;- st_warp(warped_highres[,1:11,,], crs = st_crs(warped), cellsize = 390)\n# plot(warped_lowres)\n# image(warped[,,,1], text_values = TRUE)\n\nt1 &lt;- floor(matrix(runif(42, -30, 150), ncol = 7)) # create timesteps 2 and 3 randomly\nt2 &lt;- floor(matrix(runif(42, -250, 50), ncol = 7))\n\n# create dummy stars object ----\nmake_dummy_stars &lt;- function(x, y, d1, d2, aff) {\n  m = warped_highres[[1]][1:x,1:y,1] # underlying raster doesn't matter because it's just dummy construct\n  dim(m) = c(x = x, y = y) # named dim\n  dummy = st_as_stars(m)\n  attr(dummy, \"dimensions\")[[1]]$delta = d1\n  attr(dummy, \"dimensions\")[[2]]$delta = d2\n  attr(attr(dummy, \"dimensions\"), \"raster\")$affine = c(aff, 0.0)\n  return(dummy)\n}\n\ns &lt;- make_dummy_stars(6, 7, 2.5, -.5714286, -1.14) # mainly used, perspective\nf &lt;- make_dummy_stars(6, 7, 1, 1, 0) # flat\nhighres &lt;- make_dummy_stars(12, 14, 1.25, -.2857143, -.57) # for resampling\nlowres &lt;- make_dummy_stars(3, 4, 5, -1, -2) # for resampling\n\n# matrices from image ----\nmake_matrix &lt;- function(image, band, n = 42, ncol = 7, t = 0) {\n  # this is based on an input image with &gt;= 4 input bands\n  # n is meant to cut off NAs, ncol is y, t is random matrix for time difference\n  return(matrix(image[,,,band][[1]][1:n], ncol = ncol) - t)\n  # before: b3 &lt;- matrix(warped[,,,1][[1]][1:42], ncol = 7) - t2\n}\n\n# now use function: \nb1 &lt;- make_matrix(warped, 1)\nb2 &lt;- make_matrix(warped, 1, t = t1)\nb3 &lt;- make_matrix(warped, 1, t = t2)\ng1 &lt;- make_matrix(warped, 2)\ng2 &lt;- make_matrix(warped, 2, t = t1)\ng3 &lt;- make_matrix(warped, 2, t = t2)\nr1 &lt;- make_matrix(warped, 3)\nr2 &lt;- make_matrix(warped, 3, t = t1)\nr3 &lt;- make_matrix(warped, 3, t = t2)\nn1 &lt;- make_matrix(warped, 4)\nn2 &lt;- make_matrix(warped, 4, t = t1)\nn3 &lt;- make_matrix(warped, 4, t = t2)\n\n# plot functions ----\nplt &lt;- function(x, yoffset = 0, add, li = TRUE, pal, print_geom = TRUE, border = .75, breaks = \"equal\") {\n  # pal is color palette\n  attr(x, \"dimensions\")[[2]]$offset = attr(x, \"dimensions\")[[2]]$offset + yoffset \n  l = st_as_sf(x, as_points = FALSE)\n  if (li)\n    pal &lt;- lighten(pal, 0.2) # + rnorm(1, 0, 0.1))\n  if (! add)\n    plot(l, axes = FALSE, breaks = breaks, pal = pal, reset = FALSE, border = grey(border), key.pos = NULL, main = NULL, xlab = \"time\")\n  else\n    plot(l, axes = TRUE, breaks = breaks, pal = pal, add = TRUE, border = grey(border))\n  u &lt;- st_union(l)\n  # print(u)\n  if(print_geom) {\n    plot(st_geometry(u), add = TRUE, col = NA, border = 'black', lwd = 2.5)\n  } else {\n    # not print geometry\n  }\n}\n\npl_stack &lt;- function(s, x, y, add = TRUE, nrM, imgY = 7, inner = 1) {\n  # nrM is the timestep {1, 2, 3}, cause this function\n  # prints all 4 bands at once\n  attr(s, \"dimensions\")[[1]]$offset = x\n  attr(s, \"dimensions\")[[2]]$offset = y\n  # m &lt;- r[[1]][y + 1:nrow,x + 1:ncol,1]\n  m &lt;- eval(parse(text=paste0(\"n\", nrM)))\n  s[[1]] &lt;- m[,c(imgY:1)] # turn around to have same orientation as flat plot\n  plt(s, 0, TRUE,  pal = purples)\n  m &lt;- eval(parse(text=paste0(\"r\", nrM)))\n  s[[1]] &lt;- m[,c(imgY:1)]\n  plt(s, 1*inner, TRUE,  pal = reds)\n  m &lt;- eval(parse(text=paste0(\"g\", nrM)))\n  s[[1]] &lt;- m[,c(imgY:1)]\n  plt(s, 2*inner, TRUE,  pal = greens)\n  m &lt;- eval(parse(text=paste0(\"b\", nrM)))\n  s[[1]] &lt;- m[,c(imgY:1)]\n  plt(s, 3*inner, TRUE, pal = blues) # li FALSE deleted\n}\n\n# flat plot function\n# prints any dummy stars with any single matrix to position\npl &lt;- function(s, x, y, add = TRUE, randomize = FALSE, pal, m, print_geom = TRUE, border = .75, breaks = \"equal\") {\n  # m is matrix to replace image with\n  # m &lt;- t(m)\n  attr(s, \"dimensions\")[[1]]$offset = x\n  attr(s, \"dimensions\")[[2]]$offset = y\n  # print(m)\n  s[[1]] &lt;- m\n  plt(s, 0, add = TRUE, pal = pal, print_geom = print_geom, border = border, breaks = breaks)\n  #plot(s, text_values = TRUE)\n}\n\nprint_segments &lt;- function(x, y, seg, by = 1, lwd = 4, col = \"black\") {\n  seg &lt;- seg * by\n  seg[,1] &lt;- seg[,1] + x\n  seg[,3] &lt;- seg[,3] + x\n  seg[,2] &lt;- seg[,2] + y\n  seg[,4] &lt;- seg[,4] + y\n  segments(seg[,1], seg[,2], seg[,3], seg[,4], lwd = lwd, col = col)\n}\n\n# time series ----\n\n# from: cube1_ts_6x7_bigger.png\noffset = 26\nplot.new()\n#par(mar = c(3, 2, 7, 2))\npar(mar = c(0, 0, 0, 0))\n#plot.window(xlim = c(10, 50), ylim = c(-3, 10), asp = 1)\nplot.window(xlim = c(-15, 75), ylim = c(-3, 10), asp = 1)\npl_stack(s, 0, 0, nrM = 3)\npl_stack(s, offset, 0, nrM = 2)\npl_stack(s, 2 * offset, 0, nrM = 1)\n# po &lt;- matrix(c(0,-8,7,0,15,3.5,  0,1,1,5,5,14), ncol = 2)\nheads &lt;- matrix(c(3.5, 3.5 + offset, 3.5 + 2*offset, 14,14,14), ncol = 2)\npoints(heads, pch = 16) # 4 or 16\nsegments(c(-8, 7, 0, 15), c(-1,-1,3,3), 3.5, 14) # first stack pyramid\nsegments(c(-8, 7, 0, 15) + offset, c(-1,-1,3,3), 3.5 + offset, 14) # second stack pyramid\nsegments(c(-8, 7, 0, 15) + 2*offset, c(-1,-1,3,3), 3.5 + 2*offset, 14) # third stack pyramid\narrows(-13, 14, 72, 14, angle = 20, lwd = 2)  # timeline\ntext(7.5, 3.8, \"x\", col = \"black\")\ntext(-10, -2.5, \"bands\", srt = 90, col = \"black\")\ntext(-4.5, 1.8, \"y\", srt = 27.5, col = \"black\")\ny &lt;- 15.8\ntext(69, y, \"time\", col = \"black\")\ntext(3.5, y, \"2020-10-01\", col = \"black\")\ntext(3.5 + offset, y, \"2020-10-13\", col = \"black\")\ntext(3.5 + 2*offset, y, \"2020-10-25\", col = \"black\")\n\n\n\n\n\n\nFigure 6.2: Four-dimensional raster data cube with dimensions x, y, bands, and time\nFigure 6.2 depicts a four-dimensional raster data cube (Appel and Pebesma 2019), where three-dimensional raster data cubes with a spectral dimension (“bands”) are organised along a fourth dimension, a time axis. Colour image data always has three bands (blue, green, red), and this example has a fourth band (near infrared, NIR), which is commonly found in spectral remote sensing data.\nFigure 6.3 shows exactly the same data, but layed out flat as a facet plot (or scatterplot matrix), where two dimensions (\\(x\\) and \\(y\\)) are aligned with (or nested within) the dimensions bands and time, respectively.\nCode# flat ----\nxlabels &lt;- seq(attr(warped, \"dimensions\")[[1]]$offset + attr(warped, \"dimensions\")[[1]]$delta / 2, length.out = attr(warped, \"dimensions\")[[1]]$to, by = attr(warped, \"dimensions\")[[1]]$delta)\nylabels &lt;- seq(attr(warped, \"dimensions\")[[2]]$offset + attr(warped, \"dimensions\")[[2]]$delta / 2, length.out = attr(warped, \"dimensions\")[[2]]$to, by = attr(warped, \"dimensions\")[[2]]$delta)\n\nprint_labels &lt;- function(x, y, off, lab, horizontal, cex = 1) {\n  if(horizontal) { # x\n    for(i in 0:(length(lab)-1)) {\n      text(x + i*off, y, lab[i+1], cex = cex, srt = 90)\n    }\n  } else { # y\n    lab &lt;- lab[length(lab):0]\n    for(i in 0:(length(lab)-1)) {\n      text(x, y + i*off, lab[i+1], cex = cex)\n    }\n  }\n}\n\n# before: width=1000, xlim(-2, 33), date labels x=31\nplot.new()\n# par(mar = c(0,0,0,0))\npar(mar = c(3,0,0,0))\nplot.window(xlim = c(-2, 40), ylim = c(0, 25), asp = 1)\npl(f, 7, 0, pal = blues, m = b1)\npl(f, 7, 10, pal = blues, m = b2)\npl(f, 7, 20, pal = blues, m = b3)\npl(f, 14, 0, pal = greens, m = g1)\npl(f, 14, 10, pal = greens, m = g2)\npl(f, 14, 20, pal = greens, m = g3)\npl(f, 21, 0, pal = reds, m = r1)\npl(f, 21, 10, pal = reds, m = r2)\npl(f, 21, 20, pal = reds, m = r3)\npl(f, 28, 0, pal = purples, m = n1)\npl(f, 28, 10, pal = purples, m = n2)\npl(f, 28, 20, pal = purples, m = n3)\nprint_labels(28.5, -2, 1, xlabels, horizontal = TRUE, cex = 0.7)\nprint_labels(36, 0.5, 1, ylabels, horizontal = FALSE, cex = 0.7)\n# arrows(6, 27, 6, 0, angle = 20, lwd = 2)\n# text(5, 14, \"time\", srt = 90, col = \"black\")\ntext(10, 28, \"blue\", col = \"black\")\ntext(17, 28, \"green\", col = \"black\")\ntext(24, 28, \"red\", col = \"black\")\ntext(31, 28, \"nir\", col = \"black\")\ntext(3, 23.5, \"2020-10-01\", col = \"black\")\ntext(3, 13.5, \"2020-10-13\", col = \"black\")\ntext(3, 3.5, \"2020-10-25\", col = \"black\")\n\n\n\n\n\n\nFigure 6.3: Four-dimensional raster data cube layed out flat over two dimensions",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Cubes</span>"
    ]
  },
  {
    "objectID": "06-Cubes.html#dimensions-attributes-and-support",
    "href": "06-Cubes.html#dimensions-attributes-and-support",
    "title": "6  Data Cubes",
    "section": "\n6.2 Dimensions, attributes, and support",
    "text": "6.2 Dimensions, attributes, and support\n \nPhenomena in space and time can be thought of as functions with domain space and time, and with range one or more observed attributes. For clearly identifiable discrete events or objects, the range is typically discrete, and precise delineation involves describing the precise coordinates where a thing starts or stops, which is best suited by vector geometries. For continuous phenomena, variables that take on a value everywhere such as air temperature or land use type, there are infinitely many values to represent and a common approach is to discretise space and time regularly over the spatiotemporal domain (extent) of interest. This leads to a number of familiar data structures:\n\ntime series, depicted as time lines for functions of time\nimage or raster data for two-dimensional spatial data\ntime sequences of images for dynamic spatial data\n\nThe third form of this, where a variable \\(Z\\) depends on \\(x\\), \\(y\\) and \\(t\\), as in\n\\[Z = f(x, y, t)\\]\nis the archetype of a spatiotemporal array or data cube: the shape of the volume where points regularly discretising the domain forms a cube. We call the variables that form the range (here: \\(x, y, t\\)) the cube dimensions. Data cubes may have multiple attributes, as in\n\\[\\{Z_1,Z_2,...,Z_p\\} = f(x, y, t)\\]\nand if \\(Z\\) is functional, for instance reflectance values measured over the electromagnetic spectrum, the spectral wavelengths \\(\\lambda\\) may form an additional dimension, as in \\(Z = f(x,y,t,\\lambda)\\). Section 6.5 discusses the alternative of representing colour bands as attributes.\nMultiple time dimensions arise for instance when making forecasts for different times in the future \\(t'\\) at different times \\(t\\), or when time is split over multiple dimensions (as in year, day-of-year, and hour-of-day). The most general definition of a data cube is a functional mapping from \\(n\\) dimensions to \\(p\\) attributes:\n\\[\\{Z_1,Z_2,...,Z_p\\} = f(D_1,D_2,...,D_n)\\]\nHere, we will consider any dataset with one or more space dimensions and zero or more time dimensions as data cubes. That includes:\n\nsimple features (Section 3.1)\ntime series for sets of features\nraster data\nmulti-spectral raster data (images)\ntime series of multi-spectral raster data (video)\n\n \nRegular dimensions, GDAL’s geotransform\n \nData cubes are usually stored in multi-dimensional arrays, and the usual relationship between 1-based array index \\(i\\) and an associated regularly discretised dimension variable \\(x\\) is\n\\[x = o_x + (i-1) d_x\\]\nwith \\(o_x\\) the origin, and \\(d_x\\) the grid spacing for this dimension.\nFor more general cases like those in Figures 1.6 b-c, the relation between \\(x\\) and \\(y\\) and array indexes \\(i\\) and \\(j\\) is\n\\[x = o_x + (i-1) d_x + (j-1) a_1\\]\n\\[y = o_y + (i-1) a_2 + (j-1) d_y\\]\nWith two affine parameters \\(a_1\\) and \\(a_2\\); this is the so-called geotransform as used in GDAL. When \\(a_1=a_2=0\\), this reduces to the regular raster of Figure 1.6 a with square cells if \\(d_x = d_y\\). For integer indexes, the coordinates are that of the starting edge of a grid cell, and the cell area (pixel) spans a range corresponding to index values ranging from \\(i\\) (inclusive) to \\(i+1\\) (exclusive). For most common imagery formats, \\(d_y\\) is negative, indicating that image row index increases with decreasing \\(y\\) values (southward). To get the \\(x\\)- and \\(y\\)-coordinate of the grid cell centre of the top left grid cell (in case of a negative \\(d_y\\)), we use \\(i=1.5\\) and \\(j=1.5\\).\nFor rectilinear rasters, a table that maps array index to dimension values is needed. NetCDF files for instance always store all values of spatial dimension (coordinate) variables that may correspond to the centre or offset of spatial grid cells, and in addition may store grid cell boundaries (to define rectilinear dimensions or to disambiguate the relation of the coordinate variable values to the cell boundaries).\n \nFor curvilinear rasters, an array that maps every combination of \\(i,j\\) into \\(x,y\\) pairs is needed, or a parametric function that does this (a projection or its inverse). NetCDF files often provide both, when available. GDAL calls such arrays geolocation arrays, and has extensive support for transformations involving them.\n \nSupport along cube dimensions\n \nSection 5.1 defined spatial support of an attribute variable as the size (length, area, volume) of a geometry associated with a particular observation or prediction. The same notion applies to temporal support. Although time is rarely reported by explicit time periods having a start- and end-time, in many cases either the time stamp implies a period (ISO-8601 uses “2021” for the full year, “2021-01” for the full month) or the time period is taken as the period from the time stamp of the current record up to but not including the time stamp of the next record.\nAn example is MODIS satellite imagery, where vegetation indexes (NDVI and EVI) are available as 16-day composites, meaning that over 16-day periods all available imagery is aggregated into a single image; such composites have temporal “block support”. Sentinel-2 or Landsat-8 data on the other hand are “snapshot” images and have temporal “point support”. When temporally aggregating data with temporal point support, for instance to monthly values, all images falling in the target time interval are selected. When aggregating temporal block support imagery such as the MODIS 16-day composite, one might weigh images according to the amount of overlap of the 16-day composite period and the target period, similar to area-weighted interpolation Section 5.3 but over the time dimension.",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Cubes</span>"
    ]
  },
  {
    "objectID": "06-Cubes.html#sec-dcoperations",
    "href": "06-Cubes.html#sec-dcoperations",
    "title": "6  Data Cubes",
    "section": "\n6.3 Operations on data cubes",
    "text": "6.3 Operations on data cubes\nSlicing a cube: filter\n\nData cubes can be sliced into sub-cubes by fixing a dimension at a particular value. Figure 6.4 shows the sub-cubes obtained by doing this with the temporal, spectral, or spatial dimension. In this figure, the spatial filtering does not happen by fixing a single spatial dimension at a particular value, but by selecting a particular sub-region, which is a more common operation. Fixing \\(x\\) or \\(y\\) would give a sub-cube along a transect of constant \\(x\\) or \\(y\\), which can be used to show a Hovmöller diagram, where an attribute is plotted (coloured) in the space of one space and one time dimension.\n\nCode# filter ----\n# mask &lt;- matrix(c(rep(NA, 26), 1,NA,1,NA,1,1,1, rep(NA, 9)), ncol = 7)\nmask &lt;- matrix(c(NA,NA,NA,NA,NA,NA,\n                 NA,NA,NA,NA,NA,NA,\n                 NA,NA,NA, 1, 1, 1,\n                 NA,NA, 1, 1, 1,NA,\n                 NA,NA,NA, 1, 1,NA,\n                 NA,NA,NA,NA,NA,NA,\n                 NA,NA,NA,NA,NA,NA), ncol = 7)\n\nprint_grid &lt;- function(x, y) {\n  pl(f, 0+x, 0+y, pal = blues, m = b1)\n  pl(f, 0+x, 10+y, pal = blues, m = b2)\n  pl(f, 0+x, 20+y, pal = blues, m = b3)\n  pl(f, 7+x, 0+y, pal = greens, m = g1)\n  pl(f, 7+x, 10+y, pal = greens, m = g2)\n  pl(f, 7+x, 20+y, pal = greens, m = g3)\n  pl(f, 14+x, 0+y, pal = reds, m = r1)\n  pl(f, 14+x, 10+y, pal = reds, m = r2)\n  pl(f, 14+x, 20+y, pal = reds, m = r3)\n  pl(f, 21+x, 0+y, pal = purples, m = n1)\n  pl(f, 21+x, 10+y, pal = purples, m = n2)\n  pl(f, 21+x, 20+y, pal = purples, m = n3)\n}\nprint_alpha_grid &lt;- function(x,y, alp = 0.2, geom = FALSE) {\n  pl(f, 0+x, 0+y, pal = alpha(blues, alp), print_geom = geom,  m = b1, border = 1)\n  pl(f, 0+x, 10+y, pal = alpha(blues, alp), print_geom = geom,  m = b2, border = 1)\n  pl(f, 0+x, 20+y, pal = alpha(blues, alp), print_geom = geom,  m = b3, border = 1)\n  pl(f, 7+x, 0+y, pal = alpha(greens, alp), print_geom = geom,  m = g1, border = 1)\n  pl(f, 7+x, 10+y, pal = alpha(greens, alp), print_geom = geom,  m = g2, border = 1)\n  pl(f, 7+x, 20+y, pal = alpha(greens, alp), print_geom = geom,  m = g3, border = 1)\n  pl(f, 14+x, 0+y, pal = alpha(reds, alp), print_geom = geom,  m = r1, border = 1)\n  pl(f, 14+x, 10+y, pal = alpha(reds, alp), print_geom = geom,  m = r2, border = 1)\n  pl(f, 14+x, 20+y, pal = alpha(reds, alp), print_geom = geom,  m = r3, border = 1)\n  pl(f, 21+x, 0+y, pal = alpha(purples, alp), print_geom = geom,  m = n1, border = 1)\n  pl(f, 21+x, 10+y, pal = alpha(purples, alp), print_geom = geom,  m = n2, border = 1)\n  invisible(pl(f, 21+x, 20+y, pal = alpha(purples, alp), print_geom = geom,  m = n3, border = 1))\n}\n\nprint_grid_filter &lt;- function(x, y) {\n  pl(f, 0+x, 0+y, pal = blues, m = matrix(b1[mask == TRUE], ncol = 7))\n  pl(f, 0+x, 10+y, pal = blues, m = matrix(b2[mask == TRUE], ncol = 7))\n  pl(f, 0+x, 20+y, pal = blues, m = matrix(b3[mask == TRUE], ncol = 7))\n  pl(f, 7+x, 0+y, pal = greens, m = matrix(g1[mask == TRUE], ncol = 7))\n  pl(f, 7+x, 10+y, pal = greens, m = matrix(g2[mask == TRUE], ncol = 7))\n  pl(f, 7+x, 20+y, pal = greens, m = matrix(g3[mask == TRUE], ncol = 7))\n  pl(f, 14+x, 0+y, pal = reds, m = matrix(r1[mask == TRUE], ncol = 7))\n  pl(f, 14+x, 10+y, pal = reds, m = matrix(r2[mask == TRUE], ncol = 7))\n  pl(f, 14+x, 20+y, pal = reds, m = matrix(r3[mask == TRUE], ncol = 7))\n  pl(f, 21+x, 0+y, pal = purples, m = matrix(n1[mask == TRUE], ncol = 7))\n  pl(f, 21+x, 10+y, pal = purples, m = matrix(n2[mask == TRUE], ncol = 7))\n  pl(f, 21+x, 20+y, pal = purples, m = matrix(n3[mask == TRUE], ncol = 7))\n}\n\nprint_grid_time_filter &lt;- function(x, y) { # 3x1, 28x7\n  pl(f, 0+x, 10+y, pal = blues, m = b3)\n  pl(f, 7+x, 10+y, pal = greens, m = g3)\n  pl(f, 14+x, 10+y, pal = reds, m = r3)\n  pl(f, 21+x, 10+y, pal = purples, m = n3)\n}\n\nprint_grid_bands_filter &lt;- function(x, y, pal = greys) { # 1x3 6x27\n  pl(f, 0+x, 0+y, pal = pal, m = n1)\n  pl(f, 0+x, 10+y, pal = pal, m = n2)\n  pl(f, 0+x, 20+y, pal = pal, m = n3)\n}\n\n# build exactly like reduce\nplot.new()\npar(mar = c(3,3,3,3))\nx &lt;- 120\ny &lt;- 100\ndown &lt;- 0\nplot.window(xlim = c(0, x), ylim = c(0, y), asp = 1)\nprint_grid(x/2-28/2,y-27)\nprint_alpha_grid((x/3-28)/2, 0-down) # alpha grid\nprint_grid_time_filter((x/3-28)/2, -10-down) # select 3rd\nprint_alpha_grid(x/3+((x/3-6)/2) -10.5, 0-down) # alpha grid\nprint_grid_bands_filter(x/3+((x/3-12.4)), 0-down, pal = purples)\nprint_alpha_grid(2*(x/3)+((x/3-28)/2), 0-down) # alpha grid\nprint_grid_filter(2*(x/3)+((x/3-28)/2), 0-down)\ntext(3, 13.5-down, \"time\", srt = 90, col = \"black\")\ntext(43, 13.5-down, \"time\", srt = 90, col = \"black\")\ntext(83, 13.5-down, \"time\", srt = 90, col = \"black\")\ntext(20, 30, \"bands\", col = \"black\")\ntext(60, 30, \"bands\", col = \"black\")\ntext(100, 30, \"bands\", col = \"black\")\narrows(x/2-28/2,y-30, x/6,32, angle = 20, lwd = 2)\narrows(x/2,y-30, x/2,32, angle = 20, lwd = 2)\narrows(x/2+28/2,y-30, 100, 32, angle = 20, lwd = 2)\n# points(seq(1,120,10), seq(1,120,10))\ntext(28.5,49, \"filter temporally\", srt = 55.5, col = \"black\", cex = 0.8)\ntext(57,49, \"filter bands\", srt = 90, col = \"black\", cex = 0.8)\ntext(91.5,49, \"filter spatially\", srt = -55.5, col = \"black\", cex = 0.8)\nprint_labels(x = x/2-28/2 + 3, y = y+4, off = 7, lab = c(\"blue\", \"green\", \"red\", \"nir\"),\n             horizontal = TRUE, cex = 0.6)\nprint_labels(x = x/2-28/2 - 9, y = y-23, off = 10, lab = c(\"2020-10-01\", \"2020-10-13\", \"2020-10-25\"),\n             horizontal = FALSE, cex = 0.6)\nprint_labels(x = x/2-28/2 + 21.5, y = y-30, off = 1, lab = xlabels,\n             horizontal = TRUE, cex = 0.3)\nprint_labels(x = x/2-28/2 + 30, y = y-26.5, off = 1, lab = ylabels,\n             horizontal = FALSE, cex = 0.3)\n\n\n\n\n\n\nFigure 6.4: Data cube filtering by time, band or spatially\n\n\n\n\nApplying functions to dimensions\n \nA common analysis involves applying a function over one or more cube dimensions. Simple cases arise where a function such as abs, sin, or sqrt is applied to all values in the cube, or when a function takes all values in the cube and returns a single scalar, such as when computing the mean or maximum value over the entire cube. Other options include applying the function to selected dimensions, such as applying a temporal low-pass filter to every individual (pixel/band) time series as shown in Figure 6.5, or applying a spatial low-pass filter to every spatial slice for every band/time combination, shown in Figure 6.6.\n\nCodeprint_text = function(s, x, y, m) {\n  # m &lt;- t(m) # transverse for correct order\n  # print(m)\n  r &lt;- rep(seq(0.5, 5.5, 1), 7)\n  r &lt;- r + x # consider offsets\n  u &lt;- c(rep(0.5, 6), rep(1.5, 6), rep(2.5, 6), \n         rep(3.5, 6), rep(4.5, 6), rep(5.5, 6), rep(6.5, 6))\n  u &lt;- u + y # offset\n  tab &lt;- matrix(c(r,u), nrow = 42, byrow = FALSE) # make point table\n  for (i in 1:42) {\n    #text(tab[i, 1], tab[i, 2], labels = paste0(\"\", m[i]), cex = 1.1)\n    text(tab[i, 1], tab[i, 2], labels = paste0(\"\", m[i]), cex = 0.8)\n    }\n}\ntime_arrow_seg &lt;- matrix(c(c(-1.0, 0.3, 1.5, 2.7, 3.9, 5.1), c(-1.0, 0.3, 1.5, 2.7, 3.9, 5.1),\n                           c(-0.5, 0.7, 1.9, 3.1, 4.3, 5.6), c(-0.5, 0.7, 1.9, 3.1, 4.3, 5.6)), ncol = 4)\ntime_arrow_flag_seg &lt;- matrix(c(c(-1.0, 1.5, 2.7, 3.9), c(-1.0, 1.5, 2.7, 3.9),\n                                c(0.7, 1.9, 3.1, 5.6), c(0.7, 1.9, 3.1, 5.6)), ncol = 4)\n\nb11 &lt;- b2 - t1\nb12 &lt;- b1 - t2 + t1\n\nbrks &lt;- seq(0, 1000, 50)\n# png(\"exp_apply_ts.png\", width = 2400, height = 1000, pointsize = 24)\nplot.new()\npar(mar = c(2,2,2,2))\nx &lt;- 30\ny &lt;- 10 # 7.5\nplot.window(xlim = c(0, x), ylim = c(0, y), asp = 1)\npl(f, 4.8, 3.8, pal = blues, m = b3, breaks = brks)\nprint_text(s, 4.8, 3.8, m = b3)\npl(f, 3.6, 2.6, pal = blues, m = b11, breaks = brks)\nprint_text(s, 3.6, 2.6, m = b11)\npl(f, 2.4, 1.4, pal = blues, m = b12, breaks = brks)\nprint_text(s, 2.4, 1.4, m = b12)\npl(f, 1.2, .2, pal = blues, m = b2, breaks = brks)\nprint_text(s, 1.2, .2, m = b2)\npl(f, 0, -1, pal = blues, m = b1, breaks = brks)\nprint_text(s, 0, -1, m = b1) # print text on left first stack\npl(f, 24.8, 3.8, pal = alpha(greys, 0.1), m = matrix(rep(\"NA\", 42), ncol = 7))\npl(f, 23.6, 2.6, pal = blues, m = (b12 + b11 + b3) / 3, breaks = brks)\nprint_text(s, 23.6, 2.6, m = floor((b12 + b11 + b3) / 3))\npl(f, 22.4, 1.4, pal = blues, m = (b2 + b12 + b11) / 3, breaks = brks)\nprint_text(s, 22.4, 1.4, m = floor((b2 + b12 + b11) / 3))\npl(f, 21.2, .2, pal = blues, m = (b1 + b2 + b12) / 3, breaks = brks)\nprint_text(s, 21.2, .2, m = floor((b1 + b2 + b12) / 3))\npl(f, 20, -1, pal = alpha(greys, 0.1), m = matrix(rep(\"NA\", 42), ncol = 7))\nprint_segments(5.7, 1.7, seg = time_arrow_seg, col = \"forestgreen\")\narrows(12.5, 9, 20, 9, lwd = 2)\ncex &lt;- .9\ntext(16.3, 8.3, \"apply_dimension(dimension = 't')\", cex = cex)\nprint_segments(9.7, 1.7, time_arrow_seg, col = \"forestgreen\") # draw ma explanation\ntext(-0.5 + 10, -0.5 + 2, \"496\", cex = cex)\ntext(.7 + 10, .7 + 2, \"363\", cex = cex)\ntext(1.9 + 10, 1.9 + 2, \"658\", cex = cex)\ntext(3.1 + 10, 3.1 + 2, \"230\", cex = cex)\ntext(4.3 + 10, 4.3 + 2, \"525\", cex = cex)\nt_formula &lt;- expression(\"t\"[n]*\" = (t\"[n-1]*\" + t\"[n]*\" + t\"[n+1]*\") / 3\")\n# text(13.8, 3, t_formula, srt = 45, cex = 1.2)\ntext(14.4, 3.6, \"calculate moving average\", srt = 45, cex = cex)\narrows(15, 5.7, 18, 5.7, lwd = 2)\nprint_segments(15.4, 1.7, seg = time_arrow_seg, col = \"forestgreen\") # draw ma explanation\ntext(-0.5 + 15.7, -0.5 + 2, \"NA\", cex = cex)\ntext(.7 + 15.7, .7 + 2, \"505\", cex = cex)\ntext(1.9 + 15.7, 1.9 + 2, \"417\", cex = cex)\ntext(3.1 + 15.7, 3.1 + 2, \"471\", cex = cex)\ntext(4.3 + 15.7, 4.3 + 2, \"NA\", cex = cex)\nprint_segments(25.7, 1.7, seg = time_arrow_seg, col = \"forestgreen\")\n\n\n\n\n\n\nFigure 6.5: Low-pass filtering of time series\n\n\n\n\n\nCode# apply ----\n\nabs_brks &lt;- seq(-500,500, 50)\nabs_pal &lt;- sequential_hcl(n = 20, h1 = 211, c1 = 80, l1 = 30, l2 = 100, p1 = 1.2)\n\nvNeumann_seg &lt;- matrix(c(c(0,0,1,1,2,2,1,1,0,0,-1,-1), c(0,-1,-1,0,0,1,1,2,2,1,1,0), \n                         c(0,1,1,2,2,1,1,0,0,-1,-1,0), c(-1,-1,0,0,1,1,2,2,1,1,0,0)), ncol = 4)\n\napply_filter &lt;- function(input, pad = TRUE, padValue = 1) {\n  ras &lt;- raster::focal(raster::raster(input), w = matrix(c(0,0.2,0, 0.2,0.2,0.2, 0,0.2,0), ncol = 3), pad = pad, padValue = padValue)\n  ras &lt;- raster::as.matrix(ras)\n  ras[ras == \"NaN\"] &lt;- -999\n  return(floor(ras))\n}\n\nbrks &lt;- seq(0, 1000, 50)\nplot.new()\npar(mar = c(0,2,0,0))\nx = 30\ny = 7.5\nplot.window(xlim = c(0, x), ylim = c(0, y), asp = 1)\npl(f, 3, 2, pal = blues, m = b3, breaks = brks)\npl(f, 1.5, .5, pal = blues, m = b2, breaks = brks)\npl(f, 0, -1, pal = blues, m = b1, breaks = brks)\nprint_text(s, 0, -1, m = b1) # print text on left first stack\nprint_segments(2, 3, seg = vNeumann_seg, lwd = 3)\npl(f, 23, 2, pal = blues, m = apply_filter(b3), breaks = brks)\npl(f, 21.5, 0.5, pal = blues, m = apply_filter(b2), breaks = brks)\npl(f, 20, -1, pal = blues, m = apply_filter(b1), breaks = brks)\nprint_text(s, 20, -1, m = apply_filter(b1)) # set pad = FALSE for -99\nprint_segments(22, 3, seg = vNeumann_seg, lwd = 3)\narrows(11, 4, 17.5, 4, lwd = 3)\ntext(14.3, 3.5, \"apply_kernel()\", cex = 1.4)\nprint_segments(13.8, 1, seg = vNeumann_seg, lwd = 3)\ncex = .8\ntext(14.3, 1.5, \"0.2\", cex = cex)\ntext(13.3, 1.5, \"0.2\", cex = cex)\ntext(15.3, 1.5, \"0.2\", cex = cex)\ntext(14.3, 2.5, \"0.2\", cex = cex)\ntext(14.3, .5, \"0.2\", cex = cex)\n\n\n\n\n\n\nFigure 6.6: Low pass filtering of spatial slices\n\n\n\n\nReducing dimensions\n \nWhen applying function mean to an entire data cube, all dimensions vanish: the resulting “data cube” has dimensionality zero. We can also apply functions to a limited set of dimensions such that selected dimensions vanish, or are reduced. We already saw that filtering is a special case of this, but more in general we could for instance compute the maximum of every time series, the mean over every spatial slice, or a band index such as NDVI that summarises different spectral values into a single new “band” with the index value. Figure 6.7 illustrates these options.\n\nCode# reduce ----\n\n# calc mean over time\ntimeB &lt;- (b1 + b2 + b3) / 3\ntimeG &lt;- (g1 + g2 + g3) / 3\ntimeR &lt;- (r1 + r2 + r3) / 3\ntimeN &lt;- (n1 + n2 + n3) / 3\n\nprint_grid_time &lt;- function(x, y) { # 3x1, 28x7\n  pl(f, 0+x, 10+y, pal = blues, m = timeB)\n  pl(f, 7+x, 10+y, pal = greens, m = timeG)\n  pl(f, 14+x, 10+y, pal = reds, m = timeR)\n  pl(f, 21+x, 10+y, pal = purples, m = timeN)\n}\n\n# calc ndvi\nndvi1 &lt;- (n1 - r1) / (n1 + r1)\nndvi2 &lt;- (n2 - r2) / (n2 + r2)\nndvi3 &lt;- (n3 - r3) / (n3 + r3)\n\nprint_grid_bands &lt;- function(x, y, pal = greys) { # 1x3 6x27\n  pl(f, 0+x, 0+y, pal = pal, m = ndvi1)\n  pl(f, 0+x, 10+y, pal = pal, m = ndvi2)\n  pl(f, 0+x, 20+y, pal = pal, m = ndvi3)\n}\n\nplte = function(s, x, y, add = TRUE, randomize = FALSE, pal, m) {\n  attr(s, \"dimensions\")[[1]]$offset = x\n  attr(s, \"dimensions\")[[2]]$offset = y\n  # m = r[[1]][y + 1:nrow,x + 1:ncol,1]\n  # dim(m) = c(x = nrow, y = ncol) # named dim\n  # s[[1]] = m\n  # me &lt;- floor(mean(s[[1]]))\n  me &lt;- floor(mean(m))\n  if (me[1] &gt; 100) { # in case non-artificial grids with very high\n    me &lt;- m / 10     # numbers are used, make them smaller\n    me &lt;- floor(mean(me))\n  }\n  text(x,y,me,cex = 0.8)\n}\n\nprint_grid_spat &lt;- function(x, y) {\n  x = x + 3\n  y = y + 3.5\n  plte(s, 0+x, 0+y, pal = blues, m = b1)\n  plte(s, 0+x, 10+y, pal = blues, m = b2)\n  plte(s, 0+x, 20+y, pal = blues, m = b3)\n  plte(s, 7+x, 0+y, pal = greens, m = g1)\n  plte(s, 7+x, 10+y, pal = greens, m = g2)\n  plte(s, 7+x, 20+y, pal = greens, m = g3)\n  plte(s, 14+x, 0+y, pal = reds, m = r1)\n  plte(s, 14+x, 10+y, pal = reds, m = r2)\n  plte(s, 14+x, 20+y, pal = reds, m = r3)\n  plte(s, 21+x, 0+y, pal = purples, m = n1)\n  plte(s, 21+x, 10+y, pal = purples, m = n2)\n  plte(s, 21+x, 20+y, pal = purples, m = n3)\n}\n\n# png(\"exp_reduce.png\", width = 1200, height = 1000, pointsize = 32)\nplot.new()\n#par(mar = c(3,3,3,3))\npar(mar = c(3,0,2,0))\nx = 120\ny = 100\nplot.window(xlim = c(0, x), ylim = c(0, y), asp = 1)\nprint_grid(x/2-28/2,y-27)\n# print_alpha_grid((x/3-28)/2, 0) # alpha grid\nprint_grid_time((x/3-28)/2, 0) # off = 5.5\n# print_alpha_grid(x/3+((x/3-6)/2) -10.5, 0) # alpha grid\nprint_grid_bands(x/3+((x/3-6)/2), 0)\nprint_alpha_grid(2*(x/3)+((x/3-28)/2), 0, alp = 0, geom = TRUE) # alpha grid\nprint_grid_spat(2*(x/3)+((x/3-28)/2), 0)\ntext(3, 13.5, \"time\", srt = 90, col = \"black\")\n#segments(3.6, 8, 3.7, 19, col = \"red\", lwd=3)\nsegments(3.4, 8, 3.4, 19, col = \"red\", lwd = 3)\ntext(43, 13.5, \"time\", srt = 90, col = \"black\")\ntext(83, 13.5, \"time\", srt = 90, col = \"black\")\ntext(20, 30, \"bands\", col = \"black\")\ntext(60, 30, \"bands\", col = \"black\")\nsegments(53,29.8, 67,29.8, col = \"red\", lwd = 3)\ntext(100, 30, \"bands\", col = \"black\")\ntext(30, 7, \"x\", col = \"black\")\ntext(36, 13, \"y\", col = \"black\")\ntext(60, -3, \"x\", col = \"black\")\ntext(66, 3, \"y\", col = \"black\")\ntext(110, -3, \"x\", col = \"black\")\ntext(116, 3, \"y\", col = \"black\")\nsegments(108,-2.4, 112,-3.2, col = \"red\", lwd = 3)\nsegments(114,3.2, 118,2.4, col = \"red\", lwd = 3)\ntext(60, y+4, \"bands\", col = \"black\") # dim names on main\ntext(43, y-14, \"time\", srt = 90, col = \"black\")\ntext(x/2-28/2 + 24, y-30, \"x\", col = \"black\")\ntext(x/2-28/2 + 30, y-24, \"y\", col = \"black\")\narrows(x/2-28/2,y-30, x/6,32, angle = 20, lwd = 2)\narrows(x/2,y-30, x/2,32, angle = 20, lwd = 2)\narrows(x/2+28/2,y-30, 100, 32, angle = 20, lwd = 2)\n# points(seq(1,120,10), seq(1,120,10))\ntext(28.5,49, \"reduce temporally\", srt = 55.5, col = \"black\", cex = 0.8)\ntext(57,49, \"reduce bands\", srt = 90, col = \"black\", cex = 0.8)\ntext(91.5,49, \"reduce spatially\", srt = -55.5, col = \"black\", cex = 0.8)\n\n\n\n\n\n\nFigure 6.7: Reducing data cube dimensions",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Cubes</span>"
    ]
  },
  {
    "objectID": "06-Cubes.html#sec-vectordatacubes",
    "href": "06-Cubes.html#sec-vectordatacubes",
    "title": "6  Data Cubes",
    "section": "\n6.4 Aggregating raster to vector cubes",
    "text": "6.4 Aggregating raster to vector cubes\n \nFigure 6.8 illustrates how a four-dimensional raster data cube can be aggregated to a three-dimensional vector data cube. Pixels in the raster are grouped by spatial intersection with a set of vector geometries, and each group is then reduced to a single value by an aggregation function such as mean or max. In the example, the two spatial dimensions \\(x\\) and \\(y\\) reduce to a single dimension, the one-dimensional sequence of feature geometries, with geometries that are defined in the space of \\(x\\) and \\(y\\). Grouping geometries can also be POINT geometries, in which case the aggregation function is obsolete as single values at the POINT locations are extracted by querying a pixel value or by interpolating from the nearest pixels.\n\nCode# aggregate ----\n\nmask_agg &lt;- matrix(c(NA,NA,NA,NA,NA,NA,\n                 NA, 1, 1,NA,NA,NA,\n                  1, 1,NA,NA, 1,NA,\n                  1,NA,NA, 1, 1,NA,\n                  1,NA,NA, 1, 1,NA,\n                  1,NA,NA,NA, 1,NA,\n                 NA,NA,NA,NA,NA,NA), ncol = 7)\n\npl_stack_agg &lt;- function(s, x, y, add = TRUE, nrM, imgY = 7, inner = 1) {\n  # pl_stack that masks the added matrices\n  # nrM is the timestep {1, 2, 3}, cause this function\n  # prints all 4 bands at once\n  attr(s, \"dimensions\")[[1]]$offset = x\n  attr(s, \"dimensions\")[[2]]$offset = y\n  # m = r[[1]][y + 1:nrow,x + 1:ncol,1]\n  m &lt;- eval(parse(text=paste0(\"n\", nrM)))\n  m &lt;- matrix(m[mask_agg == TRUE], ncol = 7)\n  s[[1]] = m[,c(imgY:1)] # turn around to have same orientation as flat plot\n  plt(s, 0, TRUE,  pal = purples)\n  m &lt;- eval(parse(text=paste0(\"r\", nrM)))\n  m &lt;- matrix(m[mask_agg == TRUE], ncol = 7)\n  s[[1]] = m[,c(imgY:1)]\n  plt(s, 1*inner, TRUE,  pal = reds)\n  m &lt;- eval(parse(text=paste0(\"g\", nrM)))\n  m &lt;- matrix(m[mask_agg == TRUE], ncol = 7)\n  s[[1]] = m[,c(imgY:1)]\n  plt(s, 2*inner, TRUE,  pal = greens)\n  m &lt;- eval(parse(text=paste0(\"b\", nrM)))\n  m &lt;- matrix(m[mask_agg == TRUE], ncol = 7)\n  s[[1]] = m[,c(imgY:1)]\n  plt(s, 3*inner, TRUE, pal = blues) # li FALSE\n}\n\npolygon_1 &lt;- matrix(c(c(0.0, 5.1, 4.9,-2.3), c(0.0, 2.4, 3.1, 1.8),\n                      c(5.1, 4.9,-2.3, 0.0), c(2.4, 3.1, 1.8, 0.0)), ncol = 4)\n\na &lt;- make_dummy_stars(6, 7, 5, -1.14, -2.28)\n\nprint_vector_content &lt;- function(x, y, cex = 0.8) {\n  vec &lt;- floor(rnorm(8, 250, 100))\n  text( 0 + x,12 + y, vec[1], cex = cex)\n  text( 0 + x, 8 + y, vec[2], cex = cex)\n  text( 0 + x, 4 + y, vec[3], cex = cex)\n  text( 0 + x, 0 + y, vec[4], cex = cex)\n  text( 12 + x,12 + y, vec[5], cex = cex)\n  text( 12 + x, 8 + y, vec[6], cex = cex)\n  text( 12 + x, 4 + y, vec[7], cex = cex)\n  text( 12 + x, 0 + y, vec[8], cex = cex)\n}\n\nprint_ts &lt;- function(off2, yoff2) {\n  pl_stack(s, 0 + off2, yoff2, nrM = 3) # input 2\n  pl_stack(s, off + off2, yoff2, nrM = 2)\n  pl_stack(s, 2 * off + off2, yoff2, nrM = 1)\n  arrows(-13 + off2, 14 + yoff2, 72 + off2, 14 + yoff2, angle = 20, lwd = 2)  # timeline\n  heads &lt;- matrix(c(3.5+off2, 3.5 + off + off2, 3.5 + 2*off + off2, 14+yoff2,14+yoff2,14+yoff2), ncol = 2)\n  points(heads, pch = 16) # 4 or 16\n  segments(c(-8, 7, 0, 15)+off2, c(-1,-1,3,3)+yoff2, 3.5+off2, 14+yoff2) # first stack pyramid\n  segments(c(-8, 7, 0, 15) + off + off2, c(-1,-1,3,3)+yoff2, 3.5 + off + off2, 14+yoff2) # second stack pyramid\n  segments(c(-8, 7, 0, 15) + 2*off + off2, c(-1,-1,3,3)+yoff2, 3.5 + 2*off + off2, 14+yoff2) # third stack pyramid\n  text(7.5+off2, 4.3+yoff2, \"x\", col = \"black\", cex = secText)\n  text(-9.5+off2, -2.5+yoff2, \"bands\", srt = 90, col = \"black\", cex = secText)\n  text(-4.5+off2, 2+yoff2, \"y\", srt = 27.5, col = \"black\", cex = secText)\n  text(69+off2, 15.5+yoff2+1, \"time\", col = \"black\")\n  text(3.5+off2, 15.5+yoff2, \"2020-10-01\", col = \"black\")\n  text(3.5 + off + off2, 15.5+yoff2, \"2020-10-13\", col = \"black\")\n  text(3.5 + 2*off + off2, 15.5+yoff2, \"2020-10-25\", col = \"black\")\n}\n\nsecText &lt;- 0.8 # secondary text size (dimension naming)\noff &lt;- 26 # image stacks are always 26 apart\nx &lt;- 72 # png X\ny &lt;- 48 # png Y\nyoff &lt;- 30\nplot.new()\npar(mar = c(5,3,3,3))\nplot.window(xlim = c(-5, x+4), ylim = c(-1, y), asp = 1)\nprint_ts(5, yoff)\ncol &lt;- '#ff5555'\nprint_segments(10.57, yoff-.43, seg = polygon_1, col = col)\nx = 3\nsegments(c(2, 0, -1.3)+x, c(-.2, 0, 1)+yoff, c(0, -1.3, 1)+x, c(0, 1, 2)+yoff, lwd = 4, col = col)\nprint_segments(10.57+off, yoff-.43, seg = polygon_1, col = col)\nx = 3 + off\nsegments(c(2, 0, -1.3)+x, c(-.2, 0, 1)+yoff, c(0, -1.3, 1)+x, c(0, 1, 2)+yoff, lwd = 4, col = col)\nprint_segments(10.57+2*off, yoff-.43, seg = polygon_1, col = col)\nx = 3 + 2 * off\nsegments(c(2, 0, -1.3)+x, c(-.2, 0, 1)+yoff, c(0, -1.3, 1)+x, c(0, 1, 2)+yoff, lwd = 4, col = col)\n# old 75 28 poly 86.25, 27.15\npl_stack_agg(a, 5, 5, nrM = 1, inner = 3) # print masked enlargement\nprint_segments(16.25, 7.15, seg = polygon_1, by = 2, col = col)\nx &lt;- 1\ny &lt;- 8\nsegments(c(4, 0, -2.6)+x, c(-.4, 0, 2)+y, c(0, -2.6, 2)+x, c(0, 2, 4)+y, lwd = 4, col = col) # line in large\nsegments(-3, 25, -7, 9, lwd = 3, col = 'grey')\nsegments(21, 29, 27.5, 14, lwd = 3, col = 'grey')\ntext(10, 20, \"1. Group by geometry\", cex = 1.3)\nvecM &lt;- matrix(rep(1,8), ncol = 2)\ntext(57, 21, \"2. Reduce to vector cube\", cex = 1.3)\nb &lt;- make_dummy_stars(2, 4, 12, 4, 0)\npl(b, 48, -5, m = vecM, pal = alpha(\"white\", 0.9), border = 0)\nprint_vector_content(54, -3)\npl(b, 46.5, -3.5, m = vecM, pal = alpha(\"white\", 0.9), border = 0)\nprint_vector_content(52.5, -1.5)\npl(b, 45, -2, m = vecM, pal = alpha(\"white\", 0.9), border = 0)\nprint_vector_content(51, 0)\ntext(51.5, 15, \"Line_1\", col = col)\ntext(63, 15, \"Polygon_1\", col = col)\ntext(57, 17.5, \"Geometries\", cex = 1.1)\ntext(42, 12, \"blue\")\ntext(42,  8, \"green\")\ntext(42,  4, \"red\")\ntext(42,  0, \"nir\")\ntext(38, 6, \"Bands\", srt = 90, cex = 1.1)\n# arrows(13.5, -2, 13.5, -6, angle = 20, lwd = 3)\ntext(72, 15.5, \"time\", srt = 315, cex = 1.1)\narrows(69.5, 15, 72.5, 12, angle = 20, lwd = 2)\n# print_segments(30, 35, seg = arrow_seg)\n\n\n\n\n\n\nFigure 6.8: Aggregating a raster data cube to a vector data cube\n\n\n\n\nFurther examples of vector data cubes include air quality data, where we could have \\(PM_{10}\\) measurements over two dimensions, as a sequence of\n\nmonitoring stations, and\ntime intervals\n\nor where we consider time series of demographic or epidemiological data, consisting of (population, disease) counts, with number of persons by\n\nregion, for a sequence of \\(n\\) regions\nage class, for \\(m\\) age classes, and\nyear, for \\(p\\) years\n\nwhich forms an array with \\(n m p\\) elements.\nFor spatial data science, handling vector and raster data cubes is extremely useful, because many variables are both spatially and temporaly varying, and because we often want to either change dimensions or aggregate them out, but in a fully flexible manner and order. Examples of changing dimensions are:\n\ninterpolating air quality measurements to values on a regular grid (raster; Chapter 12)\nestimating density maps from points or lines, for instance estimating the number of flights passing by per week within a range of 1 km (Chapter 11)\naggregating climate model predictions to summary indicators for administrative regions\ncombining Earth observation data from different sensors, such as MODIS (250~m pixels, every 16 days) with Sentinel-2 (10~m pixels, every 5 days)\n\nExamples of aggregating one or more full dimensions are assessments of:\n\nwhich air quality monitoring stations indicate unhealthy conditions (time)\nwhich region has the highest increase in disease incidence (space, time)\nglobal warming (global change in degrees Celsius per decade)",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Cubes</span>"
    ]
  },
  {
    "objectID": "06-Cubes.html#sec-switching",
    "href": "06-Cubes.html#sec-switching",
    "title": "6  Data Cubes",
    "section": "\n6.5 Switching dimension with attributes",
    "text": "6.5 Switching dimension with attributes\n \nWhen we accept that a dimension can also reflect an unordered, categorical variable, then one can easily swap a set of attributes for a single dimension, by replacing\n\\[\\{Z_1,Z_2,...,Z_p\\} = f(D_1,D_2,...,D_n)\\]\nwith\n\\[Z = f(D_1,D_2,...,D_n, D_{n+1})\\]\nwhere \\(D_{n+1}\\) has cardinality \\(p\\) and has as labels (the names of) \\(Z_1,Z_2,...,Z_p\\). Figure 6.9 shows a vector data cube for air quality stations where one cube dimension reflects air quality parameters. When the \\(Z_i\\) have incompatible measurement units, as in Figure 6.9, one would have to take care when reducing the “parameter” dimension \\(D_{n+1}\\): numeric functions like mean or max would be meaningless. Counting the number of variables that exceed their respective threshold values may however be meaningful.\n\nCodeset.seed(1331)\nlibrary(stars)\nlibrary(colorspace)\ntif &lt;- system.file(\"tif/L7_ETMs.tif\", package = \"stars\")\nr &lt;- read_stars(tif)\n\nnrow &lt;- 5\nncol &lt;- 8\n#m = matrix(runif(nrow * ncol), nrow = nrow, ncol = ncol)\nm &lt;- r[[1]][1:nrow,1:ncol,1]\ndim(m) &lt;- c(x = nrow, y = ncol) # named dim\ns &lt;- st_as_stars(m)\n# s\nattr(s, \"dimensions\")[[1]]$delta = 3 \nattr(s, \"dimensions\")[[2]]$delta = -.5\nattr(attr(s, \"dimensions\"), \"raster\")$affine = c(-1.2, 0.0)\n\nplt &lt;- function(x, yoffset = 0, add, li = TRUE) {\n    attr(x, \"dimensions\")[[2]]$offset = attr(x, \"dimensions\")[[2]]$offset + yoffset \n    l = st_as_sf(x, as_points = FALSE)\n    pal = sf.colors(10)\n    if (li)\n        pal = lighten(pal, 0.3 + rnorm(1, 0, 0.1))\n    if (! add)\n        plot(l, axes = FALSE, breaks = \"equal\", pal = pal, reset = FALSE, border = grey(.75), key.pos = NULL, main = NULL, xlab = \"time\")\n    else\n        plot(l, axes = TRUE, breaks = \"equal\", pal = pal, add = TRUE, border = grey(.75))\n    u = st_union(l)\n    plot(st_geometry(u), add = TRUE, col = NA, border = 'black', lwd = 2.5)\n}\n\npl &lt;- function(s, x, y, add = TRUE, randomize = FALSE) {\n  attr(s, \"dimensions\")[[1]]$offset = x\n  attr(s, \"dimensions\")[[2]]$offset = y\n  m = r[[1]][y + 1:nrow,x + 1:ncol,1]\n  if (randomize)\n    m = m[sample(y + 1:nrow),x + 1:ncol]\n  dim(m) = c(x = nrow, y = ncol) # named dim\n  s[[1]] = m\n  plt(s, 0, add)\n  plt(s, 1, TRUE)\n  plt(s, 2, TRUE)\n  plt(s, 3, TRUE)\n  plt(s, 4, TRUE)\n  plt(s, 5, TRUE)\n  plt(s, 6, TRUE)\n  plt(s, 7, TRUE)\n  plt(s, 8, TRUE, FALSE)\n}\n\n# point vector data cube:\nplot.new()\npar(mar = c(5, 0, 5, 0))\nplot.window(xlim = c(-10, 16), ylim = c(-2,12), asp = 1)\nlibrary(spacetime)\ndata(air)\nde = st_geometry(st_normalize(st_as_sf(DE)))\n# \npl(s, 0, 0, TRUE, randomize = TRUE)\nde = de * 6 + c(-7, 9)\nplot(de, add = TRUE, border = grey(.5))\ntext(-10, 0, \"time\", srt = -90, col = 'black')\ntext(-5,  7.5, \"sensor location\", srt = 25, col = 'black')\ntext( 7,  10.5, \"air quality parameter\", srt = 0, col = 'black')\ntext( 1.5,  8.5, expression(PM[10]), col = 'black', cex = .75)\ntext( 4.5,  8.5, expression(NO[x]), col = 'black', cex = .75)\ntext( 8,  8.5, expression(SO[4]), col = 'black', cex = .75)\ntext( 11,  8.5, expression(O[3]), col = 'black', cex = .75)\ntext( 14,  8.5, expression(CO), col = 'black', cex = .75)\n# location points:\np &lt;- st_coordinates(s[,1])\np[,1] &lt;- p[,1]-1.4\np[,2] &lt;- p[,2] + 8.2\npoints(p, col = grey(.7), pch = 16)\n# centroids:\nset.seed(131)\ncent &lt;- st_coordinates(st_sample(de, 8))\npoints(cent, col = grey(.7), pch = 16)\ncent &lt;- cent[rev(order(cent[,1])),]\nseg &lt;- cbind(p, cent[1:8,])\nsegments(seg[,1], seg[,2], seg[,3], seg[,4], col = 'grey')\n\n\n\n\n\n\nFigure 6.9: Vector data cube with air quality time series\n\n\n\n\nBeing able to swap dimensions to attributes flexibly and vice versa leads to highly flexible analysis possibilities (Brown 2010).",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Cubes</span>"
    ]
  },
  {
    "objectID": "06-Cubes.html#sec-otherdynamic",
    "href": "06-Cubes.html#sec-otherdynamic",
    "title": "6  Data Cubes",
    "section": "\n6.6 Other dynamic spatial data",
    "text": "6.6 Other dynamic spatial data\n \nWe have seen several dynamic raster and vector data examples that match the data cube structure well. Other data examples do less so: in particular spatiotemporal point patterns (Chapter 11) and trajectories [movement data; for a recent review, see Joo et al. (2020)] are often more straightforward to not handle as a data cube. Spatiotemporal point patterns are the sets of spatiotemporal coordinates of events or objects: accidents, disease cases, traffic jams, lightning strikes, and so on. Trajectory data are time sequences of spatial locations of moving objects (persons, cars, satellites, animals). For such data, the primary information is in the coordinates, and shifting these to a limited set of regularly discretised grid cells covering the space may help some analysis, for instance to quickly explore patterns in areas of higher densities, but the loss of the exact coordinates also hinders a number of analysis approaches involving distance, direction, or speed calculations. Nevertheless, for such data often the first computational steps involves generation of data cube representations by aggregating to a time-fixed spatial and/or space-fixed temporal discretisation.\nUsing sparse array representations of data cubes to represent point pattern or trajectory data, which is possible for instance with SciDB (Brown 2010) or TileDB (Papadopoulos et al. 2016), may strongly limit the loss of coordinate accuracy by choosing dimensions that represent an extremely dense grid, and storing only those grid cells that contain data points. For trajectory data, such representations would need to add a grouping dimension to identify individuals, or individual sequences of consecutive movement observations.",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Cubes</span>"
    ]
  },
  {
    "objectID": "06-Cubes.html#exercises",
    "href": "06-Cubes.html#exercises",
    "title": "6  Data Cubes",
    "section": "\n6.7 Exercises",
    "text": "6.7 Exercises\nUse words to solve the following exercises. If needed or relevant use R code to illustrate the argument(s).\n\nWhy is it difficult to represent trajectories, sequences of \\((x,y,t)\\) obtained by tracking moving objects, by data cubes as described in this chapter?\nIn a socio-economic vector data cube with variables population, life expectancy, and gross domestic product ordered by dimensions country and year, which variables have block support for the spatial dimension, and which have block support for the temporal dimension?\nThe Sentinel-2 satellites collect images in 12 spectral bands; list advantages and disadvantages to represent them as (i) different data cubes, (ii) a data cube with 12 attributes, one for each band, and (iii) a single attribute data cube with a spectral dimension.\nExplain why a curvilinear raster as shown in Figure 1.6 can be considered a special case of a data cube.\n\nExplain how the following problems can be solved with data cube operations filter, apply, reduce and/or aggregate, and in which order. Also mention for each which function is applied, and what the dimensionality of the resulting data cube is (if any):\n\nfrom hourly \\(PM_{10}\\) measurements for a set of air quality monitoring stations, compute per station the amount of days per year that the average daily \\(PM_{10}\\) value exceeds 50 \\(\\mu g/m^3\\)\n\nfor a sequence of aerial images of an oil spill, find the time at which the oil spill had its largest extent, and the corresponding extent\nfrom a 10-year period with global daily sea surface temperature (SST) raster maps, find the area with the 10% largest and 10% smallest temporal trends in SST values.\n\n\n\n\n\n\n\n\n\nAppel, Marius, and Edzer Pebesma. 2019. “On-Demand Processing of Data Cubes from Satellite Image Collections with the Gdalcubes Library.” Data 4 (3): 92. https://www.mdpi.com/2306-5729/4/3/92.\n\n\nBrown, P. G. 2010. “Overview of SciDB: Large Scale Array Storage, Processing and Analysis.” In Proceedings of the 2010 ACM SIGMOD International Conference on Management of Data, 963–68. ACM.\n\n\nGalton, A. 2004. “Fields and Objects in Space, Time and Space-Time.” Spatial Cognition and Computation 4.\n\n\nJoo, Rocío, Matthew E. Boone, Thomas A. Clay, Samantha C. Patrick, Susana Clusella-Trullas, and Mathieu Basille. 2020. “Navigating Through the R Packages for Movement.” Journal of Animal Ecology 89 (1): 248–67. https://doi.org/10.1111/1365-2656.13116.\n\n\nLu, Meng, Marius Appel, and Edzer Pebesma. 2018. “Multidimensional Arrays for Analysing Geoscientific Data.” ISPRS International Journal of Geo-Information 7 (8): 313.\n\n\nPapadopoulos, Stavros, Kushal Datta, Samuel Madden, and Timothy Mattson. 2016. “The Tiledb Array Data Storage Manager.” Proceedings of the VLDB Endowment 10 (4): 349–60.",
    "crumbs": [
      "Spatial Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Cubes</span>"
    ]
  },
  {
    "objectID": "part-2.html",
    "href": "part-2.html",
    "title": "R for Spatial Data Science",
    "section": "",
    "text": "The second part of this book explains how the concepts introduced in the first part are dealt with using R. 7  Introduction to sf and stars deals with basic handling of spatial data: reading, writing, subsetting, selecting by spatial predicates, geometry transformers like buffers or intersections, raster-vector and vector-raster conversion, handling of data cubes, spherical geometry, coordinate transformations and conversions. This is followed by 8  Plotting spatial data which is dedicated to plotting of spatial and spatiotemporal data with base plot, and packages ggplot2, tmap and mapview. The chapter deals with projection, colours, colour breaks, graticules, graphic elements on maps like legends, and interactive maps. 9  Large data and cloud native discusses approaches to handle large vector or raster datasets or data cubes, where “large” either means too large to fit in memory or too large to download.\nThe material covered in this part is not meant as a complete tutorial nor a manual of the packages covered, but rather as an explanation and illustration of a number of common workflows. More complete and detailed information is found in the package documentation, in particular in the package vignettes for packages sf and stars. Links to them are found on the CRAN landing pages of the packages.",
    "crumbs": [
      "R for Spatial Data Science"
    ]
  },
  {
    "objectID": "07-Introsf.html",
    "href": "07-Introsf.html",
    "title": "7  Introduction to sf and stars",
    "section": "",
    "text": "7.1 Package sf\nThis chapter introduces R packages sf and stars. sf provides a table format for simple features, where feature geometries are stored in a list-column. R package stars was written to support raster and vector data cubes (Chapter 6), supporting raster layers, raster stacks and feature time series as special cases. sf first appeared on CRAN in 2016, stars in 2018. Development of both packages received support from the R Consortium as well as strong community engagement. The packages were designed to work together. Functions or methods operating on sf or stars objects start with st_, making it easy to recognise them or to search for them when using command line completion.\nIntended to succeed and replace R packages sp, rgeos and the vector parts of rgdal, R package sf (Pebesma 2018) was developed to move spatial data analysis in R closer to standards-based approaches seen in the industry and open source projects, to build upon more modern versions of the open source geospatial software stack (Figure 1.7), and to allow for integration of R spatial software with the tidyverse (Wickham et al. 2019), if desired.\nTo do so, R package sf provides simple features access (Herring et al. 2011), natively, to R. It provides an interface to several tidyverse packages, in particular to ggplot2, dplyr, and tidyr. It can read and write data through GDAL, execute geometrical operations using GEOS (for projected coordinates) or s2geometry (for ellipsoidal coordinates), and carry out coordinate transformations or conversions using PROJ. External C++ libraries are interfaced using R package Rcpp (Eddelbuettel 2013).\nPackage sf represents sets of simple features in sf objects, a sub-class of a data.frame or tibble. sf objects contain at least one geometry list-column of class sfc, which for each element contains the geometry as an R object of class sfg. A geometry list-column acts as a variable in a data.frame or tibble, but has a more complex structure than basic vectors such as numeric or character variables Section B.3.\nAn sf object has the following metadata:\nAn sfc geometry list-column is extracted from an sf object with st_geometry and has the following metadata:\nThese attributes may best be accessed or set by using functions like st_bbox, st_crs, st_set_crs, st_agr, st_set_agr, st_precision, and st_set_precision.\nGeometry columns in sf objects can be set or replaced using st_geometry&lt;- or st_set_geometry.",
    "crumbs": [
      "R for Spatial Data Science",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to sf and stars</span>"
    ]
  },
  {
    "objectID": "07-Introsf.html#sec-sfintro",
    "href": "07-Introsf.html#sec-sfintro",
    "title": "7  Introduction to sf and stars",
    "section": "",
    "text": "the name of the (active) geometry column, held in attribute sf_column\n\nfor each non-geometry variable, the attribute-geometry relationship (Section 5.1), held in attribute agr\n\n\n\n\n\n\ncoordinate reference system held in attribute crs\n\nbounding box held in attribute bbox\n\nprecision held in attribute precision\n\nnumber of empty geometries held in attribute n_empty\n\n\n\n\n\nCreation\n An sf object can be created from scratch by\n\nlibrary(sf)\n# Linking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.4.0; sf_use_s2() is TRUE\np1 &lt;- st_point(c(7.35, 52.42))\np2 &lt;- st_point(c(7.22, 52.18))\np3 &lt;- st_point(c(7.44, 52.19))\nsfc &lt;- st_sfc(list(p1, p2, p3), crs = 'OGC:CRS84')\nst_sf(elev = c(33.2, 52.1, 81.2), \n      marker = c(\"Id01\", \"Id02\", \"Id03\"), geom = sfc)\n# Simple feature collection with 3 features and 2 fields\n# Geometry type: POINT\n# Dimension:     XY\n# Bounding box:  xmin: 7.22 ymin: 52.2 xmax: 7.44 ymax: 52.4\n# Geodetic CRS:  WGS 84 (CRS84)\n#   elev marker              geom\n# 1 33.2   Id01 POINT (7.35 52.4)\n# 2 52.1   Id02 POINT (7.22 52.2)\n# 3 81.2   Id03 POINT (7.44 52.2)\n\n\n\n\n\n\n\n\nFigure 7.1: components of an sf object\n\n\n\n\nFigure 7.1 gives an explanation of the components printed. Rather than creating objects from scratch, spatial data in R are typically read from an external source, which can be:\n\n\nexternal file\ntable (or set of tables) in a database\nrequest to a web service\ndataset held in some form in another R package\n\nThe next section introduces reading from files. Section 9.1 discusses handling of datasets too large to fit into working memory.\nReading and writing\nReading datasets from an external “data source” (file, web service, or even string) is done using st_read:\n\n\nlibrary(sf)\n(file &lt;- system.file(\"gpkg/nc.gpkg\", package = \"sf\"))\n# [1] \"/home/runner/work/_temp/Library/sf/gpkg/nc.gpkg\"\nnc &lt;- st_read(file)\n# Reading layer `nc.gpkg' from data source \n#   `/home/runner/work/_temp/Library/sf/gpkg/nc.gpkg' \n#   using driver `GPKG'\n# Simple feature collection with 100 features and 14 fields\n# Geometry type: MULTIPOLYGON\n# Dimension:     XY\n# Bounding box:  xmin: -84.3 ymin: 33.9 xmax: -75.5 ymax: 36.6\n# Geodetic CRS:  NAD27\n\nHere, the file name and path file is read from the sf package, which has a different path on every machine, and hence is guaranteed to be present on every sf installation.\n\nCommand st_read has two arguments: the data source name (dsn) and the layer. In the example above, the geopackage (GPKG) file contains only a single layer that is being read. If it had contained multiple layers, then the first layer would have been read and a warning would have been emitted. The available layers of a dataset can be queried by\n\nst_layers(file)\n# Driver: GPKG \n# Available layers:\n#   layer_name geometry_type features fields crs_name\n# 1    nc.gpkg Multi Polygon      100     14    NAD27\n\nSimple feature objects can be written with st_write, as in\n\n(file = tempfile(fileext = \".gpkg\"))\n# [1] \"/tmp/RtmpuS8E48/file3351764cee79.gpkg\"\nst_write(nc, file, layer = \"layer_nc\")\n# Writing layer `layer_nc' to data source \n#   `/tmp/RtmpuS8E48/file3351764cee79.gpkg' using driver `GPKG'\n# Writing 100 features with 14 fields and geometry type Multi Polygon.\n\nwhere the file format (GPKG) is derived from the file name extension. Using argument append, st_write can either append records to an existing layer or replace it; if unset, it will err if a layer already exists. The tidyverse-style write_sf will replace silently if append has not been set. Layers can also be deleted using st_delete, which is convenient in particular when they are associated with tables in a database.\nFor file formats supporting a WKT-2 coordinate reference system, sf_read and sf_write will read and write it. For simple formats such as csv this will not work. The shapefile format supports only a very limited encoding of the CRS.\n \nSubsetting\n \nA very common operation is to subset objects; base R can use [ for this. The rules that apply to data.frame objects also apply to sf objects: records 2-5 and columns 3-7 are selected by\n\nnc[2:5, 3:7]\n\nbut with a few additional features, in particular:\n\nthe drop argument is by default FALSE meaning that the geometry column is always selected, and an sf object is returned; when it is set to TRUE and the geometry column is not selected, it is dropped and a data.frame is returned\nselection with a spatial (sf, sfc or sfg) object as first argument leads to selection of the features that spatially intersect with that object (see next section); other predicates than intersects can be chosen by setting parameter op to a function such as st_covers or any other binary predicate function listed in Section 3.2.2.\nBinary predicates\n\nBinary predicates like st_intersects, st_covers and such (Section 3.2.2) take two sets of features or feature geometries and return for all pairs whether the predicate is TRUE or FALSE. For large sets this would potentially result in a huge matrix, typically filled mostly with FALSE values and for that reason a sparse representation is returned by default:\n\nnc5 &lt;- nc[1:5, ]\nnc7 &lt;- nc[1:7, ]\n(i &lt;- st_intersects(nc5, nc7))\n# Sparse geometry binary predicate list of length 5, where the\n# predicate was `intersects'\n#  1: 1, 2\n#  2: 1, 2, 3\n#  3: 2, 3\n#  4: 4, 7\n#  5: 5, 6\n\n\nCodeplot(st_geometry(nc7))\nplot(st_geometry(nc5), add = TRUE, border = \"brown\")\ncc = st_coordinates(st_centroid(st_geometry(nc7)))\ntext(cc, labels = 1:nrow(nc7), col = \"blue\")\n\n\n\n\n\n\nFigure 7.2: First seven North Carolina counties\n\n\n\n\nFigure 7.2 shows how the intersections of the first five with the first seven counties can be understood. We can transform the sparse logical matrix into a dense matrix by\n\nas.matrix(i)\n#       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]\n# [1,]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n# [2,]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n# [3,] FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE\n# [4,] FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE\n# [5,] FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE\n\nThe number of counties that each of nc5 intersects with is\n\nlengths(i)\n# [1] 2 3 2 2 2\n\nand the other way around, the number of counties in nc5 that intersect with each of the counties in nc7 is\n\nlengths(t(i))\n# [1] 2 3 2 1 1 1 1\n\nThe object i is of class sgbp (sparse geometrical binary predicate), and is a list of integer vectors, with each element representing a row in the logical predicate matrix holding the column indices of the TRUE values for that row. It further holds some metadata like the predicate used, and the total number of columns. Methods available for sgbp objects include\n\nmethods(class = \"sgbp\")\n#  [1] as.data.frame as.matrix     coerce        dim          \n#  [5] initialize    Ops           print         show         \n#  [9] slotsFromS3   t            \n# see '?methods' for accessing help and source code\n\nwhere the only Ops method available is !, the negation operation.\ntidyverse\n \nThe tidyverse package loads a collection of data science packages that work well together (Wickham and Grolemund 2017; Wickham et al. 2019). Package sf has tidyverse-style read and write functions, read_sf and write_sf that\n\nreturn a tibble rather than a data.frame,\ndo not print any output, and\noverwrite existing data by default.\n\nFurther tidyverse generics with methods for sf objects include filter, select, group_by, ungroup, mutate, transmute, rowwise, rename, slice, summarise, distinct, gather, pivot_longer, spread, nest, unnest, unite, separate, separate_rows, sample_n, and sample_frac. Most of these methods simply manage the metadata of sf objects and make sure the geometry remains present. In case a user wants the geometry to be removed, one can use st_drop_geometry or simply coerce to a tibble or data.frame before selecting:\n\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nnc |&gt; as_tibble() |&gt; select(BIR74) |&gt; head(3)\n# # A tibble: 3 × 1\n#   BIR74\n#   &lt;dbl&gt;\n# 1  1091\n# 2   487\n# 3  3188\n\n\nThe summarise method for sf objects has two special arguments:\n\n\ndo_union (default TRUE) determines whether grouped geometries are unioned on return, so that they form a valid geometry\n\nis_coverage (default FALSE) in case the geometries grouped form a coverage (do not have overlaps), setting this to TRUE speeds up the unioning\n\nThe distinct method selects distinct records, where st_equals is used to evaluate distinctness of geometries.\nfilter can be used with the usual predicates; when one wants to use it with a spatial predicate, for instance to select all counties less than 50 km away from Orange County, one could use\n\norange &lt;- nc |&gt; dplyr::filter(NAME == \"Orange\")\nwd &lt;- st_is_within_distance(nc, orange, \n                            units::set_units(50, km))\no50 &lt;- nc |&gt; dplyr::filter(lengths(wd) &gt; 0)\nnrow(o50)\n# [1] 17\n\n(where we use dplyr::filter rather than filter to avoid confusion with filter from base R.)\nFigure 7.3 shows the results of this analysis, and in addition a buffer around the county borders. Note that this buffer serves for illustration: it was not used to select the counties.\n\nCodeog &lt;- st_geometry(orange)\nbuf50 &lt;- st_buffer(og, units::set_units(50, km))\nall &lt;- c(buf50, st_geometry(o50))\nplot(st_geometry(o50), lwd = 2, extent = all)\nplot(og, col = 'orange', add = TRUE)\nplot(buf50, add = TRUE, col = NA, border = 'brown')\nplot(st_geometry(nc), add = TRUE, border = 'grey')\n\n\n\n\n\n\nFigure 7.3: Orange County (orange), counties within a 50 km radius (black), a 50~km buffer around Orange County (brown), and remaining counties (grey)",
    "crumbs": [
      "R for Spatial Data Science",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to sf and stars</span>"
    ]
  },
  {
    "objectID": "07-Introsf.html#spatial-joins",
    "href": "07-Introsf.html#spatial-joins",
    "title": "7  Introduction to sf and stars",
    "section": "\n7.2 Spatial joins",
    "text": "7.2 Spatial joins\n \nIn regular (left, right, or inner) joins, joined records from a pair of tables are reported when one or more selected attributes match (are identical) in both tables. A spatial join is similar, but the criterion to join records is not equality of attributes but a spatial predicate. This leaves a wide variety of options in order to define spatially matching records, using binary predicates listed in Section 3.2.2. The concepts of “left”, “right”, “inner”, or “full” joins remain identical to the non-spatial join as the options for handling records that have no spatial match.\nWhen using spatial joins, each record may have several matched records, yielding a large result table. A way to reduce this complexity may be to select from the matching records the one with the largest overlap with the target geometry. An example of this is shown (visually) in Figure 7.4; this is done using st_join with argument largest = TRUE.\n\nCode# example of largest = TRUE:\nsystem.file(\"gpkg/nc.gpkg\", package=\"sf\") |&gt; \n    read_sf() |&gt;\n    st_transform('EPSG:2264') -&gt; nc\ngr &lt;- st_sf(\n         label = apply(expand.grid(1:10, LETTERS[10:1])[,2:1], 1, paste0, collapse = \"\"),\n         geom = st_make_grid(nc))\ngr$col &lt;- sf.colors(10, categorical = TRUE, alpha = .3)\n# cut, to verify that NA's work out:\ngr &lt;- gr[-(1:30),]\nsuppressWarnings(nc_j &lt;- st_join(nc, gr, largest = TRUE))\npar(mfrow = c(2,1), mar = rep(0,4))\nplot(st_geometry(nc_j), border = 'grey')\nplot(st_geometry(gr), add = TRUE, col = gr$col)\ntext(st_coordinates(st_centroid(st_geometry(gr))), labels = gr$label, cex = .85)\n# the joined dataset:\nplot(st_geometry(nc_j), border = 'grey', col = nc_j$col)\ntext(st_coordinates(st_centroid(st_geometry(nc_j))), labels = nc_j$label, cex = .7)\nplot(st_geometry(gr), border = '#88ff88aa', add = TRUE)\n\n\n\n\n\n\nFigure 7.4: Example of st_join with largest = TRUE: the label of the polygon in the top figure with the largest intersection with polygons in the bottom figure is assigned to the polygons of the bottom figure.\n\n\n\n\n \nAnother way to reduce the result set is to use aggregate after a join, to merge all matching records, and union their geometries; see Section 5.4.\nSampling, gridding, interpolating\nSeveral convenience functions are available in package sf, some of which will be discussed here. Function st_sample generates a sample of points randomly sampled from target geometries, where target geometries can be point, line, or polygon geometries. Sampling strategies can be (completely) random, regular, or (with polygons) triangular. Chapter 11 explains how spatial sampling (or point pattern simulation) methods available in package spatstat are interfaced through st_sample.\nFunction st_make_grid creates a square, rectangular, or hexagonal grid over a region, or points with the grid centres or corners. It was used to create the rectangular grid in Figure 7.4.\n\nFunction st_interpolate_aw “interpolates” area values to new areas, as explained in Section 5.3, both for intensive and extensive variables.",
    "crumbs": [
      "R for Spatial Data Science",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to sf and stars</span>"
    ]
  },
  {
    "objectID": "07-Introsf.html#sec-ccw",
    "href": "07-Introsf.html#sec-ccw",
    "title": "7  Introduction to sf and stars",
    "section": "\n7.3 Ellipsoidal coordinates",
    "text": "7.3 Ellipsoidal coordinates\nUnprojected data have ellipsoidal coordinates, expressed in degrees east and north. As explained in Section 4.1, “straight” lines between points are the curved shortest paths (“geodesic”). By default, sf uses geometrical operations from the s2geometry library, interfaced through the s2 package (Dunnington, Pebesma, and Rubak 2023), and we see for instance that the point\n\n\"POINT(50 50.1)\" |&gt; st_as_sfc(crs = \"OGC:CRS84\") -&gt; pt\n\nfalls inside the polygon:\n\n\"POLYGON((40 40, 60 40, 60 50, 40 50, 40 40))\" |&gt;\n  st_as_sfc(crs = \"OGC:CRS84\") -&gt; pol\nst_intersects(pt, pol)\n# Sparse geometry binary predicate list of length 1, where the\n# predicate was `intersects'\n#  1: 1\n\nas illustrated by Figure 7.5 (left: straight lines on an orthographic projection centred at the plot area).\n\nCodepar(mfrow = c(1, 2))\npar(mar = c(2.1, 2.1, 1.2, .5))\northo &lt;- st_crs(\"+proj=ortho +lon_0=50 +lat_0=45\")\npol |&gt; st_transform(ortho) |&gt; plot(axes = TRUE, graticule = TRUE, \n                                   main = 's2geometry')\npt |&gt; st_transform(ortho) |&gt; plot(add = TRUE, pch = 16, col = 'red')\n# second plot:\nplot(pol, axes = TRUE, graticule = TRUE, main = 'GEOS')\nplot(pt, add = TRUE, pch = 16, col = 'red')\n\n\n\n\n\n\nFigure 7.5: Intersection depends on whether we use geodesics/great circle arcs (left: s2) or Cartesian coordinates (right)\n\n\n\n\nIf one wants sf to use ellipsoidal coordinates as if they are Cartesian coordinates, the use of s2 can be switched off:\n\nold &lt;- sf_use_s2(FALSE)\n# Spherical geometry (s2) switched off\nst_intersects(pol, pt)\n# although coordinates are longitude/latitude, st_intersects assumes\n# that they are planar\n# Sparse geometry binary predicate list of length 1, where the\n# predicate was `intersects'\n#  1: (empty)\nsf_use_s2(old) # restore\n# Spherical geometry (s2) switched on\n\nand the intersection is empty, as illustrated by Figure 7.5 (right: straight lines on an equidistant cylindrical projection). The warning indicates the planar assumption for ellipsoidal coordinates.\nUse of s2 can be switched off for reasons of performance or compatibility with legacy implementations. The underlying libraries, GEOS for Cartesian and s2geometry for spherical geometry (Figure 1.7) were developed with different motivations, and their use through sf differs in some respects:\n\ncertain operations may be much faster in one compared to the other,\ncertain functions may be available in only one of them (like st_relate being only present in GEOS),\nwhen using transformers, GEOS returns exterior polygon rings noded clockwise (st_sfc(..., check_ring_dir = TRUE) can be used to revert to counter-clockwise), s2geometry returns them counter-clockwise",
    "crumbs": [
      "R for Spatial Data Science",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to sf and stars</span>"
    ]
  },
  {
    "objectID": "07-Introsf.html#package-stars",
    "href": "07-Introsf.html#package-stars",
    "title": "7  Introduction to sf and stars",
    "section": "\n7.4 Package stars\n",
    "text": "7.4 Package stars\n\n \nAlthough package sp has always had limited support for raster data, over the last decade R package raster (Hijmans 2023a) has clearly been dominant as the prime package for powerful, flexible, and scalable raster analysis. The raster data model of package raster (and its successor, terra (Hijmans 2023b)) is that of a 2D regular raster, or a set of raster layers (“raster stack”). This aligns with the classical static “GIS view”, where the world is modelled as a set of layers, each representing a different theme. A lot of data available today however is dynamic, and comes as time series of rasters or raster stacks. A raster stack does not meaningfully reflect this, requiring the user to keep a register of which layer represents what.\nAlso, the raster package and its successor terra do an excellent job in scaling computations up to data sizes no larger than the local storage (the computer’s hard drives), and doing this fast. Recent datasets, however, including satellite imagery, climate model or weather forecasting data, often no longer fit in local storage (Chapter 9). Package spacetime (Pebesma 2012, 2022) did address the analysis of time series of vector geometries or raster grid cells, but did not extend to higher-dimensional arrays or datasets too large to fit in main memory.\n \nHere, we introduce package stars for analysing raster and vector data cubes. The package:\n\nallows for representing dynamic (time varying) raster stacks\naims at being scalable, also beyond local disk size\nprovides a strong integration of raster functions in the GDAL library\nhandles, in addition to regular grids, rotated, sheared, rectilinear, and curvilinear rasters (Figure 1.6)\nprovides a tight integration with package sf\n\nhandles array data with non-raster spatial dimensions, the vector data cubes\n\nfollows the tidyverse design principles\n\nVector data cubes include for instance time series for simple features, or spatial graph data such as potentially dynamic origin-destination matrices. The concept of spatial vector and raster data cubes was explained in Chapter 6. Irregular spacetime observations can be represented by sftime objects provided by package sftime (Teickner, Pebesma, and Graeler 2022), which extend sf objects with a time column (Section 13.3).\nReading and writing raster data\n \nRaster data typically are read from a file. We use a dataset containing a section of Landsat 7 scene, with the six 30~m-resolution bands (bands 1-5 and 7) for a region covering the city of Olinda, Brazil. We can read the example GeoTIFF file holding a regular, non-rotated grid from the package stars:\n\ntif &lt;- system.file(\"tif/L7_ETMs.tif\", package = \"stars\")\nlibrary(stars)\n# Loading required package: abind\n(r &lt;- read_stars(tif))\n# stars object with 3 dimensions and 1 attribute\n# attribute(s):\n#              Min. 1st Qu. Median Mean 3rd Qu. Max.\n# L7_ETMs.tif     1      54     69 68.9      86  255\n# dimension(s):\n#      from  to  offset delta            refsys point x/y\n# x       1 349  288776  28.5 SIRGAS 2000 / ... FALSE [x]\n# y       1 352 9120761 -28.5 SIRGAS 2000 / ... FALSE [y]\n# band    1   6      NA    NA                NA    NA\n\nwhere we see the offset, cell size, coordinate reference system, and dimensions. The dimension table contains the following fields for each dimension:\n\n\nfrom: starting index\n\nto: ending index\n\noffset: dimension value at the start (edge) of the first pixel\n\ndelta: cell size; negative delta values indicate that pixel index increases with decreasing dimension values\n\nrefsys: reference system\n\npoint: logical, indicates whether cell values have point support or cell support\n\nx/y: indicates whether a dimension is associated with a spatial raster x- or y-axis\n\nOne further field, values, is hidden here as it is not used. For regular, rotated, or sheared grids or other regularly discretised dimensions (such as time), offset and delta are not NA; for irregular cases, offset and delta are NA and the values property contains one of:\n\nin case of a rectilinear spatial raster or irregular time dimension, the sequence of values or intervals\nin case of a vector data cube, geometries associated with the spatial dimension\nin case of a curvilinear raster, the matrix with coordinate values for each raster cell\nin case of a discrete dimension, the band names or labels associated with the dimension values\n\nThe object r is of class stars and is a simple list of length one, holding a three-dimensional array:\n\nlength(r)\n# [1] 1\nclass(r[[1]])\n# [1] \"array\"\ndim(r[[1]])\n#    x    y band \n#  349  352    6\n\nand in addition holds an attribute with a dimensions table with all the metadata required to know what the array dimensions refer to, obtained by\n\nst_dimensions(r)\n\nWe can get the spatial extent of the array by\n\nst_bbox(r)\n#    xmin    ymin    xmax    ymax \n#  288776 9110729  298723 9120761\n\nRaster data can be written to the local disk using write_stars:\n\ntf &lt;- tempfile(fileext = \".tif\")\nwrite_stars(r, tf)\n\nwhere again the data format (in this case, GeoTIFF) is derived from the file extension. As for simple features, reading and writing uses the GDAL library; the list of available drivers for raster data is obtained by\n\nst_drivers(\"raster\")\n\nSubsetting stars data cubes\n \nData cubes can be subsetted using the [ operator, or using tidyverse verbs. The first option uses [ with the following comma-separated arguments:\n\nattributes first (by name, index, or logical vector)\nfollowed by each dimension.\n\nThis means that r[1:2, 101:200, , 5:10] selects from r attributes 1-2, index 101-200 for dimension 1, and index 5-10 for dimension 3; omitting dimension 2 means that no subsetting takes place. For attributes, attribute name, index or logical vectors can be used. For dimensions, logical vectors are not supported. Selecting discontinuous ranges is supported only when it is a regular sequence. By default, drop is FALSE, when set to TRUE dimensions with a single value are dropped:\n\nr[,1:100, seq(1, 250, 5), 4] |&gt; dim()\n#    x    y band \n#  100   50    1\nr[,1:100, seq(1, 250, 5), 4, drop = TRUE] |&gt; dim()\n#   x   y \n# 100  50\n\nFor selecting particular ranges of dimension values, one can use filter (after loading dplyr):\n\nlibrary(dplyr, warn.conflicts = FALSE)\nfilter(r, x &gt; 289000, x &lt; 290000)\n# stars object with 3 dimensions and 1 attribute\n# attribute(s):\n#              Min. 1st Qu. Median Mean 3rd Qu. Max.\n# L7_ETMs.tif     5      51     63 64.3      75  242\n# dimension(s):\n#      from  to  offset delta            refsys point x/y\n# x       1  35  289004  28.5 SIRGAS 2000 / ... FALSE [x]\n# y       1 352 9120761 -28.5 SIRGAS 2000 / ... FALSE [y]\n# band    1   6       1     1                NA    NA\n\nwhich changes the offset of the \\(x\\) dimension. Particular cube slices can also be obtained with slice, as in\n\nslice(r, band, 3)\n# stars object with 2 dimensions and 1 attribute\n# attribute(s):\n#              Min. 1st Qu. Median Mean 3rd Qu. Max.\n# L7_ETMs.tif    21      49     63 64.4      77  255\n# dimension(s):\n#   from  to  offset delta            refsys point x/y\n# x    1 349  288776  28.5 SIRGAS 2000 / ... FALSE [x]\n# y    1 352 9120761 -28.5 SIRGAS 2000 / ... FALSE [y]\n\nwhich drops the singular dimension band. mutate can be used on stars objects to add new arrays as functions of existing ones, transmute drops existing ones.\nCropping\n \nFurther subsetting can be done using spatial objects of class sf, sfc or bbox, for instance\n\nb &lt;- st_bbox(r) |&gt;\n    st_as_sfc() |&gt;\n    st_centroid() |&gt;\n    st_buffer(units::set_units(500, m))\nr[b]\n# stars object with 3 dimensions and 1 attribute\n# attribute(s):\n#              Min. 1st Qu. Median Mean 3rd Qu. Max. NA's\n# L7_ETMs.tif    22      54     66 67.7    78.2  174 2184\n# dimension(s):\n#      from  to  offset delta            refsys point x/y\n# x     157 193  288776  28.5 SIRGAS 2000 / ... FALSE [x]\n# y     159 194 9120761 -28.5 SIRGAS 2000 / ... FALSE [y]\n# band    1   6      NA    NA                NA    NA\n\nselects the circular centre region with a diameter of 500 metre, which for the first band is shown in Figure 7.6,\n\nCodeplot(r[b][,,,1], reset = FALSE)\nplot(b, border = 'brown', lwd = 2, col = NA, add = TRUE)\n\n\n\n\n\n\nFigure 7.6: Circular centre region of the Landsat 7 scene (band 1)\n\n\n\n\nwhere we see that pixels outside the spatial object are assigned NA values. This object still has dimension indexes relative to the offset and delta values of r; we can reset these to a new offset with\n\nr[b] |&gt; st_normalize() |&gt; st_dimensions()\n#      from to  offset delta            refsys point x/y\n# x       1 37  293222  28.5 SIRGAS 2000 / ... FALSE [x]\n# y       1 36 9116258 -28.5 SIRGAS 2000 / ... FALSE [y]\n# band    1  6      NA    NA                NA    NA\n\nBy default, the resulting raster is cropped to the extent of the selection object; an object with the same dimensions as the input object is obtained with\n\nr[b, crop = FALSE]\n# stars object with 3 dimensions and 1 attribute\n# attribute(s):\n#              Min. 1st Qu. Median Mean 3rd Qu. Max.   NA's\n# L7_ETMs.tif    22      54     66 67.7    78.2  174 731280\n# dimension(s):\n#      from  to  offset delta            refsys point x/y\n# x       1 349  288776  28.5 SIRGAS 2000 / ... FALSE [x]\n# y       1 352 9120761 -28.5 SIRGAS 2000 / ... FALSE [y]\n# band    1   6      NA    NA                NA    NA\n\nCropping a stars object can alternatively be done directly with st_crop, as in\n\nst_crop(r, b)\n\nRedimensioning and combining stars objects\n \nPackage stars uses package abind (Plate and Heiberger 2016) for a number of array manipulations. One of them is aperm which transposes an array by permuting it. A method for stars objects is provided, and\n\naperm(r, c(3, 1, 2))\n# stars object with 3 dimensions and 1 attribute\n# attribute(s):\n#              Min. 1st Qu. Median Mean 3rd Qu. Max.\n# L7_ETMs.tif     1      54     69 68.9      86  255\n# dimension(s):\n#      from  to  offset delta            refsys point x/y\n# band    1   6      NA    NA                NA    NA    \n# x       1 349  288776  28.5 SIRGAS 2000 / ... FALSE [x]\n# y       1 352 9120761 -28.5 SIRGAS 2000 / ... FALSE [y]\n\npermutes the order of dimensions of the resulting object.\nAttributes and dimensions can be swapped, using split and merge:\n\n(rs &lt;- split(r))\n# stars object with 2 dimensions and 6 attributes\n# attribute(s):\n#     Min. 1st Qu. Median Mean 3rd Qu. Max.\n# X1    47      67     78 79.1      89  255\n# X2    32      55     66 67.6      79  255\n# X3    21      49     63 64.4      77  255\n# X4     9      52     63 59.2      75  255\n# X5     1      63     89 83.2     112  255\n# X6     1      32     60 60.0      88  255\n# dimension(s):\n#   from  to  offset delta            refsys point x/y\n# x    1 349  288776  28.5 SIRGAS 2000 / ... FALSE [x]\n# y    1 352 9120761 -28.5 SIRGAS 2000 / ... FALSE [y]\nmerge(rs, name = \"band\") |&gt; setNames(\"L7_ETMs\")\n# stars object with 3 dimensions and 1 attribute\n# attribute(s):\n#          Min. 1st Qu. Median Mean 3rd Qu. Max.\n# L7_ETMs     1      54     69 68.9      86  255\n# dimension(s):\n#      from  to  offset delta            refsys point    values x/y\n# x       1 349  288776  28.5 SIRGAS 2000 / ... FALSE      NULL [x]\n# y       1 352 9120761 -28.5 SIRGAS 2000 / ... FALSE      NULL [y]\n# band    1   6      NA    NA                NA    NA X1,...,X6\n\nsplit distributes the band dimension over six attributes of a two-dimensional array, merge reverses this operation. st_redimension can be used for more generic operations, such as splitting a single array dimension over two new dimensions:\n\nst_redimension(r, c(x = 349, y = 352, b1 = 3, b2 = 2))\n# stars object with 4 dimensions and 1 attribute\n# attribute(s):\n#              Min. 1st Qu. Median Mean 3rd Qu. Max.\n# L7_ETMs.tif     1      54     69 68.9      86  255\n# dimension(s):\n#    from  to  offset delta            refsys point x/y\n# x     1 349  288776  28.5 SIRGAS 2000 / ... FALSE [x]\n# y     1 352 9120761 -28.5 SIRGAS 2000 / ... FALSE [y]\n# b1    1   3      NA    NA                NA    NA    \n# b2    1   2      NA    NA                NA    NA\n\nMultiple stars object with identical dimensions can be combined using c. By default, the arrays are combined as additional attributes, but by specifying an along argument, the arrays are merged along a new dimension:\n\nc(r, r, along = \"new_dim\")\n# stars object with 4 dimensions and 1 attribute\n# attribute(s), summary of first 1e+05 cells:\n#              Min. 1st Qu. Median Mean 3rd Qu. Max.\n# L7_ETMs.tif    47      65     76 77.3      87  255\n# dimension(s):\n#         from  to  offset delta            refsys point x/y\n# x          1 349  288776  28.5 SIRGAS 2000 / ... FALSE [x]\n# y          1 352 9120761 -28.5 SIRGAS 2000 / ... FALSE [y]\n# band       1   6      NA    NA                NA    NA    \n# new_dim    1   2      NA    NA                NA    NA\n\nthe use of this is illustrated in Section 7.5.2.\nExtracting point samples, aggregating\n \nA very common use case for raster data cube analysis is the extraction of values at certain locations, or computing aggregations over certain geometries. st_extract extracts point values. We will do this for a few randomly sampled points over the bounding box of r:\n\nset.seed(115517)\npts &lt;- st_bbox(r) |&gt; st_as_sfc() |&gt; st_sample(20)\n(e &lt;- st_extract(r, pts))\n# stars object with 2 dimensions and 1 attribute\n# attribute(s):\n#              Min. 1st Qu. Median Mean 3rd Qu. Max.\n# L7_ETMs.tif    12    41.8     63   61    80.5  145\n# dimension(s):\n#          from to            refsys point\n# geometry    1 20 SIRGAS 2000 / ...  TRUE\n# band        1  6                NA    NA\n#                                           values\n# geometry POINT (293002 ...,...,POINT (290941 ...\n# band                                        NULL\n\nwhich results in a vector data cube with 20 points and 6 bands. (Setting the seed guarantees an identical sample when reproducing, it should not be set to generate further randomly generated points.)\nAnother way of extracting information from data cubes is by aggregating it. One way of doing this is by spatially aggregating values over spatial polygons or lines (Section 6.4). We can for instance compute the maximum pixel value for each of the six bands and for each of the three circles shown in Figure 1.4 (d) by\n\ncircles &lt;- st_sample(st_as_sfc(st_bbox(r)), 3) |&gt;\n    st_buffer(500)\naggregate(r, circles, max)\n# stars object with 2 dimensions and 1 attribute\n# attribute(s):\n#              Min. 1st Qu. Median Mean 3rd Qu. Max.\n# L7_ETMs.tif    73    94.2    117  121     142  205\n# dimension(s):\n#          from to            refsys point\n# geometry    1  3 SIRGAS 2000 / ... FALSE\n# band        1  6                NA    NA\n#                                           values\n# geometry POLYGON ((2913...,...,POLYGON ((2921...\n# band                                        NULL\n\nwhich gives a (vector) data cube with three geometries and six bands. Aggregation over a temporal dimension is done by passing a time variable as the second argument to aggregate, as a\n\nset of time stamps indicating the start of time intervals,\nset of time intervals defined by make_intervals, or\ntime period like \"weeks\", \"5 days\", or \"years\".\nPredictive models\n \nThe typical model prediction workflow in R is as follows:\n\nuse a data.frame with response and predictor variables (covariates)\ncreate a model object based on this data.frame\n\ncall predict with this model object and the data.frame with target predictor variable values\n\nPackage stars provides a predict method for stars objects that essentially wraps the last step, by creating the data.frame, calling the predict method for that, and reconstructing a stars object with the predicted values.\nWe will illustrate this with a trivial two-class example mapping land from sea in the example Landsat dataset, using the sample points extracted above, shown in Figure 7.7.\n\nCodeplot(r[,,,1], reset = FALSE)\ncol &lt;- rep(\"yellow\", 20)\ncol[c(8, 14, 15, 18, 19)] = \"red\"\nst_as_sf(e) |&gt; st_coordinates() |&gt; text(labels = 1:20, col = col)\n\n\n\n\n\n\nFigure 7.7: Randomly chosen sample locations for training data; red: water, yellow: land\n\n\n\n\nFrom this figure, we read “by eye” that the points 8, 14, 15, 18, and 19 are on water, the others on land. Using a linear discriminant (“maximum likelihood”) classifier, we find model predictions as shown in Figure 7.8 by\n\nrs &lt;- split(r)\ntrn &lt;- st_extract(rs, pts)\ntrn$cls &lt;- rep(\"land\", 20)\ntrn$cls[c(8, 14, 15, 18, 19)] &lt;- \"water\"\nmodel &lt;- MASS::lda(cls ~ ., st_drop_geometry(trn))\npr &lt;- predict(rs, model)\n\nHere, we used the MASS:: prefix to avoid loading MASS, as that would mask select from dplyr. The split step is needed to convert the band dimension into attributes, so that they are offered as a set of predictors.\n\n\nCodeplot(pr[1], key.pos = 4, key.width = lcm(3.5), key.length = lcm(2))\n\n\n\n\n\n\nFigure 7.8: Linear discriminant classifier for land/water, based on training data of Figure 7.7\n\n\n\n\nWe also see that the layer plotted in Figure 7.8 is a factor variable, with class labels.\nPlotting raster data\n \n\nCodeplot(r)\n\n\n\n\n\n\nFigure 7.9: Six 30 m Landsat bands downsampled to 90m for Olinda, Br.\n\n\n\n\nWe can use the base plot method for stars objects, where the plot created with plot(r) is shown in Figure 7.9. The default colour scale uses grey tones and stretches them such that colour breaks correspond to data quantiles over all bands (“histogram equalization”). Setting breaks = \"equal\" gives equal width colour breaks; alternatively a sequence of numeric break values can be given. A more familiar view may be the RGB or false colour composite shown in Figure 7.10.\n\nCodepar(mfrow = c(1, 2))\nplot(r, rgb = c(3,2,1), reset = FALSE, main = \"RGB\")    # rgb\nplot(r, rgb = c(4,3,2), main = \"False colour (NIR-R-G)\") # false colour\n\n\n\n\n\n\nFigure 7.10: Two colour composites\n\n\n\n\nFurther details and options are given in Chapter 8.\nAnalysing raster data\n \nElement-wise mathematical functions (Section 6.3.2) on stars objects are just passed on to the arrays. This means that we can call functions and create expressions:\n\nlog(r)\n# stars object with 3 dimensions and 1 attribute\n# attribute(s):\n#              Min. 1st Qu. Median Mean 3rd Qu. Max.\n# L7_ETMs.tif     0    3.99   4.23 4.12    4.45 5.54\n# dimension(s):\n#      from  to  offset delta            refsys point x/y\n# x       1 349  288776  28.5 SIRGAS 2000 / ... FALSE [x]\n# y       1 352 9120761 -28.5 SIRGAS 2000 / ... FALSE [y]\n# band    1   6      NA    NA                NA    NA\nr + 2 * log(r)\n# stars object with 3 dimensions and 1 attribute\n# attribute(s):\n#              Min. 1st Qu. Median Mean 3rd Qu. Max.\n# L7_ETMs.tif     1      62   77.5 77.1    94.9  266\n# dimension(s):\n#      from  to  offset delta            refsys point x/y\n# x       1 349  288776  28.5 SIRGAS 2000 / ... FALSE [x]\n# y       1 352 9120761 -28.5 SIRGAS 2000 / ... FALSE [y]\n# band    1   6      NA    NA                NA    NA\n\nor even mask out certain values:\n\nr2 &lt;- r\nr2[r &lt; 50] &lt;- NA\nr2\n# stars object with 3 dimensions and 1 attribute\n# attribute(s):\n#              Min. 1st Qu. Median Mean 3rd Qu. Max.   NA's\n# L7_ETMs.tif    50      64     75   79      90  255 149170\n# dimension(s):\n#      from  to  offset delta            refsys point x/y\n# x       1 349  288776  28.5 SIRGAS 2000 / ... FALSE [x]\n# y       1 352 9120761 -28.5 SIRGAS 2000 / ... FALSE [y]\n# band    1   6      NA    NA                NA    NA\n\nor unmask areas:\n\nr2[is.na(r2)] &lt;- 0\nr2\n# stars object with 3 dimensions and 1 attribute\n# attribute(s):\n#              Min. 1st Qu. Median Mean 3rd Qu. Max.\n# L7_ETMs.tif     0      54     69   63      86  255\n# dimension(s):\n#      from  to  offset delta            refsys point x/y\n# x       1 349  288776  28.5 SIRGAS 2000 / ... FALSE [x]\n# y       1 352 9120761 -28.5 SIRGAS 2000 / ... FALSE [y]\n# band    1   6      NA    NA                NA    NA\n\n \nDimension-wise, we can apply functions to selected array dimensions (Section 6.3.3) of stars objects similar to how apply does this to arrays. For instance, we can compute for each pixel the mean of the six band values by\n\nst_apply(r, c(\"x\", \"y\"), mean)\n# stars object with 2 dimensions and 1 attribute\n# attribute(s):\n#       Min. 1st Qu. Median Mean 3rd Qu. Max.\n# mean  25.5    53.3   68.3 68.9      82  255\n# dimension(s):\n#   from  to  offset delta            refsys point x/y\n# x    1 349  288776  28.5 SIRGAS 2000 / ... FALSE [x]\n# y    1 352 9120761 -28.5 SIRGAS 2000 / ... FALSE [y]\n\nA more meaningful function would for instance compute the NDVI (normalised differenced vegetation index):\n\nndvi &lt;- function(b1, b2, b3, b4, b5, b6) (b4 - b3)/(b4 + b3)\nst_apply(r, c(\"x\", \"y\"), ndvi)\n# stars object with 2 dimensions and 1 attribute\n# attribute(s):\n#         Min. 1st Qu.  Median    Mean 3rd Qu.  Max.\n# ndvi  -0.753  -0.203 -0.0687 -0.0643   0.187 0.587\n# dimension(s):\n#   from  to  offset delta            refsys point x/y\n# x    1 349  288776  28.5 SIRGAS 2000 / ... FALSE [x]\n# y    1 352 9120761 -28.5 SIRGAS 2000 / ... FALSE [y]\n\nAlternatively, one could have defined\n\nndvi2 &lt;- function(x) (x[4]-x[3])/(x[4]+x[3])\n\nwhich is more convenient if the number of bands is large, but also much slower than ndvi as it needs to be called for every pixel whereas ndvi can be called once for all pixels, or for large chunks of pixels. The mean for each band over the whole image is computed by\n\nst_apply(r, c(\"band\"), mean) |&gt; as.data.frame()\n#   band mean\n# 1    1 79.1\n# 2    2 67.6\n# 3    3 64.4\n# 4    4 59.2\n# 5    5 83.2\n# 6    6 60.0\n\nthe result of which is small enough to be printed here as a data.frame. In these two examples, entire dimensions disappear. Sometimes, this does not happen (Section 6.3.2); we can for instance compute the three quartiles for each band\n\nst_apply(r, c(\"band\"), quantile, c(.25, .5, .75))\n# stars object with 2 dimensions and 1 attribute\n# attribute(s):\n#              Min. 1st Qu. Median Mean 3rd Qu. Max.\n# L7_ETMs.tif    32    60.8   66.5 69.8    78.8  112\n# dimension(s):\n#          from to        values\n# quantile    1  3 25%, 50%, 75%\n# band        1  6          NULL\n\nand see that this creates a new dimension, quantile, with three values. Alternatively, the three quantiles over the six bands for each pixel are obtained by\n\nst_apply(r, c(\"x\", \"y\"), quantile, c(.25, .5, .75))\n# stars object with 3 dimensions and 1 attribute\n# attribute(s):\n#              Min. 1st Qu. Median Mean 3rd Qu. Max.\n# L7_ETMs.tif     4      55   69.2 67.2    81.2  255\n# dimension(s):\n#          from  to  offset delta            refsys point\n# quantile    1   3      NA    NA                NA    NA\n# x           1 349  288776  28.5 SIRGAS 2000 / ... FALSE\n# y           1 352 9120761 -28.5 SIRGAS 2000 / ... FALSE\n#                 values x/y\n# quantile 25%, 50%, 75%    \n# x                 NULL [x]\n# y                 NULL [y]\n\nCurvilinear rasters\n\nThere are several reasons why non-regular rasters occur (Figure 1.6). For one, when the data is Earth-bound, a regular raster does not fit the Earth’s surface, which is curved. Other reasons are:\n\nwhen we convert or transform a regular raster data into another coordinate reference system, it will become curvilinear unless we resample (warp; Section 7.8); warping always comes at the cost of some loss of data and is not reversible\nobservation may lead to irregular rasters: for lower-level satellite products, we may have a regular raster in the direction of the satellite (not aligned with \\(x\\) or \\(y\\)), and rectilinear in the direction perpendicular to that (e.g., when the sensor discretises the viewing angle in equal parts)\nGDAL utils\n \nThe GDAL library typically ships with a number of executable binaries, the GDAL command line utilities for data translation and processing. Several of these utilities (all except for those written in Python) are also available as C functions in the GDAL library, through the “GDAL Algorithms C API”. If an R package like sf that links to the GDAL library uses these C API algorithms, it means that the user no longer needs to install any GDAL binary command line utilities in addition to the R package.\nPackage sf allows calling these C API algorithms through function gdal_utils, where the first argument is the name of the utility (stripped from the gdal prefix):\n\n\ninfo prints information on GDAL (raster) datasets\n\nwarp warps a raster to a new raster, possibly in another CRS\n\nrasterize rasterises a vector dataset\n\ntranslate translates a raster file to another format\n\nvectortranslate (for ogr2ogr) translates a vector file to another format\n\nbuildvrt creates a virtual raster tile (a raster created from several files)\n\ndemprocessing does various processing steps of digital elevation models (dems)\n\nnearblack converts nearly black/white borders to black\n\ngrid creates a regular grid from scattered data\n\nmdiminfo prints information on a multi-dimensional array\n\nmdimtranslate translates a multi-dimensional array into another format\n\nThese utilities work on files, and not directly on sf or stars objects. However, stars_proxy objects are essentially pointers to files, and other objects can be written to file. Several of these utilities are (always or optionally) used, for example by st_mosaic, st_warp, or st_write. R package gdalUtilities (O’Brien 2022) provides further wrapper functions around sf::gdal_utils with function argument names matching the command line arguments of the binary utils.",
    "crumbs": [
      "R for Spatial Data Science",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to sf and stars</span>"
    ]
  },
  {
    "objectID": "07-Introsf.html#vector-data-cube-examples",
    "href": "07-Introsf.html#vector-data-cube-examples",
    "title": "7  Introduction to sf and stars",
    "section": "\n7.5 Vector data cube examples",
    "text": "7.5 Vector data cube examples\n \nExample: aggregating air quality time series\n\nWe use a small excert from the European air quality data base to illustrate aggregation operations on vector data cubes. The same data source was used by Gräler, Pebesma, and Heuvelink (2016), and will be used in Chapter 12 and Chapter 13. Daily average PM\\(_{10}\\) values were computed for rural background stations in Germany, over the period 1998-2009.\nWe can create a stars object from the air matrix, the dates Date vector and the stations SpatialPoints objects by\n\nload(\"data/air.rda\") # this loads several datasets in .GlobalEnv\ndim(air)\n# space  time \n#    70  4383\nstations |&gt;\n    st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) |&gt;\n    st_geometry() -&gt; st\nd &lt;- st_dimensions(station = st, time = dates)\n(aq &lt;- st_as_stars(list(PM10 = air), dimensions = d))\n# stars object with 2 dimensions and 1 attribute\n# attribute(s):\n#       Min. 1st Qu. Median Mean 3rd Qu. Max.   NA's\n# PM10     0    9.92   14.8 17.7      22  274 157659\n# dimension(s):\n#         from   to     offset  delta refsys point\n# station    1   70         NA     NA WGS 84  TRUE\n# time       1 4383 1998-01-01 1 days   Date FALSE\n#                                          values\n# station POINT (9.59 53.7),...,POINT (9.45 49.2)\n# time                                       NULL\n\nWe can see from Figure 7.11 that the time series are quite long, but also have large missing value gaps. Figure 7.12 shows the spatial distribution of measurement stations along with mean PM\\(_{10}\\) values.\n\nCodepar(mar = c(5.1, 4.1, 0.3, 0.1))\nimage(aperm(log(aq), 2:1), main = NULL)\n\n\n\n\n\n\nFigure 7.11: Space time diagram of PM\\(_{10}\\) measurements by time and station\n\n\n\n\n\nCodede_nuts1 &lt;- read_sf(\"data/de_nuts1.gpkg\")\nst_as_sf(st_apply(aq, 1, mean, na.rm = TRUE)) |&gt;\n    plot(reset = FALSE, pch = 16, extent = de_nuts1)\nst_union(de_nuts1) |&gt; plot(add = TRUE)\n\n\n\n\n\n\nFigure 7.12: Locations of PM\\(_{10}\\) measurement stations, showing mean values\n\n\n\n\nWe can aggregate these station time series to area means, mostly as a simple exercise. For this, we use the aggregate method for stars objects\n\n(a &lt;- aggregate(aq, de_nuts1, mean, na.rm = TRUE))\n# stars object with 2 dimensions and 1 attribute\n# attribute(s):\n#       Min. 1st Qu. Median Mean 3rd Qu. Max.  NA's\n# PM10  1.08    10.9   15.3 17.9    21.8  172 25679\n# dimension(s):\n#      from   to     offset  delta refsys point\n# geom    1   16         NA     NA WGS 84 FALSE\n# time    1 4383 1998-01-01 1 days   Date FALSE\n#                                       values\n# geom MULTIPOLYGON (...,...,MULTIPOLYGON (...\n# time                                    NULL\n\nand we can now show the maps for six arbitrarily chosen days (Figure 7.13), using\n\nlibrary(tidyverse)\na |&gt; filter(time &gt;= \"2008-01-01\", time &lt; \"2008-01-07\") |&gt; \n    plot(key.pos = 4)\n\n\n\n\n\n\nFigure 7.13: Areal mean PM\\(_{10}\\) values, for six days\n\n\n\n\n\nor create a time series plot of mean values for a single state (Figure 7.14) by\n\nlibrary(xts) |&gt; suppressPackageStartupMessages()\nplot(as.xts(a)[,4], main = de_nuts1$NAME_1[4])\n\n\n\n\n\n\nFigure 7.14: Areal mean PM\\(_{10}\\) time series for a single state\n\n\n\n\nExample: Bristol origin-destination data cube\n \nThe data used for this example come from Lovelace, Nowosad, and Muenchow (2019), and concern origin-destination (OD) counts: the number of persons going from zone A to zone B, by transportation mode. We have feature geometries in sf object bristol_zones for the 102 origin and destination regions, shown in Figure 7.15.\n\nCodelibrary(spDataLarge)\nplot(st_geometry(bristol_zones), axes = TRUE, graticule = TRUE)\nplot(st_geometry(bristol_zones)[33], col = 'red', add = TRUE)\n\n\n\n\n\n\nFigure 7.15: Origin destination data zones for Bristol, UK, with zone 33 (E02003043) coloured red\n\n\n\n\nOD counts come in a table bristol_od with non-zero OD pairs as records, and transportation mode as variables:\n\nhead(bristol_od)\n# # A tibble: 6 × 7\n#   o         d           all bicycle  foot car_driver train\n#   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n# 1 E02002985 E02002985   209       5   127         59     0\n# 2 E02002985 E02002987   121       7    35         62     0\n# 3 E02002985 E02003036    32       2     1         10     1\n# 4 E02002985 E02003043   141       1     2         56    17\n# 5 E02002985 E02003049    56       2     4         36     0\n# 6 E02002985 E02003054    42       4     0         21     0\n\nThe number of zero OD values is found by subtracting the number of non-zero records from the total number of OD combinations:\n\nnrow(bristol_zones)^2 - nrow(bristol_od) \n# [1] 7494\n\nWe will form a three-dimensional vector data cube with origin, destination, and transportation mode as dimensions. For this, we first “tidy” the bristol_od table to have origin (o), destination (d), transportation mode (mode), and count (n) as variables, using pivot_longer:\n\n# create O-D-mode array:\nbristol_tidy &lt;- bristol_od |&gt; \n    select(-all) |&gt; \n    pivot_longer(3:6, names_to = \"mode\", values_to = \"n\")\nhead(bristol_tidy)\n# # A tibble: 6 × 4\n#   o         d         mode           n\n#   &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt;\n# 1 E02002985 E02002985 bicycle        5\n# 2 E02002985 E02002985 foot         127\n# 3 E02002985 E02002985 car_driver    59\n# 4 E02002985 E02002985 train          0\n# 5 E02002985 E02002987 bicycle        7\n# 6 E02002985 E02002987 foot          35\n\nNext, we form the three-dimensional array a, filled with zeroes:\n\nod &lt;- bristol_tidy |&gt; pull(\"o\") |&gt; unique()\nnod &lt;- length(od)\nmode &lt;- bristol_tidy |&gt; pull(\"mode\") |&gt; unique()\nnmode = length(mode)\na = array(0L,  c(nod, nod, nmode), \n    dimnames = list(o = od, d = od, mode = mode))\ndim(a)\n# [1] 102 102   4\n\nWe see that the dimensions are named with the zone names (o, d) and the transportation mode name (mode). Every row of bristol_tidy denotes a single array entry, and we can use this to fill the non-zero entries of a using the bristol_tidy table to provide index (o, d and mode) and value (n):\n\na[as.matrix(bristol_tidy[c(\"o\", \"d\", \"mode\")])] &lt;- \n        bristol_tidy$n\n\nTo be sure that there is not an order mismatch between the zones in bristol_zones and the zone names in bristol_tidy, we can get the right set of zones by:\n\norder &lt;- match(od, bristol_zones$geo_code)\nzones &lt;- st_geometry(bristol_zones)[order]\n\n(It happens that the order is already correct, but it is good practice to not assume this.)\nNext, with zones and modes we can create a stars dimensions object:\n\nlibrary(stars)\n(d &lt;- st_dimensions(o = zones, d = zones, mode = mode))\n#      from  to refsys point                                  values\n# o       1 102 WGS 84 FALSE MULTIPOLYGON (...,...,MULTIPOLYGON (...\n# d       1 102 WGS 84 FALSE MULTIPOLYGON (...,...,MULTIPOLYGON (...\n# mode    1   4     NA FALSE                       bicycle,...,train\n\nand finally build a stars object from a and d:\n\n(odm &lt;- st_as_stars(list(N = a), dimensions = d))\n# stars object with 3 dimensions and 1 attribute\n# attribute(s):\n#    Min. 1st Qu. Median Mean 3rd Qu. Max.\n# N     0       0      0  4.8       0 1296\n# dimension(s):\n#      from  to refsys point                                  values\n# o       1 102 WGS 84 FALSE MULTIPOLYGON (...,...,MULTIPOLYGON (...\n# d       1 102 WGS 84 FALSE MULTIPOLYGON (...,...,MULTIPOLYGON (...\n# mode    1   4     NA FALSE                       bicycle,...,train\n\nWe can take a single slice through this three-dimensional array, for instance for zone 33 (Figure 7.15), by odm[ , , 33], and plot it with\n\nplot(adrop(odm[,,33]) + 1, logz = TRUE)\n\n\n\n\n\n\nFigure 7.16: OD matrix sliced for destination zone 33, by transportation mode\n\n\n\n\nthe result of which is shown in Figure 7.16. Subsetting this way, we take all attributes (there is only one: N) since the first argument is empty, we take all origin regions (second argument empty), we take destination zone 33 (third argument), and all transportation modes (fourth argument empty, or missing).\nWe plotted this particular zone because it has the largest number of travellers as its destination. We can find this out by summing all origins and travel modes by destination:\n\nd &lt;- st_apply(odm, 2, sum)\nwhich.max(d[[1]])\n# [1] 33\n\nOther aggregations we can carry out include:\nTotal transportation by OD (102 x 102):\n\nst_apply(odm, 1:2, sum)\n# stars object with 2 dimensions and 1 attribute\n# attribute(s):\n#      Min. 1st Qu. Median Mean 3rd Qu. Max.\n# sum     0       0      0 19.2      19 1434\n# dimension(s):\n#   from  to refsys point                                  values\n# o    1 102 WGS 84 FALSE MULTIPOLYGON (...,...,MULTIPOLYGON (...\n# d    1 102 WGS 84 FALSE MULTIPOLYGON (...,...,MULTIPOLYGON (...\n\nOrigin totals, by mode:\n\nst_apply(odm, c(1,3), sum)\n# stars object with 2 dimensions and 1 attribute\n# attribute(s):\n#      Min. 1st Qu. Median Mean 3rd Qu. Max.\n# sum     1    57.5    214  490     771 2903\n# dimension(s):\n#      from  to refsys point                                  values\n# o       1 102 WGS 84 FALSE MULTIPOLYGON (...,...,MULTIPOLYGON (...\n# mode    1   4     NA FALSE                       bicycle,...,train\n\nDestination totals, by mode:\n\nst_apply(odm, c(2,3), sum)\n# stars object with 2 dimensions and 1 attribute\n# attribute(s):\n#      Min. 1st Qu. Median Mean 3rd Qu.  Max.\n# sum     0      13    104  490     408 12948\n# dimension(s):\n#      from  to refsys point                                  values\n# d       1 102 WGS 84 FALSE MULTIPOLYGON (...,...,MULTIPOLYGON (...\n# mode    1   4     NA FALSE                       bicycle,...,train\n\nOrigin totals, summed over modes:\n\no &lt;- st_apply(odm, 1, sum)\n\nDestination totals, summed over modes (we had this):\n\nd &lt;- st_apply(odm, 2, sum)\n\nWe plot o and d together after joining them by\n\nx &lt;- (c(o, d, along = list(od = c(\"origin\", \"destination\"))))\nplot(x, logz = TRUE)\n\n\n\n\n\n\nFigure 7.17: Total commutes, summed by origin (left) or destination (right)\n\n\n\n\nthe result of which is shown in Figure 7.17.\nThere is something to say for the argument that such maps give the wrong message, as both amount (colour) and polygon size give an impression of amount. To take out the amount in the count, we can compute densities (count / km\\(^2\\)), by\n\nlibrary(units)\na &lt;- set_units(st_area(st_as_sf(o)), km^2)\no$sum_km &lt;- o$sum / a\nd$sum_km &lt;- d$sum / a\nod &lt;- c(o[\"sum_km\"], d[\"sum_km\"], along = \n        list(od = c(\"origin\", \"destination\")))\nplot(od, logz = TRUE)\n\n\n\n\n\n\nFigure 7.18: Total commutes per square km, by area of origin (left) or destination (right)\n\n\n\n\nshown in Figure 7.18. Another way to normalize these totals would be to divide them by population size.\nTidy array data\n\nThe tidy data paper (Wickham 2014) may suggest that such array data should be processed not as an array, but in a long (unnormalised) table form where each row holds (region, class, year, value), and it is always good to be able to do this. For primary handling and storage, however, this is often not an option, because:\n\na lot of array data are collected or generated as array data, for instance by imagery or other sensory devices, or by climate models\nit is easier to derive the long table form from the array than vice versa\nthe long table form requires much more memory, since the space occupied by dimension values is \\(O(\\Pi n_i)\\), rather than \\(O(\\Sigma n_i)\\), with \\(n_i\\) the cardinality (size) of dimension \\(i\\)\n\nwhen missing-valued cells are dropped, the long table form loses the implicit indexing of the array form\n\nTo put this argument to the extreme, consider for instance that all image, video and sound data are stored in array form; few people would make a real case for storing them in a long table form instead. Nevertheless, R packages like tsibble (Wang et al. 2022) take this approach, and have to deal with ambiguous ordering of multiple records with identical time steps for different spatial features and index them, which is solved for both automatically by using the array form – at the cost of using dense arrays, in package stars.\nPackage stars tries to follow the tidy manifesto to handle array sets and has particularly developed support for the case where one or more of the dimensions refer to space and/or time.\nFile formats for vector data cubes\nRegular table forms, including the long table form, are possible but clumsy to use: the origin-destination data example above and Chapter 13 illustrate the complexity of recreating a vector data cube from table forms. Array formats like NetCDF or Zarr are designed for storing array data. They can however be used for any data structure, and carry the risk that files once written are hard to reuse. For vector cubes that have a single geometry dimension that consists of either points, (multi)linestrings or (multi)polygons, the CF conventions (Eaton et al. 2022) describe a way to encode such geometries. stars::read_mdim and stars::write_mdim can read and write vector data cubes following these conventions.",
    "crumbs": [
      "R for Spatial Data Science",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to sf and stars</span>"
    ]
  },
  {
    "objectID": "07-Introsf.html#sec-raster-to-vector",
    "href": "07-Introsf.html#sec-raster-to-vector",
    "title": "7  Introduction to sf and stars",
    "section": "\n7.6 Raster-to-vector, vector-to-raster",
    "text": "7.6 Raster-to-vector, vector-to-raster\nSection 1.3 already showed some examples of raster-to-vector and vector-to-raster conversions. This section will add some code details and examples.\nVector-to-raster\n \nst_as_stars is meant as a method to transform objects into stars objects. However, not all stars objects are raster objects, and the method for sf objects creates a vector data cube with the geometry as its spatial (vector) dimension, and attributes as attributes. When given a feature geometry (sfc) object, st_as_stars will rasterise it, as shown in Section 7.8 and in Figure 7.19.\n\nfile &lt;- system.file(\"gpkg/nc.gpkg\", package=\"sf\")\nread_sf(file) |&gt; \n    st_geometry() |&gt;\n    st_as_stars() |&gt;\n    plot(key.pos = 4)\n\n\n\n\n\n\nFigure 7.19: Rasterising vector geometry using st_as_stars\n\n\n\n\nHere, st_as_stars can be parameterised to control cell size, number of cells, and/or extent. The cell values returned are 0 for cells with centre point outside the geometry and 1 for cell with centre point inside or on the border of the geometry. Rasterising existing features is done using st_rasterize, as also shown in Figure 1.5:\n\nlibrary(dplyr)\nread_sf(file) |&gt;\n    mutate(name = as.factor(NAME)) |&gt;\n    select(SID74, SID79, name) |&gt;\n    st_rasterize()\n# stars object with 2 dimensions and 3 attributes\n# attribute(s):\n#      SID74           SID79            name       \n#  Min.   : 0      Min.   : 0      Sampson :  655  \n#  1st Qu.: 3      1st Qu.: 3      Columbus:  648  \n#  Median : 5      Median : 6      Robeson :  648  \n#  Mean   : 8      Mean   :10      Bladen  :  604  \n#  3rd Qu.:10      3rd Qu.:13      Wake    :  590  \n#  Max.   :44      Max.   :57      (Other) :30952  \n#  NA's   :30904   NA's   :30904   NA's    :30904  \n# dimension(s):\n#   from  to offset   delta refsys point x/y\n# x    1 461  -84.3  0.0192  NAD27 FALSE [x]\n# y    1 141   36.6 -0.0192  NAD27 FALSE [y]\n\nSimilarly, line and point geometries can be rasterised, as shown in Figure 7.20.\n\nread_sf(file) |&gt;\n    st_cast(\"MULTILINESTRING\") |&gt;\n    select(CNTY_ID) |&gt;\n    st_rasterize() |&gt;\n    plot(key.pos = 4)\n\n\n\n\n\n\nFigure 7.20: Rasterising the North Carolina county boundaries",
    "crumbs": [
      "R for Spatial Data Science",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to sf and stars</span>"
    ]
  },
  {
    "objectID": "07-Introsf.html#sec-projsf",
    "href": "07-Introsf.html#sec-projsf",
    "title": "7  Introduction to sf and stars",
    "section": "\n7.7 Coordinate transformations and conversions",
    "text": "7.7 Coordinate transformations and conversions\n \nst_crs\n\nSpatial objects of class sf or stars contain a coordinate reference system that can be retrieved or replaced with st_crs, or can be set or replaced in a pipe with st_set_crs. Coordinate reference systems can be set with an EPSG code, like st_crs(4326) which will be converted to st_crs('EPSG:4326'), or with a PROJ.4 string like \"+proj=utm +zone=25 +south\", a name like “WGS84”, or a name preceded by an authority like “OGC:CRS84”. Alternatives include a coordinate reference system definition in WKT, WKT-2 (Section 2.5) or PROJJSON. The object returned by st_crs contains two fields:\n\n\nwkt with the WKT-2 representation\n\ninput with the user input, if any, or a human readable description of the coordinate reference system, if available\n\nNote that PROJ.4 strings can be used to define some coordinate reference systems, but they cannot be used to represent coordinate reference systems. Conversion of a WKT-2 in a crs object to a proj4string using the $proj4string method, as in\n\nx &lt;- st_crs(\"OGC:CRS84\")\nx$proj4string\n# [1] \"+proj=longlat +datum=WGS84 +no_defs\"\n\nmay succeed but is not in general lossless or invertible. Using PROJ.4 strings, for instance to define a parameterised, projected coordinate reference system is fine as long as it is associated with the WGS84 datum.\n\nst_transform, sf_project\n\n \nCoordinate transformations or conversions (Section 2.4) for sf or stars objects are carried out with st_transform, which takes as its first argument a spatial object of class sf or stars that has a coordinate reference system set, and as a second argument with an crs object (or something that can be converted to it with st_crs). When PROJ finds more than one possibility to transform or convert from the source crs to the target crs, it chooses the one with the highest declared accuracy. More fine-grained control over the options is explained in Section 7.7.5. For stars objects with regular raster dimensions, st_transform will only transform coordinates and always result in a curvilinear grid. st_warp can be used to create a regular raster in a new coordinate reference system, using regridding (Section 7.8).\nA lower-level function to transform or convert coordinates not in sf or stars objects is sf_project: it takes a matrix with coordinates and a source and target crs, and it returns transformed or converted coordinates.\nsf_proj_info\n\nFunction sf_proj_info can be used to query the available projections, ellipsoids, units and prime meridians available in the PROJ software. It takes a single parameter, type, which can have the following values:\n\n\ntype = \"proj\" lists the short and long names of available projections; short names can be used in a “+proj=name” string\n\ntype = \"ellps\" lists the available ellipses, with name, long name, and ellipsoidal parameters\n\ntype = \"units\" lists the available length units, with conversion constant to meters\n\ntype = \"prime_meridians\" lists the prime meridians with their position with respect to the Greenwich meridian\nDatum grids, proj.db, cdn.proj.org, local cache\n \nDatum grids (Section 2.4) can be installed locally, or be read from the PROJ datum grid CDN at https://cdn.proj.org/. If installed locally, they are read from the PROJ search path, which is shown by\n\nsf_proj_search_paths()\n# [1] \"/home/runner/.local/share/proj\"\n# [2] \"/usr/share/proj\"\n\nThe main PROJ database is proj.db, an sqlite3 database typically found at\n\npaste0(tail(sf_proj_search_paths(), 1), .Platform$file.sep, \n       \"proj.db\")\n# [1] \"/usr/share/proj/proj.db\"\n\nwhich can be read. The version of the snapshot of the EPSG database included in each PROJ release is stated in the \"metadata\" table of proj.db; the version of the PROJ runtime used by sf is shown by\n\nsf_extSoftVersion()[\"PROJ\"]\n#    PROJ \n# \"9.4.0\"\n\nIf for a particular coordinate transformation datum grids are not locally found, PROJ will search for online datum grids in the PROJ CDN when\n\nsf_proj_network()\n# [1] FALSE\n\nreturns TRUE. By default it is set to FALSE, but\n\nsf_proj_network(TRUE)\n# [1] \"https://cdn.proj.org\"\n\nsets it to TRUE and returns the URL of the network resource used. This resource can also be set to another resource, that may be faster or less limited.\nAfter querying a datum grid on the CDN, PROJ writes the portion of the grid queried (not, by default, the entire grid) to a local cache, which is another sqlite3 database found locally in a user directory that is listed by\n\nlist.files(sf_proj_search_paths()[1], full.names = TRUE)\n# character(0)\n\nand that will be searched first in subsequent datum grid queries.\nTransformation pipelines\n\nInternally, PROJ uses a so-called coordinate operation pipeline, to represent the sequence of operations to get from a source CRS to a target CRS. Given multiple options to go from source to target, st_transform chooses the one with highest accuracy. We can query the options available by sf_proj_pipelines:\n\n(p &lt;- sf_proj_pipelines(\"OGC:CRS84\", \"EPSG:22525\"))\n# Candidate coordinate operations found:  5 \n# Strict containment:     FALSE \n# Axis order auth compl:  FALSE \n# Source:  OGC:CRS84 \n# Target:  EPSG:22525 \n# Best instantiable operation has accuracy: 2 m\n# Description: axis order change (2D) + Inverse of Corrego Alegre\n#              1970-72 to WGS 84 (2) + UTM zone 25S\n# Definition:  +proj=pipeline +step +proj=unitconvert +xy_in=deg\n#              +xy_out=rad +step +inv +proj=hgridshift\n#              +grids=br_ibge_CA7072_003.tif +step\n#              +proj=utm +zone=25 +south +ellps=intl\n\nand see that pipeline with the highest accuracy is summarised; we can see that it specifies use of a datum grid. Had we not switched on the network search, we would have obtained a different result:\n\nsf_proj_network(FALSE)\n# character(0)\nsf_proj_pipelines(\"OGC:CRS84\", \"EPSG:22525\")\n# Candidate coordinate operations found:  5 \n# Strict containment:     FALSE \n# Axis order auth compl:  FALSE \n# Source:  OGC:CRS84 \n# Target:  EPSG:22525 \n# Best instantiable operation has accuracy: 5 m\n# Description: axis order change (2D) + Inverse of Corrego Alegre\n#              1970-72 to WGS 84 (4) + UTM zone 25S\n# Definition:  +proj=pipeline +step +proj=unitconvert +xy_in=deg\n#              +xy_out=rad +step +proj=push +v_3 +step\n#              +proj=cart +ellps=WGS84 +step\n#              +proj=helmert +x=206.05 +y=-168.28\n#              +z=3.82 +step +inv +proj=cart\n#              +ellps=intl +step +proj=pop +v_3 +step\n#              +proj=utm +zone=25 +south +ellps=intl\n# Operation 4 is lacking 1 grid with accuracy 2 m\n# Missing grid: br_ibge_CA7072_003.tif \n# URL: https://cdn.proj.org/br_ibge_CA7072_003.tif\n\nand a report that a datum grid is missing. The object returned by sf_proj_pipelines is a sub-classed data.frame, with columns\n\nnames(p)\n# [1] \"id\"           \"description\"  \"definition\"   \"has_inverse\" \n# [5] \"accuracy\"     \"axis_order\"   \"grid_count\"   \"instantiable\"\n# [9] \"containment\"\n\nand we can list for instance the accuracies by\n\np |&gt; pull(accuracy)\n# [1]  2  5  5  8 NA\n\nHere, NA refers to “ballpark accuracy”, which may be anything in the 30-120 m range:\n\np |&gt; filter(is.na(accuracy))\n# Candidate coordinate operations found:  1 \n# Strict containment:     FALSE \n# Axis order auth compl:  FALSE \n# Source:  OGC:CRS84 \n# Target:  EPSG:22525 \n# Best instantiable operation has only ballpark accuracy \n# Description: Ballpark geographic offset from WGS 84 (CRS84) to\n#              Corrego Alegre 1970-72 + UTM zone 25S\n# Definition:  +proj=pipeline +step +proj=unitconvert +xy_in=deg\n#              +xy_out=rad +step +proj=utm +zone=25\n#              +south +ellps=intl\n\nThe default, most accurate pipeline chosen by st_transform can be overridden by specifying pipeline argument, as selected from the set of options in p$definition.\nAxis order and direction\n \nAs mentioned in Section 2.5, EPSG:4326 defines the first axis to be associated with latitude and the second with longitude; this is also the case for a number of other ellipsoidal coordinate reference systems. Although this is how the authority (EPSG) prescribes this, it is not how most datasets are currently stored. As most other software, package sf by default ignores this and interprets ellipsoidal coordinate pairs as (longitude, latitude) by default. If however data needs to be read from a data source that is compliant to the authority, for instance from a WFS service, one can set\n\nst_axis_order(TRUE)\n\nto globally instruct sf, when calling GDAL and PROJ routines, that authority compliance (latitude, longitude order) is assumed. It is anticipated that problems may happen in case of authority compliance, for instance when with plotting data. The plot method for sf objects respects the axis order flag and will swap coordinates using the transformation pipeline \"+proj=pipeline +step +proj=axisswap +order=2,1\" before plotting them, but geom_sf in ggplot2 has not been modified to do this. As mentioned earlier, the axis order ambiguity of EPSG:4326 is resolved by replacing it with OGC:CRS84.\nIndependent from the axis order, not all coordinate reference systems have an axis direction that is positive in North and East directions. Most plotting functions in R will not work with data that have axes defined in opposite directions. Information on axes directions and units can be retrieved by\n\nst_crs(4326)$axes\n#                 name orientation\n# 1  Geodetic latitude           1\n# 2 Geodetic longitude           3\nst_crs(4326)$ud_unit\n# 1 [°]\nst_crs(\"EPSG:2053\")$axes\n#       name orientation\n# 1  Westing           4\n# 2 Southing           2\nst_crs(\"EPSG:2053\")$ud_unit\n# 1 [m]",
    "crumbs": [
      "R for Spatial Data Science",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to sf and stars</span>"
    ]
  },
  {
    "objectID": "07-Introsf.html#sec-warp",
    "href": "07-Introsf.html#sec-warp",
    "title": "7  Introduction to sf and stars",
    "section": "\n7.8 Transforming and warping rasters",
    "text": "7.8 Transforming and warping rasters\n \nWhen using st_transform on a raster dataset, as in\n\ntif &lt;- system.file(\"tif/L7_ETMs.tif\", package = \"stars\")\nread_stars(tif) |&gt;\n    st_transform('OGC:CRS84')\n# stars object with 3 dimensions and 1 attribute\n# attribute(s):\n#              Min. 1st Qu. Median Mean 3rd Qu. Max.\n# L7_ETMs.tif     1      54     69 68.9      86  255\n# dimension(s):\n#      from  to         refsys point                    values x/y\n# x       1 349 WGS 84 (CRS84) FALSE [349x352] -34.9,...,-34.8 [x]\n# y       1 352 WGS 84 (CRS84) FALSE [349x352] -8.04,...,-7.95 [y]\n# band    1   6             NA    NA                      NULL    \n# curvilinear grid\n\nwe see that a curvilinear is created, which means that for every grid cell the coordinates are computed in the new CRS, which no longer form a regular grid. Plotting such data is extremely slow, as small polygons are computed for every grid cell and then plotted. The advantage of this is that no information is lost: grid cell values remain identical after the projection.\nWhen we start with a raster on a regular grid and want to obtain a regular grid in a new coordinate reference system, we need to warp the grid: we need to recreate a grid at new locations and use some rule to assign values to new grid cells. Rules can involve using the nearest value or using some form of interpolation. This operation is not lossless and not invertible.\nThe best approach for warping is to specify the target grid as a stars object. When only a target CRS is specified, default options for the target grid are picked that may be completely inappropriate for the problem at hand. An example workflow that uses only a target CRS is\n\nread_stars(tif) |&gt;\n    st_warp(crs = st_crs('OGC:CRS84')) |&gt;\n    st_dimensions()\n#      from  to offset     delta         refsys x/y\n# x       1 350  -34.9  0.000259 WGS 84 (CRS84) [x]\n# y       1 352  -7.95 -0.000259 WGS 84 (CRS84) [y]\n# band    1   6     NA        NA             NA\n\nwhich creates a pretty close raster, but then the transformation is also relatively modest. For a workflow that creates a target raster first, here with exactly the same number of rows and columns as the original raster, one could use:\n\nr &lt;- read_stars(tif)\ngrd &lt;- st_bbox(r) |&gt;\n        st_as_sfc() |&gt;\n        st_transform('OGC:CRS84') |&gt;\n        st_bbox() |&gt;\n        st_as_stars(nx = dim(r)[\"x\"], ny = dim(r)[\"y\"])\nst_warp(r, grd)\n# stars object with 3 dimensions and 1 attribute\n# attribute(s):\n#              Min. 1st Qu. Median Mean 3rd Qu. Max. NA's\n# L7_ETMs.tif     1      54     69 68.9      86  255 6180\n# dimension(s):\n#      from  to offset     delta         refsys x/y\n# x       1 349  -34.9   0.00026 WGS 84 (CRS84) [x]\n# y       1 352  -7.95 -0.000259 WGS 84 (CRS84) [y]\n# band    1   6     NA        NA             NA\n\nwhere we see that grid resolution in \\(x\\) and \\(y\\) directions slightly varies.",
    "crumbs": [
      "R for Spatial Data Science",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to sf and stars</span>"
    ]
  },
  {
    "objectID": "07-Introsf.html#exercises",
    "href": "07-Introsf.html#exercises",
    "title": "7  Introduction to sf and stars",
    "section": "\n7.9 Exercises",
    "text": "7.9 Exercises\nUse R to solve the following exercises.\n\nFind the names of the nc counties that intersect LINESTRING(-84 35,-78 35); use [ for this, and as an alternative use st_join for this.\nRepeat this after setting sf_use_s2(FALSE), and compute the difference (hint: use setdiff), and colour the counties of the difference using colour ‘#88000088’.\nPlot the two different lines in a single plot; note that R will plot a straight line always straight in the projection currently used; st_segmentize can be used to add points on a straight line, or on a great circle for ellipsoidal coordinates.\nNDVI, normalised differenced vegetation index, is computed as (NIR-R)/(NIR+R), with NIR the near infrared and R the red band. Read the L7_ETMs.tif file into object x, and distribute the band dimensions over attributes by split(x, \"band\"). Then, add attribute NDVI to this object by using an expression that uses the NIR (band 4) and R (band 3) attributes directly.\nCompute NDVI for the L7_ETMs.tif image by reducing the band dimension, using st_apply and a function ndvi = function(x) { (x[4]-x[3])/(x[4]+x[3]) }. Plot the result, and write the result to a GeoTIFF.\nUse st_transform to transform the stars object read from L7_ETMs.tif to OGC:CRS84. Print the object. Is this a regular grid? Plot the first band using arguments axes=TRUE and border=NA, and explain why this takes such a long time.\nUse st_warp to warp the L7_ETMs.tif object to OGC:CRS84, and plot the resulting object with axes=TRUE. Why is the plot created much faster than after st_transform?\nUsing a vector representation of the raster L7_ETMs, plot the intersection with a circular area around POINT(293716 9113692) with radius 75 m, and compute the area-weighted mean pixel values for this circle. Compare the area-weighted values with those obtained by aggregate using the vector data, and by aggregate using the raster data, using exact=FALSE (default) and exact=TRUE. Explain the differences.\n\n\n\n\n\n\n\nDunnington, Dewey, Edzer Pebesma, and Ege Rubak. 2023. S2: Spherical Geometry Operators Using the S2 Geometry Library. https://CRAN.R-project.org/package=s2.\n\n\nEaton, Brian, Jonathan Gregory, Bob Drach, Karl Taylor, Steve Hankin, Jon Blower, John Caron, et al. 2022. NetCDF Climate and Forecast (CF) Metadata Conventions, Version 1.10. https://cfconventions.org/.\n\n\nEddelbuettel, Dirk. 2013. Seamless R and C++ Integration with Rcpp. Springer.\n\n\nGräler, Benedikt, Edzer Pebesma, and Gerard Heuvelink. 2016. “Spatio-Temporal Interpolation using gstat.” The R Journal 8 (1): 204–18. https://doi.org/10.32614/RJ-2016-014.\n\n\nHerring, J. R. et al. 2011. “Opengis Implementation Standard for Geographic Information-Simple Feature Access-Part 1: Common Architecture [Corrigendum].”\n\n\nHijmans, Robert J. 2023a. Raster: Geographic Data Analysis and Modeling. https://rspatial.org/raster.\n\n\n———. 2023b. Terra: Spatial Data Analysis. https://rspatial.org/terra/.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019. Geocomputation with R. Chapman & Hall/CRC.https://geocompr.robinlovelace.net/ .\n\n\nO’Brien, Joshua. 2022. gdalUtilities: Wrappers for GDAL Utilities Executables. https://github.com/JoshOBrien/gdalUtilities/.\n\n\nPebesma, Edzer. 2012. “spacetime: Spatio-Temporal Data in R.” Journal of Statistical Software 51 (7): 1–30. https://www.jstatsoft.org/v51/i07/.\n\n\n———. 2018. “Simple Features for R: Standardized Support for Spatial Vector Data.” The R Journal 10 (1): 439–46. https://doi.org/10.32614/RJ-2018-009.\n\n\n———. 2022. Spacetime: Classes and Methods for Spatio-Temporal Data. https://github.com/edzer/spacetime.\n\n\nPlate, Tony, and Richard Heiberger. 2016. Abind: Combine Multidimensional Arrays. https://CRAN.R-project.org/package=abind.\n\n\nTeickner, Henning, Edzer Pebesma, and Benedikt Graeler. 2022. Sftime: Classes and Methods for Simple Feature Objects That Have a Time Column. https://CRAN.R-project.org/package=sftime.\n\n\nWang, Earo, Di Cook, Rob Hyndman, and Mitchell O’Hara-Wild. 2022. Tsibble: Tidy Temporal Data Frames and Tools. https://tsibble.tidyverts.org.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (1).https://www.jstatsoft.org/article/view/v059i10 .\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://joss.theoj.org/papers/10.21105/joss.01686.\n\n\nWickham, Hadley, and Garret Grolemund. 2017. R for Data Science. O’Reilly. http://r4ds.had.co.nz/.",
    "crumbs": [
      "R for Spatial Data Science",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to sf and stars</span>"
    ]
  },
  {
    "objectID": "08-Plotting.html",
    "href": "08-Plotting.html",
    "title": "8  Plotting spatial data",
    "section": "",
    "text": "8.1 Every plot is a projection\nTogether with timelines, maps belong to the most powerful graphs, perhaps because we can immediately relate to where we are, or once have been, on the space of the plot. Two recent books on visualisation (Healy 2018; Wilke 2019) contain chapters on visualising geospatial data or maps. Here, we will not try to point out which maps are good and which are bad, but rather a number of possibilities for creating them, challenges along the way, and possible ways to mitigate them.\nThe world is round, but plotting devices are flat. As mentioned in Section 2.2.2, any time we visualise, in any way, the world on a flat device, we project: we convert ellipsoidal coordinates into Cartesian coordinates. This includes the cases where we think we “do nothing” as in Figure 8.1 (left), or where we show the world “as it is”, as one would see it from space (Figure 8.1, right).\nlibrary(sf)\nlibrary(rnaturalearth)\nw &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\nsuppressWarnings(st_crs(w) &lt;- st_crs('OGC:CRS84'))\nlayout(matrix(1:2, 1, 2), c(2,1))\npar(mar = rep(0, 4))\nplot(st_geometry(w))\n\n# sphere:\nold &lt;- options(s2_oriented = TRUE) # don't change orientation from here on\ncountries &lt;- s2::s2_data_countries() |&gt; st_as_sfc()\nstopifnot(sf_use_s2()) # make sure it is, otherwise \"POLYGON FULL\" won't work\nglobe &lt;- st_as_sfc(\"POLYGON FULL\", crs = st_crs(countries))\noceans &lt;- st_difference(globe, st_union(countries))\nvisible &lt;- st_buffer(st_as_sfc(\"POINT(-30 -10)\", crs = st_crs(countries)), 9800000) # visible half\nvisible_ocean &lt;- st_intersection(visible, oceans)\nvisible_countries &lt;- st_intersection(visible, countries)\nst_transform(visible_ocean, \"+proj=ortho +lat_0=-10 +lon_0=-30\") |&gt;\n    plot(col = 'lightblue')\nst_transform(visible_countries, \"+proj=ortho +lat_0=-10 +lon_0=-30\") |&gt;\n    plot(col = NA, add = TRUE)\noptions(old)\n\n\n\n\n\n\nFigure 8.1: Earth country boundaries; left: mapping long/lat linearly to \\(x\\) and \\(y\\) (plate carrée); right: as seen from an infinite distance (orthographic)\nThe left plot of Figure 8.1 was obtained by\nlibrary(sf)\nlibrary(rnaturalearth)\nw &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\nplot(st_geometry(w))\nindicating that this is the default projection for global data with ellipsoidal coordinates:\nst_is_longlat(w)\n# [1] TRUE\nThe projection taken in Figure 8.1 (left) is the equirectangular (or equidistant cylindrical) projection, which maps longitude and latitude linearly to the \\(x\\)- and \\(y\\)-axis, keeping an aspect ratio of 1. If we would do this for smaller areas not on the equator, then it would make sense to choose a plot ratio such that one distance unit E-W equals one distance unit N-S at the centre of the plotted area, and this is the default behaviour of the plot method for unprojected sf or stars datasets, as well as the default for ggplot2::geom_sf (Section 8.4).\nWe can also carry out this projection before plotting. Say we want to plot Germany, then after loading the (rough) country outline, we use st_transform to project:\nDE &lt;- st_geometry(ne_countries(country = \"germany\",\n                              returnclass = \"sf\"))\nDE |&gt; st_transform(\"+proj=eqc +lat_ts=51.14 +lon_0=90w\") -&gt;\n    DE.eqc\nHere, eqc refers to the “equidistant cylindrical” projection of PROJ. The projection parameter here is lat_ts, the latitude of true scale, where one length unit N-S equals one length unit E-W. This was chosen at the middle of the bounding box latitudes\nCodeprint(mean(st_bbox(DE)[c(\"ymin\", \"ymax\")]), digits = 4)\n# [1] 51.14\nWe plot both maps in Figure 8.2, and they look identical up to the values along the axes: degrees for ellipsoidal (left) and metres for projected (Cartesian, right) coordinates.\nCodepar(mfrow = c(1, 2), mar = c(2.2, 2.2, 0.3, 0.5))\nplot(DE, axes = TRUE)\nplot(DE.eqc, axes = TRUE)\n\n\n\n\n\n\nFigure 8.2: Germany in equirectangular projection: with axis units degrees (left) and metres in the equidistant cylindrical projection (right)",
    "crumbs": [
      "R for Spatial Data Science",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Plotting spatial data</span>"
    ]
  },
  {
    "objectID": "08-Plotting.html#sec-transform",
    "href": "08-Plotting.html#sec-transform",
    "title": "8  Plotting spatial data",
    "section": "",
    "text": "What is a good projection for my data?\n\nThere is unfortunately no silver bullet here. Projections that maintain all distances do not exist; only globes do. The most used projections try to preserve:\n\nareas (equal area)\ndirections (conformal, such as Mercator)\nsome properties of distances (equirectangular preserves distances along meridians, azimuthal equidistant preserves distances to a central point)\n\nor some compromise of these. Parameters of projections decide what is shown in the centre of a map and what is shown on the fringes, which areas are up and which are down, and which areas are most enlarged. All these choices are in the end political decisions.\nIt is often entertaining and at times educational to play around with the different projections and understand their consequences. When the primary purpose of the map however is not to entertain or educate projection varieties, it may be preferable to choose a well-known or less surprising projection and move the discussion which projection to use to a decision process of its own. For global maps however, in almost all cases, equal area projections are preferred over plate carrée or web Mercator projections.",
    "crumbs": [
      "R for Spatial Data Science",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Plotting spatial data</span>"
    ]
  },
  {
    "objectID": "08-Plotting.html#plotting-points-lines-polygons-grid-cells",
    "href": "08-Plotting.html#plotting-points-lines-polygons-grid-cells",
    "title": "8  Plotting spatial data",
    "section": "\n8.2 Plotting points, lines, polygons, grid cells",
    "text": "8.2 Plotting points, lines, polygons, grid cells\n\nSince maps are just a special form of plots of statistical data, the usual rules hold. Frequently occurring challenges include:\n\npolygons may be very small, and vanish when plotted\ndepending on the data, polygons for different features may well overlap, and be visible only partially; using transparent fill colours may help identify them\nwhen points are plotted with symbols, they may easily overlap and be hidden; density maps (Chapter 11) may be more helpful\nlines may be hard to read when coloured and may overlap regardless the line width\n\nColours\n\nWhen plotting polygons filled with colours, one has the choice to plot polygon boundaries or to suppress these. If polygon boundaries draw too much attention, an alternative is to colour them in a grey tone, or another colour that does not interfere with the fill colours. When suppressing boundaries entirely, polygons with (nearly) identical colours will no longer be visually distinguishable. If the property indicating the fill colour is constant over the region, such as land cover type, then this is not a problem, but if the property is an aggregation then the region over which it was aggregated gets lost, and by that the proper interpretation. Especially for extensive variables, such as the amount of people living in a polygon, this strongly misleads. But even with polygon boundaries, using filled polygons for extensive variables may not be a good idea because the map colours conflate amount and area size.\nThe use of continuous colour scales that have no noticeable colour breaks for continuously varying variables may look attractive, but is often more fancy than useful:\n\nit is impracticable to match a colour on the map with a legend value\ncolour ramps often stretch non-linearly over the value range, making it hard to convey magnitude\n\nOnly for cases where the identification of values is less important than the continuity of the map, such as the colouring of a high resolution digital terrain model, it does serve its goal. Good colours scales and palettes are found in functions hcl.colors or palette.colors, and in packages RColorBrewer (Neuwirth 2022), viridis (Garnier 2021), or colorspace (Ihaka et al. 2023; Zeileis et al. 2020).\nColour breaks: classInt\n\n \nWhen plotting continuous geometry attributes using a limited set of colours (or symbols), classes need to be made from the data. R package classInt (Bivand 2022) provides a number of methods to do so. The default method is “quantile”:\n\nlibrary(classInt)\n# set.seed(1) if needed ?\nr &lt;- rnorm(100)\n(cI &lt;- classIntervals(r))\n# style: quantile\n#   one of 1.49e+10 possible partitions of this variable into 8 classes\n#   [-2.24,-1.04)  [-1.04,-0.622) [-0.622,-0.267)  [-0.267,0.157) \n#              13              12              13              12 \n#   [0.157,0.375)   [0.375,0.669)    [0.669,1.26)        [1.26,3] \n#              12              13              12              13\ncI$brks\n# [1] -2.242 -1.041 -0.622 -0.267  0.157  0.375  0.669  1.257  3.003\n\nit takes argument n for the number of intervals, and a style that can be one of “fixed”, “sd”, “equal”, “pretty”, “quantile”, “kmeans”, “hclust”, “bclust”, “fisher” or “jenks”. Style “pretty” may not obey n; if n is missing, nclass.Sturges is used; two other methods are available for choosing n automatically. If the number of observations is greater than 3000, a 10% sample is used to create the breaks for “fisher” and “jenks”.\nGraticule and other navigation aids\n \nA graticule is a network of lines on a map that follow constant latitude or longitude. Figure 1.1 shows a graticule drawn in grey, on Figure 1.2 it is white. Graticules are often drawn in maps to give place reference. In our first map in Figure 1.1 we can read that the area plotted is near 35\\(^o\\) North and 80\\(^o\\) West. Had we plotted the lines in the projected coordinate system, they would have been straight and their actual numbers would not have been very informative, apart from giving an interpretation of size or distances when the unit is known, and familiar to the map reader. Graticules also shed light on which projection was used: equirectangular or Mercator projections have straight vertical and horizontal lines, conic projections have straight but diverging meridians, and equal area projections may have curved meridians.\n\nOn Figure 8.1 and most other maps the real navigation aid comes from geographical features like the state outline, country outlines, coast lines, rivers, roads, railways and so on. If these are added sparsely and sufficiently, a graticule can as well be omitted. In such cases, maps look good without axes, tics, and labels, leaving up a lot of plotting space to be filled with actual map data.",
    "crumbs": [
      "R for Spatial Data Science",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Plotting spatial data</span>"
    ]
  },
  {
    "objectID": "08-Plotting.html#base-plot",
    "href": "08-Plotting.html#base-plot",
    "title": "8  Plotting spatial data",
    "section": "\n8.3 Base plot\n",
    "text": "8.3 Base plot\n\n\nThe plot method for sf and stars objects try to make quick, useful, exploratory plots; for higher quality plots and more configurability, alternatives with more control and/or better defaults are offered for instance by packages ggplot2 (Wickham et al. 2022), tmap (Tennekes 2022, 2018), or mapsf (Giraud 2022).\nBy default, the plot method tries to plot “all” it is given. This means that:\n\ngiven a geometry only (sfc), the geometry is plotted, without colours\ngiven a geometry and an attribute, the geometry is coloured according to the values of the attribute, using a qualitative colour scale for factor or logical attributes and a continuous scale otherwise, and a colour key is added\ngiven multiple attributes, multiple maps are plotted, each with a colour scale but a key is by default omitted, as colour assignment is done on a per sub-map basis\nfor stars objects with multiple attributes, only the first attribute is plotted; for three-dimensional raster cubes, all slices over the third dimension are plotted as sub-plots\n\nAdding to plots with legends\n\nThe plot methods for stars and sf objects may show a colour key on one of the sides (Figure 1.1). To do this with base::plot, the plot region is split in two and two plots are created: one with the map, and one with the legend. By default, the plot function resets the graphics device (using layout(matrix(1)) so that subsequent plots are not hindered by the device being split in two, but this prevents adding graphic elements subsequently. To add to an existing plot with a colour legend, the device reset needs to be prevented by using reset = FALSE in the plot command, and using add = TRUE in subsequent calls to plot. An example is\n\nlibrary(sf)\nnc &lt;- read_sf(system.file(\"gpkg/nc.gpkg\", package = \"sf\"))\nplot(nc[\"BIR74\"], reset = FALSE, key.pos = 4)\nplot(st_buffer(nc[1,1], units::set_units(10, km)), col = 'NA', \n     border = 'red', lwd = 2, add = TRUE)\n\n\n\n\n\n\nFigure 8.3: Annotating base plots with a legend\n\n\n\n\nwhich is shown in Figure 8.3. Annotating stars plots can be done in the same way when a single stars layer is shown. Annotating stars facet plots with multiple cube slices can be done by adding a “hook” function that will be called on every slice shown, as in\n\nlibrary(stars)\n# Loading required package: abind\nsystem.file(\"tif/L7_ETMs.tif\", package = \"stars\") |&gt;\n    read_stars() -&gt; r\nst_bbox(r) |&gt; st_as_sfc() |&gt; st_sample(5) |&gt; \n    st_buffer(300) -&gt; circ\nhook &lt;- function() { \n    plot(circ, col = NA, border = 'yellow', add = TRUE)\n}\nplot(r, hook = hook, key.pos = 4)\n\n\n\n\n\n\nFigure 8.4: Annotated multi-slice stars plot\n\n\n\n\nand as shown in Figure 8.4. Hook functions have access to facet parameters, facet label and bounding box.\nBase plot methods have access to the resolution of the screen device, and hence the base plot method for stars and stars_proxy object will downsample dense rasters and only plot pixels at a density that makes sense for the device available.\nProjections in base plots\nThe base plot method plots data with ellipsoidal coordinates using the equirectangular projection, using a latitude parameter equal to the middle latitude of the data bounding box (Figure 8.2). To control this parameter, either a projection to another equirectangular can be applied before plotting, or the parameter asp can be set to override: asp=1 would lead to plate carrée (Figure 8.1) left. Subsequent plots need to be in the same coordinate reference system in order to make sense with over-plotting; this is not being checked.\nColours and colour breaks\nIn base plots, argument nbreaks can be used to set the number of colour breaks and argument breaks either to the numeric vector with actual breaks, or to a style value for the style argument in classInt::classIntervals.",
    "crumbs": [
      "R for Spatial Data Science",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Plotting spatial data</span>"
    ]
  },
  {
    "objectID": "08-Plotting.html#sec-geomsf",
    "href": "08-Plotting.html#sec-geomsf",
    "title": "8  Plotting spatial data",
    "section": "\n8.4 Maps with ggplot2\n",
    "text": "8.4 Maps with ggplot2\n\n \nPackage ggplot2 (Wickham et al. 2022; Wickham 2016) can create more complex and nicer looking graphs; it has a geometry geom_sf that was developed in conjunction with the development of sf and helps creating beautiful maps. An introduction to this is found in Moreno and Basille (2018). A first example is shown in Figure 1.2. The code used for this plot is:\n\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nnc.32119 &lt;- st_transform(nc, 32119) \nyear_labels &lt;- \n    c(\"SID74\" = \"1974 - 1978\", \"SID79\" = \"1979 - 1984\")\nnc.32119 |&gt; select(SID74, SID79) |&gt; \n    pivot_longer(starts_with(\"SID\")) -&gt; nc_longer\n\n\nggplot() + geom_sf(data = nc_longer, aes(fill = value), linewidth = 0.4) + \n  facet_wrap(~ name, ncol = 1, \n             labeller = labeller(name = year_labels)) +\n  scale_y_continuous(breaks = 34:36) +\n  scale_fill_gradientn(colours = sf.colors(20)) +\n  theme(panel.grid.major = element_line(colour = \"white\"))\n\nwhere we see that two attributes had to be stacked (pivot_longer) before plotting them as facets: this is the idea behind “tidy” data, and the pivot_longer method for sf objects automatically stacks the geometry column too.\nBecause ggplot2 creates graphics objects before plotting them, it can control the coordinate reference system of all elements involved, and will transform or convert all subsequent objects to the coordinate reference system of the first. It will also draw a graticule for the (default) thin white lines on a grey background, and uses a datum (by default: WGS84) for this. geom_sf can be combined with other geoms, for instance to allow for annotating plots.\n\nFor package stars, a geom_stars has, at the moment of writing this, rather limited scope: it uses geom_sf for map layout and vector data cubes, and adds geom_raster for regular rasters and geom_rect for rectilinear rasters. It downsamples if the user specifies a downsampling rate, but has no access to the screen dimensions to automatically choose a downsampling rate. This may be just enough, for instance Figure 8.5 is created by the following commands:\n\nlibrary(ggplot2)\nlibrary(stars)\nr &lt;- read_stars(system.file(\"tif/L7_ETMs.tif\", package = \"stars\"))\nggplot() + geom_stars(data = r) +\n        facet_wrap(~band) + coord_equal() +\n        theme_void() +\n        scale_x_discrete(expand = c(0,0)) + \n        scale_y_discrete(expand = c(0,0)) +\n        scale_fill_viridis_c()\n\n\n\n\n\n\nFigure 8.5: Simple facet raster plot with ggplot2 and geom_stars\n\n\n\n\nMore elaborate ggplot2-based plots with stars objects may be obtained using package ggspatial (Dunnington 2022). Non-compatible but nevertheless ggplot2-style plots can be created with tmap, a package dedicated to creating high quality maps (Section 8.5).\n\nWhen combining several feature sets with varying coordinate reference systems, using geom_sf, all sets are transformed to the reference system of the first set. To further control the “base” coordinate reference system, coord_sf can be used. This allows for instance working in a projected system, while combining graphics elements that are not sf objects but regular data.frames with ellipsoidal coordinates associated to WGS84.\nA twitter thread by Claus Wilke illustrating this is found here.",
    "crumbs": [
      "R for Spatial Data Science",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Plotting spatial data</span>"
    ]
  },
  {
    "objectID": "08-Plotting.html#sec-tmap",
    "href": "08-Plotting.html#sec-tmap",
    "title": "8  Plotting spatial data",
    "section": "\n8.5 Maps with tmap\n",
    "text": "8.5 Maps with tmap\n\n\nPackage tmap (Tennekes 2022, 2018) takes a fresh look at plotting spatial data in R. It resembles ggplot2 in the sense that it composes graphics objects before printing by building on the grid package, and by concatenating map elements with a + between them, but otherwise it is entirely independent from, and incompatible with, ggplot2. It has a number of options that allow for highly professional looking maps, and many defaults have been carefully chosen. Creating a map with two similar attributes can be done using tm_polygons with two attributes, we can use\n\nlibrary(tmap)\nsystem.file(\"gpkg/nc.gpkg\", package = \"sf\") |&gt;\n    read_sf() |&gt; st_transform('EPSG:32119') -&gt; nc.32119\ntm_shape(nc.32119) + \n    tm_polygons(c(\"SID74\", \"SID79\"), title = \"SIDS\") +\n    tm_layout(legend.outside = TRUE, \n              panel.labels = c(\"1974-78\", \"1979-84\")) +\n    tm_facets(free.scales=FALSE)\n\nto create Figure 8.6:\n\nCodelibrary(tmap)\nsystem.file(\"gpkg/nc.gpkg\", package = \"sf\") |&gt;\n    read_sf() |&gt;\n    st_transform('EPSG:32119') -&gt; nc.32119\ntm_shape(nc.32119) + tm_polygons(c(\"SID74\", \"SID79\"), title=\"SIDS\") + \n    tm_layout(legend.outside=TRUE, panel.labels=c(\"1974-78\", \"1979-84\")) + \n    tm_facets(free.scales=FALSE)\n# \n# ── tmap v3 code detected ───────────────────────────────────────────\n# [v3-&gt;v4] `tm_polygons()`: migrate the argument(s) related to the\n# legend of the visual variable `fill` namely 'title' to 'fill.legend\n# = tm_legend(&lt;HERE&gt;)'\n# tm_facets(): the argument free.scales is deprecated. Specify this via the layer functions (e.g. fill.free in tm_polygons)\n\n\n\n\n\n\nFigure 8.6: tmap: using tm_polygons() with two attribute names\n\n\n\n\n\nAlternatively, from the long table form obtained by pivot_longer one could use + tm_polygons(\"SID\") + tm_facets(by = \"name\").\nPackage tmap also has support for stars objects, an example created with\n\ntm_shape(r) + tm_raster()\n\n\n\n\n\n\n\n\nFigure 8.7: Simple raster plot with tmap\n\n\n\n\nis shown in Figure 8.7. More examples of the use of tmap are given in Chapters 14-16.",
    "crumbs": [
      "R for Spatial Data Science",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Plotting spatial data</span>"
    ]
  },
  {
    "objectID": "08-Plotting.html#interactive-maps-leaflet-mapview-tmap",
    "href": "08-Plotting.html#interactive-maps-leaflet-mapview-tmap",
    "title": "8  Plotting spatial data",
    "section": "\n8.6 Interactive maps: leaflet, mapview, tmap\n",
    "text": "8.6 Interactive maps: leaflet, mapview, tmap\n\n \nInteractive maps as shown in Figure 1.3 can be created with R packages leaflet, mapview, or tmap. Package mapview adds a number of capabilities to leaflet including a map legend, configurable pop-up windows when clicking features, support for raster data, and scalable maps with very large feature sets using the FlatGeobuf file format, as well as facet maps that react synchronously to zoom and pan actions. Package tmap has the option that after giving\n\ntmap_mode(\"view\")\n\nall usual tmap commands are applied to an interactive html/leaflet widget, whereas after\n\ntmap_mode(\"plot\")\n\nagain all output is sent to R’s own (static) graphics device.",
    "crumbs": [
      "R for Spatial Data Science",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Plotting spatial data</span>"
    ]
  },
  {
    "objectID": "08-Plotting.html#exercises",
    "href": "08-Plotting.html#exercises",
    "title": "8  Plotting spatial data",
    "section": "\n8.7 Exercises",
    "text": "8.7 Exercises\n\nFor the countries Indonesia and Canada, create individual plots using equirectangular, orthographic, and Lambert equal area projections, while choosing projection parameters sensible for the area.\nRecreate the plot in Figure 8.3 with ggplot2 and with tmap.\nRecreate the plot in Figure 8.7 using the viridis colour ramp.\nView the interactive plot in Figure 8.7 using the “view” (interactive) mode of tmap, and explore which interactions are possible; also explore adding + tm_facets(as.layers=TRUE) and try switching layers on and off. Try also setting a transparency value to 0.5.\n\n\n\n\n\n\n\nBivand, Roger. 2022. classInt: Choose Univariate Class Intervals. https://CRAN.R-project.org/package=classInt.\n\n\nDunnington, Dewey. 2022. Ggspatial: Spatial Data Framework for Ggplot2. https://CRAN.R-project.org/package=ggspatial.\n\n\nGarnier, Simon. 2021. Viridis: Colorblind-Friendly Color Maps for r. https://CRAN.R-project.org/package=viridis.\n\n\nGiraud, Timothée. 2022. Mapsf: Thematic Cartography. https://riatelab.github.io/mapsf/.\n\n\nHealy, Kieran. 2018. Data Visualization, a Practical Introduction. Princeton University Press. http://socviz.co/index.html.\n\n\nIhaka, Ross, Paul Murrell, Kurt Hornik, Jason C. Fisher, Reto Stauffer, Claus O. Wilke, Claire D. McWhite, and Achim Zeileis. 2023. Colorspace: A Toolbox for Manipulating and Assessing Colors and Palettes. https://CRAN.R-project.org/package=colorspace.\n\n\nMoreno, Mel, and Mathieu Basille. 2018. Drawing Beautiful Maps Programmatically with r, Sf and Ggplot2 — Part 1: Basics. https://www.r-spatial.org/r/2018/10/25/ggplot2-sf.html.\n\n\nNeuwirth, Erich. 2022. RColorBrewer: ColorBrewer Palettes. https://CRAN.R-project.org/package=RColorBrewer.\n\n\nTennekes, Martijn. 2018. “tmap: Thematic Maps in R.” Journal of Statistical Software 84 (6): 1–39. https://doi.org/10.18637/jss.v084.i06.\n\n\n———. 2022. Tmap: Thematic Maps. https://github.com/r-tmap/tmap.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, and Dewey Dunnington. 2022. Ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://CRAN.R-project.org/package=ggplot2.\n\n\nWilke, Claus O. 2019. Fundamentals of Data Visualization. O’Reilly Media, Inc. https://serialmentor.com/dataviz/.\n\n\nZeileis, Achim, Jason C. Fisher, Kurt Hornik, Ross Ihaka, Claire D. McWhite, Paul Murrell, Reto Stauffer, and Claus O. Wilke. 2020. “colorspace: A Toolbox for Manipulating and Assessing Colors and Palettes.” Journal of Statistical Software 96 (1): 1–49. https://doi.org/10.18637/jss.v096.i01.",
    "crumbs": [
      "R for Spatial Data Science",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Plotting spatial data</span>"
    ]
  },
  {
    "objectID": "09-Large.html",
    "href": "09-Large.html",
    "title": "9  Large data and cloud native",
    "section": "",
    "text": "9.1 Vector data: sf\nThis chapter describes how large spatial and spatiotemporal datasets can be handled with R, with a focus on packages sf and stars. For practical use, we classify large datasets as too large\nThese three categories may (today) correspond very roughly to Gigabyte-, Terabyte- and Petabyte-sized datasets. Besides size considerations, access and processing speed also play a role, in particular for larger datasets or interactive applications. Cloud native geospatial formats are formats optimised with processing on cloud infrastructure in mind, where costs of computing and storage need to be considered and optimised. Such costs can be reduced by\nIt should be noted that there are no silver bullets in this area: optimising storage for one particular access pattern will lead to slow access for other ways. If raster data is for instance stored for optimal access of spatial regions at different spatial resolutions, reading the data as pixel time series may be very slow. Compression leads to low storage and bandwidth costs but to higher processing costs when reading, because of the decompression involved.",
    "crumbs": [
      "R for Spatial Data Science",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Large data and cloud native</span>"
    ]
  },
  {
    "objectID": "09-Large.html#sec-largesf",
    "href": "09-Large.html#sec-largesf",
    "title": "9  Large data and cloud native",
    "section": "",
    "text": "Reading from local disk\nFunction st_read reads vector data from disk, using GDAL, and then keeps the data read in working memory. In case the file is too large to be read in working memory, several options exist to read parts of the file. The first is to set argument wkt_filter with a WKT text string containing a geometry; only geometries from the target file that intersect with this geometry will be returned. An example is\n\nlibrary(sf)\n# Linking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.4.0; sf_use_s2() is TRUE\nfile &lt;- system.file(\"gpkg/nc.gpkg\", package = \"sf\")\nc(xmin = -82,ymin = 36, xmax = -80, ymax = 37) |&gt;\n    st_bbox() |&gt; st_as_sfc() |&gt; st_as_text() -&gt; bb\nread_sf(file, wkt_filter = bb) |&gt; nrow() # out of 100\n# [1] 16\n\nHere, read_sf is used as an alternative to st_read to suppress output.\n\nThe second option is to use the query argument to st_read, which can be any query in “OGR SQL” dialect. This can for instance be used to select features from a layer, or limit fields. An example is:\n\nq &lt;- \"select BIR74,SID74,geom from 'nc.gpkg' where BIR74 &gt; 1500\"\nread_sf(file, query = q) |&gt; nrow()\n# [1] 61\n\nNote that nc.gpkg is the layer name, which can be obtained from file using st_layers. Sequences of records can be read using LIMIT and OFFSET, to read records 51-60 use\n\nq &lt;- \"select BIR74,SID74,geom from 'nc.gpkg' LIMIT 10 OFFSET 50\"\nread_sf(file, query = q) |&gt; nrow()\n# [1] 10\n\nFurther query options include selection on geometry type or polygon area. When the dataset queried is a spatial database, then the query is passed on to the database and not interpreted by GDAL; this means that more powerful features will be available. Further information is found in the GDAL documentation under “OGR SQL dialect”.\n\nVery large files or directories that are zipped can be read without the need to unzip them, using the /vsizip (for zip), /vsigzip (for gzip), or /vsitar (for tar files) prefix to files. This is followed by the path to the zip file, and then followed by the file inside this zip file. Reading files this way may come at some computational cost.\nReading from databases, dbplyr\n\n\nAlthough GDAL has support for several spatial databases, and as mentioned above it passes on SQL in the query argument to the database, it is sometimes beneficial to directly read from and write to a spatial database using the DBI R database drivers for this. An example of this is:\n\npg &lt;- DBI::dbConnect(\n    RPostgres::Postgres(),\n    host = Sys.getenv(\"DB_HOST\"),\n    user = Sys.getenv(\"DB_USERNAME\"),\n    dbname = \"postgis\")\nread_sf(pg, query = \n        \"select BIR74,wkb_geometry from nc limit 3\") |&gt; nrow()\n# [1] 3\n\nwhere the database host and user name are obtained from environment variables, and the database name is postgis.\nA spatial query might look like\n\nq &lt;- \"SELECT BIR74,wkb_geometry FROM nc WHERE \\\nST_Intersects(wkb_geometry, 'SRID=4267;POINT (-81.50 36.43)');\"\nread_sf(pg, query = q) |&gt; nrow()\n# [1] 1\n\nHere, the intersection is carried out inside the database, and uses a spatial index when present. The same mechanism works when using dplyr with a database backend:\n\n\nlibrary(dplyr, warn.conflicts = FALSE)\nnc_db &lt;- tbl(pg, \"nc\")\n\nSpatial queries can be formulated and are passed on to the database:\n\nnc_db |&gt; \n     filter(ST_Intersects(wkb_geometry, \n                        'SRID=4267;POINT (-81.50 36.43)')) |&gt;\n     collect()\n# # A tibble: 1 × 16\n#   ogc_fid  area perimeter cnty_ cnty_id name  fips  fipsno cress_id\n#     &lt;int&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;    &lt;int&gt;\n# 1       1 0.114      1.44  1825    1825 Ashe  37009  37009        5\n# # ℹ 7 more variables: bir74 &lt;dbl&gt;, sid74 &lt;dbl&gt;, nwbir74 &lt;dbl&gt;,\n# #   bir79 &lt;dbl&gt;, sid79 &lt;dbl&gt;, nwbir79 &lt;dbl&gt;,\n# #   wkb_geometry &lt;pq_gmtry&gt;\nnc_db |&gt; filter(ST_Area(wkb_geometry) &gt; 0.1) |&gt; head(3)\n# # Source:   SQL [?? x 16]\n# # Database: postgres  [postgres@localhost:5432/postgis]\n#   ogc_fid  area perimeter cnty_ cnty_id name   fips  fipsno cress_id\n#     &lt;int&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt;    &lt;int&gt;\n# 1       1 0.114      1.44  1825    1825 Ashe   37009  37009        5\n# 2       3 0.143      1.63  1828    1828 Surry  37171  37171       86\n# 3       5 0.153      2.21  1832    1832 North… 37131  37131       66\n# # ℹ 7 more variables: bir74 &lt;dbl&gt;, sid74 &lt;dbl&gt;, nwbir74 &lt;dbl&gt;,\n# #   bir79 &lt;dbl&gt;, sid79 &lt;dbl&gt;, nwbir79 &lt;dbl&gt;,\n# #   wkb_geometry &lt;pq_gmtry&gt;\n\n(Note that PostGIS’ ST_Area computes the same area as the AREA field in nc, which is the meaningless value obtained by interpreting the coordinates as projected, where they are ellipsoidal.)\nReading from online resources or web services\n\nGDAL drivers support reading from online resources, by prepending the URL starting with https:// with /vsicurl/. A number of similar drivers specialised for particular clouds include /vsis3/ for Amazon S3, /vsigs/ for Google Cloud Storage, /vsiaz/ for Azure, /vsioss/ for Alibaba Cloud, or /vsiswift/ for OpenStack Swift Object Storage. Section 9.3.2 has an example of using /vsicurl/.\nThese prepositions can be combined with /vsizip/ to read a file from a zipped online resource. Depending on the file format used, reading information this way may require reading the entire file, or reading it multiple times, and may not always be the most efficient way of handling resources. Cloud native formats are optimised to work efficiently using HTTP requests.\n \nAPIs, OpenStreetMap\n \nTypical web services for geospatial data create data on the fly and give access to this through an API. As an example, data from OpenStreetMap can be bulk downloaded and read locally, for instance using the GDAL vector driver, but more typical a user wants to obtain a small subset of the data or use the data for a small query. Several R packages exist that query OpenStreetMap data:\n\nPackage OpenStreetMap (Fellows and JMapViewer library by Jan Peter Stotz 2019) downloads data as raster tiles, typically used as backdrop or reference for plotting other features\nPackage osmdata (Mark Padgham et al. 2017) downloads vector data as points, lines, or polygons in sf or sp format\nPackage osmar (from CRAN archive) returns vector data, but in addition the network topology (as an igraph object) that contains how road elements are connected, and has functions for computing the shortest route\n\nWhen provided with a correctly formulated API call in the URL the highly configurable GDAL OSM driver (in st_read) can read an “.osm” file (xml) and return a dataset with five layers: points that have significant tags, lines with non-area “way” features, multilinestrings with “relation” features, multipolygons with “relation” features , and other_relations. A simple and very small bounding box query to OpenStreetMap could look like\n\ndownload.file(paste0(\"https://openstreetmap.org/api/0.6/map?\",\n       \"bbox=7.595,51.969,7.598,51.970\"),\n    \"data/ms.osm\", method = \"auto\")\n\nand from this file we can read the layer lines, and plot its first attribute by\n\no &lt;- read_sf(\"data/ms.osm\", \"lines\")\np &lt;- read_sf(\"data/ms.osm\", \"multipolygons\")\nst_bbox(c(xmin = 7.595, ymin = 51.969, \n          xmax = 7.598, ymax = 51.970), crs = 'OGC:CRS84') |&gt;\n    st_as_sfc() |&gt;\n    plot(axes = TRUE, lwd = 2, lty = 2, cex.axis = .5)\nplot(o[,1], lwd = 2, add = TRUE)\nplot(st_geometry(p), border = NA, col = '#88888888', add = TRUE)\n\n\n\n\n\n\nFigure 9.1: OpenStreetMap vector data\n\n\n\n\nthe result of which is shown in Figure 9.1. The overpass API provides a more generic and powerful query functionality to OpenStreetMap data.\nGeoParquet and GeoArrow\n \nTwo formats dedicated to cloud native analysis are derived from the Apache projects Parquet and Arrow. Both provide column oriented storage of tabular data, meaning that the reading of a single field for many records is fast, compared to record oriented storage of most other databases. The Geo- extensions of both involve\n\na way of storing a geometry column, either in a well-known binary or text form, or in a more efficient form where sub-geometries are indexed up front\nstorage of a coordinate reference system.\n\nAt the time of writing this book, both formats are under active development, but drivers for reading or creating them are available in GDAL starting from version 3.5. Both formats allow for compressed storage. The difference is that (Geo)Parquet is more oriented towards persistent storage, where, (Geo)Arrow is more oriented to fast access and fast computation. Arrow can for instance be both an in-memory and an on-disk format.",
    "crumbs": [
      "R for Spatial Data Science",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Large data and cloud native</span>"
    ]
  },
  {
    "objectID": "09-Large.html#raster-data-stars",
    "href": "09-Large.html#raster-data-stars",
    "title": "9  Large data and cloud native",
    "section": "\n9.2 Raster data: stars\n",
    "text": "9.2 Raster data: stars\n\nA common challenge with raster datasets is not only that they come in large files (single Sentinel-2 tiles are around 1 GB), but that many of these files, potentially thousands or millions, are needed to address the area and time period of interest. In 2022, Copernicus, the program that runs all Sentinel satellites, published 160 TB of images per day. This means that a classic pattern in using R consisting of downloading data to local disc, loading the data in memory, and analysing it is not going to work.\n \nCloud-based Earth Observation processing platforms like Google Earth Engine (Gorelick et al. 2017), Sentinel Hub or openEO.cloud recognise this and let users work with datasets up to the petabyte range rather easily and with a great deal of interactivity. They share the following properties:\n\ncomputations are postponed as long as possible (lazy evaluation)\nonly the data asked for by the user is being computed and returned, and nothing more\nstoring intermediate results is avoided in favour of on-the-fly computations\nmaps with useful results are generated and shown quickly to allow for interactive model development\n\nThis is similar to the dbplyr interface to databases and cloud-based analytics environments, but differs in the aspect of what we want to see quickly: rather than the first \\(n\\) records of a dbplyr lazy table, we want a quick overview of the results, in the form of a map covering the whole area, or part of it, but at screen resolution rather than native (observation) resolution.\n \nIf for instance we want to “see” results for the United States on a screen with 1000 \\(\\times\\) 1000 pixels, we only need to compute results for this many pixels, which corresponds roughly to data on a grid with 3000 m \\(\\times\\) 3000 m grid cells. For Sentinel-2 data with 10 m resolution, this means we can downsample with a factor 300, giving 3 km \\(\\times\\) 3 km resolution. Processing, storage, and network requirements then drop a factor \\(300^2 \\approx 10^5\\), compared to working on the native 10 m \\(\\times\\) 10 m resolution. On the platforms mentioned, zooming in the map triggers further computations on a finer resolution and smaller extent.\nA simple optimisation that follows these lines is how the plot method for stars objects works. In the case of plotting large rasters, it downsamples the array before it plots, drastically saving time. The degree of downsampling is derived from the plotting region size and the plotting resolution (pixel density). For vector devices, such as pdf, R sets plot resolution to 75 dpi, corresponding to 0.3 mm per pixel. Enlarging plots may reveal this, but replotting to an enlarged devices will create a plot at target density. For geom_stars the user has to specify the downsample rate, because the ggplot2 does not make the device size available to that function.\n\nstars proxy objects\n \nTo handle datasets that are too large to fit in memory, stars provides stars_proxy objects. To demonstrate its use, we will use the starsdata package, an R data package with larger datasets (around 1 GB total). It can be installed by\n\noptions(timeout = 600) # or larger in case of slow network\ninstall.packages(\"starsdata\", \n  repos = \"http://cran.uni-muenster.de/pebesma/\", type = \"source\")\n\nWe can “load” a Sentinel-2 image from it by\n\nlibrary(stars) |&gt; suppressPackageStartupMessages()\nf &lt;- paste0(\"sentinel/S2A_MSIL1C_20180220T105051_N0206\",\n           \"_R051_T32ULE_20180221T134037.zip\")\ngranule &lt;- system.file(file = f, package = \"starsdata\")\nfile.size(granule)\n# [1] 7.69e+08\nbase_name &lt;- strsplit(basename(granule), \".zip\")[[1]]\ns2 &lt;- paste0(\"SENTINEL2_L1C:/vsizip/\", granule, \"/\", base_name, \n    \".SAFE/MTD_MSIL1C.xml:10m:EPSG_32632\")\n(p &lt;- read_stars(s2, proxy = TRUE))\n# stars_proxy object with 1 attribute in 1 file(s):\n# $EPSG_32632\n# [1] \"[...]/MTD_MSIL1C.xml:10m:EPSG_32632\"\n# \n# dimension(s):\n#      from    to offset delta            refsys    values x/y\n# x       1 10980  3e+05    10 WGS 84 / UTM z...      NULL [x]\n# y       1 10980  6e+06   -10 WGS 84 / UTM z...      NULL [y]\n# band    1     4     NA    NA                NA B4,...,B8\nobject.size(p)\n# 12568 bytes\n\nand we see that this does not actually load any of the pixel values but keeps the reference to the dataset and fills the dimensions table. (The convoluted s2 name is needed to point GDAL to the right file inside the .zip file containing 115 files in total).\n \n\nThe idea of a proxy object is that we can build expressions like\n\np2 &lt;- p * 2\n\nbut that the computations for this are postponed. Only when we really need the data, is p * 2 evaluated. We need data when we:\n\n\nplot data,\nwrite an object to disk, with write_stars, or\nexplicitly load an object in memory, with st_as_stars\n\n\nIn case the entire object does not fit in memory, plot and write_stars choose different strategies to deal with this:\n\n\nplot fetches only the pixels that can be seen by downsampling, rather than reading all pixels available\n\nwrite_stars reads, processes, and writes data chunk-by-chunk\n\n\nCodeplot(p)\n# downsample set to 18\n\n\n\n\n\n\nFigure 9.2: Downsampled 10 m bands of a Sentinel-2 scene\n\n\n\n\nDownsampling and chunking is implemented for spatially dense images, but not for dense time series or other dense dimensions. As an example, the output of plot(p), shown in Figure 9.2 only fetches the pixels that can be seen on the plot device, rather than the 10980 \\(\\times\\) 10980 pixels available in each band. The downsampling ratio taken is\n\nCode(ds &lt;- floor(sqrt(prod(dim(p)) / prod(dev.size(\"px\")))))\n# [1] 19\n\n\nmeaning that for every 19 \\(\\times\\) 19 sub-image in the original image, only one pixel is read and plotted.\nOperations on proxy objects\n Several dedicated methods are available for stars_proxy objects:\n\nmethods(class = \"stars_proxy\")\n#  [1] [               [[&lt;-            [&lt;-             adrop          \n#  [5] aggregate       aperm           as.data.frame   c              \n#  [9] coerce          dim             droplevels      filter         \n# [13] hist            image           initialize      is.na          \n# [17] Math            merge           mutate          Ops            \n# [21] plot            prcomp          predict         print          \n# [25] pull            rename          select          show           \n# [29] slice           slotsFromS3     split           st_apply       \n# [33] st_as_sf        st_as_stars     st_crop         st_dimensions&lt;-\n# [37] st_downsample   st_mosaic       st_normalize    st_redimension \n# [41] st_sample       st_set_bbox     transmute       write_stars    \n# see '?methods' for accessing help and source code\n\nWe have seen plot and print in action; dim reads out the dimension from the dimensions metadata table.\n \nThe three methods that actually fetch data are st_as_stars, plot and write_stars. st_as_stars reads the actual data into a stars object, its argument downsample controls the downsampling rate. plot does this too, choosing an appropriate downsample value from the device resolution, and plots the object. write_stars writes a stars_proxy object to disk.\nAll other methods for stars_proxy objects do not actually operate on the raster data but add the operations to a to do list attached to the object. Only when actual raster data are fetched, for instance when plot or st_as_stars are called, the commands in this list are executed.\n st_crop limits the extent (area) of the raster that will be read. c combines stars_proxy objects, but still doesn’t read any data. adrop drops empty dimensions and aperm can change dimension order.\nwrite_stars reads and processes its input chunk-wise; it has an argument chunk_size that lets users control the size of spatial chunks.\nRemote raster resources\n \nA format like “cloud-optimised GeoTIFF” (COG) has been specially designed to be efficient and resource-friendly in many cases, e.g., for only reading the metadata, or for only reading overviews (low-resolution versions of the full imagery) or spatial regions using the /vsixxx/ mechanisms (Section 9.1.3). COGs can also be created using the GeoTIFF driver of GDAL, and setting the right dataset creation options in a write_stars call.",
    "crumbs": [
      "R for Spatial Data Science",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Large data and cloud native</span>"
    ]
  },
  {
    "objectID": "09-Large.html#very-large-data-cubes",
    "href": "09-Large.html#very-large-data-cubes",
    "title": "9  Large data and cloud native",
    "section": "\n9.3 Very large data cubes",
    "text": "9.3 Very large data cubes\n \nAt some stage, datasets need to be analysed that are so large that downloading them is no longer feasible; even when local storage would be sufficient, network bandwidth may become limiting. Examples are satellite image archives such as those from Landsat and Copernicus (Sentinel-x), or model computations such as the ERA5 (Hersbach et al. 2020), a model reanalysis of the global atmosphere, land surface, and ocean waves from 1950 onwards. In such cases it may be most helpful to gain access to virtual machines in a cloud that have these data available, or to use a system that lets the user carry out computations without having to worry about virtual machines and storage. Both options will be discussed.\nFinding and processing assets\nWhen working on a virtual machine on a cloud, a first task is usually to find the assets (files) to work on. It looks attractive to obtain a file listing, and then parse file names such as\nS2A_MSIL1C_20180220T105051_N0206_R051_T32ULE_20180221T134037.zip\nfor their metadata including the date of acquisition and the code of the spatial tile covered. Obtaining such a file listing however is usually computationally very demanding, as is the processing of the result, when the number of tiles runs in the many millions.\nA solution to this is to use a catalogue. The recently developed and increasingly deployed STAC, short for spatiotemporal asset catalogue, provides an API that can be used to query image collections by properties like bounding box, date, band, and cloud coverage. The R package rstac (Simoes, Carvalho, and Brazil Data Cube Team 2023) provides an R interface to create queries and manage the information returned.\nProcessing the resulting files may involve creating a data cube at a lower spatial and/or temporal resolution, from images that may span a range of coordinate reference systems (e.g., several UTM zones). An R package that creates a regular data cube from such a collection of images is gdalcubes (Appel 2023; Appel and Pebesma 2019), which can also directly use a STAC (Appel, Pebesma, and Mohr 2021) to identify images.\nCloud native storage: Zarr\n \nWhere COG provides cloud native storage for raster imagery, Zarr is a format for cloud native storage of large multi-dimensional arrays. It can be seen as a successor of NetCDF and seems to follow similar conventions, being used by the climate and forecast communities (Eaton et al. 2022). Zarr “files” are really directories with sub-directories containing compressed chunks of the data. The compression algorithm and the chunking strategy will have an effect on how fast particular sub-cubes can be read or written.\nFunction stars::read_mdim can read entire data cubes but has options for reading sub-cubes by specifying for each dimension offset, number of pixels, and the step size to read a dimension at a lower resolution (Pebesma 2022). Similarly, stars::write_mdim can write multi-dimensional arrays to Zarr or NetCDF files, or other formats that support the GDAL C++ multi-dimensional array API.\n \nTo read a remote (cloud-based) Zarr file, one needs to prepend the URL with indicators about the format and the access protocol:\n\ndsn = paste0('ZARR:\"/vsicurl/https://ncsa.osn.xsede.org',\n       '/Pangeo/pangeo-forge/gpcp-feedstock/gpcp.zarr\"')\n\nafter which we can read the first 10 time steps using\n\nlibrary(stars)\nbounds = c(longitude = \"lon_bounds\", latitude = \"lat_bounds\")\n(r = read_mdim(dsn, bounds = bounds, count = c(NA, NA, 10)))\n# stars object with 3 dimensions and 1 attribute\n# attribute(s):\n#               Min. 1st Qu. Median Mean 3rd Qu. Max.\n# precip [mm/d]    0       0      0 2.25     1.6  109\n# dimension(s):\n#           from  to     offset  delta refsys x/y\n# longitude    1 360          0      1     NA [x]\n# latitude     1 180        -90      1     NA [y]\n# time         1  10 1996-10-01 1 days   Date\nst_bbox(r)\n# xmin ymin xmax ymax \n#    0  -90  360   90\n\nIn this example,\n\n\nNA values for count indicate to get all values available for that dimension\nhere, bounds variables needed explicit specification because the data source did not follow the more recent CF (1.10) conventions; and ignoring bounds would lead to a raster with a bounding box having latitude values outside \\([-90,90]\\).\nAPIs for data: GEE, openEO\n \nPlatforms that do not require the management and programming of virtual machines in the cloud but provide direct access to the imagery managed include GEE, openEO, and the climate data store.\nGoogle Earth Engine (GEE) is a cloud platform that allows users to compute on large amounts of Earth Observation data as well as modelling products (Gorelick et al. 2017). It has powerful analysis capabilities, including most of the data cube operations explained in Section 6.3. It has an interface where scripts can be written in JavaScript, and a Python interface to the same functionality. The code of GEE is not open-source, and cannot be extended by arbitrary user-defined functions in languages like Python or R. R package rgee (Aybar 2022) provides an R client interface to GEE.\nCloud-based data cube processing platforms built entirely around open source software are emerging, several of which using the openEO API (Schramm et al. 2021). This API allows for user-defined functions (UDFs) written in Python or R that are being passed on through the API and executed at the pixel level, for instance to aggregate or reduce dimensions using custom reducers. UDFs in R represent the data chunk to be processed as a stars object; in Python xarray objects are used.\nOther platforms include the Copernicus climate data store (Raoult et al. 2017) or atmosphere data store, which allow processing of atmospheric or climate data from ECMWF, including ERA5. An R package with an interface to both data stores is ecmwfr (Hufkens 2023).",
    "crumbs": [
      "R for Spatial Data Science",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Large data and cloud native</span>"
    ]
  },
  {
    "objectID": "09-Large.html#exercises",
    "href": "09-Large.html#exercises",
    "title": "9  Large data and cloud native",
    "section": "\n9.4 Exercises",
    "text": "9.4 Exercises\nUse R to solve the following exercises.\n\nFor the S2 image (above), find out in which order the bands are by using st_get_dimension_values(), and try to find out which spectral bands / colours they correspond to.\nCompute NDVI for the S2 image, using st_apply and an appropriate ndvi function. Plot the result to screen, and then write the result to a GeoTIFF. Explain the difference in runtime between plotting and writing.\nPlot an RGB composite of the S2 image, using the rgb argument to plot(), and then by using st_rgb() first.\nSelect five random points from the bounding box of S2, and extract the band values at these points; convert the object returned to an sf object.\nFor the 10-km radius circle around POINT(390000  5940000), use aggregate to compute the mean pixel values of the S2 image when downsampling the images with factor 30, and on the original resolution. Compute the relative difference between the results.\nUse hist to compute the histogram on the downsampled S2 image. Also do this for each of the bands. Use ggplot2 to compute a single plot with all four histograms.\nUse st_crop to crop the S2 image to the area covered by the 10-km circle. Plot the results. Explore the effect of setting argument crop = FALSE\n\nWith the downsampled image, compute the logical layer where all four bands have pixel values higher than 1000. Use a raster algebra expression on the four bands (use split first), or use st_apply for this.\n\n\n\n\n\n\n\nAppel, Marius. 2023. Gdalcubes: Earth Observation Data Cubes from Satellite Image Collections. https://github.com/appelmar/gdalcubes_R.\n\n\nAppel, Marius, and Edzer Pebesma. 2019. “On-Demand Processing of Data Cubes from Satellite Image Collections with the Gdalcubes Library.” Data 4 (3): 92. https://www.mdpi.com/2306-5729/4/3/92.\n\n\nAppel, Marius, Edzer Pebesma, and Matthias Mohr. 2021. Cloud-Based Processing of Satellite Image Collections in R Using STAC, COGs, and on-Demand Data Cubes. https://r-spatial.org/r/2021/04/23/cloud-based-cubes.html.\n\n\nAybar, Cesar. 2022. Rgee: R Bindings for Calling the Earth Engine API. https://CRAN.R-project.org/package=rgee.\n\n\nEaton, Brian, Jonathan Gregory, Bob Drach, Karl Taylor, Steve Hankin, Jon Blower, John Caron, et al. 2022. NetCDF Climate and Forecast (CF) Metadata Conventions, Version 1.10. https://cfconventions.org/.\n\n\nFellows, Ian, and using the JMapViewer library by Jan Peter Stotz. 2019. OpenStreetMap: Access to Open Street Map Raster Images. https://CRAN.R-project.org/package=OpenStreetMap.\n\n\nGorelick, Noel, Matt Hancher, Mike Dixon, Simon Ilyushchenko, David Thau, and Rebecca Moore. 2017. “Google Earth Engine: Planetary-Scale Geospatial Analysis for Everyone.” Remote Sensing of Environment 202: 18–27. https://doi.org/10.1016/j.rse.2017.06.031.\n\n\nHersbach, Hans, Bill Bell, Paul Berrisford, Shoji Hirahara, András Horányi, Joaquín Muñoz-Sabater, Julien Nicolas, et al. 2020. “The ERA5 Global Reanalysis.” Quarterly Journal of the Royal Meteorological Society 146 (730): 1999–2049. https://doi.org/10.1002/qj.3803.\n\n\nHufkens, Koen. 2023. Ecmwfr: Interface to ECMWF and CDS Data Web Services. https://github.com/bluegreen-labs/ecmwfr.\n\n\nMark Padgham, Bob Rudis, Robin Lovelace, and Maëlle Salmon. 2017. Osmdata. The Journal of Open Source Software. Vol. 2. The Open Journal. https://doi.org/10.21105/joss.00305.\n\n\nPebesma, Edzer. 2022. Reading Zarr Files with r Package Stars. https://r-spatial.org/r/2022/09/13/zarr.html.\n\n\nRaoult, Baudouin, Cedric Bergeron, Angel López Alós, Jean-Noël Thépaut, and Dick Dee. 2017. “Climate Service Develops User-Friendly Data Store.” ECMWF Newsletter 151: 22–27.\n\n\nSchramm, Matthias, Edzer Pebesma, Milutin Milenković, Luca Foresta, Jeroen Dries, Alexander Jacob, Wolfgang Wagner, et al. 2021. “The openEO API–Harmonising the Use of Earth Observation Cloud Services Using Virtual Data Cube Functionalities.” Remote Sensing 13 (6). https://doi.org/10.3390/rs13061125.\n\n\nSimoes, Rolf, Felipe Carvalho, and Brazil Data Cube Team. 2023. Rstac: Client Library for SpatioTemporal Asset Catalog. https://brazil-data-cube.github.io/rstac/.",
    "crumbs": [
      "R for Spatial Data Science",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Large data and cloud native</span>"
    ]
  },
  {
    "objectID": "part-3.html",
    "href": "part-3.html",
    "title": "Models for Spatial Data",
    "section": "",
    "text": "The first two parts of this book contain a considerable amount of concepts that one could classify as “models for spatial data”, including:\n\nhow numbers relate to real-world phenomena (2  Coordinates)\nhow coordinates are defined in different spaces (2  Coordinates, 4  Spherical Geometries)\nsimple feature geometries (3  Geometries), how straight lines between points can be used to define linestrings and polygons\nthe set of geometry types (Simple feature geometries)\nsupport and the way feature attributes can relate to geometries (5  Attributes and Support)\nhow simple tesselations can describe space-time volumes (6  Data Cubes)\nhow these concepts can be made operational using data science software (7  Introduction to sf and stars)\n\nThe third and largest part of this book is dedicated to statistical modelling of spatial data. The scientific discipline statistics is concerned with describing and understanding variability in observations, and predicting future observations. Observations are often modelled as\n\\[\\mbox{observed}=\\mbox{explained}+\\mbox{remainder}\\]\nwhere “remainder” refers to variation that could not be explained by predictors, including measurement error but also lack of fit or variation caused by model misspecification. For spatial data, a further term is often helpful, as in\n\\[\\mbox{observed}=\\mbox{explained} + \\mbox{smooth} + \\mbox{remainder}\\]\nwhere “smooth” refers to variation that is not explained by external predictors but that clearly shows “smooth” spatial patterns, as opposed to the “rough” remainder which does not do this. Such a “smooth” term can for instance be modelled by base functions in coordinates (splines, smoothers) or as a random term that is spatially correlated.\n10  Statistical modelling of spatial data introduces statistical modelling of spatial data, as a preparation to the subsequent chapters but also highlighting a number of relevant aspects that are not elaborated on in later chapters. It tries to bridge these chapters with concepts from the first part of this book, in particular support (5  Attributes and Support).\nIt is now obvious that a complete and comprehensive treatment of the topic of statistical models for spatial data that also includes instructions about the use of computational software in a single book is an impossible task. The spatstat book (Baddeley, Rubak, and Turner 2015) has around 850 pages for only spatial point patterns and R. This part focuses on the three “classical” spatial statistics topics: analysis of point patterns (11  Point Pattern Analysis), geostatistical data (Chapters 12  Spatial Interpolation and 13  Multivariate and Spatiotemporal Geostatistics), and lattice (areal) data (Chapters 14  Proximity and Areal Data-17  Spatial Econometrics Models). Where possible we attempt to refer to further literature on methods and software implementations in R.\n\n\n\n\n\n\nBaddeley, Adrian, Ege Rubak, and Rolf Turner. 2015. Spatial Point Patterns: Methodology and Applications with R. Chapman & Hall/CRC.",
    "crumbs": [
      "Models for Spatial Data"
    ]
  },
  {
    "objectID": "10-Models.html",
    "href": "10-Models.html",
    "title": "10  Statistical modelling of spatial data",
    "section": "",
    "text": "10.1 Mapping with non-spatial regression and ML models\nSo far in this book, we mostly addressed the problem of describing data. This included geometrical measures, predicates, or transformations that involved geometries, or by summary measures of attributes, or by plots involving variability in the geometry, the feature attributes, or both.\nStatistical modelling aims at going beyond describing the data, it considers the data as a sample drawn from a population, and tries to make assessments (inference) about the population sampled from, for instance by quantifying relationships between variables, estimating population parameters, and predicting the outcome of observations that could have been taken but were not, as is the case in interpolation problems. This is usually done by adopting a model for the data, where for instance observations are decomposed as follows:\n\\[\n\\mbox{observed} = \\mbox{explained} + \\mbox{remainder}\n\\tag{10.1}\\]\nwhere “explained” typically uses external variables (predictors, covariates, in machine learning confusingly also called features) that are related to the observed variable and some kind of regression model to translate into variability of the observed variable, and “remainder” is remaining variability that could not be explained. Interest may focus on the nature and magnitude or the relations between predictors and the observed variable, or in predicting new observations.\nStatistical models, and sampling hinge on the concept of probability, which in typical spatial data science problems is not a force of nature but has to be assumed in one way or another. If we are faced with data that come from (spatially) random sampling and we are interested in estimating means or totals, a design-based approach that assumes randomness in the sample locations is the most straightforward analysis approach, as pointed out in more detail in Section 10.4. If observations were not sampled randomly, or if our interest is in predicting values at specific locations (mapping), a model-based approach is needed. The remaining chapters in this part deal with model-based approaches.\nRegression models or other machine learning (ML) models can be applied to spatial and spatiotemporal data just the way they are applied for predicting new observations in non-spatial problems:\nObjects of class sf need no special treatment, as they are data.frames. To create maps of the resulting predictions, predicted values need to be added to the sf object, which can be done using the nc dataset loaded as in Chapter 1 by:\nnc |&gt; mutate(SID = SID74/BIR74, NWB = NWBIR74/BIR74) -&gt; nc1\nlm(SID ~ NWB, nc1) |&gt;\n  predict(nc1, interval = \"prediction\") -&gt; pr\nbind_cols(nc, pr) |&gt; names()\n#  [1] \"AREA\"      \"PERIMETER\" \"CNTY_\"     \"CNTY_ID\"   \"NAME\"     \n#  [6] \"FIPS\"      \"FIPSNO\"    \"CRESS_ID\"  \"BIR74\"     \"SID74\"    \n# [11] \"NWBIR74\"   \"BIR79\"     \"SID79\"     \"NWBIR79\"   \"geom\"     \n# [16] \"fit\"       \"lwr\"       \"upr\"\nwhere we see that\nIn general the datasets for model estimation and prediction do not have to be the same. Section 7.4.6 points out how this can be done with stars objects (essentially by going through a long data.frame representation of the datacube and converting the predicted results back, potentially in a chunked fashion).\nBecause many regression and ML type problems share this same structure, packages like caret (Kuhn 2022) or tidymodels (Kuhn and Wickham 2022) allow for automated evaluation and comparison over a large set of model alternatives, offering a large set of model evaluation criteria and cross-validation strategies. Such cross-validation approaches assume independent observations, which is often not a reasonable assumption for spatial data, for instance because of spatial correlation (Ploton et al. 2020) or because of strong spatial clustering in sample data (Meyer and Pebesma 2022), or both, and a number of R packages provide methods that are meant as replacements for naive cross-validation, including spatialsample (Silge and Mahoney 2023), CAST (Meyer, Milà, and Ludwig 2023), mlr3spatial (Becker and Schratz 2022), and mlr3spatiotempcv (Schratz and Becker 2022).\nStrong spatial clustering of sample can arise when sample data are composed by joining different databases, each with very different sampling density. This is often the case in global datasets (Meyer and Pebesma 2022). Another example of strong clustering arises when, for sampling ground truth points of a land cover class, polygons are digitised and points are sampled within these polygons at the resolution of pixels in satellite imagery.\nSpatial correlation in the “remainder” part of the model may be decreased by adding spatial coordinates or functions of spatial coordinates to the set of predictors. This also carries a risk of over-optimistic predictions in extrapolation cases, (cross-) validation, and model assessment, and is further discussed in Section 10.5.",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Statistical modelling of spatial data</span>"
    ]
  },
  {
    "objectID": "10-Models.html#sec-lm",
    "href": "10-Models.html#sec-lm",
    "title": "10  Statistical modelling of spatial data",
    "section": "",
    "text": "estimate: for a set of observations, a regression or ML model is fitted using predictor values corresponding to the observations (in ML jargon, this step is also known as “train”)\n\npredict: for a new situation, known predictor values are combined with the fitted model to predict the value of the observed variable, along with a prediction error or prediction interval if possible\n\n\n\n\n\n\nlm estimates a linear model and works directly on an sf object\nthe output is used for a predict model, which predicts values corresponding to the observations in nc1, the same sf object\n\npredict creates three columns: fit for predicted values and lwr and upr for the 95% prediction intervals\nthese three columns have been added to the final object using bind_cols.",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Statistical modelling of spatial data</span>"
    ]
  },
  {
    "objectID": "10-Models.html#sec-supportstatistical",
    "href": "10-Models.html#sec-supportstatistical",
    "title": "10  Statistical modelling of spatial data",
    "section": "\n10.2 Support and statistical modelling",
    "text": "10.2 Support and statistical modelling\nSupport of data (Section 1.6; Chapter 5) plays a lead role in the statistical analysis of spatial data. Methods for areal data (Chapters 14-17) are devised for data with area support, where the set of areas cover the entire area of interest.\nBy showing an extensive variable (Section 5.3.1) in a polygon choropleth map as done in Figure 1.1 one runs the risk that the information is related with the polygon size, and that the signal shown is actually the size of the polygons, in colour. For the variable population count one would divide by the polygon area to show the (intensive) variable population density, in order to create an informative map. In the analysis of health data, like disease incidences over a time period shown in Figure 1.1, rather than dividing by polygon area to get a spatial density, observations are usually converted to probabilities or incidence rates by dividing over the population size of the associated polygons. As such they are (still) associated with the polygon area but their support is associated with the population total. It is these totals that inform the (Poisson) variability used by subsequent modelling in for instance CAR-type models (Chapter 16).\nChapter 11 deals in principle with point support observations, but at some stage needs to acknowledge that observations have non-zero size: tree stem “points” cannot be separated distances smaller than the tree diameter. Also, points in point pattern analysis are considered in their observation window, the area for which the point dataset is exhaustive, or complete. The observation window is of influence in many of the analysis tools. If points are observed on a line network, then the observation window consists of the set of lines observed, and distances measured through this network.\nGeostatistical data (Chapters 12 and 13) usually start with point support observations and may end with predictions (spatial interpolations) for unobserved point locations distributed over the area of interest, or, may end in predictions for means over areas (block kriging; Section 12.5). Alternatively, observations may be aggregates over regions (Skøien et al. 2014). In remote sensing data, pixel values are usually associated with aggregates over the pixel area. Challenges may be the filling of gaps in images such as gaps caused by cloud coverage, from pixels neighbouring in space and time Militino et al. (2019).\nWhen combining data with different spatial supports, for instance polygons from administrative regions and raster layers, it is often seen that all information is “brought together” to the highest resolution, by simply extracting polygon values at pixel locations, and proceeding from there, with all the newly created “observations”. This of course bears a large risk of producing non-sensible results when analysing these “data”, and a proper downsampling strategy, possibly using simulations to cope with uncertainty, would be a better alternative. For naive users, using software that is not aware of support of values associated with areas and using software that does not warn against naive downsampling is of course not a helpful situation.",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Statistical modelling of spatial data</span>"
    ]
  },
  {
    "objectID": "10-Models.html#time-in-predictive-models",
    "href": "10-Models.html#time-in-predictive-models",
    "title": "10  Statistical modelling of spatial data",
    "section": "\n10.3 Time in predictive models",
    "text": "10.3 Time in predictive models\nSchabenberger and Gotway (2005) already noted that in many cases, statistical analysis of spatiotemporal data proceeds either by reducing time, then working on the problem spatially (time first, space later) or reducing space, then working on the problem temporally (space first, time later). An example of the first approach is given in Chapter 12 where a dataset with a year of hourly values (detailed in Chapter 13) are reduced to station mean values (time first) after which these means are interpolated spatially (space later). Examples from the area of remote sensing are\n\n\nSimoes et al. (2021) use supervised machine learning and time series deep learning to segmentise pixel time series into sequences of land use (time first), and then smooth the resulting sequences of maps to remove improbable transitions in isolated pixels (space later)\n\nVerbesselt et al. (2010) use (unsupervised) structural change algorithms to find breakpoints in pixel time series (time first), which are interpreted in the context deforestation later on.\n\nExamples of space first, time later in the area of remote sensing are any case where a single scene or scenes belonging to a single season are classified, and multi-year changes in land use or land cover are assessed by comparing time sequences of classified scenes. An example of this is Brown et al. (2022). Examples where space and time are considered jointly are the spatiotemporal interpolation in Chapter 13, and Lu et al. (2016) in the context of remote sensing.",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Statistical modelling of spatial data</span>"
    ]
  },
  {
    "objectID": "10-Models.html#sec-design",
    "href": "10-Models.html#sec-design",
    "title": "10  Statistical modelling of spatial data",
    "section": "\n10.4 Design-based and model-based inference",
    "text": "10.4 Design-based and model-based inference\n \nStatistical inference means the action of estimating parameters about a population from sample data. Suppose we denote the variable of interest with \\(z(s)\\), where \\(z\\) is the attribute value measured at location \\(s\\), and we are interested in estimating the mean value of \\(z(s)\\) over a domain \\(D\\), \\[z(s)=\\frac{1}{|D|} \\int_{ u \\in D} z(u)du,\\] with \\(|D|\\) the area of \\(D\\), from sample data \\(z(s_1),...,z(s_n)\\).\nThen, there are two possibilities to proceed: model-based, or design-based. A model-based approach considers \\(z(s)\\) to be a realisation of a super-population \\(Z(s)\\) (using capital letters to indicate random variables), and could for instance postulate a model for its spatial variability in the form of \\[Z(s) = m + e(s), \\  \\ \\mbox{E}(e(s)) = 0, \\  \\ \\mbox{Cov(e(s))} = \\Sigma(\\theta)\\] with \\(m\\) a constant mean and \\(e(s)\\) a residual with mean zero and covariance matrix \\(\\Sigma(\\theta)\\). This would require choosing the covariance function \\(\\Sigma()\\) and estimating its parameters \\(\\theta\\) from \\(z(s)\\), and then computing a block kriging prediction \\(\\hat{Z}(D)\\) (Section 12.5). This approach makes no assumptions about how \\(z(s)\\) was sampled spatially, but of course it should allow for choosing the covariance function and estimating its parameters; inference is conditional to the validity of the postulated model.\n \nRather than assuming a superpopulation model, the design-based approach (De Gruijter and Ter Braak 1990; Brus 2021a; Breidt, Opsomer, et al. 2017) assumes randomness in the locations, which is justified (only) when using random sampling. It requires that the sample data were obtained by probability sampling, meaning that some form of spatial random sampling was used where all elements of \\(z(s)\\) had a known and positive probability of being included in the sample obtained. The random process is that of sampling: \\(z(s_1)\\) is a realisation of the random process \\(z(S_1)\\), the first observation taken over repeated random sampling. Design-based estimators only need these inclusion probabilities to estimate mean values with standard errors. This means that for instance given a simple random sample, the unweighted sample mean is used to estimate the population mean, and no model parameters need to be fit.\nNow the question is whether \\(z(s_1)\\) and \\(z(s_2)\\) can be expected to be correlated when \\(s_1\\) and \\(s_2\\) are close together. The question does not work out as long as \\(z(s_1)\\) and \\(z(s_2)\\) are just two numbers: we need some kind of framework, random variables, that recreates this situation to form two sets of numbers for which we can consider correlation. The misconception here, as explained in Brus (2021a), is that the two are always spatially correlated, but this is only the case when working under model-based approaches: \\(Z(s_1)\\) and \\(Z(s_2)\\) may well be correlated (“model-dependent”), but although in a particular random sample (realisation) \\(z(s_1)\\) and \\(z(s_2)\\) may be close in space, the corresponding random variables \\(z(S_1)\\) and \\(z(S_2)\\) considered over repeated random sampling are not close together, and, are design-independent. Both situations can coexist without contradiction and are a consequence of choosing to work under one inference framework or the other.\nThe choice whether to work under a design-based or model-based framework depends on the purpose of the study and the data collection process. The model-based framework lends itself best for cases:\n\nwhere predictions are required for individual locations, or for areas too small to be sampled\nwhere the available data were not collected using a known random sampling scheme (i.e., the inclusion probabilities are unknown, or are zero over particular areas and or times)\n\nDesign-based approaches are most suitable when:\n\nobservations were collected using a spatial random sampling process\naggregated properties of the entire sample region (or sub-region) are needed\nestimates are required that are not sensitive to model misspecification, for instance when needed for regulatory or legal purposes\n\nIn case a sampling procedure is to be planned (De Gruijter et al. 2006), some form of spatial random sampling is definitely worth considering since it opens up the possibility of following both inference frameworks.",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Statistical modelling of spatial data</span>"
    ]
  },
  {
    "objectID": "10-Models.html#sec-models-with-coordinates",
    "href": "10-Models.html#sec-models-with-coordinates",
    "title": "10  Statistical modelling of spatial data",
    "section": "\n10.5 Predictive models with coordinates",
    "text": "10.5 Predictive models with coordinates\n \nIn data science projects, coordinates may be seen as features in a larger set of predictors (or features, or covariates) and treated accordingly. There are some pitfalls in doing so.\nAs usual when working with predictors, it is good to choose predictive methods that are not sensitive to shifts in origin or shifts in unit (scale). Assuming a two-dimensional problem, predictive models should also not be sensitive to arbitrary rotations of the \\(x\\)- and \\(y\\)- or latitude and longitude axes. For projected (2D, Cartesian) coordinates this can be assured, e.g., by using polynomials of order \\(n\\) as \\((x+y)^n\\), rather than \\((x)^n + (y)^n\\); for a second order polynomial this involves including the term \\(xy\\), so that an ellipsoidal-shape trend surface does not have to be aligned with the \\(x-\\) or \\(y-\\)axis. For a GAM model with spline components, one would use a spline in two dimensions \\(s(x,y)\\) rather than two independent splines \\(s(x)\\) and \\(s(y)\\) that do not allow for interaction. An exception to this “rule” would be when a pure latitude effect is desired for instance to account for yearly total solar energy influx.\nWhen the area covered by the data is large, the difference between using ellipsoidal coordinates and projected coordinates will automatically become larger, and hence choosing one of both will have an effect on predictive modelling. For very large extents and global models, polynomials or splines in latitude and longitude will not work well as they ignore the circular nature of longitude and the coordinate singularities at the poles. Here, spherical harmonics, base functions that are continuous on the sphere with increasing spatial frequencies can replace polynomials or be used as spline base functions.\nIn many cases, the spatial coordinates over which samples were collected also define the space over which predictions are made, setting them apart from other features. Many simple predictive approaches, including most machine learning methods, assume sample data to be independent. When samples are collected by spatially random sampling over the spatial target area, this assumption may be justified when working under a design-based context (Brus 2021b). This context, however, treats the coordinate space as the variable over which we randomise, which affords predicting values for a new randomly chosen location but rules out making predictions for fixed locations; this implies that averages of areas over which samples were collected, can be obtained, but not spatial interpolations. In case predictions for fixed locations are required, or in case data were not collected by spatial random sampling, a model-based approach (as taken in Chapter 12) is needed and typically some form of spatial and/or temporal autocorrelation of residuals must be assumed.\n \nA common case is where sample data are collected opportunistically (“whatever could be found”), and are then used in a predictive framework that does not weight them. This has a consequence that the resulting model may be biased towards over-represented areas (in predictor space and/or in spatial coordinates space), and that simple (random) cross-validation statistics may be over-optimistic when taken as performance measures for spatial prediction (Meyer and Pebesma 2021, 2022; Mila et al. 2022). Adaptive cross-validation measures such as spatial cross-validation may help getting more relevant measures for predictive performance.",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Statistical modelling of spatial data</span>"
    ]
  },
  {
    "objectID": "10-Models.html#exercises",
    "href": "10-Models.html#exercises",
    "title": "10  Statistical modelling of spatial data",
    "section": "\n10.6 Exercises",
    "text": "10.6 Exercises\nUse R to solve the following exercises.\n\nFollowing the lm example of Section 10.1 use a random forest model to predict SID values (e.g., using package randomForest), and plot the random forest predictions against observations, along with the \\(x=y\\) line.\nCreate a new dataset by randomly sampling 1000 points from the nc dataset, and rerun the linear regression model of Section 10.1 on this dataset. Consider the summary of the fitted models, in particular the estimated coefficients, their standard errors, and the residual standard error. What has changed?\nRedo the water-land classification of Section 7.4.6 using class::knn instead of lda, using a value of k = 5, and compare the resulting predictions with those of lda.\nFor the linear model using nc and for the knn example of the previous exercise, add a first and a second order linear model in the spatial coordinates and compare the results (use st_centroid to obtain polygon centroids, and st_coordinates to extract the x and y coordinates in matrix form).\n\n\n\n\n\n\n\nBecker, Marc, and Patrick Schratz. 2022. Mlr3spatial: Support for Spatial Objects Within the Mlr3 Ecosystem. https://CRAN.R-project.org/package=mlr3spatial.\n\n\nBreidt, F Jay, Jean D Opsomer, et al. 2017. “Model-Assisted Survey Estimation with Modern Prediction Techniques.” Statistical Science 32 (2): 190–205.\n\n\nBrown, C. F., Steven P Brumby, Brookie Guzder-Williams, Tanya Birch, Samantha Brooks Hyde, Joseph Mazzariello, Wanda Czerwinski, et al. 2022. “Dynamic World, Near Real-Time Global 10 m Land Use Land Cover Mapping.” Scientific Data 9 (1): 1–17. https://doi.org/10.1038/s41597-022-01307-4.\n\n\nBrus, Dick J. 2021a. “Statistical Approaches for Spatial Sample Survey: Persistent Misconceptions and New Developments.” European Journal of Soil Science 72 (2): 686–703. https://doi.org/10.1111/ejss.12988.\n\n\n———. 2021b. “Statistical Approaches for Spatial Sample Survey: Persistent Misconceptions and New Developments.” European Journal of Soil Science 72 (2): 686–703. https://doi.org/10.1111/ejss.12988.\n\n\nDe Gruijter, J. J., Dick J. Brus, Marc F. P. Bierkens, and Martin Knotters. 2006. Sampling for Natural Resource Monitoring. Springer Science & Business Media.\n\n\nDe Gruijter, J. J., and C. J. F. Ter Braak. 1990. “Model-Free Estimation from Spatial Samples: A Reappraisal of Classical Sampling Theory.” Mathematical Geology 22 (4): 407–15.\n\n\nGerber, Florian, Rogier de Jong, Michael E Schaepman, Gabriela Schaepman-Strub, and Reinhard Furrer. 2018. “Predicting Missing Values in Spatio-Temporal Remote Sensing Data.” IEEE Transactions on Geoscience and Remote Sensing 56 (5): 2841–53.\n\n\nHeaton, Matthew J., Abhirup Datta, Andrew O. Finley, Reinhard Furrer, Joseph Guinness, Rajarshi Guhaniyogi, Florian Gerber, et al. 2018. “A Case Study Competition Among Methods for Analyzing Large Spatial Data.” Journal of Agricultural, Biological and Environmental Statistics, December. https://doi.org/10.1007/s13253-018-00348-w.\n\n\nKuhn, Max. 2022. Caret: Classification and Regression Training. https://github.com/topepo/caret/.\n\n\nKuhn, Max, and Hadley Wickham. 2022. Tidymodels: Easily Install and Load the Tidymodels Packages. https://CRAN.R-project.org/package=tidymodels.\n\n\nLu, Meng, Edzer Pebesma, Alber Sanchez, and Jan Verbesselt. 2016. “Spatio-Temporal Change Detection from Multidimensional Arrays: Detecting Deforestation from MODIS Time Series.” ISPRS Journal of Photogrammetry and Remote Sensing 117: 227–36. https://doi.org/10.1016/j.isprsjprs.2016.03.007.\n\n\nMeyer, Hanna, Carles Milà, and Marvin Ludwig. 2023. CAST: Caret Applications for Spatial-Temporal Models. https://CRAN.R-project.org/package=CAST.\n\n\nMeyer, Hanna, and Edzer Pebesma. 2021. “Predicting into Unknown Space? Estimating the Area of Applicability of Spatial Prediction Models.” Methods in Ecology and Evolution 12 (9): 1620–33. https://doi.org/10.1111/2041-210X.13650.\n\n\n———. 2022. “Machine Learning-Based Global Maps of Ecological Variables and the Challenge of Assessing Them.” Nature Communincations 13.https://doi.org/10.1038/s41467-022-29838-9 .\n\n\nMila, Carles, Jorge Mateu, Edzer Pebesma, and Hanna Meyer. 2022. “Nearest Neighbour Distance Matching Leave-One-Out Cross-Validation for Map Validation.” Methods in Ecology and Evolution 13 (6): 1304–16. https://doi.org/10.1111/2041-210X.13851.\n\n\nMilitino, A. F., M. D. Ugarte, U. Pérez-Goya, and M. G. Genton. 2019. “Interpolation of the Mean Anomalies for Cloud-Filling in Land Surface Temperature and Normalized Difference Vegetation Index.” IEEE Transactions on Geoscience and Remote Sensing 57 (8): 6068–78. https://doi.org/10.1109/TGRS.2019.2904193.\n\n\nPloton, Pierre, Frédéric Mortier, Maxime Réjou-Méchain, Nicolas Barbier, Nicolas Picard, Vivien Rossi, Carsten Dormann, et al. 2020. “Spatial Validation Reveals Poor Predictive Performance of Large-Scale Ecological Mapping Models.” Nature Communications 11 (1): 4540. https://www.nature.com/articles/s41467-020-18321-y.\n\n\nSchabenberger, O., and C. A. Gotway. 2005. Statistical Methods for Spatial Data Analysis. Boca Raton/London: Chapman & Hall/CRC.\n\n\nSchratz, Patrick, and Marc Becker. 2022. Mlr3spatiotempcv: Spatiotemporal Resampling Methods for Mlr3. https://CRAN.R-project.org/package=mlr3spatiotempcv.\n\n\nSilge, Julia, and Michael Mahoney. 2023. Spatialsample: Spatial Resampling Infrastructure. https://CRAN.R-project.org/package=spatialsample.\n\n\nSimoes, Rolf, Gilberto Camara, Gilberto Queiroz, Felipe Souza, Pedro R. Andrade, Lorena Santos, Alexandre Carvalho, and Karine Ferreira. 2021. “Satellite Image Time Series Analysis for Big Earth Observation Data.” Remote Sensing 13 (13). https://doi.org/10.3390/rs13132428.\n\n\nSkøien, Jon O, Günter Blöschl, Gregor Laaha, E Pebesma, Juraj Parajka, and Alberto Viglione. 2014. “Rtop: An R Package for Interpolation of Data with a Variable Spatial Support, with an Example from River Networks.” Computers & Geosciences 67: 180–90.\n\n\nVerbesselt, Jan, Rob Hyndman, Glenn Newnham, and Darius Culvenor. 2010. “Detecting Trend and Seasonal Changes in Satellite Image Time Series.” Remote Sensing of Environment 114 (1): 106–15. https://doi.org/10.1016/j.rse.2009.08.014.",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Statistical modelling of spatial data</span>"
    ]
  },
  {
    "objectID": "11-PointPattern.html",
    "href": "11-PointPattern.html",
    "title": "11  Point Pattern Analysis",
    "section": "",
    "text": "11.1 Observation window\nPoint pattern analysis is concerned with describing patterns of points over space and making inference about the process that could have generated an observed pattern. The main focus here lies on the information carried in the locations of the points, and typically these locations are not controlled by sampling but a result of a process we are interested in, such as animal sightings, accidents, disease cases, or tree locations. This is opposed to geostatistical processes (Chapter 12) where we have values of some phenomenon everywhere but observations limited to a set of locations that we can control, at least in principle. Hence, in geostatistical problems the prime interest is not in the observation locations but in estimating the value of the observed phenomenon at unobserved locations. Point pattern analysis typically assumes that for an observed area, all points are available, meaning that locations without a point are not unobserved as in a geostatistical process, but are observed and contain no point. In terms of random processes, in point processes locations are random variables, where, in geostatistical processes the measured variable is a random field with locations fixed.\nThis chapter is confined to describing the very basics of point pattern analysis, using package spatstat (Baddeley, Turner, and Rubak 2022), and related packages by the same authors. The spatstat book of Baddeley, Rubak, and Turner (2015) gives a comprehensive introduction to point pattern theory and the use of the spatstat package family, which we will not try to copy. Inclusion of particular topics in this chapter should not be seen as an expression that these are more relevant than others. In particular, this chapter tries to illustrate interfaces existing between spatstat and the more spatial data science oriented packages sf and stars. A further book that introduces point patterns analysis is Stoyan et al. (2017). R package stpp (Gabriel et al. 2022) for analysing spatiotemporal point processes is discussed in Gabriel, Rowlingson, and Diggle (2013).\nImportant concepts of point patterns analysis are the distinction between a point pattern and a point process: the latter is the stochastic process that, when sampled, generates a point pattern. A dataset is always a point pattern, and inference involves figuring out the properties of a process that could have generated a pattern like the one we observed. Properties of a spatial point process include\nPoint patterns have an observation window. Consider the points generated randomly by\nlibrary(sf)\n# Linking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.4.0; sf_use_s2() is TRUE\nn &lt;- 30\nset.seed(13531) # remove this to create another random sequence\nxy &lt;- data.frame(x = runif(n), y = runif(n)) |&gt; \n    st_as_sf(coords = c(\"x\", \"y\"))\nthen these points are (by construction) uniformly distributed, or completely spatially random, over the domain \\([0,1] \\times [0,1]\\). For a larger domain, they are not uniform, for the two square windows w1 and w2 created by\nw1 &lt;- st_bbox(c(xmin = 0, ymin = 0, xmax = 1, ymax = 1)) |&gt; \n        st_as_sfc() \nw2 &lt;- st_sfc(st_point(c(1, 0.5))) |&gt; st_buffer(1.2)\nthis is shown in Figure 11.1.\nCodepar(mfrow = c(1, 2), mar = c(2.1, 2.1, 0.1, 0.5), xaxs = \"i\", yaxs = \"i\")\nplot(w1, axes = TRUE, col = 'grey')\nplot(xy, add = TRUE)\nplot(w2, axes = TRUE, col = 'grey')\nplot(xy, add = TRUE, cex = .5)\n\n\n\n\n\n\nFigure 11.1: Depending on the observation window (grey), the same point pattern can appear completely spatially random (left), or clustered (right)\nPoint patterns in spatstat are objects of class ppp that contain points and an observation window (an object of class owin). We can create a ppp from points by\nlibrary(spatstat) |&gt; suppressPackageStartupMessages()\nas.ppp(xy)\n# Planar point pattern: 30 points\n# window: rectangle = [0.009, 0.999] x [0.103, 0.996] units\nwhere we see that the bounding box of the points is used as observation window when no window is specified. If we add a polygonal geometry as the first feature of the dataset, then this is used as observation window:\n(pp1 &lt;- c(w1, st_geometry(xy)) |&gt; as.ppp())\n# Planar point pattern: 30 points\n# window: polygonal boundary\n# enclosing rectangle: [0, 1] x [0, 1] units\nc1 &lt;- st_buffer(st_centroid(w2), 1.2)\n(pp2 &lt;- c(c1, st_geometry(xy)) |&gt; as.ppp())\n# Planar point pattern: 30 points\n# window: polygonal boundary\n# enclosing rectangle: [-0.2, 2.2] x [-0.7, 1.7] units\nTo test for homogeneity, one could carry out a quadrat count, using an appropriate quadrat layout (a 3 \\(\\times\\) 3 layout is shown in Figure 11.2)\npar(mfrow = c(1, 2), mar = rep(0, 4))\nq1 &lt;- quadratcount(pp1, nx=3, ny=3)\nq2 &lt;- quadratcount(pp2, nx=3, ny=3)\nplot(q1, main = \"\")\nplot(xy, add = TRUE)\nplot(q2, main = \"\")\nplot(xy, add = TRUE)\n\n\n\n\n\n\nFigure 11.2: 3 \\(\\times\\) 3 quadrat counts for the two point patterns\nand carry out a \\(\\chi^2\\) test on these counts:\nquadrat.test(pp1, nx=3, ny=3)\n# Warning: Some expected counts are small; chi^2 approximation may be\n# inaccurate\n# \n#   Chi-squared test of CSR using quadrat counts\n# \n# data:  pp1\n# X2 = 8, df = 8, p-value = 0.9\n# alternative hypothesis: two.sided\n# \n# Quadrats: 9 tiles (irregular windows)\nquadrat.test(pp2, nx=3, ny=3)\n# Warning: Some expected counts are small; chi^2 approximation may be\n# inaccurate\n# \n#   Chi-squared test of CSR using quadrat counts\n# \n# data:  pp2\n# X2 = 43, df = 8, p-value = 2e-06\n# alternative hypothesis: two.sided\n# \n# Quadrats: 9 tiles (irregular windows)\nwhich indicates that for the second case we have an indication that this is not a CSR (completely spatially random) pattern. As indicated by the warning, we should take the p-values with a large grain of salt because we have too small expected counts.\nKernel densities can be computed using density, where kernel shape and bandwidth can be controlled. Here, cross-validation is used by function bw.diggle to specify the bandwidth parameter sigma; plots are shown in Figure 11.3.\nden1 &lt;- density(pp1, sigma = bw.diggle)\nden2 &lt;- density(pp2, sigma = bw.diggle)\nCodepar(mfrow = c(1, 2), mar = c(0,0,1.1,2))\nplot(den1)\nplot(pp1, add=TRUE)\nplot(den2)\nplot(pp1, add=TRUE)\n\n\n\n\n\n\nFigure 11.3: Kernel densities for both point patterns\nThe density maps created this way are obviously raster images, and we can convert them into stars object by\nlibrary(stars)\n# Loading required package: abind\ns1 &lt;- st_as_stars(den1)\n(s2 &lt;- st_as_stars(den2))\n# stars object with 2 dimensions and 1 attribute\n# attribute(s):\n#        Min.  1st Qu. Median Mean 3rd Qu. Max. NA's\n# v  6.28e-15 0.000153  0.304 6.77    13.1 42.7 3492\n# dimension(s):\n#   from  to offset  delta x/y\n# x    1 128   -0.2 0.0187 [x]\n# y    1 128   -0.7 0.0188 [y]\nand we can verify that the area under the density surface is similar to the sample size (30), by\ns1$a &lt;- st_area(s1) |&gt; suppressMessages()\ns2$a &lt;- st_area(s2) |&gt; suppressMessages()\nwith(s1, sum(v * a, na.rm = TRUE))\n# [1] 29\nwith(s2, sum(v * a, na.rm = TRUE))\n# [1] 30.7\nMore exciting applications involve modelling the density surface as a function of external variables. Suppose we want to model the density of pp2 as a Poisson point process (meaning that points do not interact with each other), where the intensity is a function of distance to the centre of the “cluster”, and these distance are available in a stars object:\npt &lt;- st_sfc(st_point(c(0.5, 0.5)))\nst_as_sf(s2, as_points = TRUE, na.rm = FALSE) |&gt;\n  st_distance(pt) -&gt; s2$dist\nwe can then model the densities using ppm, where the name of the point pattern object is used on the left-hand side of the formula:\n(m &lt;- ppm(pp2 ~ dist, data = list(dist = as.im(s2[\"dist\"]))))\n# Nonstationary Poisson process\n# Fitted to point pattern dataset 'pp2'\n# \n# Log intensity:  ~dist\n# \n# Fitted trend coefficients:\n# (Intercept)        dist \n#        4.54       -4.25 \n# \n#             Estimate  S.E. CI95.lo CI95.hi Ztest  Zval\n# (Intercept)     4.54 0.341    3.87    5.21   *** 13.32\n# dist           -4.25 0.701   -5.62   -2.88   *** -6.06\nThe returned object is of class ppm, and can be plotted: Figure 11.4 shows the predicted surface. The prediction standard error can also be plotted.\nCodeplot(m, se = FALSE)\n\n\n\n\n\n\nFigure 11.4: Predicted densities of a ppm model\nThe model also has a predict method, which returns an im object that can be converted into a stars object by\npredict(m, covariates = list(dist = as.im(s2[\"dist\"]))) |&gt;\n    st_as_stars()\n# stars object with 2 dimensions and 1 attribute\n# attribute(s):\n#      Min. 1st Qu. Median Mean 3rd Qu. Max. NA's\n# v  0.0694   0.527   2.12 6.62     7.3 89.9 3492\n# dimension(s):\n#   from  to offset  delta x/y\n# x    1 128   -0.2 0.0187 [x]\n# y    1 128   -0.7 0.0187 [y]",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Point Pattern Analysis</span>"
    ]
  },
  {
    "objectID": "11-PointPattern.html#coordinate-reference-systems",
    "href": "11-PointPattern.html#coordinate-reference-systems",
    "title": "11  Point Pattern Analysis",
    "section": "\n11.2 Coordinate reference systems",
    "text": "11.2 Coordinate reference systems\nAll routines in spatstat are layed out for two-dimensional data with Cartesian coordinates. If we try to convert an object with ellipsoidal coordinates, we get an error:\n\nsystem.file(\"gpkg/nc.gpkg\", package = \"sf\") |&gt; \n    read_sf() |&gt;\n    st_geometry() |&gt;\n    st_centroid() |&gt;\n    as.ppp()\n# Error: Only projected coordinates may be converted to spatstat\n# class objects\n\nWhen converting to a spatstat data structure we lose the coordinate reference system we started with. It can be set back to sf or stars objects by using st_set_crs.",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Point Pattern Analysis</span>"
    ]
  },
  {
    "objectID": "11-PointPattern.html#marked-point-patterns-points-on-linear-networks",
    "href": "11-PointPattern.html#marked-point-patterns-points-on-linear-networks",
    "title": "11  Point Pattern Analysis",
    "section": "\n11.3 Marked point patterns, points on linear networks",
    "text": "11.3 Marked point patterns, points on linear networks\n\nA few more data types can be converted to and from spatstat. Marked point patterns are point patterns that have a “mark”, which is either a categorical label or a numeric label for each point. A dataset available in spatstat with marks is the longleaf pines dataset, containing diameter at breast height as a numeric mark:\n\nlongleaf\n# Marked planar point pattern: 584 points\n# marks are numeric, of storage type  'double'\n# window: rectangle = [0, 200] x [0, 200] metres\nll &lt;- st_as_sf(longleaf)\nprint(ll, n = 3)\n# Simple feature collection with 585 features and 2 fields\n# Geometry type: GEOMETRY\n# Dimension:     XY\n# Bounding box:  xmin: 0 ymin: 0 xmax: 200 ymax: 200\n# CRS:           NA\n# First 3 features:\n#    spatstat.geom..marks.x.  label                           geom\n# NA                      NA window POLYGON ((0 0, 200 0, 200 2...\n# 1                     32.9  point                POINT (200 8.8)\n# 2                     53.5  point                 POINT (199 10)\n\nValues can be converted back to ppp with\n\nas.ppp(ll)\n# Marked planar point pattern: 584 points\n# Mark variables: spatstat.geom..marks.x., label \n# window: polygonal boundary\n# enclosing rectangle: [0, 200] x [0, 200] units\n\n Line segments, in spatstat objects of class psp can be converted back and forth to simple feature with LINESTRING geometries following a POLYGON feature with the observation window, as in\n\nprint(st_as_sf(copper$SouthLines), n = 5)\n# Simple feature collection with 91 features and 1 field\n# Geometry type: GEOMETRY\n# Dimension:     XY\n# Bounding box:  xmin: -0.335 ymin: 0.19 xmax: 35 ymax: 158\n# CRS:           NA\n# First 5 features:\n#     label                           geom\n# 1  window POLYGON ((-0.335 0.19, 35 0...\n# 2 segment LINESTRING (3.36 0.19, 10.4...\n# 3 segment LINESTRING (12.5 0.263, 11....\n# 4 segment LINESTRING (11.2 0.197, -0....\n# 5 segment LINESTRING (6.35 12.8, 16.5...\n\nFinally, point patterns on linear networks, in spatstat represented by lpp objects, can be converted to sf by\n\nprint(st_as_sf(chicago), n = 5)\n# Simple feature collection with 620 features and 4 fields\n# Geometry type: GEOMETRY\n# Dimension:     XY\n# Bounding box:  xmin: 0.389 ymin: 153 xmax: 1280 ymax: 1280\n# CRS:           NA\n# First 5 features:\n#     label seg tp marks                           geom\n# 1  window  NA NA  &lt;NA&gt; POLYGON ((0.389 153, 1282 1...\n# 2 segment  NA NA  &lt;NA&gt; LINESTRING (0.389 1254, 110...\n# 3 segment  NA NA  &lt;NA&gt; LINESTRING (110 1252, 111 1...\n# 4 segment  NA NA  &lt;NA&gt; LINESTRING (110 1252, 198 1...\n# 5 segment  NA NA  &lt;NA&gt; LINESTRING (198 1277, 198 1...\n\nwhere we only see the first five features; the points are also in this object, as variable label indicates\n\ntable(st_as_sf(chicago)$label)\n# \n#   point segment  window \n#     116     503       1\n\nPotential information about network structure, how LINESTRING geometries are connected, is not present in the sf object. Package sfnetworks (van der Meer et al. 2022) would be a candidate package to hold such information in R or to pass on network data imported from OpenStreetMaps to spatstat.",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Point Pattern Analysis</span>"
    ]
  },
  {
    "objectID": "11-PointPattern.html#spatial-sampling-and-simulating-a-point-process",
    "href": "11-PointPattern.html#spatial-sampling-and-simulating-a-point-process",
    "title": "11  Point Pattern Analysis",
    "section": "\n11.4 Spatial sampling and simulating a point process",
    "text": "11.4 Spatial sampling and simulating a point process\n Package sf contains an st_sample method that samples points from MULTIPOINT, linear or polygonal geometries, using different spatial sampling strategies. It natively supports strategies “random”, “hexagonal”, “Fibonacci” (Section 11.5), and “regular”, where “regular” refers to sampling on a square regular grid and “hexagonal” essentially gives a triangular grid. For type “random”, it can return exactly the number of requested points, for other types this is approximate.\nst_sample also interfaces point process simulation functions of spatstat, when other values for sampling type are chosen. For instance the spatstat function rThomas is invoked when setting type = Thomas (Figure 11.5):\n\nkappa &lt;- 30 / st_area(w2) # intensity\nth &lt;- st_sample(w2, kappa = kappa, mu = 3, scale = 0.05, \n    type = \"Thomas\")\nnrow(th)\n# [1] 90\n\n\nCodepar(mar = rep(0, 4))\nplot(w2)\nplot(th, add = TRUE)\n\n\n\n\n\n\nFigure 11.5: Thomas process with mu = 3 and scale = 0.05\n\n\n\n\nThe help function obtained by ?rThomas details the meaning of the parameters kappa, mu, and scale. Simulating point processes means that the intensity is given, not the sample size. The sample size within the observation window obtained this way is a random variable.",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Point Pattern Analysis</span>"
    ]
  },
  {
    "objectID": "11-PointPattern.html#sec-simsphere",
    "href": "11-PointPattern.html#sec-simsphere",
    "title": "11  Point Pattern Analysis",
    "section": "\n11.5 Simulating points on the sphere",
    "text": "11.5 Simulating points on the sphere\n Another spatial random sampling type supported by sf natively (in st_sample) is simulation of random points on the sphere. An example of this is shown in Figure 11.6, where points were constrained to those in oceans. Points approximately regularly distributed over a sphere are obtained by st_sample with type = \"Fibonacci\" (González 2010).\n\nCodepar(mar = rep(0, 4), mfrow = c(1, 2))\nlibrary(s2)\ng &lt;- as_s2_geography(TRUE) # Earth\nco &lt;- s2_data_countries()\noc &lt;- s2_difference(g, s2_union_agg(co)) # oceans\nb &lt;- s2_buffer_cells(as_s2_geography(\"POINT(-30 -10)\"), 9800000) # visible half\ni &lt;- s2_intersection(b, oc) # visible ocean\nco &lt;- s2_intersection(b, co)\northo = st_crs(\"+proj=ortho +lat_0=-10 +lon_0=-30\")\n# background:\nst_transform(st_as_sfc(i), ortho) |&gt; plot(col = 'lightblue')\nst_transform(st_as_sfc(co), ortho) |&gt; plot(col = NA, add = TRUE, border = 'grey')\n# sampling randomly from globe:\nsf_use_s2(FALSE) |&gt; suppressMessages()\nif (packageVersion(\"sf\") &gt;= \"1.0.16\") {\n  globe = st_as_stars() |&gt; st_bbox() # st_sample only needs bbox\n} else {\n  globe = st_as_stars() |&gt; st_bbox() |&gt; st_as_sfc()\n}\npts &lt;- st_sample(globe, 1000, exact = FALSE) |&gt; suppressMessages()\nsf_use_s2(TRUE) |&gt; suppressMessages()\ns2_intersection(pts, i) |&gt; st_as_sfc() -&gt; pts\n# add:\nst_transform(pts, ortho) |&gt; plot(add = TRUE, pch = 3, cex = .5)\n# right: background:\nst_transform(st_as_sfc(i), ortho) |&gt; plot(col = 'lightblue')\nst_transform(st_as_sfc(co), ortho) |&gt; plot(col = NA, add = TRUE, border = 'grey')\n# Fibonacci:\nsf_use_s2(FALSE) |&gt; suppressMessages()\nst_sample(globe, 1000, type = \"Fibonacci\", exact = FALSE) |&gt; suppressMessages() -&gt; pts\nsf_use_s2(TRUE) |&gt; suppressMessages()\npts |&gt; s2_intersection(i) |&gt; st_as_sfc() -&gt; pts\nst_transform(pts, ortho) |&gt; plot(add = TRUE, pch = 3, cex = .5)\n\n\n\n\n\n\nFigure 11.6: Points sampled on the globe over the oceans: randomly (left) and approximately regular (Fibonacci; right), shown in an orthographic projection",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Point Pattern Analysis</span>"
    ]
  },
  {
    "objectID": "11-PointPattern.html#exercises",
    "href": "11-PointPattern.html#exercises",
    "title": "11  Point Pattern Analysis",
    "section": "\n11.6 Exercises",
    "text": "11.6 Exercises\n\nAfter loading spatstat, recreate the plot obtained by plot(longleaf) by using ggplot2 and geom_sf(), and by sf::plot().\nConvert the sample locations of the NO\\(_2\\) data used in Chapter 12 to a ppp object, with a proper window.\nCompute and plot the density of the NO\\(_2\\) dataset, import the density as a stars object, and compute the volume under the surface.\n\n\n\n\n\n\n\nBaddeley, Adrian, Ege Rubak, and Rolf Turner. 2015. Spatial Point Patterns: Methodology and Applications with R. Chapman & Hall/CRC.\n\n\nBaddeley, Adrian, Rolf Turner, and Ege Rubak. 2022. Spatstat: Spatial Point Pattern Analysis, Model-Fitting, Simulation, Tests. http://spatstat.org/.\n\n\nGabriel, Edith, Peter J Diggle, Barry Rowlingson, and Francisco J Rodriguez-Cortes. 2022. Stpp: Space-Time Point Pattern Simulation, Visualisation and Analysis. https://CRAN.R-project.org/package=stpp.\n\n\nGabriel, Edith, Barry Rowlingson, and Peter Diggle. 2013. “Stpp: An R Package for Plotting, Simulating and Analyzing Spatio-Temporal Point Patterns.” Journal of Statistical Software, Articles 53 (2): 1–29. https://doi.org/10.18637/jss.v053.i02.\n\n\nGonzález, Álvaro. 2010. “Measurement of Areas on a Sphere Using Fibonacci and Latitude–Longitude Lattices.” Mathematical Geosciences 42 (1): 49–64. https://arxiv.org/pdf/0912.4540.pdf.\n\n\nStoyan, Dietrich, Francisco J. Rodríguez-Cortés, Jorge Mateu, and Wilfried Gille. 2017. “Mark Variograms for Spatio-Temporal Point Processes.” Spatial Statistics 20: 125–47. https://doi.org/10.1016/j.spasta.2017.02.006.\n\n\nvan der Meer, Lucas, Lorena Abad, Andrea Gilardi, and Robin Lovelace. 2022. Sfnetworks: Tidy Geospatial Networks. https://CRAN.R-project.org/package=sfnetworks.",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Point Pattern Analysis</span>"
    ]
  },
  {
    "objectID": "12-Interpolation.html",
    "href": "12-Interpolation.html",
    "title": "12  Spatial Interpolation",
    "section": "",
    "text": "12.1 A first dataset\nSpatial interpolation is the activity of estimating values of spatially continuous variables (fields) for spatial locations where they have not been observed, based on observations. The statistical methodology for spatial interpolation, called geostatistics, is concerned with the modelling, prediction, and simulation of spatially continuous phenomena. The typical problem is a missing value problem: we observe a property of a phenomenon \\(Z(s)\\) at a limited number of sample locations \\(s_i, i = 1,...,n\\), and are interested in the property value at all locations \\(s_0\\) covering an area of interest, so we have to predict it for unobserved locations. This is also called kriging, or Gaussian Process prediction. In case \\(Z(s)\\) contains a white noise component \\(\\epsilon\\), as in \\(Z(s)=S(s)+\\epsilon\\) (possibly reflecting measurement error) an alternative but similar goal is to predict or simulate \\(S(s)\\) rather than \\(Z(s)\\), which may be called spatial filtering or smoothing.\nIn this chapter we will show simple approaches for handling geostatistical data, demonstrate simple interpolation methods, and explore modelling spatial correlation, spatial prediction and simulation. Chapter 13 focuses on more complex multivariate and spatiotemporal geostatistical models. We will use package gstat (Pebesma and Graeler 2022; Pebesma 2004), which offers a fairly wide palette of models and options for non-Bayesian geostatistical analysis. Bayesian methods with R implementations are found in Diggle, Tawn, and Moyeed (1998), Diggle and Ribeiro Jr. (2007), Blangiardo and Cameletti (2015), and Wikle, Zammit-Mangion, and Cressie (2019). An overview and comparison of methods for large datasets is given in Heaton et al. (2018).\nWe can read station mean NO\\(_2\\) values, a dataset that is prepared in Chapter 13, by loading it from package gstat using\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nno2 &lt;- read_csv(system.file(\"external/no2.csv\", \n    package = \"gstat\"), show_col_types = FALSE)\nand convert it into an sf object with an appropriate UTM projection using\nlibrary(sf)\n# Linking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.4.0; sf_use_s2() is TRUE\ncrs &lt;- st_crs(\"EPSG:32632\")\nst_as_sf(no2, crs = \"OGC:CRS84\", coords = \n    c(\"station_longitude_deg\", \"station_latitude_deg\")) |&gt;\n    st_transform(crs) -&gt; no2.sf\nNext, we can load country boundaries and plot these data using ggplot, shown in Figure 12.1.\nread_sf(\"data/de_nuts1.gpkg\") |&gt; st_transform(crs) -&gt; de\nCodeggplot() + geom_sf(data = de) + \n    geom_sf(data = no2.sf, mapping = aes(col = NO2))\n\n\n\n\n\n\nFigure 12.1: Mean NO\\(_2\\) concentrations in air for rural background stations in Germany, in 2017\nIf we want to interpolate, we first need to decide where. This is typically done on a regular grid covering the area of interest. Starting with the country outline in object de we can create a regular 10 km \\(\\times\\) 10 km grid over Germany by\nlibrary(stars) |&gt; suppressPackageStartupMessages()\nst_bbox(de) |&gt;\n  st_as_stars(dx = 10000) |&gt;\n  st_crop(de) -&gt; grd\ngrd\n# stars object with 2 dimensions and 1 attribute\n# attribute(s):\n#         Min. 1st Qu. Median Mean 3rd Qu. Max. NA's\n# values     0       0      0    0       0    0 2076\n# dimension(s):\n#   from to  offset  delta            refsys x/y\n# x    1 65  280741  10000 WGS 84 / UTM z... [x]\n# y    1 87 6101239 -10000 WGS 84 / UTM z... [y]\nHere, we chose grid cells not too fine, so that we still see them in plots.\nPerhaps the simplest interpolation method is inverse distance weighted interpolation, which is a weighted average, using weights inverse proportional to distances from the interpolation location:\n\\[\n\\hat{z}(s_0) = \\frac{\\sum_{i=1}^{n} w_i z(s_i)}{\\sum_{i=1}^n w_i}\n\\]\nwith \\(w_i = |s_0-s_i|^{-p}\\), and the inverse distance power \\(p\\) typically taken as 2, or optimised using cross-validation. We can compute inverse distance interpolated values using gstat::idw,\nlibrary(gstat)\ni &lt;- idw(NO2~1, no2.sf, grd)\n# [inverse distance weighted interpolation]\nand plot them in Figure 12.2.\nCodeggplot() + geom_stars(data = i, \n                      aes(fill = var1.pred, x = x, y = y)) + \n    xlab(NULL) + ylab(NULL) +\n    geom_sf(data = st_cast(de, \"MULTILINESTRING\")) + \n    geom_sf(data = no2.sf)\n\n\n\n\n\n\nFigure 12.2: Inverse distance weighted interpolated values for NO\\(_2\\) over Germany",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Spatial Interpolation</span>"
    ]
  },
  {
    "objectID": "12-Interpolation.html#sample-variogram",
    "href": "12-Interpolation.html#sample-variogram",
    "title": "12  Spatial Interpolation",
    "section": "\n12.2 Sample variogram",
    "text": "12.2 Sample variogram\n \nIn order to make spatial predictions using geostatistical methods, we first need to identify a model for the mean and for the spatial correlation. In the simplest model, \\(Z(s) = m + e(s)\\), the mean is an unknown constant \\(m\\), and in this case the spatial correlation can be modelled using the variogram, \\(\\gamma(h) = 0.5 E (Z(s)-Z(s+h))^2\\). For processes with a finite variance \\(C(0)\\), the variogram is related to the covariogram or covariance function through \\(\\gamma(h) = C(0)-C(h)\\).\nThe sample variogram is obtained by computing estimates of \\(\\gamma(h)\\) for distance intervals, \\(h_i = [h_{i,0},h_{i,1}]\\): \\[\n\\hat{\\gamma}(h_i) = \\frac{1}{2N(h_i)}\\sum_{j=1}^{N(h_i)}(z(s_i)-z(s_i+h'))^2, \\ \\ h_{i,0} \\le h' &lt; h_{i,1}\n\\tag{12.1}\\]\nwith \\(N(h_i)\\) the number of sample pairs available for distance interval \\(h_i\\). Function gstat::variogram computes sample variograms,\n\nv &lt;- variogram(NO2~1, no2.sf)\n\nand the result of plotting this is shown in Figure 12.3.\n\nCodeplot(v, plot.numbers = TRUE, xlab = \"distance h [m]\",\n     ylab = expression(gamma(h)),\n     xlim = c(0, 1.055 * max(v$dist)))\n\n\n\n\n\n\nFigure 12.3: Sample variogram plot\n\n\n\n\nFunction variogram chooses default for maximum distance (cutoff: one-third of the length of the bounding box diagonal) and (constant) interval widths (width: cutoff divided by 15). These defaults can be changed by\n\nv0 &lt;- variogram(NO2~1, no2.sf, cutoff = 100000, width = 10000)\n\nshown in Figure 12.4.\n\nCodeplot(v0, plot.numbers = TRUE, xlab = \"distance h [m]\",\n     ylab = expression(gamma(h)),\n     xlim = c(0, 1.055 * max(v0$dist)))\n\n\n\n\n\n\nFigure 12.4: Sample variogram plot with adjusted cutoff and lag width\n\n\n\n\nNote that the formula NO2~1 is used to select the variable of interest from the data file (NO2), and to specify the mean model: ~1 specifies an intercept-only (unknown, constant mean) model.",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Spatial Interpolation</span>"
    ]
  },
  {
    "objectID": "12-Interpolation.html#fitting-variogram-models",
    "href": "12-Interpolation.html#fitting-variogram-models",
    "title": "12  Spatial Interpolation",
    "section": "\n12.3 Fitting variogram models",
    "text": "12.3 Fitting variogram models\n\nIn order to progress towards spatial predictions, we need a variogram model \\(\\gamma(h)\\) for (potentially) all distances \\(h\\), rather than the set of estimates derived above. If we would connect these estimates with straight lines, or assume they reflect constant values over their respective distance intervals, it would lead to statistical models with non-positive definite covariance matrices, which would block using them in prediction.\n \nTo avoid this, we fit parametric models \\(\\gamma(h)\\) to the estimates \\(\\hat{\\gamma}(h_i)\\), where we take \\(h_i\\) as the mean value of all the \\(h'\\) values involved in estimating \\(\\hat{\\gamma}(h_i)\\). We can fit for instance a model with an exponential variogram by\n\nv.m &lt;- fit.variogram(v, vgm(1, \"Exp\", 50000, 1))\n\nshown by the solid line in Figure 12.5.\n\nCodefit.variogram_ml &lt;- function(formula, data, init, ...) {\n  stopifnot(nrow(init) &lt;= 2, inherits(data, \"sf\"), inherits(formula, \"formula\"),\n    inherits(init, \"variogramModel\"))\n  if (nrow(init) == 2)\n    stopifnot(\"Nug\" %in% init$model)\n\n  # convert from parameter vector to \"variogramModel\" class:\n  # x is c(sill, range) or: c(sill, range, nugget)\n  get_model &lt;- function(x, model, min_range = 1e-10) {\n    sill &lt;- x[1]\n    range &lt;- max(x[2], min_range)\n    nugget &lt;- if (length(x) == 3)\n            x[3]\n      else\n            0.\n    m &lt;- vgm(sill, model, range, nugget)\n  }\n\n  # with A &lt;- chol(Q), solve Q x = b for x:\n  ch_solve &lt;- function(A, b) {\n    backsolve(A, forwardsolve(A, b, upper.tri = TRUE, transpose = TRUE))\n  }\n\n  # negative log likelihood, excluding the constant:\n  nll &lt;- function(x, d, res, model, ...) {\n    m &lt;- get_model(x, model, ...)\n    Q &lt;- variogramLine(m, dist_vector = d, covariance = TRUE)\n    Qc &lt;- chol(Q)\n    det &lt;- 2 * sum(log(diag(Qc)))\n    det + t(res) %*% ch_solve(Qc, res)\n  }\n\n  # distance matrix, for optim stability rescaled to [0,1] range\n  d &lt;- st_distance(data) |&gt; units::drop_units()\n  max_d &lt;- max(d)\n  d &lt;- d / max_d\n  # residuals y - X beta: scale to sd 1\n  res &lt;- residuals(lm(formula, data))\n  v &lt;- var(res)\n  res &lt;- res/sqrt(v)\n  if (nrow(init) == 2) {\n    o.init &lt;- c(init$psill[2], init$range[2], init$psill[0])\n    model &lt;- as.character(init$model[2])\n  } else {\n    o.init &lt;- c(init$psill[1], init$range[1])\n    model &lt;- as.character(init$model[1])\n  }\n  o.init[2] &lt;- o.init[2] / max_d # scale to [0,1]\n  o.init[-2] &lt;- o.init[-2] / v   # scale to sd 1\n  o &lt;- optim(o.init, nll, d = d, res = res, model = model, \n            lower = rep(0, length(o.init)), method = \"L-BFGS-B\", ...)$par\n  o[2] &lt;- o[2] * max_d # scale back to distance units\n  o[-2] &lt;- o[-2] * v # scale back to variance v\n  get_model(o, model)\n}\n# use WLS fit model v.m for initial parameters:\nv.ml &lt;- fit.variogram_ml(NO2~1, no2.sf, v.m)\n# plot(v, v.ml, plot.numbers = TRUE)\n# plot(v, v.m, plot.numbers = TRUE) ## draws a single model; draw 2 models in single plot:\npar(xaxs = \"i\", yaxs = \"i\")\nplot(gamma ~ dist, v, \n     xlim = c(0, 1.075 * max(v$dist)), ylim = c(0, 1.05 * max(v$gamma)),\n     xlab = \"distance h [m]\", ylab = expression(gamma(h)))\nlines(variogramLine(v.m, 1.075 * max(v$dist)), lty = 1, col = 'blue')\nlines(variogramLine(v.ml, 1.075 * max(v$dist)), lty = 2, col = 'blue')\ntext(v$dist, v$gamma, v$np, pos = 4)\n\n\n\n\n\n\nFigure 12.5: Sample variogram (circles) with models fitted using weighted least squares (solid line) and maximum likelihood estimation (dashed line)\n\n\n\n\nThe fitting for the drawn line was done by weighted least squares, minimising \\[\n\\sum_{i=1}^{n}w_i(\\gamma(h_i)-\\hat{\\gamma}(h_i))^2,\n\\tag{12.2}\\] with weights \\(w_i\\) by default equal to \\(N(h_i)/h^2\\). Other weight options are available through argument fit.method.\n\nAs an alternative to weighted least squares fitting, one can use maximum likelihood (ML) or restricted maximum likelihood parameter estimation (Kitanidis and Lane 1985), which for this case leads to a relatively similar fitted model, shown as the dashed line in Figure 12.5. An advantage of ML-type approaches is that they do not require choosing distance intervals \\(h_i\\) in Equation 12.1 or weights \\(w_i\\) in Equation 12.2. Disadvantages are that they lean on stronger assumptions of multivariate normally distributed data, and for larger datasets require iteratively solving linear systems of size equal to the number of observations; Heaton et al. (2018) compare approaches dedicated to fitting models to large datasets.",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Spatial Interpolation</span>"
    ]
  },
  {
    "objectID": "12-Interpolation.html#sec-kriging",
    "href": "12-Interpolation.html#sec-kriging",
    "title": "12  Spatial Interpolation",
    "section": "\n12.4 Kriging interpolation",
    "text": "12.4 Kriging interpolation\n \nTypically, when we interpolate a variable, we do that on points on a regular grid covering the target area. We first create a stars object with a raster covering the target area, and NAs outside it.\nKriging involves the prediction of \\(Z(s_0)\\) at arbitrary locations \\(s_0\\). We can krige NO\\(_2\\) by using gstat::krige, with the model for the trend, the data, the prediction grid, and the variogram model as arguments (Figure 12.6) by:\n\n\nk &lt;- krige(NO2~1, no2.sf, grd, v.m)\n# [using ordinary kriging]\n\n\nggplot() + geom_stars(data = k, aes(fill = var1.pred, x = x, y = y)) + \n    xlab(NULL) + ylab(NULL) +\n    geom_sf(data = st_cast(de, \"MULTILINESTRING\")) + \n    geom_sf(data = no2.sf) +\n    coord_sf(lims_method = \"geometry_bbox\")\n\n\n\n\n\n\nFigure 12.6: Kriged NO\\(_2\\) concentrations over Germany",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Spatial Interpolation</span>"
    ]
  },
  {
    "objectID": "12-Interpolation.html#sec-blockkriging",
    "href": "12-Interpolation.html#sec-blockkriging",
    "title": "12  Spatial Interpolation",
    "section": "\n12.5 Areal means: block kriging",
    "text": "12.5 Areal means: block kriging\n \nComputing areal means can be done in several ways. The simplest is to take the average of point samples falling inside the target polygons:\n\na &lt;- aggregate(no2.sf[\"NO2\"], by = de, FUN = mean)\n\nA more complicated way is to use block kriging (Journel and Huijbregts 1978), which uses all the data to estimate the mean of the variable over the target areas. With krige, this can be done by giving the target areas (polygons) as the newdata argument:\n\nb &lt;- krige(NO2~1, no2.sf, de, v.m)\n# [using ordinary kriging]\n\nwe can now merge the two maps into a single object to create a single plot (Figure 12.7):\n\nb$sample &lt;- a$NO2\nb$kriging &lt;- b$var1.pred\n\n\nb |&gt; select(sample, kriging) |&gt; \n        pivot_longer(1:2, names_to = \"var\", values_to = \"NO2\") -&gt; b2\nb2$var &lt;- factor(b2$var, levels = c(\"sample\", \"kriging\"))\nggplot() + geom_sf(data = b2, mapping = aes(fill = NO2)) + facet_wrap(~var) +\n     scale_fill_gradientn(colors = sf.colors(20))\n\n\n\n\n\n\nFigure 12.7: Aggregated NO\\(_2\\) values from simple averaging (left) and block kriging (right)\n\n\n\n\nWe see that the signal is similar, but that the sample means from simple averaging are more variable than the block kriging values; this may be due to the smoothing effect of kriging: data points outside the aggregation area receive weight, too.\n\nTo compare the standard errors of means, for the sample mean we can get a rough guess of the standard error by \\(\\sqrt{(\\sigma^2/n)}\\):\n\nSE &lt;- function(x) sqrt(var(x)/length(x))\na &lt;- aggregate(no2.sf[\"NO2\"], de, SE)\n\nwhich would have been the actual estimate in design-based inference (Section 10.4) if the sample were obtained by spatially random sampling. The block kriging variance is the model-based estimate and is a by-product of kriging. We can compare the two in Figure 12.8 where we see that the simple averaging approach gives more variability and mostly larger values for prediction errors of areal means, compared to block kriging.\n\nCodeb$sample &lt;- a$NO2\nb$kriging &lt;- sqrt(b$var1.var)\nb |&gt; select(sample, kriging) |&gt; \n        pivot_longer(1:2, names_to = \"var\", \n                     values_to = \"Standard_error\") -&gt; b2\nb2$var &lt;- factor(b2$var, levels = c(\"sample\", \"kriging\"))\nggplot() +\n    geom_sf(data = b2, mapping = aes(fill = Standard_error)) +\n    facet_wrap(~var, as.table = FALSE) + \n    scale_fill_gradientn(colors = sf.colors(20))\n\n\n\n\n\n\nFigure 12.8: Standard errors for mean NO\\(_2\\) values obtained by simple averaging (left) and block kriging (right)",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Spatial Interpolation</span>"
    ]
  },
  {
    "objectID": "12-Interpolation.html#conditional-simulation",
    "href": "12-Interpolation.html#conditional-simulation",
    "title": "12  Spatial Interpolation",
    "section": "\n12.6 Conditional simulation",
    "text": "12.6 Conditional simulation\n \nIn case one or more conditional realisation of the field \\(Z(s)\\) are needed rather than their conditional mean, we can obtain this by conditional simulation. A reason for wanting this may be the need to estimate areal mean values of \\(g(Z(s))\\) with \\(g(\\cdot)\\) a non-linear function; a simple example is the areal fraction where \\(Z(s)\\) exceeds a threshold.\nThe default approach used by gstat is to use the sequential simulation algorithm for this. This is a simple algorithm that randomly steps through the prediction locations and at each location:\n\ncarries out a kriging prediction\ndraws a random variable from the normal distribution with mean and variance equal to the kriging variance\nadds this value to the conditioning dataset\nfinds a new random simulation location\n\nuntil all locations have been visited.\nThis is carried out by gstat::krige when nsim is set to a positive value:\n\nset.seed(13341)\n(s &lt;- krige(NO2~1, no2.sf, grd, v.m, nmax = 30, nsim = 6))\n# drawing 6 GLS realisations of beta...\n# [using conditional Gaussian simulation]\n# stars object with 3 dimensions and 1 attribute\n# attribute(s):\n#       Min. 1st Qu. Median Mean 3rd Qu. Max.  NA's\n# var1  -5.7    6.12   8.68 8.88    11.5 23.9 12456\n# dimension(s):\n#        from to  offset  delta            refsys        values x/y\n# x         1 65  280741  10000 WGS 84 / UTM z...          NULL [x]\n# y         1 87 6101239 -10000 WGS 84 / UTM z...          NULL [y]\n# sample    1  6      NA     NA                NA sim1,...,sim6\n\nwhere set.seed() was called here to allow reproducibility.\nIt is usually needed to constrain the (maximum) number of nearest neighbours to include in kriging estimation by setting nmax because the dataset grows each step, leading otherwise quickly to very long computing times and large memory requirements. Resulting conditional simulations are shown in (Figure 12.9).\n\nCodelibrary(viridis)\ng &lt;- ggplot() + coord_equal() +\n    scale_fill_viridis() +\n    theme_void() +\n    scale_x_discrete(expand=c(0,0)) +\n    scale_y_discrete(expand=c(0,0))\ng + geom_stars(data = s[,,,1:6]) + facet_wrap(~sample)\n\n\n\n\n\n\nFigure 12.9: Six conditional simulations for NO\\(_2\\) values\n\n\n\n\nAlternative methods for conditional simulation have recently been added to gstat, and include krigeSimCE implementing the circular embedding method (Davies and Bryant 2013), and krigeSTSimTB implementing the turning bands method (Schlather 2011). These are of particular of interest for larger datasets or conditional simulations of spatiotemporal data.",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Spatial Interpolation</span>"
    ]
  },
  {
    "objectID": "12-Interpolation.html#trend-models",
    "href": "12-Interpolation.html#trend-models",
    "title": "12  Spatial Interpolation",
    "section": "\n12.7 Trend models",
    "text": "12.7 Trend models\n\nKriging and conditional simulation, as used so far in this chapter, assume that all spatial variability is a random process, characterised by a spatial covariance model. In case we have other variables that are meaningfully correlated with the target variable, we can use them in a linear regression model for the trend, \\[\nZ(s) = \\sum_{j=0}^p \\beta_j X_j(s) + e(s)\n\\] with \\(X_0(s) = 1\\) and \\(\\beta_0\\) an intercept, but with the other \\(\\beta_j\\) regression coefficients. Adding variables typically reduces both the spatial correlation in the residual \\(e(s)\\), as well as its variance, and leads to more accurate predictions and more similar conditional simulations. As an example, we will use population density to partly explain variation in NO\\(_2\\).\nA population grid\n\nAs a potential predictor for NO\\(_2\\) in the air, we use population density. NO\\(_2\\) is mostly caused by traffic, and traffic is more intense in densely populated areas. Population density is obtained from the 2011 census and is downloaded as a csv file with the number of inhabitants per 100 m \\(\\times\\) 100 m grid cell. We can aggregate these data to the target grid cells by summing the inhabitants:\n\nv &lt;- vroom::vroom(\"aq/pop/Zensus_Bevoelkerung_100m-Gitter.csv\")\nv |&gt; filter(Einwohner &gt; 0) |&gt; \n    select(-Gitter_ID_100m) |&gt;\n    st_as_sf(coords = c(\"x_mp_100m\", \"y_mp_100m\"), crs = 3035) |&gt;\n    st_transform(st_crs(grd)) -&gt; b\na &lt;- aggregate(b, st_as_sf(grd, na.rm = FALSE), sum)\n\nNow we have the population counts per grid cell in a. To get to population density, we need to find the area of each cell; for cells crossing the country border, this will be less than 10 \\(\\times\\) 10 km:\n\ngrd$ID &lt;- 1:prod(dim(grd)) # to identify grid cells\nii &lt;- st_intersects(grd[\"ID\"],\n  st_cast(st_union(de), \"MULTILINESTRING\"), as_points = FALSE)\ngrd_sf &lt;- st_as_sf(grd[\"ID\"], na.rm = FALSE)[lengths(ii) &gt; 0,]\nst_agr(grd_sf) = \"identity\"\niii &lt;- st_intersection(grd_sf, st_union(de))\ngrd$area &lt;- st_area(grd)[[1]] + \n    units::set_units(grd$values, m^2)\ngrd$area[iii$ID] &lt;- st_area(iii)\n\nInstead of doing the two-stage procedure above, first finding cells that have a border crossing it then computing its area, we could also directly use st_intersection on all cells, but that takes considerably longer. From the counts and areas we can compute densities (Figure 12.10) and verify totals\n\ngrd$pop_dens &lt;- a$Einwohner / grd$area\nsum(grd$pop_dens * grd$area, na.rm = TRUE) # verify\n# 80323301 [1]\nsum(b$Einwohner)\n# [1] 0\n\nwhich indicates strong agreement. Using st_interpolate_aw would have given an exact match.\n\nCodeg + geom_stars(data = grd, aes(fill = sqrt(pop_dens), x = x, y = y))\n\n\n\n\n\n\nFigure 12.10: Population density for 100 m \\(\\times\\) 100 m grid cells\n\n\n\n\nWe need to divide the number of inhabitants by the number of 100 m \\(\\times\\) 100 m grid cells contributing to it, in order to convert population counts into population density.\n To obtain population density values at monitoring network stations, we can use st_extract:\n\ngrd |&gt;\n  select(\"pop_dens\") |&gt;\n  st_extract(no2.sf) |&gt;\n  pull(\"pop_dens\") |&gt; \n  mutate(no2.sf, pop_dens = _) -&gt; no2.sf\n\nWe can then investigate the linear relationship between NO\\(_2\\) and population density at monitoring station locations:\n\nsummary(lm(NO2~sqrt(pop_dens), no2.sf))\n# \n# Call:\n# lm(formula = NO2 ~ sqrt(pop_dens), data = no2.sf)\n# \n# Residuals:\n#    Min     1Q Median     3Q    Max \n# -7.990 -2.052 -0.505  1.610  8.095 \n# \n# Coefficients:\n#                Estimate Std. Error t value Pr(&gt;|t|)    \n# (Intercept)       4.537      0.685    6.62  5.5e-09 ***\n# sqrt(pop_dens)  326.154     49.366    6.61  5.8e-09 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# Residual standard error: 3.13 on 72 degrees of freedom\n# Multiple R-squared:  0.377,   Adjusted R-squared:  0.369 \n# F-statistic: 43.7 on 1 and 72 DF,  p-value: 5.82e-09\n\nfor which the corresponding scatterplot is shown in Figure 12.11.\n\nCodeplot(NO2 ~ sqrt(pop_dens), no2.sf)\nabline(lm(NO2 ~ sqrt(pop_dens), no2.sf))\n\n\n\n\n\n\nFigure 12.11: Scatter plot of 2017 annual mean NO\\(_2\\) concentration against population density, for rural background air quality stations\n\n\n\n\n Prediction under this new model involves first modelling a residual variogram\n\nno2.sf &lt;- no2.sf[!is.na(no2.sf$pop_dens),]\nvr &lt;- variogram(NO2~sqrt(pop_dens), no2.sf)\nvr.m &lt;- fit.variogram(vr, vgm(1, \"Exp\", 50000, 1))\n\n\nCodeplot(vr, vr.m, plot.numbers = TRUE)\n\n\n\n\n\n\nFigure 12.12: Residual variogram after subtracting population density trend\n\n\n\n\nwhich is shown in Figure 12.12. Subsequently, kriging prediction (Figure 12.13) is done by\n\nkr &lt;- krige(NO2 ~ sqrt(pop_dens), no2.sf, \n            grd[\"pop_dens\"], vr.m)\n# [using universal kriging]\n\n\nCodek$kr1 &lt;- k$var1.pred\nk$kr2 &lt;- kr$var1.pred\nst_redimension(k[c(\"kr1\", \"kr2\")], \n    along = list(what = c(\"kriging\", \"residual kriging\"))) |&gt;\n    setNames(\"NO2\") -&gt; km\ng + geom_stars(data = km, aes(fill = NO2, x = x, y = y)) + \n    geom_sf(data = st_cast(de, \"MULTILINESTRING\")) + \n    geom_sf(data = no2.sf) + facet_wrap(~what) +\n    coord_sf(lims_method = \"geometry_bbox\")\n\n\n\n\n\n\nFigure 12.13: Kriging NO\\(_2\\) values using population density as a trend variable\n\n\n\n\nwhere, critically, the pop_dens values are now available for prediction locations in the newdata object grd.\nCompared to (ordinary) kriging We see some clear differences: the map using population density in the trend follows the extremes of the population density rather than those of the measurement stations, and has a value range that extends that of ordinary kriging. It should be taken with a large grain of salt however, since the stations used were filtered for the category “rural background”, indicating that they only represent conditions of lower populations density. The scatter-plot of Figure 12.11 reveals that the the population density at the locations of stations is much more limited than that in the population density map, and hence the right-hand side map is based on strongly extrapolating the relationship shown in Figure 12.11.",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Spatial Interpolation</span>"
    ]
  },
  {
    "objectID": "12-Interpolation.html#exercises",
    "href": "12-Interpolation.html#exercises",
    "title": "12  Spatial Interpolation",
    "section": "\n12.8 Exercises",
    "text": "12.8 Exercises\n\nCreate a plot like the one in Figure 12.13 that has the inverse distance interpolated map of Figure 12.2 added on the left side.\nCreate a scatter-plot of the map values of the idw and kriging map, and a scatter-plot of map values of idw and residual kriging.\nCarry out a block kriging, predicting block averages for blocks centred over grid cells, by setting the block argument in krige(), and do this for block sizes of 10 km (grid cell size), 50 km, and 200 km. Compare the resulting maps of estimates for these three block sizes with those obtained by point kriging, and do the same thing for all associated kriging standard errors.\nBased on the residual kriging results obtained above, compute maps of the lower and upper boundary of a 95% confidence interval, when assuming that the kriging error is normally distributed, and show them in a plot with a single (joint) legend.\nCompute and show the map with the probabilities that NO\\(_2\\) point values exceed the level of 15 ppm, assuming normally distributed kriging errors.\n\n\n\n\n\n\n\nBlangiardo, Marta, and Michela Cameletti. 2015. Spatial and Spatio-Temporal Bayesian Models with r-INLA. John Wiley & Sons.\n\n\nDavies, Tilman, and David Bryant. 2013. “On Circulant Embedding for Gaussian Random Fields in R.” Journal of Statistical Software, Articles 55 (9): 1–21. https://doi.org/10.18637/jss.v055.i09.\n\n\nDiggle, P. J., and P. J. Ribeiro Jr. 2007. Model-Based Geostatistics. New York: Springer.\n\n\nDiggle, P. J., J. A. Tawn, and R. A. Moyeed. 1998. “Model-Based Geostatistics.” Applied Statistics, 299–350.\n\n\nHeaton, Matthew J., Abhirup Datta, Andrew O. Finley, Reinhard Furrer, Joseph Guinness, Rajarshi Guhaniyogi, Florian Gerber, et al. 2018. “A Case Study Competition Among Methods for Analyzing Large Spatial Data.” Journal of Agricultural, Biological and Environmental Statistics, December. https://doi.org/10.1007/s13253-018-00348-w.\n\n\nJournel, Andre G., and Charles J. Huijbregts. 1978. Mining Geostatistics. Academic Press, London.\n\n\nKitanidis, Peter K., and Robert W. Lane. 1985. “Maximum Likelihood Parameter Estimation of Hydrologic Spatial Processes by the Gauss-Newton Method.” Journal of Hydrology 79 (1-2): 53–71. https://doi.org/10.1016/0022-1694(85)90181-7.\n\n\nPebesma, Edzer. 2004. “Multivariable Geostatistics in S: The Gstat Package.” Computers & Geosciences 30: 683–91.\n\n\nPebesma, Edzer, and Benedikt Graeler. 2022. Gstat: Spatial and Spatio-Temporal Geostatistical Modelling, Prediction and Simulation. https://github.com/r-spatial/gstat/.\n\n\nSchlather, Martin. 2011. “Construction of Covariance Functions and Unconditional Simulation of Random Fields.” In Space-Time Processes and Challenges Related to Environmental Problems, edited by E. Porcu, Montero J. M., and M. Schlather. New York: Springer.\n\n\nWikle, Christopher K, Andrew Zammit-Mangion, and Noel Cressie. 2019. Spatio-Temporal Statistics with R. CRC Press.",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Spatial Interpolation</span>"
    ]
  },
  {
    "objectID": "13-Geostatistics.html",
    "href": "13-Geostatistics.html",
    "title": "13  Multivariate and Spatiotemporal Geostatistics",
    "section": "",
    "text": "13.1 Preparing the air quality dataset\nBuilding on the simple interpolation methods presented in Chapter 12, this chapter continues with multivariate geostatistics and spatiotemporal geostatistics. The topic of multivariate geostatistics, more extensively illustrated in Bivand, Pebesma, and Gómez-Rubio (2013), is briefly introduced. Spatiotemporal geostatistics is illustrated with a worked out case study for spatiotemporal interpolation, using NO\\(_2\\) air quality data, and population density as covariate.\nThe dataset we work with is an air quality dataset obtained from the European Environmental Agency (EEA). European member states report air quality measurements to this agency. So-called validated data are quality controlled by member states, and are reported on a yearly basis. They form the basis for policy compliancy evaluations and (counter) measures.\nThe EEA’s air quality e-reporting website gives access to the data reported by European member states. We decided to download hourly (time series) data, which is the data primarily measured. A web form helps convert simple selection criteria into an http GET request. The URL was created to select all validated (Source=E1a) \\(NO_2\\) (Pollutant=8) data for 2017 (Year_from, Year_to) from Germany (CountryCode=DE). It returns a text file with a set of URLs to CSV files, each containing the hourly values for the whole period for a single measurement station. These files were downloaded and converted to the right encoding using the dos2unix command line utility.\nIn the following, we will read all the files into a list, except for the single file with the station metadata:\nfiles &lt;- list.files(\"aq\", pattern = \"*.csv\", full.names = TRUE)\nfiles &lt;- setdiff(files, \"aq/AirBase_v8_stations.csv\") # metadata file\nr &lt;- lapply(files, function(f) read.csv(f))\nthen we convert the time variable into a POSIXct variable, and put them in time order by\nSys.setenv(TZ = \"UTC\") # don't use local time zone\nr &lt;- lapply(r, function(f) {\n        f$t = as.POSIXct(f$DatetimeBegin) \n        f[order(f$t), ] \n    }\n)\nWe remove smaller sub-datasets, which for this dataset have no hourly data:\nr &lt;- r[sapply(r, nrow) &gt; 1000]\nnames(r) &lt;- sapply(r,\n               function(f) unique(f$AirQualityStationEoICode))\nlength(r) == length(unique(names(r)))\n# [1] TRUE\nand then merge all files using xts::cbind, so that records are combined based on matching times:\nlibrary(xts) |&gt; suppressPackageStartupMessages()\nr &lt;- lapply(r, function(f) xts(f$Concentration, f$t))\naq &lt;- do.call(cbind, r)\nA usual further selection for this dataset is to select stations for which 75% of the hourly values measured are valid, i.e., drop those with more than 25% missing hourly values. Knowing that mean(is.na(x)) gives the fraction of missing values in a vector x, we can apply this function to the columns (stations):\nsel &lt;- apply(aq, 2, function(x) mean(is.na(x)) &lt; 0.25)\naqsel &lt;- aq[, sel]\nNext, the station metadata was read and filtered for rural background stations in Germany (\"DE\") by\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nread.csv(\"aq/AirBase_v8_stations.csv\", sep = \"\\t\") |&gt;\n    as_tibble() |&gt; \n    filter(country_iso_code == \"DE\",\n           station_type_of_area == \"rural\",\n           type_of_station == \"Background\") -&gt; a2\nThese stations contain coordinates, and an sf object with (static) station metadata is created by\nlibrary(sf) |&gt; suppressPackageStartupMessages()\na2.sf &lt;- st_as_sf(a2, crs = 'OGC:CRS84',\n  coords = c(\"station_longitude_deg\", \"station_latitude_deg\"))\nWe now subset the air quality measurements to include only stations that are of type rural background, which we saved in a2:\nsel &lt;- colnames(aqsel) %in% a2$station_european_code\naqsel &lt;- aqsel[, sel]\ndim(aqsel)\n# [1] 8760   74\nWe can compute station means and join these to station locations by\ntb &lt;- tibble(NO2 = apply(aqsel, 2, mean, na.rm = TRUE), \n            station_european_code = colnames(aqsel))\ncrs &lt;- st_crs('EPSG:32632')\nright_join(a2.sf, tb) |&gt; st_transform(crs) -&gt; no2.sf \nread_sf(\"data/de_nuts1.gpkg\") |&gt; st_transform(crs) -&gt; de\nStation mean NO\\(_2\\) concentrations, along with country borders, are shown in in Figure 12.1.",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multivariate and Spatiotemporal Geostatistics</span>"
    ]
  },
  {
    "objectID": "13-Geostatistics.html#sec-cokriging",
    "href": "13-Geostatistics.html#sec-cokriging",
    "title": "13  Multivariate and Spatiotemporal Geostatistics",
    "section": "\n13.2 Multivariable geostatistics",
    "text": "13.2 Multivariable geostatistics\n \nMultivariable geostatics involves the joint modelling, prediction, and simulation of multiple variables, \\[Z_1(s) = X_1 \\beta_1 + e_1(s)\\] \\[...\\] \\[Z_n(s) = X_n \\beta_n + e_n(s).\\] In addition to having observations, trend models, and variograms for each variable, the cross-variogram for each pair of residual variables, describing the covariance of \\(e_i(s), e_j(s+h)\\), is required. If this cross-covariance is non-zero, knowledge of \\(e_j(s+h)\\) may help predict (or simulate) \\(e_i(s)\\). This is especially true if \\(Z_j(s)\\) is more densely sample than \\(Z_i(s)\\). Prediction and simulation under this model are called cokriging and cosimulation. Examples using gstat are found when running the demo scripts\n\nlibrary(gstat)\ndemo(cokriging)\ndemo(cosimulation)\n\nand are further illustrated and discussed in Bivand, Pebesma, and Gómez-Rubio (2013).\n \nIn case the different variables considered are observed at the same set of locations, for instance different air quality parameters, then the statistical gain of using cokriging as opposed to direct (univariable) kriging is often modest, when not negligible. A gain may however be that the prediction is truly multivariable: in addition to the prediction vector \\(\\hat{Z(s_0)}=(\\hat{Z}_1(s_0),...,\\hat{Z}_n(s_0))\\), we get the full covariance matrix of the prediction error (Ver Hoef and Cressie 1993). Using these prediction error covariances, for any linear combination of \\(\\hat{Z}(s_0)\\), such as \\(\\hat{Z}_2(s_0) - \\hat{Z}_1(s_0)\\), we can get the standard error of that combination.\nAlthough sets of direct and cross-variograms can be computed and fitted automatically, multivariable geostatistical modelling becomes quickly hard to manage when the number of variables gets large, because the number of direct and cross-variograms required is \\(n(n+1)/2\\).\nIn case different variables refer to the same variable taken at different time steps, one could use a multivariable (cokriging) prediction approach, but this would not allow for interpolation between two time steps. For this, and for handling the case of having data observed at many time instances, one can also model its variation as a function of continuous space and time, as of \\(Z(s,t)\\), which we will do in the next section.",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multivariate and Spatiotemporal Geostatistics</span>"
    ]
  },
  {
    "objectID": "13-Geostatistics.html#sec-stgeos",
    "href": "13-Geostatistics.html#sec-stgeos",
    "title": "13  Multivariate and Spatiotemporal Geostatistics",
    "section": "\n13.3 Spatiotemporal geostatistics",
    "text": "13.3 Spatiotemporal geostatistics\n Spatiotemporal geostatistical processes are modelled as variables having a value everywhere in space and time, \\(Z(s,t)\\), with \\(s\\) and \\(t\\) the continuously indexed space and time index. Given observations \\(Z(s_i,t_j)\\) and a variogram (covariance) model \\(\\gamma(s,t)\\) we can predict \\(Z(s_0,t_0)\\) at arbitrary space/time locations \\((s_0,t_0)\\) using standard Gaussian process theory.\nSeveral books have been written recently about modern approaches to handling and modelling spatiotemporal geostatistical data, including Wikle, Zammit-Mangion, and Cressie (2019) and Blangiardo and Cameletti (2015). Here, we will use Gräler, Pebesma, and Heuvelink (2016) and give some simple examples using the dataset also used for the previous chapter.\nA spatiotemporal variogram model\n\nStarting with the spatiotemporal matrix of NO\\(_2\\) data in aq constructed at the beginning of this chapter, we selected complete records taken at rural background stations into aqsel. We can select the spatial locations for these 74 stations by\n\nsfc &lt;- st_geometry(a2.sf)[match(colnames(aqsel),\n                           a2.sf$station_european_code)] |&gt;\n  st_transform(crs)\n\nand finally build a stars vector data cube with time and station as dimensions:\n\nlibrary(stars)\n# Loading required package: abind\nst_as_stars(NO2 = as.matrix(aqsel)) |&gt;\n    st_set_dimensions(names = c(\"time\", \"station\")) |&gt;\n    st_set_dimensions(\"time\", index(aqsel)) |&gt;\n    st_set_dimensions(\"station\", sfc) -&gt; no2.st\nno2.st\n# stars object with 2 dimensions and 1 attribute\n# attribute(s):\n#      Min. 1st Qu. Median Mean 3rd Qu. Max.  NA's\n# NO2  -8.1    3.02   5.66 8.39    10.4  197 16134\n# dimension(s):\n#         from   to         offset   delta            refsys point\n# time       1 8760 2017-01-01 UTC 1 hours           POSIXct    NA\n# station    1   74             NA      NA WGS 84 / UTM z...  TRUE\n#                                          values\n# time                                       NULL\n# station POINT (439814 ...,...,POINT (456668 ...\n\nFrom this, we can compute the spatiotemporal variogram using\n\nlibrary(gstat)\n\n\nv.st &lt;- variogramST(NO2~1, no2.st[,1:(24*31)], tlags = 0:48, \n    cores = getOption(\"mc.cores\", 2))\n\nwhich is shown in Figure 13.1.\n\nCodev1 &lt;- plot(v.st)\nv2 &lt;- plot(v.st, map = FALSE, legend = list())\nprint(v1, split = c(1,1,2,1), more = TRUE)\nprint(v2, split = c(2,1,2,1), more = FALSE)\n\n\n\n\n\n\nFigure 13.1: Spatiotemporal sample variogram for hourly NO\\(_2\\) concentrations at rural background stations in Germany over 2027; in the right-hand side plot colour corresponds to time lag (yellow is later); distance in m\n\n\n\n\nTo this sample variogram, we can fit a variogram model. One relatively flexible model we try here is the product-sum model (Gräler, Pebesma, and Heuvelink 2016), fitted by\n\n# product-sum\nprodSumModel &lt;- vgmST(\"productSum\",\n    space = vgm(150, \"Exp\", 200000, 0),\n    time = vgm(20, \"Sph\", 6, 0),\n    k = 2)\n#v.st$dist = v.st$dist / 1000\nStAni &lt;- estiStAni(v.st, c(0,200000))\n(fitProdSumModel &lt;- fit.StVariogram(v.st, prodSumModel,\n    fit.method = 7, stAni = StAni, method = \"L-BFGS-B\",\n    control = list(parscale = c(1,100000,1,1,0.1,1,10)),\n    lower = rep(0.0001, 7)))\n# space component: \n#   model    psill range\n# 1   Nug   0.0166     0\n# 2   Exp 152.7046 83590\n# time component: \n#   model   psill range\n# 1   Nug  0.0001  0.00\n# 2   Sph 25.5736  5.77\n# k: 0.00397635996859073\n\nand shown in Figure 13.2, which can also be plotted as wire frames, shown in Figure 13.3. Fitting this model is rather sensitive to the chosen parameters, which may be caused by the relatively small number (74) of monitoring network stations available.\n\nCodeplot(v.st, fitProdSumModel, wireframe = FALSE, all = TRUE, \n     scales = list(arrows = FALSE), zlim = c(0, 150))\n\n\n\n\n\n\nFigure 13.2: Product-sum model, fitted to the spatiotemporal sample variogram\n\n\n\n\n\nCodeplot(v.st, model = fitProdSumModel, wireframe = TRUE, all = TRUE, \n     scales = list(arrows = FALSE), zlim = c(0, 185))\n\n\n\n\n\n\nFigure 13.3: Wireframe plot of the fitted spatiotemporal variogram model\n\n\n\n\nHints about the fitting strategy and alternative models for spatiotemporal variograms are given in Gräler, Pebesma, and Heuvelink (2016).\n\nWith this fitted model, and given the observations, we can carry out kriging or simulation at arbitrary points in space and time. For instance, we could estimate (or simulate) values in the time series that are now missing: this occurs regularly, and in Section 12.4 we used means over time series based on simply ignoring up to 25% of the observations: substituting these with estimated or simulated values based on neighbouring (in space and time) observations before computing yearly mean values seems a more reasonable approach.\nMore in general, we can estimate at arbitrary locations and time points, and we will illustrate this with predicting time series at particular locations and and predicting spatial slices (Gräler, Pebesma, and Heuvelink 2016). We can create a stars object for two randomly selected spatial points and all time instances by\n\nset.seed(1331)\npt &lt;- st_sample(de, 2)\nt &lt;- st_get_dimension_values(no2.st, 1)\nst_as_stars(list(pts = matrix(1, length(t), length(pt)))) |&gt;\n    st_set_dimensions(names = c(\"time\", \"station\")) |&gt;\n    st_set_dimensions(\"time\", t) |&gt;\n    st_set_dimensions(\"station\", pt) -&gt; new_pt\n\nand we obtain the spatiotemporal predictions at these two points using krigeST by\n\nno2.st &lt;- st_transform(no2.st, crs)\nnew_ts &lt;- krigeST(NO2~1, data = no2.st[\"NO2\"], newdata = new_pt,\n         nmax = 50, stAni = StAni, modelList = fitProdSumModel,\n         progress = FALSE)\n\nwhere the results are shown in Figure 13.4.\n\nCodeplot(as.xts(new_ts[2]))\n\n\n\n\n\n\nFigure 13.4: Time series plot of spatiotemporal predictions for two points\n\n\n\n\nAlternatively, we can create spatiotemporal predictions for a set of time-stamped raster maps, evenly spaced over the year 2017, created by\n\nst_bbox(de) |&gt;\n  st_as_stars(dx = 10000) |&gt;\n  st_crop(de) -&gt; grd\nd &lt;- dim(grd)\nt4 &lt;- t[(1:4 - 0.5) * (3*24*30)]\nst_as_stars(pts = array(1, c(d[1], d[2], time = length(t4)))) |&gt;\n    st_set_dimensions(\"time\", t4) |&gt;\n    st_set_dimensions(\"x\", st_get_dimension_values(grd, \"x\")) |&gt;\n    st_set_dimensions(\"y\", st_get_dimension_values(grd, \"y\")) |&gt;\n    st_set_crs(crs) -&gt; grd.st\n\nand the subsequent predictions are obtained by\n\nnew_int &lt;- krigeST(NO2~1, data = no2.st[\"NO2\"], newdata = grd.st,\n         nmax = 200, stAni = StAni, modelList = fitProdSumModel,\n         progress = FALSE)\nnames(new_int)[2] = \"NO2\"\n\nand shown in Figure 13.5.\n\nCodelibrary(viridis)\nlibrary(viridisLite)\nlibrary(ggplot2)\ng &lt;- ggplot() + coord_equal() +\n    scale_fill_viridis() +\n    theme_void() +\n    scale_x_discrete(expand=c(0,0)) +\n    scale_y_discrete(expand=c(0,0))\ng + geom_stars(data = new_int, aes(fill = NO2, x = x, y = y)) + \n    facet_wrap(~as.Date(time)) +\n    geom_sf(data = st_cast(de, \"MULTILINESTRING\")) + \n    geom_sf(data = no2.sf, col = 'grey', cex = .5) + \n    coord_sf(lims_method = \"geometry_bbox\")\n\n\n\n\n\n\nFigure 13.5: Spatiotemporal predictions for four selected time slices\n\n\n\n\nA larger value for nmax was needed here to decrease the visible disturbance (sharp edges) caused by discrete neighbourhood selections, which are now done in space and time.\nIrregular space time data\nFor the case where observations are collected at locations that vary constantly, or at fixed locations but without a common time basis, stars objects (vector data cubes) do not represent them well. Such irregular space time observations can be represented by sftime objects, provided by package sftime (Teickner, Pebesma, and Graeler 2022), which are essentially sf objects with a specified time column. An example of its uses is found in demo(sftime), provided in package gstat.",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multivariate and Spatiotemporal Geostatistics</span>"
    ]
  },
  {
    "objectID": "13-Geostatistics.html#exercises",
    "href": "13-Geostatistics.html#exercises",
    "title": "13  Multivariate and Spatiotemporal Geostatistics",
    "section": "\n13.4 Exercises",
    "text": "13.4 Exercises\n\nWhich fraction of the stations is removed in Section 13.1 when the criterion applied that a station must be 75% complete?\nFrom the hourly time series in no2.st, compute daily mean concentrations using aggregate, and compute the spatiotemporal variogram of this. How does it compare to the variogram of hourly values?\nCarry out a spatiotemporal interpolation for daily mean values for the days corresponding to those shown in Figure 13.5, and compare the results.\nFollowing the example in the demo scripts pointed at in Section 13.2, carry out a cokriging on the daily mean station data for the four days shown in Figure 13.5.\nWhat are the differences of this latter approach to spatiotemporal kriging?\n\n\n\n\n\n\n\nBivand, Roger, Edzer Pebesma, and Virgilio Gómez-Rubio. 2013. Applied Spatial Data Analysis with R, Second Edition. Springer, NY. http://www.asdar-book.org/.\n\n\nBlangiardo, Marta, and Michela Cameletti. 2015. Spatial and Spatio-Temporal Bayesian Models with r-INLA. John Wiley & Sons.\n\n\nGräler, Benedikt, Edzer Pebesma, and Gerard Heuvelink. 2016. “Spatio-Temporal Interpolation using gstat.” The R Journal 8 (1): 204–18. https://doi.org/10.32614/RJ-2016-014.\n\n\nTeickner, Henning, Edzer Pebesma, and Benedikt Graeler. 2022. Sftime: Classes and Methods for Simple Feature Objects That Have a Time Column. https://CRAN.R-project.org/package=sftime.\n\n\nVer Hoef, Jay M, and Noel Cressie. 1993. “Multivariable Spatial Prediction.” Mathematical Geology 25 (2): 219–40.\n\n\nWikle, Christopher K, Andrew Zammit-Mangion, and Noel Cressie. 2019. Spatio-Temporal Statistics with R. CRC Press.",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multivariate and Spatiotemporal Geostatistics</span>"
    ]
  },
  {
    "objectID": "14-Areal.html",
    "href": "14-Areal.html",
    "title": "14  Proximity and Areal Data",
    "section": "",
    "text": "14.1 Representing proximity in spdep\nAreal units of observation are very often used when simultaneous observations are aggregated within non-overlapping boundaries. The boundaries may be those of administrative entities and may be related to underlying spatial processes, such as commuting flows, but are usually arbitrary. If they do not match the underlying and unobserved spatial processes in one or more variables of interest, proximate areal units will contain parts of the underlying processes, engendering spatial autocorrelation. By proximity, we mean closeness in ways that make sense for the data generation processes thought to be involved. In cross-sectional geostatistical analysis with point support, measured distance makes sense for typical data generation processes. In similar analysis of areal data, sharing a border may make more sense, because that is what we do know, but we cannot measure the distance between the areas in as adequate a way.\nBy support of data we mean the physical size (length, area, volume) associated with an individual observational unit (measurement; see Chapter 5). It is possible to represent the support of areal data by a point, despite the fact that the data have polygonal support. The centroid of the polygon may be taken as a representative point, or the centroid of the largest polygon in a multi-polygon object. When data with intrinsic point support are treated as areal data, the change of support goes the other way, from the known point to a non-overlapping tessellation such as a Voronoi diagram or Dirichlet tessellation or Thiessen polygons often through a Delaunay triangulation using projected coordinates. Here, different metrics may also be chosen, or distances measured on a network rather than on the plane. There is also a literature using weighted Voronoi diagrams in local spatial analysis (see for example Boots and Okabe 2007; Okabe et al. 2008; She et al. 2015).\nWhen the intrinsic support of the data is represented as points, but the underlying process is between proximate observations rather than driven chiefly by distance between observations, the data may be aggregate counts or totals (polling stations, retail turnover) or represent a directly observed characteristic of the observation (opening hours of the polling station). Obviously, the risk of misrepresenting the footprint of the underlying spatial processes remains in all of these cases, not least because the observations are taken as encompassing the entirety of the underlying process in the case of tessellation of the whole area of interest. This is distinct from the geostatistical setting in which observations are rather samples taken using some scheme within the area of interest. It is also partly distinct from the practice of taking areal sample plots within the area of interest but covering only a small proportion of the area, typically used in ecological and environmental research.\nIn order to explore and analyse areal data of these kinds in Chapters 15-17, methods are needed to represent the proximity of observations. This chapter considers a subset of such methods, where the spatial processes are considered as working through proximity understood in the first instance as contiguity, as a graph linking observations taken as neighbours. This graph is typically undirected and unweighted, but may be directed and/or weighted in certain settings, which then leads to further issues with regard to symmetry. In principle, proximity would be expected to operate symmetrically in space, that is that the influence of \\(i\\) on \\(j\\) and of \\(j\\) on \\(i\\) based on their relative positions should be equivalent. Edge effects are not considered in standard treatments.\nHandling spatial autocorrelation using relationships to neighbours on a graph takes the graph as given, chosen by the analyst. This differs from the geostatistical approach in which the analyst chooses the binning of the empirical variogram and function used, and then the way the variogram is fitted. Both involve a priori choices, but represent the underlying correlation in different ways (Wall 2004). In Bavaud (1998) and work citing his contribution, attempts have been made to place graph-based neighbours in a broader context.\nOne issue arising in the creation of objects representing neighbourhood relationships is that of no-neighbour areal units (Bivand and Portnov 2004). Islands or units separated by rivers may not be recognised as neighbours when the units have areal support and when using topological relationships such as shared boundaries. In some settings, for example mrf (Markov Random Field) terms in mgcv::gam and similar model fitting functions, undirected connected graphs are required, which is violated when there are disconnected subgraphs.\nNo-neighbour observations can also occur when a distance threshold is used between points, where the threshold is smaller than the maximum nearest neighbour distance. Shared boundary contiguities are not affected by using geographical, unprojected coordinates, but all point-based approaches use distance in one way or another, and need to calculate distances in an appropriate way.\nThe spdep package provides an nb class for neighbours, a list of length equal to the number of observations, with integer vector components. No-neighbours are encoded as an integer vector with a single element 0L, and observations with neighbours as sorted integer vectors containing values in 1L:n pointing to the neighbouring observations. This is a typical row-oriented sparse representation of neighbours. spdep provides many ways of constructing nb objects, and the representation and construction functions are widely used in other packages.\nspdep builds on the nb representation (undirected or directed graphs) with the listw object, a list with three components, an nb object, a matching list of numerical weights, and a single element character vector containing the single letter name of the way in which the weights were calculated. The most frequently used approach in the social sciences is calculating weights by row standardisation, so that all the non-zero weights for one observation will be the inverse of the cardinality of its set of neighbours (1/card(nb)[i]).\nWe will be using election data from the 2015 Polish presidential election in this chapter, with 2495 municipalities and Warsaw boroughs (see Figure 14.1) for a tmap map (Section 8.5) of the municipality types, and complete count data from polling stations aggregated to these areal units. The data are an sf sf object:\nlibrary(sf)\n# Linking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.4.0; sf_use_s2() is TRUE\ndata(pol_pres15, package = \"spDataLarge\")\npol_pres15 |&gt;\n    subset(select = c(TERYT, name, types)) |&gt;\n    head()\n# Simple feature collection with 6 features and 3 fields\n# Geometry type: MULTIPOLYGON\n# Dimension:     XY\n# Bounding box:  xmin: 235000 ymin: 367000 xmax: 281000 ymax: 413000\n# Projected CRS: ETRF2000-PL / CS92\n#    TERYT                name       types\n# 1 020101         BOLESŁAWIEC       Urban\n# 2 020102         BOLESŁAWIEC       Rural\n# 3 020103            GROMADKA       Rural\n# 4 020104        NOWOGRODZIEC Urban/rural\n# 5 020105          OSIECZNICA       Rural\n# 6 020106 WARTA BOLESŁAWIECKA       Rural\n#                         geometry\n# 1 MULTIPOLYGON (((261089 3855...\n# 2 MULTIPOLYGON (((254150 3837...\n# 3 MULTIPOLYGON (((275346 3846...\n# 4 MULTIPOLYGON (((251770 3770...\n# 5 MULTIPOLYGON (((263424 4060...\n# 6 MULTIPOLYGON (((267031 3870...\nif (tmap4) {\n    tm_shape(pol_pres15) +\n        tm_fill(\"types\", fill.scale = tm_scale(values = \"brewer.set3\"),\n           fill.legend = tm_legend(position = tm_pos_in(\"left\", \"bottom\"),\n               frame.lwd=0, item.r = 0)\n        )\n} else {\n    tm_shape(pol_pres15) + tm_fill(\"types\")\n}\n\n\n\n\n\n\nFigure 14.1: Polish municipality types 2015\nFor safety’s sake, we impose topological validity:\nif (!all(st_is_valid(pol_pres15)))\n        pol_pres15 &lt;- st_make_valid(pol_pres15)\nBetween early 2002 and April 2019, spdep contained functions for constructing and handling neighbour and spatial weights objects, tests for spatial autocorrelation, and model fitting functions. The latter have been split out into spatialreg, and will be discussed in subsequent chapters. spdep (Bivand 2022) now accommodates objects represented using sf classes and sp classes directly.\nlibrary(spdep) |&gt; suppressPackageStartupMessages()",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Proximity and Areal Data</span>"
    ]
  },
  {
    "objectID": "14-Areal.html#contiguous-neighbours",
    "href": "14-Areal.html#contiguous-neighbours",
    "title": "14  Proximity and Areal Data",
    "section": "\n14.2 Contiguous neighbours",
    "text": "14.2 Contiguous neighbours\n \nThe poly2nb function in spdep takes the boundary points making up the polygon boundaries in the object passed as the pl argument, typically an \"sf\" or \"sfc\" object with \"POLYGON\" or \"MULTIPOLYGON\" geometries. For each observation, the function checks whether at least one (queen=TRUE, default), or at least two (rook, queen=FALSE) points are within snap distance units of each other. The distances are planar in the raw coordinate units, ignoring geographical projections. Once the required number of sufficiently close points is found, the search is stopped.\n\nargs(poly2nb)\n\n\n#  function (pl, row.names = NULL, snap = NULL, queen = TRUE, useC =\n#    TRUE, foundInBox = NULL)\n\nFrom spdep 1.1-7, the sf package GEOS interface is used within poly2nb to find the candidate neighbours and populate foundInBox internally. In this case, the use of spatial indexing (STRtree queries) in GEOS through sf is the default:\n\npol_pres15 |&gt; poly2nb(queen = TRUE) -&gt; nb_q\n\nThe print method shows the summary structure of the neighbour object:\n\nnb_q\n# Neighbour list object:\n# Number of regions: 2495 \n# Number of nonzero links: 14242 \n# Percentage nonzero weights: 0.229 \n# Average number of links: 5.71\n\nFrom sf version 1.0-0, the s2 package (Dunnington, Pebesma, and Rubak 2023) is used by default for spherical geometries, as st_intersects used in poly2nb passes calculation to s2::s2_intersects_matrix (see Chapter 4). From spdep version 1.1-9, if sf_use_s2() is TRUE, spherical intersection is used to find candidate neighbours; as with GEOS, the underlying s2 library uses fast spatial indexing.\n\nold_use_s2 &lt;- sf_use_s2()\n\n\n\nsf_use_s2(TRUE)\n\n\n(pol_pres15 |&gt; st_transform(\"OGC:CRS84\") -&gt; pol_pres15_ll) |&gt; \n    poly2nb(queen = TRUE) -&gt; nb_q_s2\n\n \nSpherical and planar intersection of the input polygons yield the same contiguity neighbours in this case; in both cases valid input geometries are desirable:\n\nall.equal(nb_q, nb_q_s2, check.attributes=FALSE)\n# [1] TRUE\n\nNote that nb objects record both symmetric neighbour relationships i to j and j to i, because these objects admit asymmetric relationships as well, but these duplications are not needed for object construction.\nMost of the spdep functions for constructing neighbour objects take a row.names argument, the value of which is stored as a region.id attribute. If not given, the values are taken from row.names() of the first argument. These can be used to check that the neighbours object is in the same order as data. If nb objects are subsetted, the indices change to continue to be within 1:length(subsetted_nb), but the region.id attribute values point back to the object from which it was constructed. This is used in out-of-sample prediction from spatial regression models discussed briefly in Section 17.4.\nWe can also check that this undirected graph is connected using the n.comp.nb function; while some model estimation techniques do not support graphs that are not connected, it is helpful to be aware of possible problems (Freni-Sterrantino, Ventrucci, and Rue 2018):\n\n(nb_q |&gt; n.comp.nb())$nc\n# [1] 1\n\n\nThis approach is equivalent to treating the neighbour object as a graph and using graph analysis on that graph (Csardi and Nepusz 2006; Nepusz 2022), by first coercing to a binary sparse matrix (Bates, Maechler, and Jagan 2022):\n\nlibrary(Matrix, warn.conflicts = FALSE)\nlibrary(spatialreg, warn.conflicts = FALSE)\nnb_q |&gt; \n    nb2listw(style = \"B\") |&gt; \n    as(\"CsparseMatrix\") -&gt; smat\nlibrary(igraph, warn.conflicts = FALSE)\n(smat |&gt; graph_from_adjacency_matrix() -&gt; g1) |&gt; \n    count_components()\n# [1] 1\n\n \nNeighbour objects may be exported and imported in GAL format for exchange with other software, using write.nb.gal and read.gal:\n\ntf &lt;- tempfile(fileext = \".gal\")\nwrite.nb.gal(nb_q, tf)",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Proximity and Areal Data</span>"
    ]
  },
  {
    "objectID": "14-Areal.html#graph-based-neighbours",
    "href": "14-Areal.html#graph-based-neighbours",
    "title": "14  Proximity and Areal Data",
    "section": "\n14.3 Graph-based neighbours",
    "text": "14.3 Graph-based neighbours\nIf areal units are an appropriate representation, but only points on the plane have been observed, contiguity relationships may be approximated using graph-based neighbours. In this case, the imputed boundaries tessellate the plane such that points closer to one observation than any other fall within its polygon. The simplest form is by using triangulation, here using the deldir function in the deldir package. Because the function returns from \\(i\\) and to \\(j\\) identifiers, it is easy to construct a long representation of a listw object, as used in the S-Plus SpatialStats module and the sn2listw function internally to construct an nb object (ragged wide representation). Alternatives such as GEOS often fail to return sufficient information to permit the neighbours to be identified.\nThe output of these functions is then converted to the nb representation using graph2nb, with the possible use of the sym argument to coerce to symmetry. We take the centroids of the largest component polygon for each observation as the point representation; population-weighted centroids might have been a better choice if they were available:\n\npol_pres15 |&gt; \n    st_geometry() |&gt; \n    st_centroid(of_largest_polygon = TRUE) -&gt; coords \n(coords |&gt; tri2nb() -&gt; nb_tri)\n# Neighbour list object:\n# Number of regions: 2495 \n# Number of nonzero links: 14930 \n# Percentage nonzero weights: 0.24 \n# Average number of links: 5.98\n\n\nThe average number of neighbours is similar to the Queen boundary contiguity case, but if we look at the distribution of edge lengths using nbdists(), we can see that although the upper quartile is about 15 km, the maximum is almost 300 km, an edge along much of one side of the convex hull. The short minimum distance is also of interest, as many centroids of urban municipalities are very close to the centroids of their surrounding rural counterparts.\n \n\nnb_tri |&gt; \n    nbdists(coords) |&gt; \n    unlist() |&gt; \n    summary()\n#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#     247    9847   12151   13485   14994  296974\n\nTriangulated neighbours also yield a connected graph:\n\n(nb_tri |&gt; n.comp.nb())$nc\n# [1] 1\n\n\nGraph-based approaches include soi.graph - discussed here, relativeneigh and gabrielneigh.\nThe Sphere of Influence soi.graph function takes triangulated neighbours and prunes off neighbour relationships represented by edges that are unusually long for each point, especially around the convex hull (Avis and Horton 1985).\n\n(nb_tri |&gt; \n        soi.graph(coords) |&gt; \n        graph2nb() -&gt; nb_soi)\n# Warning in graph2nb(soi.graph(nb_tri, coords)): neighbour object\n# has 16 sub-graphs\n# Neighbour list object:\n# Number of regions: 2495 \n# Number of nonzero links: 12792 \n# Percentage nonzero weights: 0.205 \n# Average number of links: 5.13 \n# 16 disjoint connected subgraphs\n\nUnpicking the triangulated neighbours does however remove the connected character of the underlying graph:\n \n\n(nb_soi |&gt; n.comp.nb() -&gt; n_comp)$nc\n# [1] 16\n\nThe algorithm has stripped out longer edges leading to urban and rural municipality pairs where their centroids are very close to each other because the rural ones completely surround the urban, giving 15 pairs of neighbours unconnected to the main graph:\n\ntable(n_comp$comp.id)\n# \n#    1    2    3    4    5    6    7    8    9   10   11   12   13 \n# 2465    2    2    2    2    2    2    2    2    2    2    2    2 \n#   14   15   16 \n#    2    2    2\n\nThe largest length edges along the convex hull have been removed, but “holes” have appeared where the unconnected pairs of neighbours have appeared. The differences between nb_tri and nb_soi are shown in orange in Figure 14.2. \n\nCodeopar &lt;- par(mar = c(0,0,0,0)+0.5)\npol_pres15 |&gt; \n    st_geometry() |&gt; \n    plot(border = \"grey\", lwd = 0.5)\nnb_soi |&gt; plot(coords = coords, add = TRUE, \n               points = FALSE, lwd = 0.5)\nnb_tri |&gt; \n    diffnb(nb_soi) |&gt; \n    plot(coords = coords, col = \"orange\", add = TRUE,\n         points = FALSE, lwd = 0.5)\n# Warning in diffnb(nb_tri, nb_soi): neighbour object has 1567\n# sub-graphs\npar(opar)\n\n\n\n\n\n\nFigure 14.2: Triangulated (orange + black) and sphere of influence neighbours (black); apparent holes appear for sphere of influence neighbours where an urban municipality is surrounded by a dominant rural municipality (see Figure 14.1)",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Proximity and Areal Data</span>"
    ]
  },
  {
    "objectID": "14-Areal.html#distance-based-neighbours",
    "href": "14-Areal.html#distance-based-neighbours",
    "title": "14  Proximity and Areal Data",
    "section": "\n14.4 Distance-based neighbours",
    "text": "14.4 Distance-based neighbours\n \nDistance-based neighbours can be constructed using dnearneigh, with a distance band with lower d1 and upper d2 bounds controlled by the bounds argument. If spherical coordinates are used and either specified in the coordinates object x or with x as a two-column matrix and longlat=TRUE, great circle distances in kilometre will be calculated assuming the WGS84 reference ellipsoid, or if use_s2=TRUE (the default value) using the spheroid (see Chapter 4). If dwithin is FALSE and the version of s2 is greater than 1.0.7, s2_closest_edges may be used, if TRUE and use_s2=TRUE, s2_dwithin_matrix is used; both of these methods use fast spherical spatial indexing, but because s2_closest_edges takes minimum and maximum bounds, it only needs one pass in the R code of dnearneigh.\n\nArguments have been added to use functionality in the dbscan package (Hahsler and Piekenbrock 2022) for finding neighbours using planar spatial indexing in two or three dimensions by default, and not to test the symmetry of the output neighbour object. In addition, three arguments relate to the use of spherical geometry distance measurements.\n \nThe knearneigh function for \\(k\\)-nearest neighbours returns a knn object, converted to an nb object using knn2nb. It can also use great circle distances, not least because nearest neighbours may differ when unprojected coordinates are treated as planar. k should be a small number. For projected coordinates, the dbscan package is used to compute nearest neighbours more efficiently. Note that nb objects constructed in this way are most unlikely to be symmetric hence knn2nb has a sym argument to permit the imposition of symmetry, which will mean that all units have at least k neighbours, not that all units will have exactly k neighbours. When sf_use_s2() is TRUE, knearneigh will use fast spherical spatial indexing when the input object is of class \"sf\" or \"sfc\".\n\nThe nbdists function returns the length of neighbour relationship edges in the units of the coordinates if the coordinates are projected, in kilometre otherwise. In order to set the upper limit for distance bands, one may first find the maximum first nearest neighbour distance, using unlist to remove the list structure of the returned object. When sf_use_s2() is TRUE, nbdists will use fast spherical distance calculations when the input object is of class \"sf\" or \"sfc\".\n\ncoords |&gt; \n    knearneigh(k = 1) |&gt; \n    knn2nb() |&gt; \n    nbdists(coords) |&gt; \n    unlist() |&gt; \n    summary()\n# Warning in knn2nb(knearneigh(coords, k = 1)): neighbour object has\n# 695 sub-graphs\n#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#     247    6663    8538    8275   10124   17979\n\nHere the largest first nearest neighbour distance is just under 18 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour:\n\ncoords |&gt; dnearneigh(0, 18000) -&gt; nb_d18\n# Warning in dnearneigh(coords, 0, 18000): neighbour object has 2\n# sub-graphs\n\nFor this moderate number of observations, use of spatial indexing does not yield advantages in run times:\n\ncoords |&gt; dnearneigh(0, 18000, use_kd_tree = FALSE) -&gt; nb_d18a\n# Warning in dnearneigh(coords, 0, 18000, use_kd_tree = FALSE):\n# neighbour object has 2 sub-graphs\n\nand the output objects are the same:\n\nall.equal(nb_d18, nb_d18a, check.attributes = FALSE)\n# [1] TRUE\n\n\nnb_d18\n# Neighbour list object:\n# Number of regions: 2495 \n# Number of nonzero links: 20358 \n# Percentage nonzero weights: 0.327 \n# Average number of links: 8.16 \n# 2 disjoint connected subgraphs\n\nHowever, even though there are no no-neighbour observations (their presence is reported by the print method for nb objects), the graph is not connected, as a pair of observations are each others’ only neighbours.\n\n(nb_d18 |&gt; n.comp.nb() -&gt; n_comp)$nc\n# [1] 2\n\n\ntable(n_comp$comp.id)\n# \n#    1    2 \n# 2493    2\n\nAdding 300 m to the threshold gives us a neighbour object with no no-neighbour units, and all units can be reached from all others across the graph.\n\n(coords |&gt; dnearneigh(0, 18300) -&gt; nb_d183)\n# Neighbour list object:\n# Number of regions: 2495 \n# Number of nonzero links: 21086 \n# Percentage nonzero weights: 0.339 \n# Average number of links: 8.45\n\n\n(nb_d183 |&gt; n.comp.nb())$nc\n# [1] 1\n\nOne characteristic of distance-based neighbours is that more densely settled areas, with units which are smaller in terms of area, have higher neighbour counts (Warsaw boroughs are much smaller on average, but have almost 30 neighbours for this distance criterion). Having many neighbours smooths the neighbour relationship across more neighbours.\nFor use later, we also construct a neighbour object with no-neighbour units, using a threshold of 16 km:\n\n(coords |&gt; dnearneigh(0, 16000) -&gt; nb_d16)\n# Warning in dnearneigh(coords, 0, 16000): neighbour object has 17\n# sub-graphs\n# Neighbour list object:\n# Number of regions: 2495 \n# Number of nonzero links: 15850 \n# Percentage nonzero weights: 0.255 \n# Average number of links: 6.35 \n# 7 regions with no links:\n# 569, 1371, 1522, 2374, 2385, 2473, 2474\n# 17 disjoint connected subgraphs\n\nIt is possible to control the numbers of neighbours directly using \\(k\\)-nearest neighbours, either accepting asymmetric neighbours:\n\n((coords |&gt; knearneigh(k = 6) -&gt; knn_k6) |&gt; knn2nb() -&gt; nb_k6)\n# Neighbour list object:\n# Number of regions: 2495 \n# Number of nonzero links: 14970 \n# Percentage nonzero weights: 0.24 \n# Average number of links: 6 \n# Non-symmetric neighbours list\n\nor imposing symmetry:\n\n(knn_k6 |&gt; knn2nb(sym = TRUE) -&gt; nb_k6s)\n# Neighbour list object:\n# Number of regions: 2495 \n# Number of nonzero links: 16810 \n# Percentage nonzero weights: 0.27 \n# Average number of links: 6.74\n\nHere the size of k is sufficient to ensure connectedness, although the graph is not planar as edges cross at locations other than nodes, which is not the case for contiguous or graph-based neighbours.\n\n(nb_k6s |&gt; n.comp.nb())$nc\n# [1] 1\n\nIn the case of points on the sphere (see Chapter 4), the output of st_centroid will differ, so rather than inverse projecting the points, we extract points as geographical coordinates from the inverse projected polygon geometries:\n\nold_use_s2 &lt;- sf_use_s2()\n\n\nsf_use_s2(TRUE)\n\n\npol_pres15_ll |&gt; \n    st_geometry() |&gt; \n    st_centroid(of_largest_polygon = TRUE) -&gt; coords_ll\n\nFor spherical coordinates, distance bounds are in kilometres:\n\n(coords_ll |&gt; dnearneigh(0, 18.3, use_s2 = TRUE, \n                         dwithin = TRUE) -&gt; nb_d183_ll)\n# Neighbour list object:\n# Number of regions: 2495 \n# Number of nonzero links: 21140 \n# Percentage nonzero weights: 0.34 \n# Average number of links: 8.47\n\nThese neighbours differ from the spherical 18.3 km neighbours as would be expected:\n\nisTRUE(all.equal(nb_d183, nb_d183_ll, check.attributes = FALSE))\n# [1] FALSE\n\nIf s2 providing faster distance neighbour indexing is available, by default s2_closest_edges will be used for geographical coordinates:\n\n(coords_ll |&gt; dnearneigh(0, 18.3) -&gt; nb_d183_llce)\n# Neighbour list object:\n# Number of regions: 2495 \n# Number of nonzero links: 21140 \n# Percentage nonzero weights: 0.34 \n# Average number of links: 8.47\n\nwhere the two s2-based neighbour objects are the same:\n\nisTRUE(all.equal(nb_d183_llce, nb_d183_ll,\n                 check.attributes = FALSE))\n# [1] TRUE\n\nFast spherical spatial indexing in s2 is used to find \\(k\\) nearest neighbours:\n\n(coords_ll |&gt; knearneigh(k = 6) |&gt; knn2nb() -&gt; nb_k6_ll)\n# Neighbour list object:\n# Number of regions: 2495 \n# Number of nonzero links: 14970 \n# Percentage nonzero weights: 0.24 \n# Average number of links: 6 \n# Non-symmetric neighbours list\n\nThese neighbours differ from the planar k=6 nearest neighbours as would be expected, but will also differ slightly from legacy brute-force ellipsoid distances:\n\nisTRUE(all.equal(nb_k6, nb_k6_ll, check.attributes = FALSE))\n# [1] FALSE\n\nThe nbdists function also uses s2 to find distances on the sphere when the \"sf\" or \"sfc\"input object is in geographical coordinates (distances returned in kilometres):\n\nnb_q |&gt; nbdists(coords_ll) |&gt; unlist() |&gt; summary()\n#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#     0.2     9.8    12.2    12.6    15.1    33.0\n\nThese differ a little for the same weights object when planar coordinates are used (distances returned in the metric of the points for planar geometries and kilometres for ellipsoidal and spherical geometries):\n\nnb_q |&gt; nbdists(coords) |&gt; unlist() |&gt; summary()\n#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#     247    9822   12173   12651   15117   33102\n\n\nsf_use_s2(old_use_s2)",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Proximity and Areal Data</span>"
    ]
  },
  {
    "objectID": "14-Areal.html#weights-specification",
    "href": "14-Areal.html#weights-specification",
    "title": "14  Proximity and Areal Data",
    "section": "\n14.5 Weights specification",
    "text": "14.5 Weights specification\nOnce neighbour objects are available, further choices need to be made in specifying the weights objects. The nb2listw function is used to create a listw weights object with an nb object, a matching list of weights vectors, and a style specification. Because handling no-neighbour observations now begins to matter, the zero.policy argument is introduced. By default, this is FALSE, indicating that no-neighbour observations will cause an error, as the spatially lagged value for an observation with no neighbours is not available. By convention, zero is substituted for the lagged value, as the cross-product of a vector of zero-valued weights and a data vector, hence the name of zero.policy.\n \n\nargs(nb2listw)\n\n\n#  function (neighbours, glist = NULL, style = \"W\", zero.policy =\n#    NULL)\n\nWe will be using the helper function spweights.constants below to show some consequences of varying style choices. It returns constants for a listw object, \\(n\\) is the number of observations, n1 to n3 are \\(n-1, \\ldots\\), nn is \\(n^2\\) and \\(S_0\\), \\(S_1\\) and \\(S_2\\) are constants, \\(S_0\\) being the sum of the weights. There is a full discussion of the constants in Bivand and Wong (2018).\n\n\nargs(spweights.constants)\n\n\n#  function (listw, zero.policy = attr(listw, \"zero.policy\"),\n#    adjust.n = TRUE)\n\nThe \"B\" binary style gives a weight of unity to each neighbour relationship, and typically up-weights units with no boundaries on the edge of the study area, having a higher count of neighbours.\n\n(nb_q |&gt; \n    nb2listw(style = \"B\") -&gt; lw_q_B) |&gt; \n    spweights.constants() |&gt; \n    data.frame() |&gt; \n    subset(select = c(n, S0, S1, S2))\n#      n    S0    S1     S2\n# 1 2495 14242 28484 357280\n\n\nThe \"W\" row-standardised style up-weights units around the edge of the study area that necessarily have fewer neighbours. This style first gives a weight of unity to each neighbour relationship, then it divides these weights by the per unit sums of weights. Naturally this leads to division by zero where there are no neighbours, a not-a-number result, unless the chosen policy is to permit no-neighbour observations. We can see that \\(S_0\\) is now equal to \\(n\\).\n\n(nb_q |&gt; \n        nb2listw(style = \"W\") -&gt; lw_q_W) |&gt; \n    spweights.constants() |&gt; \n    data.frame() |&gt; \n    subset(select = c(n, S0, S1, S2))\n#      n   S0  S1    S2\n# 1 2495 2495 958 10406\n\n\nInverse distance weights are used in a number of scientific fields. Some use dense inverse distance matrices, but many of the inverse distances are close to zero, have little practical contribution, especially as the spatial process matrix is itself dense. Inverse distance weights may be constructed by taking the lengths of edges, changing units to avoid most weights being too large or small (here from metre to kilometre), taking the inverse, and passing through the glist argument to nb2listw:\n\nnb_d183 |&gt; \n    nbdists(coords) |&gt; \n    lapply(function(x) 1/(x/1000)) -&gt; gwts\n(nb_d183 |&gt; nb2listw(glist=gwts, style=\"B\") -&gt; lw_d183_idw_B) |&gt; \n    spweights.constants() |&gt; \n    data.frame() |&gt; \n    subset(select=c(n, S0, S1, S2))\n#      n   S0  S1   S2\n# 1 2495 1841 534 7265\n\nNo-neighbour handling is by default to prevent the construction of a weights object, making the analyst take a position on how to proceed.\n\ntry(nb_d16 |&gt; nb2listw(style=\"B\") -&gt; lw_d16_B)\n# Error in nb2listw(nb_d16, style = \"B\") : \n#   Empty neighbour sets found (zero.policy: FALSE)\n\nUse can be made of the zero.policy argument to many functions used with nb and listw objects.\n\nnb_d16 |&gt; \n    nb2listw(style=\"B\", zero.policy=TRUE) |&gt; \n    spweights.constants(zero.policy=TRUE) |&gt; \n    data.frame() |&gt; \n    subset(select=c(n, S0, S1, S2))\n#      n    S0    S1     S2\n# 1 2488 15850 31700 506480\n\nNote that by default the adjust.n argument to spweights.constants is set by default to TRUE, subtracting the count of no-neighbour observations from the observation count, so \\(n\\) is smaller with possible consequences for inference. The complete count can be retrieved by changing the argument.",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Proximity and Areal Data</span>"
    ]
  },
  {
    "objectID": "14-Areal.html#higher-order-neighbours",
    "href": "14-Areal.html#higher-order-neighbours",
    "title": "14  Proximity and Areal Data",
    "section": "\n14.6 Higher order neighbours",
    "text": "14.6 Higher order neighbours\n\nWe recall the characteristics of the neighbour object based on Queen contiguities:\n\nnb_q\n# Neighbour list object:\n# Number of regions: 2495 \n# Number of nonzero links: 14242 \n# Percentage nonzero weights: 0.229 \n# Average number of links: 5.71\n\nIf we wish to create an object showing \\(i\\) to \\(k\\) neighbours, where \\(i\\) is a neighbour of \\(j\\), and \\(j\\) in turn is a neighbour of \\(k\\), so taking two steps on the neighbour graph, we can use nblag, which automatically removes \\(i\\) to \\(i\\) self-neighbours:\n\n(nb_q |&gt; nblag(2) -&gt; nb_q2)[[2]]\n# Neighbour list object:\n# Number of regions: 2495 \n# Number of nonzero links: 32930 \n# Percentage nonzero weights: 0.529 \n# Average number of links: 13.2\n\nThe nblag_cumul function cumulates the list of neighbours for the whole list of lags:\n \n\nnblag_cumul(nb_q2)\n# Neighbour list object:\n# Number of regions: 2495 \n# Number of nonzero links: 47172 \n# Percentage nonzero weights: 0.758 \n# Average number of links: 18.9\n\nwhile the set operation union.nb takes two objects, giving here the same outcome: \n\nunion.nb(nb_q2[[2]], nb_q2[[1]])\n# Neighbour list object:\n# Number of regions: 2495 \n# Number of nonzero links: 47172 \n# Percentage nonzero weights: 0.758 \n# Average number of links: 18.9\n\nReturning to the graph representation of the same neighbour object, we can ask how many steps might be needed to traverse the graph:\n\n\ndiameter(g1)\n# [1] 52\n\nWe step out from each observation across the graph to establish the number of steps needed to reach each other observation by the shortest path (creating an \\(n \\times n\\) matrix sps), once again finding the same maximum count.\n\ng1 |&gt; distances() -&gt; sps\n(sps |&gt; apply(2, max) -&gt; spmax) |&gt; max()\n# [1] 52\n\n\nThe municipality with the maximum count is called Lutowiska, close to the Ukrainian border in the far south east of the country:\n\nmr &lt;- which.max(spmax)\npol_pres15$name0[mr]\n# [1] \"Lutowiska\"\n\nFigure 14.3 shows that contiguity neighbours represent the same kinds of relationships with other observations as distance. Some approaches prefer distance neighbours on the basis that, for example, inverse distance neighbours show clearly how all observations are related to each other. However, the development of tests for spatial autocorrelation and spatial regression models has involved the inverse of a spatial process model, which in turn can be represented as the sum of a power series of the product of a coefficient and a spatial weights matrix, intrinsically acknowledging the relationships of all observations with all other observations. Sparse contiguity neighbour objects accommodate rich dependency structures without the need to make the structures explicit.\n\nCodepol_pres15$sps1 &lt;- sps[,mr]\nif (!tmap4) {\n  tm1 &lt;- tm_shape(pol_pres15) +\n          tm_fill(\"sps1\", title = \"Shortest path\\ncount\")\n} else {\n  tm1 &lt;- tm_shape(pol_pres15) +\n      tm_fill(\"sps1\",\n          fill.scale = tm_scale(values = \"brewer.yl_or_br\"),\n          fill.legend = tm_legend(\"Shortest path\\ncount\", item.r = 0,\n              frame = FALSE, position = tm_pos_in(\"left\", \"bottom\")))\n}\ncoords[mr] |&gt; \n    st_distance(coords) |&gt; \n    c() |&gt; \n    (function(x) x/1000)() |&gt; \n    units::set_units(NULL) -&gt; pol_pres15$dist_52\nlibrary(ggplot2)\ng1 &lt;- ggplot(pol_pres15, aes(x = sps1, y = dist_52)) +\n        geom_point() +\n        xlab(\"Shortest path count\") +\n        ylab(\"km distance\")\ngridExtra::grid.arrange(tmap_grob(tm1), g1, nrow=1)\n\n\n\n\n\n\nFigure 14.3: Relationship of shortest paths to distance for Lutowiska; left panel: shortest path counts from Lutowiska; right panel: plot of shortest paths from Lutowiska to other observations, and distances from Lutowiska to other observations",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Proximity and Areal Data</span>"
    ]
  },
  {
    "objectID": "14-Areal.html#exercises",
    "href": "14-Areal.html#exercises",
    "title": "14  Proximity and Areal Data",
    "section": "\n14.7 Exercises",
    "text": "14.7 Exercises\n\nWhich kinds of geometry support are appropriate for which functions creating neighbour objects?\nWhich functions creating neighbour objects are only appropriate for planar representations?\nWhat difference might the choice of rook rather than queen contiguities make on a chessboard?\nWhat are the relationships between neighbour set cardinalities (neighbour counts) and row-standardised weights, and how do they open analyses up to edge effects? Use the chessboard you constructed in exercise 3 for both rook and queen neighbours.\n\n\n\n\n\n\n\nAvis, D., and J. Horton. 1985. “Remarks on the Sphere of Influence Graph.” In Discrete Geometry and Convexity, edited by J. E. Goodman, 323–27. New York: New York Academy of Sciences, New York.\n\n\nBates, Douglas, Martin Maechler, and Mikael Jagan. 2022. Matrix: Sparse and Dense Matrix Classes and Methods. https://CRAN.R-project.org/package=Matrix.\n\n\nBavaud, F. 1998. “Models for Spatial Weights: A Systematic Look.” Geographical Analysis 30: 153–71. https://doi.org/10.1111/j.1538-4632.1998.tb00394.x.\n\n\nBivand, Roger. 2022. Spdep: Spatial Dependence: Weighting Schemes, Statistics.\n\n\nBivand, Roger, and B. A. Portnov. 2004. “Exploring Spatial Data Analysis Techniques Using R: The Case of Observations with No Neighbours.” In Advances in Spatial Econometrics: Methodology, Tools, Applications, edited by Luc Anselin, Raymond J. G. M. Florax, and S. J. Rey, 121–42. Berlin: Springer.\n\n\nBivand, Roger, and David W. S. Wong. 2018. “Comparing Implementations of Global and Local Indicators of Spatial Association.” TEST 27 (3): 716–48. https://doi.org/10.1007/s11749-018-0599-x.\n\n\nBoots, B., and A. Okabe. 2007. “Local Statistical Spatial Analysis: Inventory and Prospect.” International Journal of Geographical Information Science 21 (4): 355–75. https://doi.org/10.1080/13658810601034267.\n\n\nCsardi, Gabor, and Tamas Nepusz. 2006. “The Igraph Software Package for Complex Network Research.” InterJournal Complex Systems: 1695. https://igraph.org.\n\n\nDunnington, Dewey, Edzer Pebesma, and Ege Rubak. 2023. S2: Spherical Geometry Operators Using the S2 Geometry Library. https://CRAN.R-project.org/package=s2.\n\n\nFreni-Sterrantino, Anna, Massimo Ventrucci, and Håvard Rue. 2018. “A Note on Intrinsic Conditional Autoregressive Models for Disconnected Graphs.” Spatial and Spatio-Temporal Epidemiology 26: 25–34. https://doi.org/10.1016/j.sste.2018.04.002.\n\n\nHahsler, Michael, and Matthew Piekenbrock. 2022. Dbscan: Density-Based Spatial Clustering of Applications with Noise (DBSCAN) and Related Algorithms. https://github.com/mhahsler/dbscan.\n\n\nNepusz, Tamás. 2022. Igraph: Network Analysis and Visualization. https://CRAN.R-project.org/package=igraph.\n\n\nOkabe, A., T. Satoh, T. Furuta, A. Suzuki, and K. Okano. 2008. “Generalized Network Voronoi Diagrams: Concepts, Computational Methods, and Applications.” International Journal of Geographical Information Science 22 (9): 965–94. https://doi.org/10.1080/13658810701587891.\n\n\nShe, Bing, Xinyan Zhu, Xinyue Ye, Wei Guo, Kehua Su, and Jay Lee. 2015. “Weighted Network Voronoi Diagrams for Local Spatial Analysis.” Computers, Environment and Urban Systems 52: 70–80. https://doi.org/10.1016/j.compenvurbsys.2015.03.005.\n\n\nWall, M. M. 2004. “A Close Look at the Spatial Structure Implied by the CAR and SAR Models.” Journal of Statistical Planning and Inference 121: 311–24.",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Proximity and Areal Data</span>"
    ]
  },
  {
    "objectID": "15-Measures.html",
    "href": "15-Measures.html",
    "title": "15  Measures of Spatial Autocorrelation",
    "section": "",
    "text": "15.1 Measures and process misspecification\nWhen analysing areal data, it has long been recognised that, if present, spatial autocorrelation changes how we may infer, relative to the default assumption of independent observations. In the presence of spatial autocorrelation, we can predict the values of observation \\(i\\) from the values observed at \\(j \\in N_i\\), the set of its proximate neighbours. Early results (Moran 1948; Geary 1954) entered into research practice gradually, for example the social sciences (Duncan, Cuzzort, and Duncan 1961). These results were then collated and extended to yield a set of basic tools of analysis (Cliff and Ord 1973, 1981).\nCliff and Ord (1973) generalised and extended the expression of the spatial weights matrix representation as part of the framework for establishing the distribution theory for join-count, Moran’s \\(I\\) and Geary’s \\(C\\) statistics. This development of what have become known as global measures, returning a single value of autocorrelation for the total study area, has been supplemented by local measures returning values for each areal unit (Getis and Ord 1992; Anselin 1995).\nThe measures offered by the spdep package have been written partly to provide implementations, but also to permit the comparative investigation of these measures and their implementation. For this reason, the implementations are written in R rather than compiled code, and they are generally slower but more flexible than implementations in the newly released rgeoda package (Li and Anselin 2021; Anselin, Li, and Koschinsky 2021).\nIt is not and has never been the case that Tobler’s first law of geography, “Everything is related to everything else, but near things are more related than distant things”, always holds absolutely. This is and has always been an oversimplification, disguising possible underlying entitation, support, and other misspecification problems. Are the units of observation appropriate for the scale of the underlying spatial process? Could the spatial patterning of the variable of interest for the chosen entitation be accounted for by another variable?\nTobler (1970) was published in the same special issue of Economic Geography as Olsson (1970), but Olsson does grasp the important point that spatial autocorrelation is not inherent in spatial phenomena, but often, is engendered by inappropriate entitation, by omitted variables and/or inappropriate functional form. The key quote from Olsson is on p. 228:\nThe status of the “first law” is very similar to the belief that John Snow induced from a map the cause of cholera as water-borne. It may be a good way of selling GIS, but it is inaccurate: Snow had a strong working hypothesis prior to visiting Soho, and the map was prepared after the Broad Street pump was disabled as documentation that his hypothesis held (Brody et al. 2000).\nMeasures of spatial autocorrelation unfortunately pick up other misspecifications in the way that we model data (Schabenberger and Gotway 2005; McMillen 2003). For reference, Moran’s \\(I\\) is given as (Cliff and Ord 1981, 17):\n\\[\nI = \\frac{n \\sum_{(2)} w_{ij} z_i z_j}{S_0 \\sum_{i=1}^{n} z_i^2}\n\\] where \\(x_i, i=1, \\ldots, n\\) are \\(n\\) observations on the numeric variable of interest, \\(z_i = x_i - \\bar{x}\\), \\(\\bar{x} = \\sum_{i=1}^{n} x_i / n\\), \\(\\sum_{(2)} = \\stackrel{\\sum_{i=1}^{n} \\sum_{j=1}^{n}}{i \\neq j}\\), \\(w_{ij}\\) are the spatial weights, and \\(S_0 = \\sum_{(2)} w_{ij}\\). First we test a random variable using the Moran test, here under the normality assumption (argument randomisation=FALSE, default TRUE). Inference is made on the statistic \\(Z(I) = \\frac{I - E(I)}{\\sqrt{\\mathrm{Var}(I)}}\\), the z-value compared with the Normal distribution for \\(E(I)\\) and \\(\\mathrm{Var}(I)\\) for the chosen assumptions; this x does not show spatial autocorrelation with these spatial weights:\nlibrary(spdep) |&gt; suppressPackageStartupMessages()\nlibrary(parallel)\nglance_htest &lt;- function(ht) c(ht$estimate, \n    \"Std deviate\" = unname(ht$statistic), \n    \"p.value\" = unname(ht$p.value))\nset.seed(1)\n(pol_pres15 |&gt; \n    nrow() |&gt; \n    rnorm() -&gt; x) |&gt; \n    moran.test(lw_q_B, randomisation = FALSE,\n               alternative = \"two.sided\") |&gt; \n    glance_htest()\n# Moran I statistic       Expectation          Variance \n#         -0.004772         -0.000401          0.000140 \n#       Std deviate           p.value \n#         -0.369320          0.711889\nThe test however detects quite strong positive spatial autocorrelation when we insert a gentle trend into the data, but omit to include it in the mean model, thus creating a missing variable problem but finding spatial autocorrelation instead:\nbeta &lt;- 0.0015\ncoords |&gt; \n    st_coordinates() |&gt; \n    subset(select = 1, drop = TRUE) |&gt; \n    (function(x) x/1000)() -&gt; t\n(x + beta * t -&gt; x_t) |&gt; \n    moran.test(lw_q_B, randomisation = FALSE,\n               alternative = \"two.sided\") |&gt; \n    glance_htest()\n# Moran I statistic       Expectation          Variance \n#          0.043403         -0.000401          0.000140 \n#       Std deviate           p.value \n#          3.701491          0.000214\nIf we test the residuals of a linear model including the trend, the apparent spatial autocorrelation disappears:\nlm(x_t ~ t) |&gt; \n    lm.morantest(lw_q_B, alternative = \"two.sided\") |&gt; \n    glance_htest()\n# Observed Moran I      Expectation         Variance      Std deviate \n#        -0.004777        -0.000789         0.000140        -0.337306 \n#          p.value \n#         0.735886\nA comparison of implementations of measures of spatial autocorrelation shows that a wide range of measures is available in R in a number of packages, chiefly in the spdep package (Bivand 2022b), and that differences from other implementations can be attributed to design decisions (Bivand and Wong 2018). The spdep package also includes the only implementations of exact and saddlepoint approximations to global and local Moran’s I for regression residuals (Tiefelsdorf 2002; Bivand, Müller, and Reder 2009).",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Measures of Spatial Autocorrelation</span>"
    ]
  },
  {
    "objectID": "15-Measures.html#sec-measprocmisspec",
    "href": "15-Measures.html#sec-measprocmisspec",
    "title": "15  Measures of Spatial Autocorrelation",
    "section": "",
    "text": "The existence of such autocorrelations makes it tempting to agree with Tobler (1970, 236 [the original refers to the pagination of a conference paper]) that ‘everything is related to everything else, but near things are more related than distant things.’ On the other hand, the fact that the autocorrelations seem to hide systematic specification errors suggests that the elevation of this statement to the status of ‘the first law of geography’ is at best premature. At worst, the statement may represent the spatial variant of the post hoc fallacy, which would mean that coincidence has been mistaken for a causal relation.",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Measures of Spatial Autocorrelation</span>"
    ]
  },
  {
    "objectID": "15-Measures.html#global-measures",
    "href": "15-Measures.html#global-measures",
    "title": "15  Measures of Spatial Autocorrelation",
    "section": "\n15.2 Global measures",
    "text": "15.2 Global measures\nGlobal measures consider the average level of spatial autocorrelation across all observations; they can of course be biased (as most spatial statistics) by edge effects where important spatial process components fall outside the study area.\nJoin-count tests for categorical data\n\nWe will begin by examining join-count statistics, where joincount.test takes a \"factor\" vector of values fx and a listw object, and returns a list of htest (hypothesis test) objects defined in the stats package, one htest object for each level of the fx argument. The observed counts are of neighbours with the same factor levels, known as same-colour joins.\n\n\nargs(joincount.test)\n\n\n#  function (fx, listw, zero.policy = attr(listw, \"zero.policy\"),\n#    alternative = \"greater\", sampling = \"nonfree\", spChk = NULL,\n#    adjust.n = TRUE)\n\nThe function takes an alternative argument for hypothesis testing, a sampling argument showing the basis for the construction of the variance of the measure, where the default \"nonfree\" choice corresponds to analytical permutation; the spChk argument is retained for backward compatibility. For reference, the counts of factor levels for the type of municipality or Warsaw borough are:\n\n(pol_pres15 |&gt; \n        st_drop_geometry() |&gt; \n        subset(select = types, drop = TRUE) -&gt; Types) |&gt; \n    table()\n# \n#          Rural          Urban    Urban/rural Warsaw Borough \n#           1563            303            611             18\n\nSince there are four levels, we rearrange the list of htest objects to give a matrix of estimated results. The observed same-colour join-counts are tabulated with their expectations based on the counts of levels of the input factor, so that few joins would be expected between for example Warsaw boroughs, because there are very few of them. The variance calculation uses the underlying constants of the chosen listw object and the counts of levels of the input factor. The z-value is obtained in the usual way by dividing the difference between the observed and expected join-counts by the square root of the variance.\nThe join-count test was subsequently adapted for multi-colour join-counts (Upton and Fingleton 1985). The implementation as joincount.multi in spdep returns a table based on non-free sampling, and does not report p-values.\n\n\nTypes |&gt; joincount.multi(listw = lw_q_B)\n#                               Joincount Expected Variance z-value\n# Rural:Rural                    3087.000 2793.920 1126.534    8.73\n# Urban:Urban                     110.000  104.719   93.299    0.55\n# Urban/rural:Urban/rural         656.000  426.526  331.759   12.60\n# Warsaw Borough:Warsaw Borough    41.000    0.350    0.347   68.96\n# Urban:Rural                     668.000 1083.941  708.209  -15.63\n# Urban/rural:Rural              2359.000 2185.769 1267.131    4.87\n# Urban/rural:Urban               171.000  423.729  352.190  -13.47\n# Warsaw Borough:Rural             12.000   64.393   46.460   -7.69\n# Warsaw Borough:Urban              9.000   12.483   11.758   -1.02\n# Warsaw Borough:Urban/rural        8.000   25.172   22.354   -3.63\n# Jtot                           3227.000 3795.486 1496.398  -14.70\n\nSo far, we have used binary weights, so the sum of join-counts multiplied by the weight on that join remains integer. If we change to row standardised weights, where the weights are almost always fractions of 1, the counts, expectations and variances change, but there are few major changes in the z-values.\nUsing an inverse distance based listw object does, however, change the z-values markedly, because closer centroids are up-weighted relatively strongly:\n\nTypes |&gt; joincount.multi(listw = lw_d183_idw_B)\n#                               Joincount Expected Variance z-value\n# Rural:Rural                    3.46e+02 3.61e+02 4.93e+01   -2.10\n# Urban:Urban                    2.90e+01 1.35e+01 2.23e+00   10.39\n# Urban/rural:Urban/rural        4.65e+01 5.51e+01 9.61e+00   -2.79\n# Warsaw Borough:Warsaw Borough  1.68e+01 4.53e-02 6.61e-03  206.38\n# Urban:Rural                    2.02e+02 1.40e+02 2.36e+01   12.73\n# Urban/rural:Rural              2.25e+02 2.83e+02 3.59e+01   -9.59\n# Urban/rural:Urban              3.65e+01 5.48e+01 8.86e+00   -6.14\n# Warsaw Borough:Rural           5.65e+00 8.33e+00 1.73e+00   -2.04\n# Warsaw Borough:Urban           9.18e+00 1.61e+00 2.54e-01   15.01\n# Warsaw Borough:Urban/rural     3.27e+00 3.25e+00 5.52e-01    0.02\n# Jtot                           4.82e+02 4.91e+02 4.16e+01   -1.38\n\nMoran’s \\(I\\)\n\nThe implementation of Moran’s \\(I\\) in spdep in the moran.test function has similar arguments to those of joincount.test, but sampling is replaced by randomisation to indicate the underlying analytical approach used for calculating the variance of the measure. It is also possible to use ranks rather than numerical values (Cliff and Ord 1981, 46). The drop.EI2 argument may be used to reproduce results where the final component of the variance term is omitted as found in some legacy software implementations.\n\nargs(moran.test)\n\n\n#  function (x, listw, randomisation = TRUE, zero.policy =\n#    attr(listw, \"zero.policy\"), alternative = \"greater\", rank =\n#    FALSE, na.action = na.fail, spChk = NULL, adjust.n = TRUE,\n#    drop.EI2 = FALSE)\n\n\nThe default for the randomisation argument is TRUE, but here we will simply show that the test under normality is the same as a test of least squares residuals with only the intercept used in the mean model. The analysed variable is first-round turnout proportion of registered voters in municipalities and Warsaw boroughs in the 2015 Polish presidential election. The spelling of randomisation is that of Cliff and Ord (1973).\n\npol_pres15 |&gt; \n        st_drop_geometry() |&gt; \n        subset(select = I_turnout, drop = TRUE) -&gt; I_turnout\n\n\nI_turnout |&gt; moran.test(listw = lw_q_B, randomisation = FALSE) |&gt; \n    glance_htest()\n# Moran I statistic       Expectation          Variance \n#          0.691434         -0.000401          0.000140 \n#       Std deviate           p.value \n#         58.461349          0.000000\n\n\nThe lm.morantest function also takes a resfun argument to set the function used to extract the residuals used for testing, and clearly lets us model other salient features of the response variable (Cliff and Ord 1981, 203). To compare with the standard test, we are only using the intercept here and, as can be seen, the results are the same.\n\nlm(I_turnout ~ 1, pol_pres15) |&gt; \n    lm.morantest(listw = lw_q_B) |&gt; \n    glance_htest()\n# Observed Moran I      Expectation         Variance      Std deviate \n#         0.691434        -0.000401         0.000140        58.461349 \n#          p.value \n#         0.000000\n\nThe only difference between tests under normality and randomisation is that an extra term is added if the kurtosis of the variable of interest indicates a flatter or more peaked distribution, where the measure used is the classical measure of kurtosis. Under the default randomisation assumption of analytical randomisation, the results are largely unchanged.\n\n(I_turnout |&gt; \n    moran.test(listw = lw_q_B) -&gt; mtr) |&gt; \n    glance_htest()\n# Moran I statistic       Expectation          Variance \n#          0.691434         -0.000401          0.000140 \n#       Std deviate           p.value \n#         58.459835          0.000000\n\n From the very beginning in the early 1970s, interest was shown in Monte Carlo tests, also known as Hope-type tests and as permutation bootstrap. By default, moran.mc returns a \"htest\" object, but may simply use boot::boot internally and return a \"boot\" object when return_boot=TRUE. In addition the number of simulations needs to be given as nsim; that is the number of times the values of the observations are shuffled at random.\n\n\nset.seed(1)\nI_turnout |&gt; \n    moran.mc(listw = lw_q_B, nsim = 999, \n             return_boot = TRUE) -&gt; mmc\n\nThe bootstrap permutation retains the outcomes of each of the random permutations, reporting the observed value of the statistic, here Moran’s \\(I\\), the difference between this value and the mean of the simulations under randomisation (equivalent to \\(E(I)\\)), and the standard deviation of the simulations under randomisation.\nIf we compare the Monte Carlo and analytical variances of \\(I\\) under randomisation, we typically see few differences, arguably rendering Monte Carlo testing unnecessary.\n \n\nc(\"Permutation bootstrap\" = var(mmc$t), \n  \"Analytical randomisation\" = unname(mtr$estimate[3]))\n#    Permutation bootstrap Analytical randomisation \n#                 0.000144                 0.000140\n\nGeary’s global \\(C\\) is implemented in geary.test largely following the same argument structure as moran.test. The Getis-Ord \\(G\\) test includes extra arguments to accommodate differences between implementations, as Bivand and Wong (2018) found multiple divergences from the original definitions, often to omit no-neighbour observations generated when using distance band neighbours. It is given by Getis and Ord (1992), on page 194. For \\(G^*\\), the \\(i \\neq j\\) summation constraint is relaxed by including \\(i\\) as a neighbour of itself (thereby also removing the no-neighbour problem, because all observations have at least one neighbour).\nFinally, the empirical Bayes Moran’s \\(I\\) takes account of the denominator in assessing spatial autocorrelation in rates data (Assunção and Reis 1999). Until now, we have considered the proportion of valid votes cast in relation to the numbers entitled to vote by spatial entity, but using EBImoran.mc we can try to accommodate uncertainty in extreme rates in entities with small numbers entitled to vote. There is, however, little impact on the outcome in this case.\nGlobal measures of spatial autocorrelation using spatial weights objects based on graphs of neighbours are, as we have seen, rather blunt tools, which for interpretation depend critically on a reasoned mean model of the variable in question. If the mean model is just the intercept, the global measures will respond to all kinds of misspecification, not only spatial autocorrelation. The choice of entities for aggregation of data will typically be a key source of misspecification.",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Measures of Spatial Autocorrelation</span>"
    ]
  },
  {
    "objectID": "15-Measures.html#local-measures",
    "href": "15-Measures.html#local-measures",
    "title": "15  Measures of Spatial Autocorrelation",
    "section": "\n15.3 Local measures",
    "text": "15.3 Local measures\n Building on insights from the weaknesses of global measures, local indicators of spatial association began to appear in the first half of the 1990s (Anselin 1995; Getis and Ord 1992, 1996).\nIn addition, the Moran plot was introduced, plotting the values of the variable of interest against their spatially lagged values, typically using row-standardised weights to make the axes more directly comparable (Anselin 1996). The moran.plot function also returns an influence measures object used to label observations exerting more than proportional influence on the slope of the line representing global Moran’s \\(I\\). In Figure 15.1, we can see that there are many spatial entities exerting such influence. These pairs of observed and lagged observed values make up in aggregate the global measure, but can also be explored in detail. The quadrants of the Moran plot also show low-low pairs in the lower left quadrant, high-high in the upper right quadrant, and fewer low-high and high-low pairs in the upper left and lower right quadrants. In moran.plot, the quadrants are split on the means of the variable and its spatial lag; alternative splits are on zero for the centred variable and the spatial lag of the centred variable.\n\n\nCodeI_turnout |&gt; \n    moran.plot(listw = lw_q_W, labels = pol_pres15$TERYT, \n               cex = 1, pch = \".\", xlab = \"I round turnout\", \n               ylab = \"lagged turnout\") -&gt; infl_W\n\n\n\n\n\n\nFigure 15.1: Moran plot of I round turnout, row standardised weights\n\n\n\n\nIf we extract the hat value influence measure from the returned object, Figure 15.2 suggests that some edge entities exert more than proportional influence (perhaps because of row standardisation), as do entities in or near larger urban areas.\n\npol_pres15$hat_value &lt;- infl_W$hat\nif (tmap4) {\n  tm_shape(pol_pres15) +\n  tm_fill(\"hat_value\", fill.scale = tm_scale(values = \"brewer.yl_or_br\"),\n    fill.legend = tm_legend(item.r = 0, frame = FALSE,\n    position = tm_pos_in(\"left\", \"bottom\"))\n  )\n} else {\n  tm_shape(pol_pres15) + tm_fill(\"hat_value\")\n}\n\n\n\n\n\n\nFigure 15.2: Moran plot hat values, row standardised neighbours\n\n\n\n\nLocal Moran’s \\(I_i\\)\n\n\nBivand and Wong (2018) discuss issues impacting the use of local indicators, such as local Moran’s \\(I_i\\) and local Getis-Ord \\(G_i\\). Some issues affect the calculation of the local indicators, others inference from their values. Because \\(n\\) statistics may be calculated from the same number of observations, there are multiple comparison problems that need to be addressed. Caldas de Castro and Singer (2006) conclude, based on a typical dataset and a simulation exercise, that the false discovery rate (FDR) adjustment of probability values will certainly give a better picture of interesting clusters than no adjustment. Following this up, Anselin (2019) explores the combination of FDR adjustments with the use of redefined “significance” cutoffs (Benjamin et al. 2018), for example \\(0.01\\), \\(0.005\\), and \\(0.001\\) instead of \\(0.1\\), \\(0.05\\), and \\(0.01\\); the use of the term interesting rather than significant is also preferred. This is discussed further in Bivand (2022a). As in the global case, misspecification remains a source of confusion, and, further, interpreting local spatial autocorrelation in the presence of global spatial autocorrelation is challenging (Ord and Getis 2001; Tiefelsdorf 2002; Bivand, Müller, and Reder 2009).\n\nargs(localmoran)\n\n\n#  function (x, listw, zero.policy = attr(listw, \"zero.policy\"),\n#    na.action = na.fail, conditional = TRUE, alternative =\n#    \"two.sided\", mlvar = TRUE, spChk = NULL, adjust.x = FALSE)\n\n\nIn an important clarification, Sauer et al. (2021) show that the comparison of standard deviates for local Moran’s \\(I_i\\) based on analytical formulae and conditional permutation in Bivand and Wong (2018) was based on a misunderstanding. Sokal, Oden, and Thomson (1998) provide alternative analytical formulae for standard deviates of local Moran’s \\(I_i\\) based either on total or conditional permutation, but the analytical formulae used in Bivand and Wong (2018), based on earlier practice, only use total permutation, and consequently do not match the simulation conditional permutations. Thanks to a timely pull request, localmoran now has a conditional argument (default TRUE) using alternative formulae from the appendix of Sokal, Oden, and Thomson (1998). The mlvar and adjust.x arguments to localmoran are discussed in Bivand and Wong (2018), and permit matching with other implementations. Taking \"two.sided\" probability values (the default), we obtain:\n\nI_turnout |&gt; \n    localmoran(listw = lw_q_W) -&gt; locm\n\nThe \\(I_i\\) local indicators when summed and divided by the sum of the spatial weights equal global Moran’s \\(I\\), showing the possible presence of positive and negative local spatial autocorrelation:\n\nall.equal(sum(locm[,1])/Szero(lw_q_W), \n          unname(moran.test(I_turnout, lw_q_W)$estimate[1]))\n# [1] TRUE\n\nUsing stats::p.adjust to adjust for multiple comparisons, we see that over 15% of the 2495 local measures have p-values &lt; 0.005 if no adjustment is applied, but only 1.5% using Bonferroni adjustment to control the family-wise error rate, with two other choices shown: \"fdr\" is the Benjamini and Hochberg (1995) false discovery rate (almost 6%) and \"BY\" (Benjamini and Yekutieli 2001), another false discovery rate adjustment (about 2.5%):\n\npva &lt;- function(pv) cbind(\"none\" = pv, \n    \"FDR\" = p.adjust(pv, \"fdr\"), \"BY\" = p.adjust(pv, \"BY\"),\n    \"Bonferroni\" = p.adjust(pv, \"bonferroni\"))\nlocm |&gt; \n    subset(select = \"Pr(z != E(Ii))\", drop = TRUE) |&gt; \n    pva() -&gt; pvsp\nf &lt;- function(x) sum(x &lt; 0.005)\napply(pvsp, 2, f)\n#       none        FDR         BY Bonferroni \n#        385        149         64         38\n\nIn the global measure case, bootstrap permutations may be used as an alternative to analytical methods for possible inference, where both the theoretical development of the analytical variance of the measure, and the permutation scheme, shuffle all of the observed values. In the local case, conditional permutation should be used, fixing the value at observation \\(i\\) and randomly sampling from the remaining \\(n-1\\) values to find randomised values at neighbours. Conditional permutation is provided as function localmoran_perm, which may use multiple compute nodes to sample in parallel if provided, and permits the setting of a seed for the random number generator across the compute nodes. The number of simulations nsim also controls the precision of the ranked estimates of the probability value based on the rank of observed \\(I_i\\) among the simulated values:\n\n\nlibrary(parallel)\ninvisible(spdep::set.coresOption(max(detectCores()-1L, 1L)))\nI_turnout |&gt; \n    localmoran_perm(listw = lw_q_W, nsim = 9999, \n                    iseed = 1) -&gt; locm_p\n\nThe outcome is that over 15% of observations have two sided p-values &lt; 0.005 without multiple comparison adjustment, and about 1.5% with Bonferroni adjustment, when the p-values are calculated using the standard deviate of the permutation samples and the normal distribution.\n\nlocm_p |&gt; \n    subset(select = \"Pr(z != E(Ii))\", drop = TRUE) |&gt; \n    pva() -&gt; pvsp\napply(pvsp, 2, f)\n#       none        FDR         BY Bonferroni \n#        382        147         64         38\n\nSince the variable under analysis may not be normally distributed, the p-values can also be calculated by finding the rank of the observed \\(I_i\\) among the rank-based simulated values, and looking up the probability value from the uniform distribution taking the alternative choice into account:\n\nlocm_p |&gt; \n    subset(select = \"Pr(z != E(Ii)) Sim\", drop = TRUE) |&gt; \n    pva() -&gt; pvsp\napply(pvsp, 2, f)\n#       none        FDR         BY Bonferroni \n#        386        128          0          0\n\nNow the \"BY\" and Bonferroni counts of interesting locations are zero with 9999 samples, but may be recovered by increasing the sample count to 999999 if required; the FDR adjustment and interesting cutoff 0.005 yields about 5% locations.\n\npol_pres15$locm_pv &lt;- p.adjust(locm[, \"Pr(z != E(Ii))\"], \"fdr\")\npol_pres15$locm_std_pv &lt;- p.adjust(locm_p[, \"Pr(z != E(Ii))\"], \n                                   \"fdr\")\npol_pres15$locm_p_pv &lt;- p.adjust(locm_p[, \"Pr(z != E(Ii)) Sim\"],\n                                 \"fdr\")\n\n\nCodeif (tmap4) {\npv_brks &lt;- c(0, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.5, 0.75, 1)\ntm_shape(pol_pres15) + \n    tm_polygons(fill=c(\"locm_pv\", \"locm_std_pv\", \"locm_p_pv\"),\n        fill.legend = tm_legend(\"Pseudo p-values\\nLocal Moran's I\",\n            frame=FALSE, item.r = 0),\n        fill.scale = tm_scale(breaks = pv_brks, values=\"-brewer.yl_or_br\"),\n        fill.free=FALSE, lwd=0.01) +\n    ## see https://github.com/r-tmap/tmap/issues/1111:\n    #tm_facets_grid(columns=2, rows=2) +\n    tm_layout(panel.labels = c(\"Analytical conditional\",\n        \"Permutation std. dev.\", \"Permutation rank\"))\n} else {\ntm_shape(pol_pres15) +\n        tm_fill(c(\"locm_pv\", \"locm_std_pv\", \"locm_p_pv\"), \n                breaks=pv_brks, \n                title = \"Pseudo p-values\\nLocal Moran's I\",\n                palette=\"-YlOrBr\") +\n    tm_facets(free.scales = FALSE, ncol = 2) +\n    tm_layout(panel.labels = c(\"Analytical conditional\",\n                               \"Permutation std. dev.\",\n                               \"Permutation rank\"))\n}\n# [plot mode] fit legend/component: Some legend items or map\n# compoments do not fit well, and are therefore rescaled.\n# ℹ Set the tmap option `component.autoscale = FALSE` to disable\n#   rescaling.\n\n\n\n\n\n\nFigure 15.3: Local Moran’s I FDR probability values: left upper panel: analytical conditional p-values; right upper panel: permutation standard deviate conditional p-values; left lower panel: permutation rank conditional p-values, first-round turnout, row-standardised neighbours\n\n\n\n\nProceeding using the FDR adjustment and an interesting location cutoff of \\(0.005\\), we can see from Figure 15.3 that the adjusted probability values for the analytical conditional approach, the approach using the moments of the sampled values from permutation sampling, and the approach using the ranks of observed values among permutation samples all yield similar maps, as the distribution of the input variable is quite close to normal.\n In presenting local Moran’s \\(I\\), use is often made of “hotspot” maps. Because \\(I_i\\) takes high values both for strong positive autocorrelation of low and high values of the input variable, it is hard to show where “clusters” of similar neighbours with low or high values of the input variable occur. The quadrants of the Moran plot are used, by creating a categorical quadrant variable interacting the input variable and its spatial lag split at their means. The quadrant categories are then set to NA if, for the chosen probability value and adjustment, \\(I_i\\) would not be considered interesting. Here, for the FDR adjusted conditional analytical probability values (Figure 15.3, upper left panel), 53 observations belong to \"Low-Low\" cluster cores, and 96 to \"High-High\" cluster cores, similarly for the standard deviate-based permutation p-values (Figure 15.3, upper right panel), but the rank-based permutation p-values reduce the \"High-High\" count and increase the \"Low-Low\" count Figure 15.3 lower left panel:\n\nquadr &lt;- attr(locm, \"quadr\")$mean\na &lt;- table(addNA(quadr))\nlocm |&gt; hotspot(Prname=\"Pr(z != E(Ii))\", cutoff = 0.005, \n                droplevels=FALSE) -&gt; pol_pres15$hs_an_q\nlocm_p |&gt; hotspot(Prname=\"Pr(z != E(Ii))\", cutoff = 0.005, \n                  droplevels=FALSE) -&gt; pol_pres15$hs_ac_q \nlocm_p |&gt; hotspot(Prname=\"Pr(z != E(Ii)) Sim\", cutoff = 0.005,\n                  droplevels = FALSE) -&gt; pol_pres15$hs_cp_q\nb &lt;- table(addNA(pol_pres15$hs_an_q))\nc &lt;- table(addNA(pol_pres15$hs_ac_q))\nd &lt;- table(addNA(pol_pres15$hs_cp_q))\nt(rbind(\"Moran plot quadrants\" = a, \"Analytical cond.\" = b, \n  \"Permutation std. cond.\" = c, \"Permutation rank cond.\" = d))\n#           Moran plot quadrants Analytical cond.\n# Low-Low                   1040               53\n# High-Low                   264                0\n# Low-High                   213                0\n# High-High                  978               96\n# &lt;NA&gt;                         0             2346\n#           Permutation std. cond. Permutation rank cond.\n# Low-Low                       54                     62\n# High-Low                       0                      0\n# Low-High                       0                      0\n# High-High                     93                     66\n# &lt;NA&gt;                        2348                   2367\n\n\npol_pres15$hs_an_q &lt;- droplevels(pol_pres15$hs_an_q)\npol_pres15$hs_ac_q &lt;- droplevels(pol_pres15$hs_ac_q)\npol_pres15$hs_cp_q &lt;- droplevels(pol_pres15$hs_cp_q)\n\n\nCodeif (tmap4) {\n    pal &lt;- rev(RColorBrewer::brewer.pal(4, \"Set3\")[-c(2,3)])\n    tm_shape(pol_pres15) +\n    tm_polygons(fill = c(\"hs_an_q\", \"hs_ac_q\", \"hs_cp_q\"),\n    fill.legend = tm_legend(\"Turnout hotspot status \\nLocal Moran's I\",\n            frame = FALSE, item.r = 0),\n    fill.scale = tm_scale(values = pal, value.na = \"grey95\",\n            label.na = \"Not \\\"interesting\\\"\"),\n        lwd = 0.01, fill.free = FALSE) +\n    tm_facets_wrap(ncol = 2, nrow = 2) +\n    tm_layout(panel.labels = c(\"Analytical conditional\",\n        \"Permutation std. cond.\", \"Permutation rank cond.\"))\n} else {\ntm_shape(pol_pres15) +\n    tm_fill(c(\"hs_an_q\", \"hs_ac_q\", \"hs_cp_q\"),\n        colorNA = \"grey95\", textNA=\"Not \\\"interesting\\\"\",\n        title = \"Turnout hotspot status \\nLocal Moran's I\",\n        palette = RColorBrewer::brewer.pal(4, \"Set3\")[-c(2,3)]) +\n    tm_facets(free.scales = FALSE, ncol = 2) +\n    tm_layout(panel.labels = c(\"Analytical conditional\",\n                               \"Permutation std. cond.\",\n                               \"Permutation rank cond.\"))\n}\n\n\n\n\n\n\nFigure 15.4: Local Moran's I FDR hotspot cluster core maps \\(\\alpha = 0.005\\): left upper panel: analytical conditional p-values; right upper panel: permutation standard deviate conditional p-values; left lower panel: permutation rank conditional p-values, first-round turnout, row-standardised neighbours\n\n\n\n\nFigure 15.4 shows that there is very little difference between the FDR-adjusted interesting clusters with a choice of an \\(\\alpha=0.005\\) probability value cutoff for the three approaches of analytical conditional standard deviates, permutation-based standard deviates, and rank-based probability values; the \"High-High\" cluster cores are metropolitan areas.\nTiefelsdorf (2002) argues that standard approaches to the calculation of the standard deviates of local Moran’s \\(I_i\\) should be supplemented by numerical estimates, and shows that saddlepoint approximations are a computationally efficient way of achieving this goal. The localmoran.sad function takes a fitted linear model as its first argument, so we first fit a null (intercept only) model, but use case weights because the numbers entitled to vote vary greatly between observations:\n \n\nlm(I_turnout ~ 1) -&gt; lm_null\n\nSaddlepoint approximation is as computationally intensive as conditional permutation, because, rather than computing a simple measure on many samples, a good deal of numerical calculation is needed for each local approximation:\n\nlm_null |&gt; localmoran.sad(nb = nb_q, style = \"W\",\n                                  alternative = \"two.sided\") |&gt;\n        summary() -&gt; locm_sad_null\n\nThe chief advantage of the saddlepoint approximation is that it takes a fitted linear model rather than simply a numerical variable, so the residuals are analysed. With an intercept-only model, the results are similar to local Moran’s \\(I_i\\), but we can weight the observations, here by the count of those entitled to vote, which should down-weight small units of observation:\n\nlm(I_turnout ~ 1, weights = pol_pres15$I_entitled_to_vote) -&gt;\n        lm_null_weights\nlm_null_weights |&gt;\n            localmoran.sad(nb = nb_q, style = \"W\",\n                           alternative = \"two.sided\") |&gt;\n        summary() -&gt; locm_sad_null_weights\n\nNext we add the categorical variable distinguishing between rural, urban and other types of observational unit:\n\nlm(I_turnout ~ Types, weights=pol_pres15$I_entitled_to_vote) -&gt;\n        lm_types\nlm_types |&gt; localmoran.sad(nb = nb_q, style = \"W\",\n                                  alternative = \"two.sided\") |&gt;\n        summary() -&gt; locm_sad_types\n\n\nlocm_sad_null |&gt; hotspot(Prname=\"Pr. (Sad)\",\n                     cutoff=0.005) -&gt; pol_pres15$locm_sad0\nlocm_sad_null_weights |&gt; hotspot(Prname=\"Pr. (Sad)\",\n                     cutoff = 0.005) -&gt; pol_pres15$locm_sad1\nlocm_sad_types |&gt; hotspot(Prname=\"Pr. (Sad)\",\n                     cutoff = 0.005) -&gt; pol_pres15$locm_sad2\n\n\nCodeif (tmap4) {\n    pal &lt;- RColorBrewer::brewer.pal(4, \"Set3\")[c(4, 1)]\n    tm_shape(pol_pres15) +\n    tm_polygons(fill = c(\"hs_cp_q\", \"locm_sad0\", \"locm_sad1\",  \"locm_sad2\"),\n    fill.legend = tm_legend(\"Turnout hotspot status \\nLocal Moran's I\",\n            frame = FALSE, item.r = 0),\n    fill.scale = tm_scale(values = pal, value.na = \"grey95\",\n            label.na = \"Not \\\"interesting\\\"\"),\n        lwd = 0.01, fill.free = FALSE) +\n    tm_facets_wrap(ncol = 2, nrow = 2) +\n    tm_layout(panel.labels = c(\"Permutation rank\", \n         \"saddlepoint null\", \"saddlepoint weighted null\", \n     \"saddlepoint weighted types\"))\n} else {\ntm_shape(pol_pres15) +\n  tm_fill(c(\"hs_cp_q\", \"locm_sad0\", \"locm_sad1\",  \"locm_sad2\"),\n    colorNA = \"grey95\", textNA = \"Not \\\"interesting\\\"\",\n    title = \"Turnout hotspot status \\nLocal Moran's I\",\n    palette =RColorBrewer::brewer.pal(4, \"Set3\")[c(1, 4, 2)]) +\n  tm_facets(free.scales = FALSE, ncol = 2) + \n  tm_layout(panel.labels = c(\"Permutation rank\", \n     \"saddlepoint null\", \"saddlepoint weighted null\", \n     \"saddlepoint weighted types\"))\n}\n\n\n\n\n\n\nFigure 15.5: Local Moran's I FDR hotspot cluster core maps, two-sided, interesting cutoff \\(\\alpha = 0.005\\): left upper panel: permutation rank conditional p-values; right upper panel: null (intercept only) model saddlepoint p-values; left lower panel: weighted null (intercept only) model saddlepoint p-values; right lower panel: weighted types model saddlepoint p-values, for first-round turnout, row-standardised neighbours\n\n\n\n\n\nrbind(null = append(table(addNA(pol_pres15$locm_sad0)),\n                    c(\"Low-High\" = 0), 1),\n      weighted = append(table(addNA(pol_pres15$locm_sad1)),\n                        c(\"Low-High\" = 0), 1),\n      type_weighted = append(table(addNA(pol_pres15$locm_sad2)),\n                        c(\"Low-High\" = 0), 1))\n#               Low-Low Low-High High-High &lt;NA&gt;\n# null               19        0        55 2421\n# weighted            9        0        52 2434\n# type_weighted      13        0        81 2401\n\n\nFigure 15.5 includes the permutation rank cluster cores for comparison (upper left panel). Because saddlepoint approximation permits richer mean models to be used, and possibly because the approximation approach is inherently local, relating regression residual values at \\(i\\) to those of its neighbours, the remaining three panels diverge somewhat. The intercept-only (null) model is fairly similar to standard local Moran’s \\(I_i\\), but weighting by counts of eligible voters removes most of the \"Low-Low\" cluster cores. Adding the type categorical variable strengthens the urban \"High-High\" cluster cores but removes the Warsaw boroughs as interesting cluster cores. The central boroughs are surrounded by other boroughs, all with high turnout, not driven by autocorrelation but by being metropolitan boroughs. It is also possible to use saddlepoint approximation where the global spatial process has been incorporated, removing the conflation of global and local spatial autocorrelation in standard approaches.\nThe same can also be accomplished using exact methods, but may require more tuning as numerical integration may fail, returning NaN rather than the exact estimate of the standard deviate (Bivand, Müller, and Reder 2009):\n\nlm_types |&gt; localmoran.exact(nb = nb_q, style = \"W\", \n    alternative = \"two.sided\", useTP=TRUE, truncErr=1e-8) |&gt; \n    as.data.frame() -&gt; locm_ex_types\n\n\nlocm_ex_types |&gt; hotspot(Prname = \"Pr. (exact)\",\n                         cutoff = 0.005) -&gt; pol_pres15$locm_ex\n\n\nCodeif (tmap4) {\n    pal &lt;- RColorBrewer::brewer.pal(4, \"Set3\")[c(4, 1)]\n    tm_shape(pol_pres15) +\n    tm_polygons(fill = c(\"locm_sad2\", \"locm_ex\"),\n    fill.legend = tm_legend(\"Turnout hotspot status \\nLocal Moran's I\",\n            frame = FALSE, item.r = 0),\n    fill.scale = tm_scale(values = pal, value.na = \"grey95\",\n            label.na = \"Not \\\"interesting\\\"\"),\n        lwd = 0.01, fill.free = FALSE) +\n    tm_facets_wrap(nrow = 1) +\n    tm_layout(panel.labels = c(\"saddlepoint weighted types\",\n    \"Exact weighted types\"))\n\n} else {\ntm_shape(pol_pres15) +\n    tm_fill(c(\"locm_sad2\", \"locm_ex\"), colorNA = \"grey95\",\n        textNA = \"Not \\\"interesting\\\"\", \n        title = \"Turnout hotspot status \\nLocal Moran's I\",\n        palette = RColorBrewer::brewer.pal(4, \"Set3\")[c(1, 4, 2)]) +\n    tm_facets(free.scales = FALSE, ncol = 2) +\n    tm_layout(panel.labels = c(\"saddlepoint weighted types\",\n                               \"Exact weighted types\"))\n}\n\n\n\n\n\n\nFigure 15.6: Local Moran’s I FDR hotspot cluster core maps, two-sided, interesting cutoff \\(\\alpha = 0.005\\): left panel: weighted types model saddlepoint p-values; right panel: weighted types model exact p-values, for first-round turnout, row-standardised neighbours\n\n\n\n\nAs Figure 15.6 shows, the exact and saddlepoint approximation methods yield almost identical cluster classifications from the same regression residuals, multiple comparison adjustment method, and cutoff level, with the exact method returning four more interesting observations:\n\ntable(Saddlepoint = addNA(pol_pres15$locm_sad2),\n      exact = addNA(pol_pres15$locm_ex))\n#            exact\n# Saddlepoint Low-Low High-High &lt;NA&gt;\n#   Low-Low        13         0    0\n#   High-High       0        81    0\n#   &lt;NA&gt;            2         2 2397\n\nLocal Getis-Ord \\(G_i\\)\n\n \nThe local Getis-Ord \\(G_i\\) measure (Getis and Ord 1992, 1996) is reported as a standard deviate, and, may also take the \\(G^*_i\\) form where self-neighbours are inserted into the neighbour object using include.self. The observed and expected values of local \\(G\\) with their analytical variances may also be returned if return_internals=TRUE.\n\nI_turnout |&gt; \n        localG(lw_q_W, return_internals = TRUE) -&gt; locG\n\nPermutation inference is also available for this measure:\n\nI_turnout |&gt; \n        localG_perm(lw_q_W, nsim = 9999, iseed = 1) -&gt; locG_p\n\nThe correlation between the two-sided probability values for analytical and permutation-based standard deviates (first two columns and rows) and permutation rank-based probability values are very strong:\n\ncor(cbind(localG=attr(locG, \"internals\")[, \"Pr(z != E(Gi))\"], \n    attr(locG_p, \"internals\")[, c(\"Pr(z != E(Gi))\", \n                                  \"Pr(z != E(Gi)) Sim\")]))\n#                    localG Pr(z != E(Gi)) Pr(z != E(Gi)) Sim\n# localG                  1              1                  1\n# Pr(z != E(Gi))          1              1                  1\n# Pr(z != E(Gi)) Sim      1              1                  1\n\nLocal Geary’s \\(C_i\\)\n\n \nAnselin (2019) extends Anselin (1995) and has been recently added to spdep thanks to contributions by Josiah Parry (pull request https://github.com/r-spatial/spdep/pull/66). The conditional permutation framework used for \\(I_i\\) and \\(G_i\\) is also used for \\(C_i\\):\n\nI_turnout |&gt; \n        localC_perm(lw_q_W, nsim=9999, iseed=1) -&gt; locC_p\n\nThe permutation standard deviate-based and rank-based probability values are not as highly correlated as for \\(G_i\\), in part reflecting the difference in view of autocorrelation in \\(C_i\\) as represented by a function of the differences between values rather than the products of values:\n\ncor(attr(locC_p, \"pseudo-p\")[, c(\"Pr(z != E(Ci))\",\n                                 \"Pr(z != E(Ci)) Sim\")])\n#                    Pr(z != E(Ci)) Pr(z != E(Ci)) Sim\n# Pr(z != E(Ci))              1.000              0.966\n# Pr(z != E(Ci)) Sim          0.966              1.000\n\n\nlocC_p |&gt; hotspot(Prname = \"Pr(z != E(Ci)) Sim\",\n                  cutoff = 0.005) -&gt; pol_pres15$hs_C\nlocG_p |&gt; hotspot(Prname = \"Pr(z != E(Gi)) Sim\",\n                  cutoff = 0.005) -&gt; pol_pres15$hs_G\n\n\nCodeif (tmap4) {\n    pal &lt;- RColorBrewer::brewer.pal(4, \"Set3\")[-c(2,3)]\n    m1 &lt;- tm_shape(pol_pres15) +\n        tm_polygons(fill = \"hs_cp_q\",\n        fill.legend = tm_legend(\"Turnout hotspot status\\nLocal Moran I\",\n                position = tm_pos_out(\"center\",\"bottom\"),\n                frame = FALSE, item.r = 0),\n        fill.scale = tm_scale(values = pal, value.na = \"grey95\",\n                label.na = \"Not \\\"interesting\\\"\"),\n            lwd = 0.01) +\n        tm_layout(meta.margins = c(.2, 0, 0, 0))\n    m2 &lt;- tm_shape(pol_pres15) +\n        tm_polygons(fill = \"hs_G\",\n        fill.legend = tm_legend(\"Turnout hotspot status\\nLocal Getis/Ord G\",\n                position = tm_pos_out(\"center\",\"bottom\"),\n                frame = FALSE, item.r = 0),\n        fill.scale = tm_scale(values = pal, value.na = \"grey95\",\n                label.na = \"Not \\\"interesting\\\"\"),\n            lwd = 0.01) +\n        tm_layout(meta.margins = c(.2, 0, 0, 0))\n    m3 &lt;- tm_shape(pol_pres15) +\n        tm_polygons(fill = \"hs_C\",\n        fill.legend = tm_legend(\"Turnout hotspot status\\nLocal Geary C\",\n                position = tm_pos_out(\"center\",\"bottom\"),\n                frame = FALSE, item.r = 0),\n        fill.scale = tm_scale(values = rev(pal), value.na = \"grey95\",\n                label.na = \"Not \\\"interesting\\\"\"),\n            lwd = 0.01) +\n        tm_layout(meta.margins = c(.2, 0, 0, 0))\n} else {\nm1 &lt;- tm_shape(pol_pres15) +\n    tm_fill(\"hs_cp_q\", \n            palette = RColorBrewer::brewer.pal(4, \"Set3\")[-c(2,3)],\n            colorNA = \"grey95\", textNA = \"Not \\\"interesting\\\"\",\n            title = \"Turnout hotspot status\\nLocal Moran I\") + \n    tm_layout(legend.outside=TRUE, legend.outside.position=\"bottom\")\nm2 &lt;- tm_shape(pol_pres15) +\n    tm_fill(\"hs_G\",\n            palette = RColorBrewer::brewer.pal(4, \"Set3\")[-c(2,3)],\n            colorNA = \"grey95\", textNA=\"Not \\\"interesting\\\"\",\n            title = \"Turnout hotspot status\\nLocal Getis/Ord G\") +\n    tm_layout(legend.outside=TRUE, legend.outside.position=\"bottom\")\nm3 &lt;- tm_shape(pol_pres15) +\n    tm_fill(\"hs_C\",\n            palette = RColorBrewer::brewer.pal(4, \"Set3\")[c(4, 1, 3)],\n            colorNA = \"grey95\", textNA=\"Not \\\"interesting\\\"\",\n            title = \"Turnout hotspot status\\nLocal Geary C\") +\n    tm_layout(legend.outside=TRUE, legend.outside.position=\"bottom\")\n}\ntmap_arrange(m1, m2, m3, nrow=1)\n\n\n\n\n\n\nFigure 15.7: FDR hotspot cluster core maps, two-sided, interesting cutoff \\(\\alpha = 0.005\\): left panel: local Moran's \\(I_i\\); centre panel: local Getis-Ord \\(G_i\\); right panel: local Geary's \\(C_i\\); first-round turnout, row-standardised neighbours\n\n\n\n\nFigure 15.7 shows that the cluster cores identified as interesting using \\(I_i\\), \\(G_i\\) and \\(C_i\\) for the same variable, first-round turnout, and the same spatial weights, for rank-based permutation FDR adjusted probability values and an \\(\\alpha = 0.005\\) cutoff, are very similar. In most cases, the \"High-High\" cluster cores are urban areas, and \"Low-Low\" cores are sparsely populated rural areas in the North, in addition to the German national minority areas close to the southern border. The three measures use slightly different strategies for naming cluster cores: \\(I_i\\) uses quadrants of the Moran scatterplot, \\(G_i\\) splits into \"Low\" and \"High\" on the mean of the input variable (which is the same as the first component in the \\(I_i\\) tuple), and univariate \\(C_i\\) on the mean of the input variable and zero for its lag. As before, cluster categories that do not occur are dropped.\nFor comparison, and before moving to multivariate \\(C_i\\), let us take the univariate \\(C_i\\) for the second (final) round turnout. One would expect that the run-off between the two top candidates from the first-round might mobilise some voters who did not have a clear first-round preference, but that it discourages some of those with strong loyalty to a candidate eliminated after the first round:\n\npol_pres15 |&gt; \n        st_drop_geometry() |&gt; \n        subset(select = II_turnout) |&gt; \n        localC_perm(lw_q_W, nsim=9999, iseed=1) -&gt; locC_p_II\n\n\nlocC_p_II |&gt; hotspot(Prname = \"Pr(z != E(Ci)) Sim\",\n                     cutoff = 0.005) -&gt; pol_pres15$hs_C_II\n\nMultivariate \\(C_i\\) (Anselin 2019) is taken as the sum of univariate \\(C_i\\) divided by the number of variables, but permutation is fixed so that the correlation between the variables is unchanged:\n\npol_pres15 |&gt; \n        st_drop_geometry() |&gt; \n        subset(select = c(I_turnout, II_turnout)) |&gt;\n        localC_perm(lw_q_W, nsim=9999, iseed=1) -&gt; locMvC_p\n\nLet us check that the multivariate \\(C_i\\) is equal to the mean of the univariate \\(C_i\\):\n\nall.equal(locMvC_p, (locC_p+locC_p_II)/2,\n          check.attributes = FALSE)\n# [1] TRUE\n\n\nlocMvC_p |&gt; hotspot(Prname = \"Pr(z != E(Ci)) Sim\",\n                    cutoff = 0.005) -&gt; pol_pres15$hs_MvC\n\n\nCodeif (tmap4) {\n    pal &lt;- RColorBrewer::brewer.pal(4, \"Set3\")[-c(2,3)]\n    m3 &lt;- tm_shape(pol_pres15) +\n        tm_polygons(fill = \"hs_C\",\n        fill.legend = tm_legend(\"First round turnout\\nLocal Geary C\",\n                position = tm_pos_out(\"center\",\"bottom\"),\n                frame = FALSE, item.r = 0),\n        fill.scale = tm_scale(values = rev(pal), value.na = \"grey95\",\n                label.na = \"Not \\\"interesting\\\"\"),\n            lwd = 0.01) +\n        tm_layout(meta.margins = c(.2, 0, 0, 0))\n    pal &lt;- RColorBrewer::brewer.pal(4, \"Set3\")[c(4, 1, 3, 2)]\n    m4 &lt;- tm_shape(pol_pres15) +\n        tm_polygons(fill = \"hs_C_II\",\n        fill.legend = tm_legend(\"Second round turnout\\nLocal Geary C\",\n                position = tm_pos_out(\"center\",\"bottom\"),\n                frame = FALSE, item.r = 0),\n        fill.scale = tm_scale(values = pal, value.na = \"grey95\",\n                label.na = \"Not \\\"interesting\\\"\"),\n            lwd = 0.01) +\n        tm_layout(meta.margins = c(.2, 0, 0, 0))\n    pal &lt;- RColorBrewer::brewer.pal(4, \"Set3\")[c(4, 1)]\n    m5 &lt;- tm_shape(pol_pres15) +\n        tm_polygons(fill = \"hs_MvC\",\n        fill.legend = tm_legend(\"Both rounds turnout\\nLocal Multivariate Geary C\",\n                position = tm_pos_out(\"center\",\"bottom\"),\n                frame = FALSE, item.r = 0),\n        fill.scale = tm_scale_categorical(values = pal, value.na = \"grey95\",\n                label.na = \"Not \\\"interesting\\\"\"),\n            lwd = 0.01) +\n        tm_layout(meta.margins = c(.2, 0, 0, 0))\n} else {\nm3 &lt;- tm_shape(pol_pres15) +\n  tm_fill(\"hs_C\", \n    palette = RColorBrewer::brewer.pal(4, \"Set3\")[c(4, 1, 3, 2)],\n    colorNA = \"grey95\", textNA = \"Not \\\"interesting\\\"\",\n    title = \"First round turnout\\nLocal Geary C\") +\n    tm_layout(legend.outside=TRUE, legend.outside.position=\"bottom\")\nm4 &lt;- tm_shape(pol_pres15) +\n  tm_fill(\"hs_C_II\", \n    palette = RColorBrewer::brewer.pal(4, \"Set3\")[c(4, 1, 3, 2)], \n    colorNA = \"grey95\", textNA = \"Not \\\"interesting\\\"\",\n    title=\"Second round turnout\\nLocal Geary C\") +\n    tm_layout(legend.outside=TRUE, legend.outside.position=\"bottom\")\nm5 &lt;- tm_shape(pol_pres15) +\n  tm_fill(\"hs_MvC\", \n    palette = RColorBrewer::brewer.pal(4, \"Set3\")[c(4, 1)],\n    colorNA = \"grey95\", textNA = \"Not \\\"interesting\\\"\",\n    title = \"Both rounds turnout\\nLocal Multivariate Geary C\") +\n    tm_layout(legend.outside=TRUE, legend.outside.position=\"bottom\")\n}\ntmap_arrange(m3, m4, m5, nrow=1)\n\n\n\n\n\n\nFigure 15.8: FDR hotspot cluster core maps, two-sided, interesting cutoff \\(\\alpha = 0.005\\): left panel: local \\(C_i\\), first-round turnout; centre panel: local \\(C_i\\), second-round turnout; right panel: local multivariate \\(C_i\\), both turnout rounds; row-standardised neighbours\n\n\n\n\nFigure 15.8 indicates that the multivariate measure picks up aggregated elements of observations found interesting in the two univariate measures. We can break this down by interacting the first- and second-round univariate measures, and tabulating against the multivariate measure.\n\ntable(droplevels(interaction(addNA(pol_pres15$hs_C),\n                             addNA(pol_pres15$hs_C_II), sep=\":\")), \n      addNA(pol_pres15$hs_MvC))\n#                      \n#                       Positive &lt;NA&gt;\n#   High-High:High-High       82    0\n#   NA:High-High              37   21\n#   Low-Low:Low-Low           24    0\n#   NA:Low-Low                41   14\n#   NA:Other Positive          1    0\n#   NA:Negative                0    1\n#   High-High:NA              17    3\n#   Low-Low:NA                 6    2\n#   NA:NA                     43 2203\n\nFor these permutation outcomes, 47 observations in the multivariate case are found interesting where neither of the univariate \\(C_i\\) were found interesting (FDR, cutoff \\(0.005\\)). Almost all of the observations found interesting in both first and second round, are also interesting in the multivariate case, but outcomes are more mixed when observations were only found interesting in one of the two rounds.\nThe rgeoda package\n\nGeoda has been wrapped for R as rgeoda (Li and Anselin 2022), and provides very similar functionalities for the exploration of spatial autocorrelation in areal data as matching parts of spdep. The active objects are kept as pointers to a compiled code workspace; using compiled code for all operations (as in Geoda itself) makes rgeoda perform fast, but makes it less flexible when modifications or enhancements are desired.\n\n\nlibrary(rgeoda)\nGeoda_w &lt;- queen_weights(pol_pres15)\nsummary(Geoda_w)\n#                      name               value\n# 1 number of observations:                2495\n# 2          is symmetric:                 TRUE\n# 3               sparsity: 0.00228786229774178\n# 4        # min neighbors:                   1\n# 5        # max neighbors:                  13\n# 6       # mean neighbors:    5.70821643286573\n# 7     # median neighbors:                   6\n# 8           has isolates:               FALSE\n\nFor comparison, let us take the multivariate \\(C_i\\) measure of turnout in the two rounds of the 2015 Polish presidential election as above:\n\n\nlisa &lt;- local_multigeary(Geoda_w, \n    pol_pres15[c(\"I_turnout\", \"II_turnout\")], \n    cpu_threads = max(detectCores() - 1, 1),\n    permutations = 99999, seed = 1)\n\nThe contiguity neighbours are the same as those found by poly2nb:\n\n\nall.equal(card(nb_q), lisa_num_nbrs(lisa), \n          check.attributes = FALSE)\n# [1] TRUE\n\nas are the multivariate \\(C_i\\) values the same as those found above:\n\n\nall.equal(lisa_values(lisa), c(locMvC_p),\n          check.attributes = FALSE)\n# [1] TRUE\n\nOne difference is that the range of the folded two-sided rank-based permutation probability values used by rgeoda is \\([0, 0.5]\\), also reported in spdep:\n\napply(attr(locMvC_p, \"pseudo-p\")[,c(\"Pr(z != E(Ci)) Sim\", \n                                \"Pr(folded) Sim\")], 2, range)\n#      Pr(z != E(Ci)) Sim Pr(folded) Sim\n# [1,]             0.0002         0.0001\n# [2,]             0.9974         0.4987\n\nThis means that the cutoff corresponding to \\(0.005\\) over \\([0, 1]\\) is \\(0.0025\\) over \\([0, 0.5]\\):\n\nlocMvC_p |&gt; hotspot(Prname = \"Pr(folded) Sim\",\n                    cutoff = 0.0025) -&gt; pol_pres15$hs_MvCa\n\nSo although local_multigeary used the default cutoff of \\(0.05\\) in setting cluster core classes, we can sharpen the cutoff and apply the FDR adjustment on output components of the lisa object in the compiled code workspace:\n\nmvc &lt;- factor(lisa_clusters(lisa), levels=0:2,\n              labels = lisa_labels(lisa)[1:3])\nis.na(mvc) &lt;- p.adjust(lisa_pvalues(lisa), \"fdr\") &gt;= 0.0025\npol_pres15$geoda_mvc &lt;- droplevels(mvc)\n\nAbout 80 more observations are found interesting in the rgeoda permutation, and further analysis of implementation details is still in progress:\n\naddmargins(table(spdep = addNA(pol_pres15$hs_MvCa),\n                 rgeoda = addNA(pol_pres15$geoda_mvc)))\n#           rgeoda\n# spdep      Positive &lt;NA&gt;  Sum\n#   Positive      245    6  251\n#   &lt;NA&gt;           77 2167 2244\n#   Sum           322 2173 2495\n\n\nCodeif (tmap4) {\n    pal &lt;- RColorBrewer::brewer.pal(4, \"Set3\")[c(4, 1)]\n    m5 &lt;- tm_shape(pol_pres15) +\n        tm_polygons(fill = \"hs_MvC\",\n    fill.legend = tm_legend(\"Both rounds turnout spdep\\nLocal Multivariate Geary C\",\n            position = tm_pos_in(\"left\",\"bottom\"),\n            frame = FALSE, item.r = 0),\n    fill.scale = tm_scale_categorical(values = pal, value.na = \"grey95\",\n            label.na = \"Not \\\"interesting\\\"\"),\n        lwd = 0.01)\n    m6 &lt;- tm_shape(pol_pres15) +\n        tm_polygons(fill = \"geoda_mvc\",\n    fill.legend = tm_legend(\"Both rounds turnout rgeoda\\nLocal Multivariate Geary C\",\n            position = tm_pos_in(\"left\",\"bottom\"),\n            frame = FALSE, item.r = 0),\n    fill.scale = tm_scale_categorical(values = pal, value.na = \"grey95\",\n            label.na = \"Not \\\"interesting\\\"\"),\n        lwd = 0.01)\n} else {\nm5 &lt;- tm_shape(pol_pres15) +\n    tm_fill(\"hs_MvCa\", \n        palette = RColorBrewer::brewer.pal(4, \"Set3\")[c(4, 1)],\n        colorNA = \"grey95\", textNA = \"Not \\\"interesting\\\"\",\n  title = \"Both rounds turnout spdep\\nLocal Multivariate Geary C\")\nm6 &lt;- tm_shape(pol_pres15) +\n    tm_fill(\"geoda_mvc\", \n        palette = RColorBrewer::brewer.pal(4, \"Set3\")[c(4, 1)],\n        colorNA = \"grey95\", textNA = \"Not \\\"interesting\\\"\",\n  title=\"Both rounds turnout rgeoda\\nLocal Multivariate Geary C\")\n}\ntmap_arrange(m5, m6, nrow=1)\n\n\n\n\n\n\nFigure 15.9: FDR local multivariate \\(C_i\\) hotspot cluster core maps, two-sided, interesting cutoff \\(\\alpha = 0.0025\\) over \\([0, 0.5]\\): left panel: spdep, both turnout rounds; right panel: rgeoda, both turnout rounds; row-standardised neighbours\n\n\n\n\nFigure 15.9 shows that while almost all of the 242 observations found interesting in the spdep implementation were also interesting for rgeoda, the latter found a further 86 interesting. Of course, permutation outcomes are bound to vary, but it remains to establish whether either or both implementations require revision.",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Measures of Spatial Autocorrelation</span>"
    ]
  },
  {
    "objectID": "15-Measures.html#exercises",
    "href": "15-Measures.html#exercises",
    "title": "15  Measures of Spatial Autocorrelation",
    "section": "\n15.4 Exercises",
    "text": "15.4 Exercises\n\nWhy are join-count measures on a chessboard so different between rook and queen neighbours?\nPlease repeat the simulation shown in Section 15.1 using the chessboard polygons and the row-standardised queen contiguity neighbours. Why is it important to understand that spatial autocorrelation usually signals (unavoidable) misspecification in our data?\nWhy is false discovery rate adjustment recommended for local measures of spatial autocorrelation?\nCompare the local Moran’s \\(I_i\\) standard deviate values for the simulated data from exercise 2 (above) for the analytical conditional approach, and saddlepoint approximation. Consider the advantages and disadvantages of the saddlepoint approximation approach.\n\n\n\n\n\n\n\nAnselin, Luc. 1995. “Local indicators of spatial association - LISA.” Geographical Analysis 27 (2): 93–115.\n\n\n———. 1996. “The Moran Scatterplot as an ESDA Tool to Assess Local Instability in Spatial Association.” In Spatial Analytical Perspectives on GIS, edited by M. M. Fischer, H. J. Scholten, and D. Unwin, 111–25. London: Taylor & Francis.\n\n\n———. 2019. “A Local Indicator of Multivariate Spatial Association: Extending Geary’s c.” Geographical Analysis 51 (2): 133–50. https://doi.org/10.1111/gean.12164.\n\n\nAnselin, Luc, Xun Li, and Julia Koschinsky. 2021. “GeoDa, from the Desktop to an Ecosystem for Exploring Spatial Data.” Geographical Analysis. https://doi.org/10.1111/gean.12311.\n\n\nAssunção, R. M., and E. A. Reis. 1999. “A New Proposal to Adjust Moran’s \\(I\\) for Population Density.” Statistics in Medicine 18: 2147–62.\n\n\nBenjamin, Daniel J., James O. Berger, Johannesson Magnus, Brian A. Nosek, Wagenmakers E-J, Richard Berk, Kenneth A. Bollen, et al. 2018. “Redefine Statistical Significance.” Nature Human Behaviour 2 (1): 6–10.\n\n\nBenjamini, Yoav, and Yosef Hochberg. 1995. “Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing.” Journal of the Royal Statistical Society. Series B (Methodological) 57 (1): 289–300. https://doi.org/10.1111/j.2517-6161.1995.tb02031.x.\n\n\nBenjamini, Yoav, and Daniel Yekutieli. 2001. “The control of the false discovery rate in multiple testing under dependency.” The Annals of Statistics 29 (4): 1165–88. https://doi.org/10.1214/aos/1013699998.\n\n\nBivand, Roger. 2022a. “R Packages for Analyzing Spatial Data: A Comparative Case Study with Areal Data.” Geographical Analysis 54 (3): 488–518. https://doi.org/10.1111/gean.12319.\n\n\n———. 2022b. Spdep: Spatial Dependence: Weighting Schemes, Statistics.\n\n\nBivand, Roger, W. Müller, and M. Reder. 2009. “Power Calculations for Global and Local Moran’s \\(I\\).” Computational Statistics and Data Analysis 53: 2859–72.\n\n\nBivand, Roger, and David W. S. Wong. 2018. “Comparing Implementations of Global and Local Indicators of Spatial Association.” TEST 27 (3): 716–48. https://doi.org/10.1007/s11749-018-0599-x.\n\n\nBrody, Howard, Michael Russell Rip, Peter Vinten-Johansen, Nigel Paneth, and Stephen Rachman. 2000. “Map-Making and Myth-Making in Broad Street: The London Cholera Epidemic, 1854.” The Lancet 356 (9223): 64–68. https://doi.org/10.1016/S0140-6736(00)02442-9.\n\n\nCaldas de Castro, Marcia, and Burton H. Singer. 2006. “Controlling the False Discovery Rate: A New Application to Account for Multiple and Dependent Tests in Local Statistics of Spatial Association.” Geographical Analysis 38 (2): 180–208. https://doi.org/10.1111/j.0016-7363.2006.00682.x.\n\n\nCliff, A. D., and J. K. Ord. 1973. Spatial Autocorrelation. London: Pion.\n\n\n———. 1981. Spatial Processes. London: Pion.\n\n\nDuncan, O. D., R. P. Cuzzort, and B. Duncan. 1961. Statistical Geography: Problems in Analyzing Areal Data. Glencoe, IL: Free Press.\n\n\nGeary, R. C. 1954. “The Contiguity Ratio and Statistical Mapping.” The Incorporated Statistician 5: 115–45.\n\n\nGetis, A., and J. K. Ord. 1992. “The Analysis of Spatial Association by the Use of Distance Statistics.” Geographical Analysis 24 (2): 189–206.\n\n\n———. 1996. “Local Spatial Statistics: An Overview.” In Spatial Analysis: Modelling in a GIS Environment, edited by P. Longley and M Batty, 261–77. Cambridge: GeoInformation International.\n\n\nLi, Xun, and Luc Anselin. 2021. Rgeoda: R Library for Spatial Data Analysis. https://CRAN.R-project.org/package=rgeoda.\n\n\n———. 2022. Rgeoda: R Library for Spatial Data Analysis. https://CRAN.R-project.org/package=rgeoda.\n\n\nMcMillen, Daniel P. 2003. “Spatial Autocorrelation or Model Misspecification?” International Regional Science Review 26: 208–17.\n\n\nMoran, P. A. P. 1948. “The Interpretation of Statistical Maps.” Journal of the Royal Statistical Society, Series B (Methodological) 10 (2): 243–51.\n\n\nOlsson, Gunnar. 1970. “Explanation, Prediction, and Meaning Variance: An Assessment of Distance Interaction Models.” Economic Geography 46: 223–33. https://doi.org/10.2307/143140.\n\n\nOrd, J. K., and A. Getis. 2001. “Testing for Local Spatial Autocorrelation in the Presence of Global Autocorrelation.” Journal of Regional Science 41 (3): 411–32.\n\n\nSauer, Jeffery, Taylor Oshan, Sergio Rey, and Levi John Wolf. 2021. “The Importance of Null Hypotheses: Understanding Differences in Local Moran’s \\(I_i\\) Under Heteroskedasticity.” Geographical Analysis. https://doi.org/10.1111/gean.12304.\n\n\nSchabenberger, O., and C. A. Gotway. 2005. Statistical Methods for Spatial Data Analysis. Boca Raton/London: Chapman & Hall/CRC.\n\n\nSokal, R. R, N. L. Oden, and B. A. Thomson. 1998. “Local Spatial Autocorrelation in a Biological Model.” Geographical Analysis 30: 331–54.\n\n\nTiefelsdorf, M. 2002. “The Saddlepoint Approximation of Moran’s I and Local Moran’s \\({I}_i\\) Reference Distributions and Their Numerical Evaluation.” Geographical Analysis 34: 187–206.\n\n\nTobler, W. R. 1970. “A Computer Movie Simulating Urban Growth in the Detroit Region.” Economic Geography 46: 234–40. https://doi.org/10.2307/143141.\n\n\nUpton, G., and B. Fingleton. 1985. Spatial Data Analysis by Example: Point Pattern and Qualitative Data. New York: Wiley.",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Measures of Spatial Autocorrelation</span>"
    ]
  },
  {
    "objectID": "16-SpatialRegression.html",
    "href": "16-SpatialRegression.html",
    "title": "16  Spatial Regression",
    "section": "",
    "text": "16.1 Markov random field and multilevel models\nEven though it may be tempting to focus on interpreting the map pattern of an areal support response variable of interest, the pattern may largely derive from covariates (and their functional forms), as well as the respective spatial footprints of the variables in play. Spatial autoregressive models in two dimensions began without covariates and with clear links to time series (Whittle 1954). Extensions included tests for spatial autocorrelation in linear model residuals, and models applying the autoregressive component to the response or the residuals, where the latter matched the tests for residuals (Cliff and Ord 1972, 1973). These “lattice” models of areal data typically express the dependence between observations using a graph of neighbours in the form of a contiguity matrix.\nOf course, handling a spatial correlation structure in a generalised least squares model or a (generalised) linear or non-linear mixed effects model such as those provided in the nlme and many other packages does not have to use a graph of neighbours (Pinheiro and Bates 2000). These models are also spatial regression models, using functions of the distance between observations, and fitted variograms to model the spatial autocorrelation present; such models have been held to yield a clearer picture of the underlying processes (Wall 2004), building on geostatistics. For example, the glmmTMB package successfully uses this approach to spatial regression (Brooks et al. 2017). Here we will only consider spatial regression using spatial weights matrices.\nThere is a large literature in disease mapping using conditional autoregressive (CAR) and intrinsic CAR (ICAR) models in spatially structured random effects. These extend to multilevel models, in which the spatially structured random effects may apply at different levels of the model (Bivand et al. 2017). In order to try out some of the variants, we need to remove the no-neighbour observations from the tract level, and from the model output zone aggregated level, in two steps as reducing the tract level induces a no-neighbour outcome at the model output zone level. Many of the model estimating functions take family arguments, and fit generalised linear mixed effects models with per-observation spatial random effects structured using a Markov random field representation of relationships between neighbours. In the multilevel case, the random effects may be modelled at the group level, which is the case presented in the following examples.\nWe follow Gómez-Rubio (2019) in summarising Pinheiro and Bates (2000) and McCulloch and Searle (2001) to describe the mixed-effects model representation of spatial regression models. In a Gaussian linear mixed model setting, a random effect \\(u\\) is added to the model, with response \\(Y\\), fixed covariates \\(X\\), their coefficients \\(\\beta\\) and error term \\(\\varepsilon_i \\sim N(0, \\sigma^2), i=1,\\dots, n\\):\n\\[\nY = X \\beta + Z u + \\varepsilon\n\\] \\(Z\\) is a fixed design matrix for the random effects. If there are \\(n\\) random effects, it will be an \\(n \\times n\\) identity matrix if instead the observations are aggregated into \\(m\\) groups, so with \\(m &lt; n\\) random effects, it will be an \\(n \\times m\\) matrix showing which group each observation belongs to. The random effects are modelled as a multivariate Normal distribution \\(u \\sim N(0, \\sigma^2_u \\Sigma)\\), and \\(\\sigma^2_u \\Sigma\\) is the square variance-covariance matrix of the random effects.\nA division has grown up, possibly unhelpfully, between scientific fields using CAR models (Besag 1974), and simultaneous autoregressive models (SAR) (Ord 1975; Hepple 1976). Although CAR and SAR models are closely related, these fields have found it difficult to share experience of applying similar models, often despite referring to key work summarising the models (Ripley 1981, 1988; Cressie 1993). Ripley gives the SAR variance as (Ripley 1981, 89), here shown as the inverse \\(\\Sigma^{-1}\\) (also known as the precision matrix):\n\\[\n\\Sigma^{-1} = [(I - \\rho W)'(I - \\rho W)]\n\\]\nwhere \\(\\rho\\) is a spatial autocorrelation parameter and \\(W\\) is a non-singular spatial weights matrix that represents spatial dependence. The CAR variance is:\n\\[\n\\Sigma^{-1} = (I - \\rho W)\n\\] where \\(W\\) is a symmetric and strictly positive definite spatial weights matrix. In the case of the intrinsic CAR model, avoiding the estimation of a spatial autocorrelation parameter, we have:\n\\[\n\\Sigma^{-1} = M = \\mathrm{diag}(n_i) - W\n\\] where \\(W\\) is a symmetric and strictly positive definite spatial weights matrix as before and \\(n_i\\) are the row sums of \\(W\\). The Besag-York-Mollié model includes intrinsic CAR spatially structured random effects and unstructured random effects. The Leroux model combines matrix components for unstructured and spatially structured random effects, where the spatially structured random effects are taken as following an intrinsic CAR specification:\n\\[\n\\Sigma^{-1} = [(1 - \\rho) I_n + \\rho M]\n\\] References to the definitions of these models may be found in Gómez-Rubio (2020), and estimation issues affecting the Besag-York-Mollié and Leroux models are reviewed by Gerber and Furrer (2015).\nMore recent books expounding the theoretical bases for modelling with areal data simply point out the similarities between SAR and CAR models in relevant chapters (Gaetan and Guyon 2010; Van Lieshout 2019); the interested reader is invited to consult these sources for background information.",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Spatial Regression</span>"
    ]
  },
  {
    "objectID": "16-SpatialRegression.html#markov-random-field-and-multilevel-models",
    "href": "16-SpatialRegression.html#markov-random-field-and-multilevel-models",
    "title": "16  Spatial Regression",
    "section": "",
    "text": "Boston house value dataset\nHere we shall use the Boston housing dataset, which has been restructured and furnished with census tract boundaries (Bivand 2017). The original dataset used 506 census tracts and a hedonic model to try to estimate willingness to pay for clean air. The response was constructed from counts of ordinal answers to a 1970 census question about house value. The response is left- and right-censored in the census source and has been treated as Gaussian. The key covariate was created from a calibrated meteorological model showing the annual nitrogen oxides (NOX) level for a smaller number of model output zones. The numbers of houses responding also varies by tract and model output zone. There are several other covariates, some measured at the tract level, some by town only, where towns broadly correspond to the air pollution model output zones.\nWe can start by reading in the 506 tract dataset from spData (Bivand, Nowosad, and Lovelace 2022), and creating a contiguity neighbour object and from that again a row standardised spatial weights object.\n\nlibrary(sf)\nlibrary(spData)\nboston_506 &lt;- st_read(system.file(\"shapes/boston_tracts.gpkg\",\n                      package = \"spData\")[1], quiet = TRUE)\n\n\nnb_q &lt;- spdep::poly2nb(boston_506)\nlw_q &lt;- spdep::nb2listw(nb_q, style = \"W\")\n\nIf we examine the median house values, we find that those for censored values have been assigned as missing, and that 17 tracts are affected.\n\ntable(boston_506$censored)\n# \n#  left    no right \n#     2   489    15\n\n\nsummary(boston_506$median)\n#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#    5600   16800   21000   21749   24700   50000      17\n\nNext, we can subset to the remaining 489 tracts with non-censored house values, and the neighbour object to match. The neighbour object now has one observation with no neighbours.\n\nboston_506$CHAS &lt;- as.factor(boston_506$CHAS)\nboston_489 &lt;- boston_506[!is.na(boston_506$median),]\nnb_q_489 &lt;- spdep::poly2nb(boston_489)\n# Warning in spdep::poly2nb(boston_489): some observations have no neighbours;\n# if this seems unexpected, try increasing the snap argument.\n# Warning in spdep::poly2nb(boston_489): neighbour object has 3 sub-graphs;\n# if this sub-graph count seems unexpected, try increasing the snap argument.\nlw_q_489 &lt;- spdep::nb2listw(nb_q_489, style = \"W\",\n                            zero.policy = TRUE)\n\nThe NOX_ID variable specifies the upper-level aggregation, letting us aggregate the tracts to air pollution model output zones. We can create aggregate neighbour and row standardised spatial weights objects, and aggregate the NOX variable taking means, and the CHAS Charles River dummy variable for observations on the river. Here we follow the principles outlined in Section 5.3.1 for spatially extensive and intensive variables; neither NOX nor CHAS can be summed, as they are not count variables.\n\nagg_96 &lt;- list(as.character(boston_506$NOX_ID))\nboston_96 &lt;- aggregate(boston_506[, \"NOX_ID\"], by = agg_96,\n                       unique)\nnb_q_96 &lt;- spdep::poly2nb(boston_96)\nlw_q_96 &lt;- spdep::nb2listw(nb_q_96)\nboston_96$NOX &lt;- aggregate(boston_506$NOX, agg_96, mean)$x\nboston_96$CHAS &lt;-\n    aggregate(as.integer(boston_506$CHAS)-1, agg_96, max)$x\n\nThe response is aggregated using the weightedMedian function in matrixStats, and midpoint values for the house value classes. Counts of houses by value class were punched to check the published census values, which can be replicated using weightedMedian at the tract level. Here we find two output zones with calculated weighted medians over the upper census question limit of USD $50,000, and remove them subsequently as they also are affected by not knowing the appropriate value to insert for the top class by value. This is a case of spatially extensive aggregation, for which the summation of counts is appropriate:\n\nnms &lt;- names(boston_506)\nccounts &lt;- 23:31\nfor (nm in nms[c(22, ccounts, 36)]) {\n  boston_96[[nm]] &lt;- aggregate(boston_506[[nm]], agg_96, sum)$x\n}\nbr2 &lt;- \n  c(3.50, 6.25, 8.75, 12.5, 17.5, 22.5, 30, 42.5, 60) * 1000\ncounts &lt;- as.data.frame(boston_96)[, nms[ccounts]]\nf &lt;- function(x) matrixStats::weightedMedian(x = br2, w = x,\n                                     interpolate = TRUE)\nboston_96$median &lt;- apply(counts, 1, f)\nis.na(boston_96$median) &lt;- boston_96$median &gt; 50000\nsummary(boston_96$median)\n#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#    9009   20417   23523   25263   30073   49496       2\n\nBefore subsetting, we aggregate the remaining covariates by weighted mean using the tract population counts punched from the census (Bivand 2017); these are spatially intensive variables, not count data.\n\nPOP &lt;- boston_506$POP\nf &lt;- function(x) matrixStats::weightedMean(x[,1], x[,2])\nfor (nm in nms[c(9:11, 14:19, 21, 33)]) {\n  s0 &lt;- split(data.frame(boston_506[[nm]], POP), agg_96)\n  boston_96[[nm]] &lt;- sapply(s0, f)\n}\nboston_94 &lt;- boston_96[!is.na(boston_96$median),]\nnb_q_94 &lt;- spdep::subset.nb(nb_q_96, !is.na(boston_96$median))\nlw_q_94 &lt;- spdep::nb2listw(nb_q_94, style=\"W\")\n\nWe now have two datasets at each level, at the lower, census tract level, and at the upper, air pollution model output zone level, one including the censored observations, the other excluding them.\n\nboston_94a &lt;- aggregate(boston_489[,\"NOX_ID\"], \n                        list(boston_489$NOX_ID), unique)\nnb_q_94a &lt;- spdep::poly2nb(boston_94a)\n# Warning in spdep::poly2nb(boston_94a): some observations have no neighbours;\n# if this seems unexpected, try increasing the snap argument.\n# Warning in spdep::poly2nb(boston_94a): neighbour object has 2 sub-graphs;\n# if this sub-graph count seems unexpected, try increasing the snap argument.\nNOX_ID_no_neighs &lt;-\n        boston_94a$NOX_ID[which(spdep::card(nb_q_94a) == 0)]\nboston_487 &lt;- boston_489[is.na(match(boston_489$NOX_ID,\n                                     NOX_ID_no_neighs)),]\nboston_93 &lt;- aggregate(boston_487[, \"NOX_ID\"],\n                       list(ids = boston_487$NOX_ID), unique)\nrow.names(boston_93) &lt;- as.character(boston_93$NOX_ID)\nnb_q_93 &lt;- spdep::poly2nb(boston_93,\n        row.names = unique(as.character(boston_93$NOX_ID)))\n\n\nThe original model related the log of median house values by tract to the square of NOX values, including other covariates usually related to house value by tract, such as aggregate room counts, aggregate age, ethnicity, social status, distance to downtown and to the nearest radial road, a crime rate, and town-level variables reflecting land use (zoning, industry), taxation and education (Bivand 2017). This structure will be used here to exercise issues raised in fitting spatial regression models, including the presence of multiple levels.",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Spatial Regression</span>"
    ]
  },
  {
    "objectID": "16-SpatialRegression.html#sec-multilevel",
    "href": "16-SpatialRegression.html#sec-multilevel",
    "title": "16  Spatial Regression",
    "section": "\n16.2 Multilevel models of the Boston dataset",
    "text": "16.2 Multilevel models of the Boston dataset\n The ZN, INDUS, NOX, RAD, TAX, and PTRATIO variables show effectively no variability within the TASSIM zones, so in a multilevel model the random effect may absorb their influence.\n\nform &lt;- formula(log(median) ~ CRIM + ZN + INDUS + CHAS + \n                I((NOX*10)^2) + I(RM^2) + AGE + log(DIS) +\n                log(RAD) + TAX + PTRATIO + I(BB/100) + \n                log(I(LSTAT/100)))\n\nIID random effects with lme4\n The lme4 package (Bates et al. 2022) lets us add an independent and identically distributed (IID) unstructured random effect at the model output zone level by updating the model formula with a random effects term:\n\nlibrary(Matrix)\nlibrary(lme4)\nMLM &lt;- lmer(update(form, . ~ . + (1 | NOX_ID)), \n            data = boston_487, REML = FALSE)\n\nCopying the random effect into the \"sf\" object for mapping is performed below.\n\nboston_93$MLM_re &lt;- ranef(MLM)[[1]][,1]\n\nIID and CAR random effects with hglm\n The same model may be estimated using the hglm package (Alam, Ronnegard, and Shen 2019), which also permits the modelling of discrete responses, this time using an extra one-sided formula to express the random effects term:\n\nlibrary(hglm) |&gt; suppressPackageStartupMessages()\nsuppressWarnings(HGLM_iid &lt;- hglm(fixed = form,\n                                  random = ~1 | NOX_ID,\n                                  data = boston_487,\n                                  family = gaussian()))\nboston_93$HGLM_re &lt;- unname(HGLM_iid$ranef)\n\nThe same package has been extended to spatially structured SAR and CAR random effects, for which a sparse spatial weights matrix is required (Alam, Rönnegård, and Shen 2015); we choose binary spatial weights:\n\nlibrary(spatialreg)\nW &lt;- as(spdep::nb2listw(nb_q_93, style = \"B\"), \"CsparseMatrix\")\n\nWe fit a CAR model at the upper level, using the rand.family argument, where the values of the indexing variable NOX_ID match the row names of \\(W\\):\n\nsuppressWarnings(HGLM_car &lt;- hglm(fixed = form,\n                                  random = ~ 1 | NOX_ID,\n                                  data = boston_487,\n                                  family = gaussian(),\n                                  rand.family = CAR(D=W)))\nboston_93$HGLM_ss &lt;- HGLM_car$ranef[,1]\n\nIID and ICAR random effects with R2BayesX\n The R2BayesX package (Umlauf et al. 2022) provides flexible support for structured additive regression models, including spatial multilevel models. The models include an IID unstructured random effect at the upper level using the \"re\" specification in the sx model term (Umlauf et al. 2015); we choose the \"MCMC\" method:\n\nlibrary(R2BayesX) |&gt; suppressPackageStartupMessages()\n\n\nBX_iid &lt;- bayesx(update(form, . ~ . + sx(NOX_ID, bs = \"re\")),\n                 family = \"gaussian\", data = boston_487,\n                 method = \"MCMC\", iterations = 12000,\n                 burnin = 2000, step = 2, seed = 123)\n\n\nboston_93$BX_re &lt;- BX_iid$effects[\"sx(NOX_ID):re\"][[1]]$Mean\n\nand the \"mrf\" (Markov Random Field) spatially structured intrinsic CAR random effect specification based on a graph derived from converting a suitable \"nb\" object for the upper level. The \"region.id\" attribute of the \"nb\" object needs to contain values corresponding to the indexing variable in the sx effects term, to facilitate the internal construction of design matrix \\(Z\\):\n\nRBX_gra &lt;- nb2gra(nb_q_93)\nall.equal(row.names(RBX_gra), attr(nb_q_93, \"region.id\"))\n# [1] TRUE\n\nAs we saw above in the intrinsic CAR model definition, the counts of neighbours are entered on the diagonal, but the current implementation uses a dense, not sparse, matrix:\n\nall.equal(unname(diag(RBX_gra)), spdep::card(nb_q_93))\n# [1] TRUE\n\nThe sx model term continues to include the indexing variable, and now passes through the intrinsic CAR precision matrix:\n\nBX_mrf &lt;- bayesx(update(form, . ~ . + sx(NOX_ID, bs = \"mrf\",\n                                         map = RBX_gra)), \n                 family = \"gaussian\", data = boston_487,\n                 method = \"MCMC\", iterations = 12000,\n                 burnin = 2000, step = 2, seed = 123)\n\n\nboston_93$BX_ss &lt;- BX_mrf$effects[\"sx(NOX_ID):mrf\"][[1]]$Mean\n\nIID, ICAR and Leroux random effects with INLA\n Bivand, Gómez-Rubio, and Rue (2015) and Gómez-Rubio (2020) present the use of the INLA package (Rue, Lindgren, and Teixeira Krainski 2022) and the inla model fitting function with spatial regression models:\n\nlibrary(INLA) |&gt; suppressPackageStartupMessages()\n# Warning: package 'INLA' was built under R version 4.5.0\n\nAlthough differing in details, the approach by updating the fixed model formula with an unstructured random effects term is very similar to that seen above:\n\nINLA_iid &lt;- inla(update(form, . ~ . + f(NOX_ID, model = \"iid\")),\n                 family = \"gaussian\", data = boston_487)\n\n\nboston_93$INLA_re &lt;- INLA_iid$summary.random$NOX_ID$mean\n\nAs with most implementations, care is needed to match the indexing variable with the spatial weights; in this case using indices \\(1, \\dots, 93\\) rather than the NOX_ID variable directly:\n\nID2 &lt;- as.integer(as.factor(boston_487$NOX_ID))\n\nThe same sparse binary spatial weights matrix is used, and the intrinsic CAR representation is constructed internally:\n\nINLA_ss &lt;- inla(update(form, . ~ . + f(ID2, model = \"besag\",\n                                       graph = W)),\n                family = \"gaussian\", data = boston_487)\n\n\nboston_93$INLA_ss &lt;- INLA_ss$summary.random$ID2$mean\n\nThe sparse Leroux representation as given by Gómez-Rubio (2020) can be constructed in the following way:\n\nM &lt;- Diagonal(nrow(W), rowSums(W)) - W\nCmatrix &lt;- Diagonal(nrow(M), 1) -  M\n\nThis model can be estimated using the \"generic1\" model with the specified precision matrix:\n\nINLA_lr &lt;- inla(update(form, . ~ . + f(ID2, model = \"generic1\",\n                                       Cmatrix = Cmatrix)),\n                family = \"gaussian\", data = boston_487)\n\n\nboston_93$INLA_lr &lt;- INLA_lr$summary.random$ID2$mean\n\nICAR random effects with mgcv::gam()\n In a very similar way, the gam function in the mgcv package (Wood 2022) can take an \"mrf\" term using a suitable \"nb\" object for the upper level. In this case the \"nb\" object needs to have the contents of the \"region.id\" attribute copied as the names of the neighbour list components, and the indexing variable needs to be a factor (Wood 2017):\n\nlibrary(mgcv)\nnames(nb_q_93) &lt;- attr(nb_q_93, \"region.id\")\nboston_487$NOX_ID &lt;- as.factor(boston_487$NOX_ID)\n\nThe specification of the spatially structured term again differs in details from those above, but achieves the same purpose. The \"REML\" method of bayesx gives the same results as gam using \"REML\" in this case:\n\nGAM_MRF &lt;- gam(update(form, . ~ . + s(NOX_ID, bs = \"mrf\",\n                                      xt = list(nb = nb_q_93))),\n               data = boston_487, method = \"REML\")\n\nThe upper-level random effects may be extracted by predicting terms; as we can see, the values in all lower-level tracts belonging to the same upper-level air pollution model output zones are identical:\n\nssre &lt;- predict(GAM_MRF, type = \"terms\", \n                se = FALSE)[, \"s(NOX_ID)\"]\nall(sapply(tapply(ssre, list(boston_487$NOX_ID), c),\n           function(x) length(unique(round(x, 8))) == 1))\n# [1] TRUE\n\nso we can return the first value for each upper-level unit:\n\nboston_93$GAM_ss &lt;- aggregate(ssre, list(boston_487$NOX_ID), \n                              head, n=1)$x\n\nUpper-level random effects: summary\nIn the cases of hglm, bayesx, inla and gam, we could also model discrete responses without further major difficulty, and bayesx, inla and gam also facilitate the generalisation of functional form fitting for included covariates.\nUnfortunately, the coefficient estimates for the air pollution variable for these multilevel models are not helpful. All are negative as expected, but the inclusion of the model output zone level effects, IID or spatially structured, makes it is hard to disentangle the influence of the scale of observation from that of covariates observed at that scale rather than at the tract level.\nFigure 16.1 shows that the air pollution model output zone level IID random effects are very similar across the four model fitting functions reported. In all the maps, the central downtown zones have stronger negative random effect values, but strong positive values are also found in close proximity; suburban areas take values closer to zero.\n\nCodelibrary(tmap, warn.conflicts=FALSE)\ntmap4 &lt;- packageVersion(\"tmap\") &gt;= \"3.99\"\nif (tmap4) {\n    tm_shape(boston_93) +\n    tm_polygons(fill = c(\"MLM_re\", \"HGLM_re\", \"INLA_re\", \"BX_re\"),\n        fill.legend = tm_legend(\"IID\", frame=FALSE, item.r = 0),\n        fill.free = FALSE, lwd = 0.01,\n        fill.scale = tm_scale(midpoint = 0, values = \"brewer.rd_yl_gn\")) +\n    tm_facets_wrap(ncol = 2, nrow = 2) + \n    tm_layout(panel.labels = c(\"lmer\", \"hglm\", \"inla\", \"bayesx\"))\n} else {\ntm_shape(boston_93) +\n  tm_fill(c(\"MLM_re\", \"HGLM_re\", \"INLA_re\", \"BX_re\"),\n          midpoint = 0, title = \"IID\") +\n  tm_facets(free.scales = FALSE) +\n  tm_borders(lwd = 0.3, alpha = 0.4) + \n  tm_layout(panel.labels = c(\"lmer\", \"hglm\", \"inla\", \"bayesx\"))\n}\n\n\n\n\n\n\nFigure 16.1: Air pollution model output zone level IID random effects estimated using lme4, hglm, INLA and R2BayesX; the range of the response, log(median) is 2.1893\n\n\n\n\nFigure 16.2 shows that the spatially structured random effects are also very similar to each other, with the \"SAR\" spatial smooth being perhaps a little smoother than the \"CAR\" smooths when considering the range of values taken by the random effect term.\n\nCodeif (tmap4) {\n    tm_shape(boston_93) +\n    tm_polygons(fill = c(\"HGLM_ss\", \"INLA_lr\", \"INLA_ss\", \"BX_ss\", \n            \"GAM_ss\"),\n        fill.legend = tm_legend(\"SSRE\", frame=FALSE, item.r = 0),\n        fill.free = FALSE, lwd = 0.1,\n        fill.scale = tm_scale(midpoint = 0, values = \"brewer.rd_yl_gn\")) +\n    tm_facets_wrap(ncol = 3, nrow = 2) + \n    tm_layout(panel.labels = c(\"hglm CAR\", \"inla Leroux\",\n             \"inla ICAR\", \"bayesx ICAR\", \"gam ICAR\"))\n} else {\ntm_shape(boston_93) +\n  tm_fill(c(\"HGLM_ss\", \"INLA_lr\", \"INLA_ss\", \"BX_ss\", \n            \"GAM_ss\"), midpoint = 0, title = \"SSRE\") +\n  tm_facets(free.scales = FALSE) + \n  tm_borders(lwd = 0.3, alpha = 0.4) +\n  tm_layout(panel.labels = c(\"hglm CAR\", \"inla Leroux\",\n             \"inla ICAR\", \"bayesx ICAR\", \"gam ICAR\"))\n}\n\n\n\n\n\n\nFigure 16.2: Air pollution model output zone level spatially structured random effects estimated using hglm, HSAR, INLA, R2BayesX and mgcv\n\n\n\n\nAlthough there is still a great need for more thorough comparative studies of model fitting functions for spatial regression including multilevel capabilities, there has been much progress over recent years. Vranckx, Neyens, and Faes (2019) offer a recent comparative survey of disease mapping spatial regression, typically set in a Poisson regression framework offset by an expected count. In Bivand and Gómez-Rubio (2021), methods for estimating spatial survival models using spatial weights matrices are compared with spatial probit models.",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Spatial Regression</span>"
    ]
  },
  {
    "objectID": "16-SpatialRegression.html#exercises",
    "href": "16-SpatialRegression.html#exercises",
    "title": "16  Spatial Regression",
    "section": "\n16.3 Exercises",
    "text": "16.3 Exercises\n\nConstruct a multilevel dataset using the Athens housing data from the archived HSAR package: https://cran.r-project.org/src/contrib/Archive/HSAR/HSAR_0.5.1.tar.gz, and included in spData from version 2.2.1. At which point do the municipality department attribute values get copied out to all the point observations within each municipality department?\nCreate neighbour objects at both levels. Test greensp for spatial autocorrelation at the upper level, and then at the lower level. What has been the chief consequence of copying out the area of green spaces in square meters for the municipality departments to the point support property level?\nUsing the formula object from the vignette, assess whether adding the copied out upper-level variables seems sensible. Use mgcv::gam to fit a linear mixed effects model (IID of num_dep identifying the municipality departments) using just the lower-level variables and the lower- and upper-level variables. Do your conclusions differ?\nComplete the analysis by replacing the IID random effects with an \"mrf\" Markov random field and the contiguity neighbour object created above. Do you think that it is reasonable to, for example, draw any conclusions based on the municipality department level variables such as greensp?\n\n\n\n\n\n\n\nAlam, Moudud, Lars Ronnegard, and Xia Shen. 2019. Hglm: Hierarchical Generalized Linear Models. https://CRAN.R-project.org/package=hglm.\n\n\nAlam, Moudud, Lars Rönnegård, and Xia Shen. 2015. “Fitting Conditional and Simultaneous Autoregressive Spatial Models in Hglm.” The R Journal 7 (2): 5–18. https://doi.org/10.32614/RJ-2015-017.\n\n\nBates, Douglas, Martin Maechler, Ben Bolker, and Steven Walker. 2022. Lme4: Linear Mixed-Effects Models Using Eigen and S4. https://github.com/lme4/lme4/.\n\n\nBesag, Julian. 1974. “Spatial Interaction and the Statistical Analysis of Lattice Systems.” Journal of the Royal Statistical Society. Series B (Methodological) 36: pp. 192–236.\n\n\nBivand, Roger. 2017. “Revisiting the Boston Data Set — Changing the Units of Observation Affects Estimated Willingness to Pay for Clean Air.” REGION 4 (1): 109–27. https://doi.org/10.18335/region.v4i1.107.\n\n\nBivand, Roger, and Virgilio Gómez-Rubio. 2021. “Spatial Survival Modelling of Business Re-Opening After Katrina: Survival Modelling Compared to Spatial Probit Modelling of Re-Opening Within 3, 6 or 12 Months.” Statistical Modelling 21 (1-2): 137–60. https://doi.org/10.1177/1471082X20967158.\n\n\nBivand, Roger, Virgilio Gómez-Rubio, and Håvard Rue. 2015. “Spatial Data Analysis with r-INLA with Some Extensions.” Journal of Statistical Software, Articles 63 (20): 1–31. https://doi.org/10.18637/jss.v063.i20.\n\n\nBivand, Roger, Jakub Nowosad, and Robin Lovelace. 2022. spData: Datasets for Spatial Analysis. https://jakubnowosad.com/spData/.\n\n\nBivand, Roger, Zhe Sha, Liv Osland, and Ingrid Sandvig Thorsen. 2017. “A Comparison of Estimation Methods for Multilevel Models of Spatially Structured Data.” Spatial Statistics. https://doi.org/10.1016/j.spasta.2017.01.002.\n\n\nBrooks, Mollie E., Kasper Kristensen, Koen J. van Benthem, Arni Magnusson, Casper W. Berg, Anders Nielsen, Hans J. Skaug, Martin Maechler, and Benjamin M. Bolker. 2017. “glmmTMB Balances Speed and Flexibility Among Packages for Zero-Inflated Generalized Linear Mixed Modeling.” The R Journal 9 (2): 378–400. https://journal.r-project.org/archive/2017/RJ-2017-066/index.html.\n\n\nCliff, A. D., and J. K. Ord. 1972. “Testing for Spatial Autocorrelation Among Regression Residuals.” Geographical Analysis 4: 267–84.\n\n\n———. 1973. Spatial Autocorrelation. London: Pion.\n\n\nCressie, N. A. C. 1993. Statistics for Spatial Data. New York: Wiley.\n\n\nGaetan, Carlo, and Xavier Guyon. 2010. Spatial Statistics and Modeling. New York: Springer.\n\n\nGerber, Florian, and Reinhard Furrer. 2015. “Pitfalls in the Implementation of Bayesian Hierarchical Modeling of Areal Count Data: An Illustration Using BYM and Leroux Models.” Journal of Statistical Software, Code Snippets 63 (1): 1–32. https://doi.org/10.18637/jss.v063.c01.\n\n\nGómez-Rubio, Virgilio. 2019. “Spatial Data Analysis with INLA. Coding Club UC3M Tutorial Series. Universidad Carlos III de Madrid.” https://codingclubuc3m.rbind.io/talk/2019-11-05/.\n\n\n———. 2020. Bayesian Inference with INLA. Boca Raton, FL: CRC Press.\n\n\nHepple, Leslie W. 1976. “A Maximum Likelihood Model for Econometric Estimation with Spatial Series.” In Theory and Practice in Regional Science, edited by I. Masser, 90–104. London Papers in Regional Science. London: Pion.\n\n\nMcCulloch, Charles E., and Shayle R. Searle. 2001. Generalized, Linear, and Mixed Models. New York: Wiley.\n\n\nOrd, J. K. 1975. “Estimation Methods for Models of Spatial Interaction.” Journal of the American Statistical Association 70 (349): 120–26.\n\n\nPinheiro, Jose C., and Douglas M. Bates. 2000. Mixed-Effects Models in S and S-Plus. New York: Springer.\n\n\nRipley, B. D. 1981. Spatial Statistics. New York: Wiley.\n\n\n———. 1988. Statistical Inference for Spatial Processes. Cambridge: Cambridge University Press.\n\n\nRue, Havard, Finn Lindgren, and Elias Teixeira Krainski. 2022. INLA: Full Bayesian Analysis of Latent Gaussian Models Using Integrated Nested Laplace Approximations.\n\n\nUmlauf, Nikolaus, Daniel Adler, Thomas Kneib, Stefan Lang, and Achim Zeileis. 2015. “Structured Additive Regression Models: An R Interface to BayesX.” Journal of Statistical Software 63 (21): 1–46. http://www.jstatsoft.org/v63/i21/.\n\n\nUmlauf, Nikolaus, Thomas Kneib, Stefan Lang, and Achim Zeileis. 2022. R2BayesX: Estimate Structured Additive Regression Models with BayesX. https://CRAN.R-project.org/package=R2BayesX.\n\n\nVan Lieshout, M. N. M. 2019. Theory of Spatial Statistics. Boca Raton, FL: Chapman & Hall/CRC.\n\n\nVranckx, M., T. Neyens, and C. Faes. 2019. “Comparison of Different Software Implementations for Spatial Disease Mapping.” Spatial and Spatio-Temporal Epidemiology 31: 100302. https://doi.org/10.1016/j.sste.2019.100302.\n\n\nWall, M. M. 2004. “A Close Look at the Spatial Structure Implied by the CAR and SAR Models.” Journal of Statistical Planning and Inference 121: 311–24.\n\n\nWhittle, P. 1954. “On Stationary Processes in the Plane.” Biometrika 41 (3-4): 434–49. https://doi.org/10.1093/biomet/41.3-4.434.\n\n\nWood, S. N. 2017. Generalized Additive Models: An Introduction with R. 2nd ed. Chapman & Hall/CRC.\n\n\n———. 2022. Mgcv: Mixed GAM Computation Vehicle with Automatic Smoothness Estimation. https://CRAN.R-project.org/package=mgcv.",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Spatial Regression</span>"
    ]
  },
  {
    "objectID": "17-Econometrics.html",
    "href": "17-Econometrics.html",
    "title": "17  Spatial Econometrics Models",
    "section": "",
    "text": "17.1 Spatial econometric models: definitions\nSpatial autoregression models using spatial weights matrices were described in some detail using maximum likelihood estimation some time ago (Cliff and Ord 1973, 1981). A family of models was elaborated in spatial econometric terms extending earlier work, and in many cases using the simultaneous autoregressive framework and row standardisation of spatial weights (Anselin 1988). The simultaneous and conditional autoregressive frameworks can be compared, and both can be supplemented using case weights to reflect the relative importance of different observations (Waller and Gotway 2004).\nBefore moving to presentations of issues raised in fitting spatial regression models, it is worth making a few further points. A recent review of spatial regression in a spatial econometrics setting is given by Kelejian and Piras (2017); note that their usage is to call the spatial coefficient of the lagged response \\(\\lambda\\) and that of the lagged residuals \\(\\rho\\), the reverse of other usage (Anselin 1988; LeSage and Pace 2009); here we use \\(\\rho_{\\mathrm{Lag}}\\) for the spatial coefficient in the spatial lag model, and \\(\\rho_{\\mathrm{Err}}\\) for the spatial error model. One interesting finding is that relatively dense spatial weights matrices may down-weight model estimates, suggesting that sparser weights are preferable (Smith 2009). Another useful finding is that the presence of residual spatial autocorrelation need not bias the estimates of variance of regression coefficients, provided that the covariates themselves do not exhibit spatial autocorrelation (Smith and Lee 2012). In general, however, the footprints of the spatial processes of the response and covariates may not be aligned, and if covariates and the residual are autocorrelated, it is likely that the estimates of variance of regression coefficients will be biased downwards if attempts are not made to model the spatial processes.\nIn trying to model spatial processes, one of the earliest spatial econometric representations is to model the spatial autocorrelation in the residual (spatial error model, SEM): \\[\n{\\mathbf y} = {\\mathbf X}{\\mathbf \\beta} + {\\mathbf u},\n\\qquad {\\mathbf u} = \\rho_{\\mathrm{Err}} {\\mathbf W} {\\mathbf u} + {\\mathbf \\varepsilon},\n\\] where \\({\\mathbf y}\\) is an \\((N \\times 1)\\) vector of observations on a response variable taken at each of \\(N\\) locations, \\({\\mathbf X}\\) is an \\((N \\times k)\\) matrix of covariates, \\({\\mathbf \\beta}\\) is a \\((k \\times 1)\\) vector of parameters, \\({\\mathbf u}\\) is an \\((N \\times 1)\\) spatially autocorrelated disturbance vector, \\({\\mathbf \\varepsilon}\\) is an \\((N \\times 1)\\) vector of independent and identically distributed disturbances and \\(\\rho_{\\mathrm{Err}}\\) is a scalar spatial parameter.\nThis model, and other spatial econometric models, do not fit into the mixed models framework. Here the modelled spatial process interacts directly with the response, covariates, and their coefficients. This modelling framework appears to draw on an older tradition extending time series to two dimensions:\n\\[\n{\\mathbf u} = ({\\mathbf I} - \\rho_{\\mathrm{Err}} {\\mathbf W})^{-1} {\\mathbf \\varepsilon},\n\\ \\ {\\mathbf y} = {\\mathbf X}{\\mathbf \\beta} + ({\\mathbf I} - \\rho_{\\mathrm{Err}} {\\mathbf W})^{-1} {\\mathbf \\varepsilon},\n\\ \\ ({\\mathbf I} - \\rho_{\\mathrm{Err}} {\\mathbf W}) {\\mathbf y} = ({\\mathbf I} - \\rho_{\\mathrm{Err}} {\\mathbf W}) {\\mathbf X}{\\mathbf \\beta} + {\\mathbf \\varepsilon}.\n\\]\nIf the processes in the covariates and the response match, we should find little difference between the coefficients of a least squares and a SEM, but very often they diverge, suggesting that a Hausman test for this condition should be employed (Pace and LeSage 2008). This may be related to earlier discussions of a spatial equivalent to the unit root and cointegration where spatial processes match (Fingleton 1999).\nA model with a spatial process in the response only is termed a spatial lag model (SLM, often SAR - spatial autoregressive) (LeSage and Pace 2009). Durbin models add the spatially lagged covariates to the covariates included in the spatial model; spatial Durbin models are reviewed by Mur and Angulo (2006). If it is chosen to admit a spatial process in the residuals in addition to a spatial process in the response, again two models are formed, a general nested model (GNM) nesting all the others, and a model without spatially lagged covariates (SAC, also known as SARAR - Spatial AutoRegressive-AutoRegressive model). If neither the residuals nor the response are modelled with spatial processes, spatially lagged covariates may be added to a linear model, as a spatially lagged X model (SLX) (Elhorst 2010; Bivand 2012; LeSage 2014; Halleck Vega and Elhorst 2015). We can write the GNM as: \\[\n{\\mathbf y} = \\rho_{\\mathrm{Lag}} {\\mathbf W}{\\mathbf y} + {\\mathbf X}{\\mathbf \\beta} + {\\mathbf W}{\\mathbf X}{\\mathbf \\gamma} + {\\mathbf u},\n\\qquad {\\mathbf u} = \\rho_{\\mathrm{Err}} {\\mathbf W} {\\mathbf u} + {\\mathbf \\varepsilon},\n\\] where \\({\\mathbf \\gamma}\\) is a \\((k' \\times 1)\\) vector of parameters. \\(k'\\) defines the subset of the intercept and covariates, often \\(k' = k-1\\) when using row standardised spatial weights and omitting the spatially lagged intercept.\nThis may be constrained to the double spatial coefficient model SAC/SARAR by setting \\({\\mathbf \\gamma} = 0\\), to the spatial Durbin (SDM) by setting \\(\\rho_{\\mathrm{Err}} = 0\\), and to the error Durbin model (SDEM) by setting \\(\\rho_{\\mathrm{Lag}} = 0\\). Imposing more conditions gives the spatial lag model (SLM) with \\({\\mathbf \\gamma} = 0\\) and \\(\\rho_{\\mathrm{Err}} = 0\\), the SEM with \\({\\mathbf \\gamma} = 0\\) and \\(\\rho_{\\mathrm{Lag}} = 0\\), and the SLX with \\(\\rho_{\\mathrm{Lag}} = 0\\) and \\(\\rho_{\\mathrm{Err}} = 0\\).\nAlthough making predictions for new locations for which covariates are observed was raised as an issue some time ago, it has taken many years to make progress in reviewing the possibilities (Bivand 2002; Goulard, Laurent, and Thomas-Agnan 2017; Laurent and Margaretic 2021). The prediction methods for SLM, SDM, SEM, SDEM, SAC, and GNM models fitted with maximum likelihood were contributed as a Google Summer of Coding project by Martin Gubri. This work, and work on similar models with missing data (Suesse 2018) is also relevant for exploring censored median house values in the Boston dataset. Work on prediction also exposed the importance of the reduced form of these models, in which the spatial process in the response interacts with the regression coefficients in the SLM, SDM, SAC and GNM models.\nThe consequence of these interactions is that a unit change in a covariate will only impact the response as the value of the regression coefficient if the spatial coefficient of the lagged response is zero. Where it is non-zero, global spillovers, impacts, come into play, and these impacts should be reported rather than the regression coefficients (LeSage and Pace 2009; Elhorst 2010; Bivand 2012; LeSage 2014; Halleck Vega and Elhorst 2015). Local impacts may be reported for SDEM and SLX models, using a linear combination to calculate standard errors for the total impacts of each covariate (sums of coefficients on the covariates and their spatial lags).\nThis can be seen from the GNM data generation process:\n\\[\n({\\mathbf I} - \\rho_{\\mathrm{Err}} {\\mathbf W})({\\mathbf I} - \\rho_{\\mathrm{Lag}} {\\mathbf W}){\\mathbf y} = ({\\mathbf I} - \\rho_{\\mathrm{Err}} {\\mathbf W})({\\mathbf X}{\\mathbf \\beta} + {\\mathbf W}{\\mathbf X}{\\mathbf \\gamma}) + {\\mathbf \\varepsilon},\n\\] re-writing:\n\\[\n{\\mathbf y} = ({\\mathbf I} - \\rho_{\\mathrm{Lag}} {\\mathbf W})^{-1}({\\mathbf X}{\\mathbf \\beta} + {\\mathbf W}{\\mathbf X}{\\mathbf \\gamma}) + ({\\mathbf I} - \\rho_{\\mathrm{Lag}} {\\mathbf W})^{-1}({\\mathbf I} - \\rho_{\\mathrm{Err}} {\\mathbf W})^{-1}{\\mathbf \\varepsilon}.\n\\] There is interaction between the \\(\\rho_{\\mathrm{Lag}}\\) and \\({\\mathbf \\beta}\\) (and \\({\\mathbf \\gamma}\\) if present) coefficients. This can be seen from the partial derivatives: \\(\\partial y_i / \\partial x_{jr} = (({\\mathbf I} - \\rho_{\\mathrm{Lag}} {\\mathbf W})^{-1} ({\\mathbf I} \\beta_r + {\\mathbf W} \\gamma_r))_{ij}\\). This dense matrix \\(S_r({\\mathbf W}) = (({\\mathbf I} - \\rho_{\\mathrm{Lag}} {\\mathbf W})^{-1} ({\\mathbf I} \\beta_r + {\\mathbf W} \\gamma_r))\\) expresses the direct impacts (effects) on its principal diagonal, and indirect impacts in off-diagonal elements.\nPiras and Prucha (2014) revisit and correct Raymond J. G. M. Florax, Folmer, and Rey (2003) (see also comments by Hendry (2006) and Raymond J. G. M. Florax, Folmer, and Rey (2006)), finding that the common use of pre-test strategies for model selection probably ought to be replaced by the estimation of the most general model appropriate for the relationships being modelled. In light of this finding, pre-test model selection will not be used here.\nCurrent work in the spatialreg package is focused on refining the handling of spatially lagged covariates using a consistent Durbin argument taking either a logical value or a formula giving the subset of covariates to add in spatially lagged form. There is a speculation that some covariates, for example some dummy variables, should not be added in spatially lagged form. This then extends to handling these included spatially lagged covariates appropriately in calculating impacts. This work applies to cross-sectional models fitted using MCMC or maximum likelihood, and will offer facilities to spatial panel models.\nIt is worth mentioning the almost unexplored issues of functional form assumptions, for which flexible structures are useful, including spatial quantile regression presented in the McSpatial package (McMillen 2013). There are further issues with discrete response variables, covered by some functions in McSpatial, by the new package spldv (Sarrias and Piras 2022), and in the spatialprobit and ProbitSpatial packages (Wilhelm and Matos 2013; Martinetti and Geniaux 2017); the MCMC implementations of the former are based on LeSage and Pace (2009). Finally, Wagner and Zeileis (2019) show how an SLM model may be used in the setting of recursive partitioning, with an implementation using spatialreg::lagsarlm() in the lagsarlmtree package.\nThe review of cross-sectional maximum likelihood and generalised method of moments (GMM) estimators in spatialreg (Bivand and Piras 2022) and sphet for spatial econometrics style spatial regression models by Bivand and Piras (2015) is still largely valid. In the review, estimators in these R packages were compared with alternative implementations available in other programming languages elsewhere. The review did not cover Bayesian spatial econometrics style spatial regression. More has changed with respect to spatial panel estimators described in Millo and Piras (2012) but will not be covered here.\nBecause Bivand, Millo, and Piras (2021) covers many of the features of R packages for spatial econometrics, updating Bivand and Piras (2015), and including recent advances in General Method of Moments and spatial panel modelling, this chapter will be restricted to a small number of examples drawing on Bivand (2017) using the Boston house value dataset.",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Spatial Econometrics Models</span>"
    ]
  },
  {
    "objectID": "17-Econometrics.html#maximum-likelihood-estimation-in-spatialreg",
    "href": "17-Econometrics.html#maximum-likelihood-estimation-in-spatialreg",
    "title": "17  Spatial Econometrics Models",
    "section": "\n17.2 Maximum likelihood estimation in spatialreg\n",
    "text": "17.2 Maximum likelihood estimation in spatialreg\n\nFor models with single spatial coefficients (SEM and SDEM using errorsarlm(), SLM and SDM using lagsarlm()), the methods initially described by Ord (1975) are used. The following table shows the functions that can be used to estimate the models described above using maximum likelihood.\n\n\n\n\n\n\n\nmodel\nmodel name\nmaximum likelihood estimation function\n\n\n\nSEM\nspatial error\nerrorsarlm(..., Durbin=FALSE)\n\n\nSEM\nspatial error\nspautolm(..., family=\"SAR\")\n\n\nSDEM\nspatial Durbin error\nerrorsarlm(..., Durbin=TRUE)\n\n\nSLM\nspatial lag\nlagsarlm(..., Durbin=FALSE)\n\n\nSDM\nspatial Durbin\nlagsarlm(..., Durbin=TRUE)\n\n\nSAC\nspatial autoregressive combined\nsacsarlm(..., Durbin=FALSE)\n\n\nGNM\ngeneral nested\nsacsarlm(..., Durbin=TRUE)\n\n\n\n \nThe estimating functions errorsarlm() and lagsarlm() take similar arguments, where the first two, formula and data are shared by most model estimating functions. The third argument is a listw spatial weights object, while na.action behaves as in other model estimating functions if the spatial weights can reasonably be subsetted to avoid observations with missing values. The weights argument may be used to provide weights indicating the known degree of per-observation variability in the variance term - this is not available for lagsarlm().\nThe Durbin argument replaces the earlier type and etype arguments, and if not given is taken as FALSE. If given, it may be FALSE, TRUE in which case all spatially lagged covariates are included, or a one-sided formula specifying which spatially lagged covariates should be included. The method argument gives the method for calculating the log determinant term in the log likelihood function, and defaults to \"eigen\", suitable for moderately sized datasets. The interval argument gives the bounds of the domain for the line search using stats::optimize() used for finding the spatial coefficient. The tol.solve() argument, passed through to base::solve(), was needed to handle datasets with differing numerical scales among the coefficients which hindered inversion of the variance-covariance matrix; the default value in base::solve() used to be much larger. The control argument takes a list of control values to permit more careful adjustment of the running of the estimation function.\nThe sacsarlm() function may take second spatial weights and interval arguments if the spatial weights used to model the two spatial processes in the SAC and GNM specifications differ. By default, the same spatial weights are used. By default, stats::nlminb() is used for numerical optimisation, using a heuristic to choose starting values. Like lagsarlm(), this function does not take a weights argument.\nWhere larger datasets are used, a numerical Hessian approach is used to calculate the variance-covariance matrix of coefficients, rather than an analytical asymptotic approach.\nBoston house value dataset examples\nThe examples use the objects read and created in Chapter 16, based on Bivand (2017).\n\nlibrary(spatialreg)\neigs_489 &lt;- eigenw(lw_q_489)\nSDEM_489 &lt;- errorsarlm(form, data = boston_489, \n      listw = lw_q_489, Durbin = TRUE, zero.policy = TRUE,\n      control = list(pre_eig = eigs_489))\nSEM_489 &lt;- errorsarlm(form, data = boston_489, \n      listw = lw_q_489, zero.policy = TRUE,\n      control = list(pre_eig = eigs_489))\n\nHere we are using the control list argument to pass through pre-computed eigenvalues for the default \"eigen\" method.\n\ncbind(data.frame(model=c(\"SEM\", \"SDEM\")), \n      rbind(broom::tidy(Hausman.test(SEM_489)), \n            broom::tidy(Hausman.test(SDEM_489))))[,1:4]\n#   model statistic  p.value parameter\n# 1   SEM      52.0 2.83e-06        14\n# 2  SDEM      48.7 6.48e-03        27\n\nBoth Hausman test results for the 489 tract dataset suggest that the regression coefficients do differ from their non-spatial counterparts, perhaps indicating that the footprints of the spatial processes do not match.\n\neigs_94 &lt;- eigenw(lw_q_94)\nSDEM_94 &lt;- errorsarlm(form, data=boston_94, listw=lw_q_94,\n                      Durbin = TRUE,\n                      control = list(pre_eig=eigs_94))\nSEM_94 &lt;- errorsarlm(form, data = boston_94, listw = lw_q_94,\n                     control = list(pre_eig = eigs_94))\n\nFor the 94 air pollution model output zones, the Hausman tests find little difference between coefficients:\n\ncbind(data.frame(model=c(\"SEM\", \"SDEM\")), \n      rbind(broom::tidy(Hausman.test(SEM_94)), \n            broom::tidy(Hausman.test(SDEM_94))))[, 1:4]\n#   model statistic p.value parameter\n# 1   SEM     15.66   0.335        14\n# 2  SDEM      9.21   0.999        27\n\nThis is related to the fact that the SEM and SDEM models add little to least squares or SLX at the air pollution model output zone level, using likelihood ratio tests:\n\ncbind(data.frame(model=c(\"SEM\", \"SDEM\")),\n      rbind(broom::tidy(LR1.Sarlm(SEM_94)),\n            broom::tidy(LR1.Sarlm(SDEM_94))))[,c(1, 4:6)]\n#   model statistic p.value parameter\n# 1   SEM     2.593   0.107         1\n# 2  SDEM     0.216   0.642         1\n\nWe can use spatialreg::LR.Sarlm() to apply a likelihood ratio test between nested models, but here choose lmtest::lrtest(), which gives the same results, preferring models including spatially lagged covariates both for tracts and model output zones:\n\no &lt;- lmtest::lrtest(SEM_489, SDEM_489)\nattr(o, \"heading\")[2] &lt;- \"Model 1: SEM_489\\nModel 2: SDEM_489\"\no\n# Likelihood ratio test\n# \n# Model 1: SEM_489\n# Model 2: SDEM_489\n#   #Df LogLik Df Chisq Pr(&gt;Chisq)    \n# 1  16    274                        \n# 2  29    311 13  74.4    1.2e-10 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\no &lt;- lmtest::lrtest(SEM_94, SDEM_94)\nattr(o, \"heading\")[2] &lt;- \"Model 1: SEM_94\\nModel 2: SDEM_94\"\no\n# Likelihood ratio test\n# \n# Model 1: SEM_94\n# Model 2: SDEM_94\n#   #Df LogLik Df Chisq Pr(&gt;Chisq)    \n# 1  16   59.7                        \n# 2  29   81.3 13  43.2    4.2e-05 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe SLX model is fitted using least squares and also returns a log likelihood value, letting us test whether we need a spatial process in the residuals. In the tract dataset, we obviously do:\n\nSLX_489 &lt;- lmSLX(form, data = boston_489, listw = lw_q_489,\n                 zero.policy = TRUE)\no &lt;- lmtest::lrtest(SLX_489, SDEM_489)\nattr(o, \"heading\")[2] &lt;- \"Model 1: SLX_489\\nModel 2: SDEM_489\"\no\n# Likelihood ratio test\n# \n# Model 1: SLX_489\n# Model 2: SDEM_489\n#   #Df LogLik Df Chisq Pr(&gt;Chisq)    \n# 1  28    231                        \n# 2  29    311  1   159     &lt;2e-16 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nbut in the output zone case, we do not.\n\nSLX_94 &lt;- lmSLX(form, data = boston_94, listw = lw_q_94)\no &lt;- lmtest::lrtest(SLX_94, SDEM_94)\nattr(o, \"heading\")[2] &lt;- \"Model 1: SLX_94\\nModel 2: SDEM_94\"\no\n# Likelihood ratio test\n# \n# Model 1: SLX_94\n# Model 2: SDEM_94\n#   #Df LogLik Df Chisq Pr(&gt;Chisq)\n# 1  28   81.2                    \n# 2  29   81.3  1  0.22       0.64\n\nThese outcomes are sustained also when we use the counts of house units by tract and output zones as case weights:\n\nSLX_489w &lt;- lmSLX(form, data = boston_489, listw = lw_q_489,\n                  weights = units, zero.policy = TRUE)\nSDEM_489w &lt;- errorsarlm(form, data = boston_489,\n                        listw = lw_q_489, Durbin = TRUE,\n                        weights = units, zero.policy = TRUE,\n                        control = list(pre_eig = eigs_489))\no &lt;- lmtest::lrtest(SLX_489w, SDEM_489w)\nattr(o, \"heading\")[2] &lt;- \"Model 1: SLX_489w\\nModel 2: SDEM_489w\"\no\n# Likelihood ratio test\n# \n# Model 1: SLX_489w\n# Model 2: SDEM_489w\n#   #Df LogLik Df Chisq Pr(&gt;Chisq)    \n# 1  28    311                        \n# 2  29    379  1   136     &lt;2e-16 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSLX_94w &lt;- lmSLX(form, data = boston_94, listw = lw_q_94,\n                 weights = units)\nSDEM_94w &lt;- errorsarlm(form, data = boston_94, listw = lw_q_94,\n                       Durbin = TRUE, weights = units,\n                       control = list(pre_eig = eigs_94))\no &lt;- lmtest::lrtest(SLX_94w, SDEM_94w)\nattr(o, \"heading\")[2] &lt;- \"Model 1: SLX_94w\\nModel 2: SDEM_94w\"\no\n# Likelihood ratio test\n# \n# Model 1: SLX_94w\n# Model 2: SDEM_94w\n#   #Df LogLik Df Chisq Pr(&gt;Chisq)\n# 1  28   97.5                    \n# 2  29   98.0  1  0.92       0.34\n\nIn this case and based on arguments advanced in Bivand (2017), the use of weights is justified because tract counts of reported housing units underlying the weighted median values vary from 5 to 3,031, and air pollution model output zone counts vary from 25 to 12,411. Because of this, and because a weighted general nested model has not been developed, we cannot take the GNM as the starting point for general-to-simpler testing, but we start rather from the SDEM model and use the Hausman test to guide the choice of units of observation.",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Spatial Econometrics Models</span>"
    ]
  },
  {
    "objectID": "17-Econometrics.html#impacts",
    "href": "17-Econometrics.html#impacts",
    "title": "17  Spatial Econometrics Models",
    "section": "\n17.3 Impacts",
    "text": "17.3 Impacts\n\nGlobal impacts have been seen as crucial for reporting results from fitting models including the spatially lagged response (SLM, SDM, SAC, GNM) for over 10 years (LeSage and Pace 2009). Extension to other models including spatially lagged covariates (SLX, SDEM) has followed (Elhorst 2010; Bivand 2012; Halleck Vega and Elhorst 2015). For SLM, SDM, SAC, and GNM models fitted with maximum likelihood or GMM, the variance-covariance matrix of the coefficients is available, and can be used to make random draws from a multivariate Normal distribution with mean set to coefficient values and variance to the estimated variance-covariance matrix. For these models fitted using Bayesian methods, draws are already available. In the SDEM case, the draws on the regression coefficients of the unlagged covariates represent direct impacts, and draws on the coefficients of the spatially lagged covariates represent indirect impacts, and their by-draw sums the total impacts.\nSince sampling is not required for inference for SLX and SDEM models, linear combination is used for models fitted using maximum likelihood; results are shown here for the air pollution variable only. The literature has not yet resolved the question of how to report model output, as each covariate is now represented by three impacts. Where spatially lagged covariates are included, two coefficients are replaced by three impacts, here for the air pollution variable of interest.\n\n\nsum_imp_94_SDEM &lt;- summary(impacts(SDEM_94))\nrbind(Impacts = sum_imp_94_SDEM$mat[5,], \n      SE = sum_imp_94_SDEM$semat[5,])\n#           Direct Indirect   Total\n# Impacts -0.01276 -0.01845 -0.0312\n# SE       0.00235  0.00472  0.0053\n\nIn the SLX and SDEM models, the direct impacts are the consequences for the response of changes in air pollution in the same observational entity, and the indirect (local) impacts are the consequences for the response of changes in air pollution in neighbouring observational entities.\n\nsum_imp_94_SLX &lt;- summary(impacts(SLX_94))\nrbind(Impacts = sum_imp_94_SLX$mat[5,], \n      SE = sum_imp_94_SLX$semat[5,])\n#          Direct Indirect    Total\n# Impacts -0.0128 -0.01874 -0.03151\n# SE       0.0028  0.00556  0.00611\n\nApplying the same approaches to the weighted spatial regressions, the total impacts of air pollution on house values are reduced, but remain significant:\n\nsum_imp_94_SDEMw &lt;- summary(impacts(SDEM_94w))\nrbind(Impacts = sum_imp_94_SDEMw$mat[5,], \n      SE = sum_imp_94_SDEMw$semat[5,])\n#           Direct Indirect    Total\n# Impacts -0.00592 -0.01076 -0.01668\n# SE       0.00269  0.00531  0.00559\n\nOn balance, using a weighted spatial regression representation including only the spatially lagged covariates aggregated to the air pollution model output zone level seems to clear most of the misspecification issues, and as Bivand (2017) discusses in more detail, it gives a willingness to pay for pollution abatement that is much larger than misspecified alternative models:\n\nsum_imp_94_SLXw &lt;- summary(impacts(SLX_94w))\nrbind(Impacts = sum_imp_94_SLXw$mat[5,], \n      SE = sum_imp_94_SLXw$semat[5,])\n#           Direct Indirect    Total\n# Impacts -0.00620 -0.01221 -0.01842\n# SE       0.00326  0.00628  0.00629",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Spatial Econometrics Models</span>"
    ]
  },
  {
    "objectID": "17-Econometrics.html#sec-spateconpred",
    "href": "17-Econometrics.html#sec-spateconpred",
    "title": "17  Spatial Econometrics Models",
    "section": "\n17.4 Predictions",
    "text": "17.4 Predictions\nIn the Boston tracts dataset, 17 observations of median house values, the response, are censored. We will use the predict() method for \"Sarlm\" objects to fill in these values; the method was rewritten by Martin Gubri based on Goulard, Laurent, and Thomas-Agnan (2017), see also Laurent and Margaretic (2021). The pred.type argument specifies the prediction strategy among those presented in the article.\nUsing these as an example and comparing some pred.type variants for the SDEM model and predicting out-of-sample, we can see that there are differences, suggesting that this is a fruitful area for study. There have been a number of alternative proposals for handling missing variables (Gómez-Rubio, Bivand, and Rue 2015; Suesse 2018). Another reason for increasing attention on prediction is that it is fundamental for machine learning approaches, in which prediction for validation and test datasets drives model specification choice. The choice of training and other datasets with dependent spatial data remains an open question, and, is certainly not as simple as with independent data.\nHere, we’ll list the predictions for the censored tract observations using three different prediction types, taking the exponent to get back to the USD median house values. Note that the row.names() of the newdata object are matched with the whole-data spatial weights matrix \"region.id\" attribute to make out-of-sample prediction possible:\n\nnd &lt;- boston_506[is.na(boston_506$median),]\nt0 &lt;- exp(predict(SDEM_489, newdata = nd, listw = lw_q,\n                  pred.type = \"TS\", zero.policy  =TRUE))\n# Warning in subset.nb(x = nb, subset = subset): subsetting caused\n# increase in subgraph count\nsuppressWarnings(t1 &lt;- exp(predict(SDEM_489, newdata = nd,\n                                    listw = lw_q,\n                                    pred.type = \"KP2\",\n                                    zero.policy = TRUE)))\nsuppressWarnings(t2 &lt;- exp(predict(SDEM_489, newdata = nd,\n                                    listw = lw_q,\n                                    pred.type = \"KP5\",\n                                    zero.policy = TRUE)))\n\nWe can also use the \"slm\" model in INLA to predict missing response values as part of the model fitting function call. A certain amount of set-up code is required as the \"slm\" model is still experimental:\n\n\nlibrary(INLA)\n# This is INLA_25.06.07 built 2025-06-11 18:54:45\n#                            UTC.\n#  - See www.r-inla.org/contact-us for how to get help.\n#  - List available models/likelihoods/etc with inla.list.models()\n#  - Use inla.doc(&lt;NAME&gt;) to access documentation\n# \n# Attaching package: 'INLA'\n# The following object is masked _by_ '.GlobalEnv':\n# \n#     f\nW &lt;- as(lw_q, \"CsparseMatrix\")\nn &lt;- nrow(W)\ne &lt;- eigenw(lw_q)\nre.idx &lt;- which(abs(Im(e)) &lt; 1e-6)\nrho.max &lt;- 1 / max(Re(e[re.idx]))\nrho.min &lt;- 1 / min(Re(e[re.idx]))\nrho &lt;- mean(c(rho.min, rho.max))\nboston_506$idx &lt;- 1:n\nzero.variance = list(prec = list(initial = 25, fixed = TRUE))\nargs.slm &lt;- list(rho.min = rho.min, rho.max = rho.max, W = W,\n                 X = matrix(0, n, 0), Q.beta = matrix(1,0,0))\nhyper.slm &lt;- list(prec = list(prior = \"loggamma\", \n                              param = c(0.01, 0.01)),\n                  rho = list(initial = 0, prior = \"logitbeta\",\n                             param = c(1,1)))\nWX &lt;- create_WX(model.matrix(update(form, CMEDV ~ .), \n                             data = boston_506), lw_q)\nSDEM_506_slm &lt;- inla(update(form, \n                            . ~ . + WX + f(idx, model = \"slm\",\n                                         args.slm = args.slm,\n                                         hyper = hyper.slm)),\n                 data = boston_506, family = \"gaussian\",\n                 control.family = list(hyper = zero.variance),\n                 control.compute = list(dic = TRUE, cpo = TRUE))\nmv_mean &lt;- exp(SDEM_506_slm$summary.fitted.values$mean[\n               which(is.na(boston_506$median))])\n\nINLA also provide gridded estimates of the marginal distributions of the predictions, offering a way to assess the uncertainty associated with the predicted values.\n\ndata.frame(fit_TS = t0[,1], fit_KP2 = c(t1), fit_KP5 = c(t2),\n    INLA_slm = mv_mean, censored = \n      boston_506$censored[as.integer(attr(t0, \"region.id\"))])\n#     fit_TS fit_KP2 fit_KP5 INLA_slm censored\n# 13   23912   29477   28147    31145    right\n# 14   28126   27001   28516    31243    right\n# 15   30553   36184   32476    41623    right\n# 17   18518   19621   18878    21256    right\n# 43    9564    6817    7561     6767     left\n# 50    8371    7196    7383     6876     left\n# 312  51477   53301   54173    56188    right\n# 313  45921   45823   47095    46381    right\n# 314  44196   44586   45361    42780    right\n# 317  43427   45707   45442    48124    right\n# 337  39879   42072   41127    41522    right\n# 346  44708   46694   46108    45908    right\n# 355  48188   49068   48911    49176    right\n# 376  42881   45883   44966    47883    right\n# 408  44294   44615   45670    46087    right\n# 418  38211   43375   41914    44096    right\n# 434  41647   41690   42398    41637    right\n\nThe spatial regression toolbox remains incomplete, and it will take time to fill in blanks. It remains unfortunate that the several traditions in spatial regression seldom seem to draw on each others’ understandings and advances.",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Spatial Econometrics Models</span>"
    ]
  },
  {
    "objectID": "17-Econometrics.html#exercises",
    "href": "17-Econometrics.html#exercises",
    "title": "17  Spatial Econometrics Models",
    "section": "\n17.5 Exercises",
    "text": "17.5 Exercises\n\nReferring to Piras and Prucha (2014) and Raymond J. G. M. Florax, Folmer, and Rey (2003), if we choose to use a pre-test strategy, do linear models of the properties-only dataset and the properties with added municipality department variables show residual spatial dependence? Which model specifications might the pre-tests indicate?\nCould the inclusion of municipality department dummies, or a municipality department regimes model assist in reducing residual spatial dependence?\nAttempt to fit a SEM specification by maximum likelihood (see Bivand, Millo, and Piras (2021) for GMM code examples) to the properties-only and the properties with added municipality department variables models; extend to an SDEM model. Repeat with SLX models; how might the changes in the tests of residual autocorrelation in the SLX models be interpreted? How might you interpret the highly significant outcomes of Hausman tests on the SEM and SDEM models?\nFit GNM specifications to the properties-only and the properties with added municipality department variables models; can these models be simplified to say SDM or SDEM representations?\nDo the model estimates reached in the Chapter 16 exercises provide more clarity than those in this chapter?\n\n\n\n\n\n\n\nAnselin, Luc. 1988. Spatial Econometrics: Methods and Models. Kluwer Academic Publishers.\n\n\nBivand, Roger. 2002. “Spatial Econometrics Functions in R: Classes and Methods.” Journal of Geographical Systems 4: 405–21.\n\n\n———. 2012. “After ’Raising the Bar’: Applied Maximum Likelihood Estimation of Families of Models in Spatial Econometrics.” Estadística Española 54: 71–88.\n\n\n———. 2017. “Revisiting the Boston Data Set — Changing the Units of Observation Affects Estimated Willingness to Pay for Clean Air.” REGION 4 (1): 109–27. https://doi.org/10.18335/region.v4i1.107.\n\n\nBivand, Roger, Giovanni Millo, and Gianfranco Piras. 2021. “A Review of Software for Spatial Econometrics in R.” Mathematics 9 (11). https://doi.org/10.3390/math9111276.\n\n\nBivand, Roger, and Gianfranco Piras. 2015. “Comparing Implementations of Estimation Methods for Spatial Econometrics.” Journal of Statistical Software 63 (1): 1–36. https://doi.org/10.18637/jss.v063.i18.\n\n\n———. 2022. Spatialreg: Spatial Regression Analysis. https://CRAN.R-project.org/package=spatialreg.\n\n\nCliff, A. D., and J. K. Ord. 1973. Spatial Autocorrelation. London: Pion.\n\n\n———. 1981. Spatial Processes. London: Pion.\n\n\nElhorst, J. Paul. 2010. “Applied Spatial Econometrics: Raising the Bar.” Spatial Economic Analysis 5: 9–28.\n\n\nFingleton, B. 1999. “Spurious spatial regression: Some Monte Carlo results with a spatial unit root and spatial cointegration.” Journal of Regional Science 9: 1–19.\n\n\nFlorax, Raymond J. G. M., Hendrik Folmer, and Sergio J. Rey. 2006. “A Comment on Specification Searches in Spatial Econometrics: The Relevance of Hendry’s Methodology: A Reply.” Regional Science and Urban Economics 36 (2): 300–308. https://doi.org/10.1016/j.regsciurbeco.2005.10.002.\n\n\nFlorax, Raymond J. G. M, Hendrik Folmer, and Sergio J Rey. 2003. “Specification Searches in Spatial Econometrics: The Relevance of Hendry’s Methodology.” Regional Science and Urban Economics 33 (5): 557–79. https://doi.org/10.1016/S0166-0462(03)00002-4.\n\n\nGómez-Rubio, Virgilio, Roger Bivand, and Håvard Rue. 2015. “A New Latent Class to Fit Spatial Econometrics Models with Integrated Nested Laplace Approximations.” Procedia Environmental Sciences 27: 116–18. https://doi.org/10.1016/j.proenv.2015.07.119.\n\n\nGoulard, Michel, Thibault Laurent, and Christine Thomas-Agnan. 2017. “About Predictions in Spatial Autoregressive Models: Optimal and Almost Optimal Strategies.” Spatial Economic Analysis 12 (2-3): 304–25. https://doi.org/10.1080/17421772.2017.1300679.\n\n\nHalleck Vega, Solmaria, and J. Paul Elhorst. 2015. “The SLX Model.” Journal of Regional Science 55 (3): 339–63. https://doi.org/10.1111/jors.12188.\n\n\nHendry, David F. 2006. “A Comment on ‘Specification Searches in Spatial Econometrics: The Relevance of Hendry’s Methodology’.” Regional Science and Urban Economics 36 (2): 309–12. https://doi.org/10.1016/j.regsciurbeco.2005.10.001.\n\n\nKelejian, Harry, and Gianfranco Piras. 2017. Spatial Econometrics. London: Academic Press.\n\n\nLaurent, Thibault, and Paula Margaretic. 2021. “Predictions in Spatial Econometric Models: Application to Unemployment Data.” In Advances in Contemporary Statistics and Econometrics: Festschrift in Honor of Christine Thomas-Agnan, edited by Abdelaati Daouia and Anne Ruiz-Gazen, 409–26. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-73249-3_21.\n\n\nLeSage, James P. 2014. “What Regional Scientists Need to Know about Spatial Econometrics.” Review of Regional Studies 44: 13–32. https://journal.srsa.org/ojs/index.php/RRS/article/view/44.1.2.\n\n\nLeSage, James P., and Kelley R. Pace. 2009. Introduction to Spatial Econometrics. Boca Raton, FL: CRC Press.\n\n\nMartinetti, Davide, and Ghislain Geniaux. 2017. “Approximate Likelihood Estimation of Spatial Probit Models.” Regional Science and Urban Economics 64: 30–45. https://doi.org/10.1016/j.regsciurbeco.2017.02.002.\n\n\nMcMillen, Daniel P. 2013. Quantile Regression for Spatial Data. Heidelberg: Springer-Verlag.\n\n\nMillo, Giovanni, and Gianfranco Piras. 2012. “splm: Spatial Panel Data Models in R.” Journal of Statistical Software 47 (1): 1–38.\n\n\nMur, Jesús, and Ana Angulo. 2006. “The Spatial Durbin Model and the Common Factor Tests.” Spatial Economic Analysis 1 (2): 207–26. https://doi.org/10.1080/17421770601009841.\n\n\nOrd, J. K. 1975. “Estimation Methods for Models of Spatial Interaction.” Journal of the American Statistical Association 70 (349): 120–26.\n\n\nPace, R. K., and James P. LeSage. 2008. “A Spatial Hausman Test.” Economics Letters 101: 282–84.\n\n\nPiras, Gianfranco, and Ingmar R. Prucha. 2014. “On the Finite Sample Properties of Pre-Test Estimators of Spatial Models.” Regional Science and Urban Economics 46: 103–15. https://doi.org/10.1016/j.regsciurbeco.2014.03.002.\n\n\nSarrias, Mauricio, and Gianfranco Piras. 2022. Spldv: Spatial Models for Limited Dependent Variables. https://CRAN.R-project.org/package=spldv.\n\n\nSmith, Tony E. 2009. “Estimation Bias in Spatial Models with Strongly Connected Weight Matrices.” Geographical Analysis 41 (3): 307–32. https://doi.org/10.1111/j.1538-4632.2009.00758.x.\n\n\nSmith, Tony E., and K. L. Lee. 2012. “The effects of spatial autoregressive dependencies on inference in ordinary least squares: a geometric approach.” Journal of Geographical Systems 14 (January): 91–124. https://doi.org/10.1007/s10109-011-0152-x.\n\n\nSuesse, Thomas. 2018. “Marginal Maximum Likelihood Estimation of SAR Models with Missing Data.” Computational Statistics & Data Analysis 120: 98–110. https://doi.org/10.1016/j.csda.2017.11.004.\n\n\nWagner, Martin, and Achim Zeileis. 2019. “Heterogeneity and Spatial Dependence of Regional Growth in the EU: A Recursive Partitioning Approach.” German Economic Review 20 (1): 67–82. https://doi.org/10.1111/geer.12146.\n\n\nWaller, Lance A., and Carol A. Gotway. 2004. Applied Spatial Statistics for Public Health Data. Hoboken, NJ: John Wiley & Sons.\n\n\nWilhelm, Stefan, and Miguel Godinho de Matos. 2013. “Estimating Spatial Probit Models in R.” The R Journal 5 (1): 130–43. https://doi.org/10.32614/RJ-2013-013.",
    "crumbs": [
      "Models for Spatial Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Spatial Econometrics Models</span>"
    ]
  },
  {
    "objectID": "sp-raster.html",
    "href": "sp-raster.html",
    "title": "Appendix A — Older R Spatial Packages",
    "section": "",
    "text": "A.1 Retiring rgdal and rgeos\nR users who have been around a bit longer, in particular before packages like sf and stars were developed, may be more familiar with older packages like maptools, sp, rgeos, and rgdal. These users need to migrate existing code and/or existing R packages depending on these packages.\nPackages maptools, rgdal, and rgeos retired in 2023. Retirement means that maintenance halts, and that as a consequence the packages has been archived on CRAN. The source code repositories on R-Forge will remain as long as R-Forge does itself. One reason for retirement is that their maintainer has retired, a more important reason that their role has been superseded by the newer packages. We have refrained from seeking new maintainers because much of the code of these packages has gradually evolved along with developments in the GEOS, GDAL, and PROJ libraries, and contains numerous constructs that are outdated and make it forbidding to read.\nWith retirement of rgeos and rgdal, existing ties that package sp had to rgdal and rgeos has been replaced by ties to package sf. This involved for example validation of coordinate reference system identifiers, and checking whether rings are holes or exterior rings. Chosen maptools functions were also be moved to sp.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Older R Spatial Packages</span>"
    ]
  },
  {
    "objectID": "sp-raster.html#links-and-differences-between-sf-and-sp",
    "href": "sp-raster.html#links-and-differences-between-sf-and-sp",
    "title": "Appendix A — Older R Spatial Packages",
    "section": "\nA.2 Links and differences between sf and sp",
    "text": "A.2 Links and differences between sf and sp\n\nThere are a number of differences between sf and sp. The most notable is that sp classes are formal, S4 classes where sf uses the (more) informal S3 class hierarchy. sf objects are derived from data.frames or tibbles and because of that are more readily interfaceable with much of the existing R ecosystem, especially with the tidyverse package family. sf objects keep geometry in a list-column, meaning that a geometry is always a list element. Package sp used data structures much less strictly, and for instance all coordinates of SpatialPoints or SpatialPixels are kept in matrices, which is much more performant for certain problems but is not possible with a list-column. Conversion from an sf object x to its sp equivalent is done by\n\nlibrary(sp)\ny = as(x, \"Spatial\")\n\nand the conversion the other way around is done by\n\nx0 = st_as_sf(y)\n\nThere are some limitations to conversions like this:\n\n\nsp does not distinguish between LINESTRING and MULTILINESTRING geometries, or between POLYGON or MULTIPOLYGON. For example, a LINESTRING will after conversion to sp come back as a MULTILINESTRING\n\n\nsp does not have a representation for GEOMETRYCOLLECTION geometries, or sf objects with geometry types not in the “big seven” (Section 3.1.1)\n\nsf or sfc objects of geometry type GEOMETRY, with mixed geometry types, cannot be converted into sp objects\nattribute-geometry relationship attributes get lost when converting to sp\n\n\nsf objects with more than one geometry list-column will, when converting to sp, lose their secondary list-column(s).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Older R Spatial Packages</span>"
    ]
  },
  {
    "objectID": "sp-raster.html#migration-code-and-packages",
    "href": "sp-raster.html#migration-code-and-packages",
    "title": "Appendix A — Older R Spatial Packages",
    "section": "\nA.3 Migration code and packages",
    "text": "A.3 Migration code and packages\n\nThe wiki page of the GitHub site for sf, found at https://github.com/r-spatial/sf/wiki/Migrating contains a list of methods and functions in rgeos, rgdal, and sp and the corresponding sf method or function. This may help converting existing code or packages.\nA simple approach to migrate code is when only rgdal::readOGR is used to read file. As an alternative, one might use\n\nx = as(sf::read_sf(\"file\"), \"Spatial\")\n\nhowever possible arguments to readOGR, when used, would need more care. An effort by us is underway to convert all code of our earlier book Applied Spatial Data Analysis with R (with Virgilio Gómez-Rubio, Bivand, Pebesma, and Gómez-Rubio (2013)) to run entirely without rgdal, rgeos, and maptools and where possible without sp. The scripts are found at https://github.com/rsbivand/sf_asdar2ed.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Older R Spatial Packages</span>"
    ]
  },
  {
    "objectID": "sp-raster.html#packages-raster-and-terra",
    "href": "sp-raster.html#packages-raster-and-terra",
    "title": "Appendix A — Older R Spatial Packages",
    "section": "\nA.4 Packages raster and terra",
    "text": "A.4 Packages raster and terra\n \nPackage raster has been a workhorse package for analysing raster data with R since 2010, and has since grown into a package for “Geographic Data Analysis and Modeling” (Hijmans 2023a), indicating that it is used for all kinds of spatial data. The raster package uses sp objects for vector data, and terra to read and write data to formats served by the GDAL library. Its successor package terra, for “Spatial Data Analysis” (Hijmans 2023b), “is very similar to the raster package; but […] can do more, is easier to use, and […] is faster”. The terra package comes with its own classes for vector data, but accepts many sf objects, with similar restrictions as listed above for conversion to sp. Package terra has its own direct links to GDAL, GEOS, and PROJ, so, no longer needs other packages for that.\nRaster maps, or stacks of them from package raster or terra can be converted to stars objects using st_as_stars(). Package sf contains an st_as_sf() method for SpatVector objects from package terra.\nThe online book Spatial Data Science with R and “terra”, written by Robert Hijmans and found at https://rspatial.org/terra details the terra approach to spatial data analysis. Package sf and stars and several other r-spatial packages discussed in this book reside on the r-spatial GitHub organisation (note the hyphen between r and spatial, which is absent on Hijmans’ organisation), which has a blog site, with links to this book, found at https://r-spatial.org/book.\nPackages sf and stars on one hand and terra on the other have many goals in common, but try to reach them in slightly different ways, emphasising different aspects of data analysis, software engineering, and community management. Although this may confuse some users, we believe that these differences enrich the R package ecosystem, are beneficial to users, encourage diversity and choice, and hopefully work as an encouragement for others to continue trying out new ideas when using R for spatial data problems, and to help carrying the R spatial flag.\n\n\n\n\n\n\nBivand, Roger, Edzer Pebesma, and Virgilio Gómez-Rubio. 2013. Applied Spatial Data Analysis with R, Second Edition. Springer, NY. http://www.asdar-book.org/.\n\n\nHijmans, Robert J. 2023a. Raster: Geographic Data Analysis and Modeling. https://rspatial.org/raster.\n\n\n———. 2023b. Terra: Spatial Data Analysis. https://rspatial.org/terra/.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Older R Spatial Packages</span>"
    ]
  },
  {
    "objectID": "rbasics.html",
    "href": "rbasics.html",
    "title": "Appendix B — R Basics",
    "section": "",
    "text": "B.1 Pipes\nThis chapter provides some minimal R basics that may make it easier to read this book. A more comprehensive book on R basics is given in Wickham (2014), chapter 2.\nThe |&gt; (pipe) symbols should be read as then: we read\na |&gt; b() |&gt; c() |&gt; d(n = 10)\nas with a do b, then c, then d with n being 10, and that is just alternative syntax for\nd(c(b(a)), n = 10)\nor\ntmp1 &lt;- b(a)\ntmp2 &lt;- c(tmp1)\nd(tmp2, n = 10)\nTo many, the pipe-form is easier to read because execution order follows reading order, from left to right. Like nested function calls, it avoids the need to choose names for intermediate results. As with nested function calls, it is hard to debug intermediate results that diverge from our expectations. Note that the intermediate results do exist in memory, so neither form saves memory allocation. The |&gt; native pipe that appeared in R 4.1.0 as used in this book, can be safely substituted by the %&gt;% pipe of package magrittr.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "rbasics.html#sec-datastructures",
    "href": "rbasics.html#sec-datastructures",
    "title": "Appendix B — R Basics",
    "section": "\nB.2 Data structures",
    "text": "B.2 Data structures\n\nAs pointed out by Chambers (2016), everything that exists in R is an object. This includes objects that make things happen, such as language objects or functions, but also the more basic “things”, such as data objects. Some basic R data structures will now be discussed.\nHomogeneous vectors\n\nData objects contain data, and possibly metadata. Data is always in the form of a vector, which can have different types. We can find the type by typeof, and vector length by length. Vectors are created by c, which combines individual elements:\n\ntypeof(1:10)\n# [1] \"integer\"\nlength(1:10)\n# [1] 10\ntypeof(1.0)\n# [1] \"double\"\nlength(1.0)\n# [1] 1\ntypeof(c(\"foo\", \"bar\"))\n# [1] \"character\"\nlength(c(\"foo\", \"bar\"))\n# [1] 2\ntypeof(c(TRUE, FALSE))\n# [1] \"logical\"\n\nVectors of this kind can only have a single type.\nNote that vectors can have zero length:\n\ni &lt;- integer(0)\ntypeof(i)\n# [1] \"integer\"\ni\n# integer(0)\nlength(i)\n# [1] 0\n\n\nWe can retrieve (or in assignments: replace) elements in a vector using [ or [[:\n\na &lt;- c(1,2,3)\na[2]\n# [1] 2\na[[2]]\n# [1] 2\na[2:3]\n# [1] 2 3\na[2:3] &lt;- c(5,6)\na\n# [1] 1 5 6\na[[3]] &lt;- 10\na\n# [1]  1  5 10\n\nwhere the difference is that [ can operate on an index range (or multiple indexes), and [[ operates on a single vector value.\nHeterogeneous vectors: list\n\n\nAn additional vector type is the list, which can combine any types in its elements:\n\nl &lt;- list(3, TRUE, \"foo\")\ntypeof(l)\n# [1] \"list\"\nlength(l)\n# [1] 3\n\nFor lists, there is a further distinction between [ and [[: the single [ returns always a list, and [[ returns the contents of a list element:\n\nl[1]\n# [[1]]\n# [1] 3\nl[[1]]\n# [1] 3\n\nFor replacement, one case use [ when providing a list, and [[ when providing a new value:\n\nl[1:2] &lt;- list(4, FALSE)\nl\n# [[1]]\n# [1] 4\n# \n# [[2]]\n# [1] FALSE\n# \n# [[3]]\n# [1] \"foo\"\nl[[3]] &lt;- \"bar\"\nl\n# [[1]]\n# [1] 4\n# \n# [[2]]\n# [1] FALSE\n# \n# [[3]]\n# [1] \"bar\"\n\nIn case list elements are named, as in\n\nl &lt;- list(first = 3, second = TRUE, third = \"foo\")\nl\n# $first\n# [1] 3\n# \n# $second\n# [1] TRUE\n# \n# $third\n# [1] \"foo\"\n\nwe can use names as in l[[\"second\"]] and this can be abbreviated to\n\nl$second\n# [1] TRUE\nl$second &lt;- FALSE\nl\n# $first\n# [1] 3\n# \n# $second\n# [1] FALSE\n# \n# $third\n# [1] \"foo\"\n\nThis is convenient, but it also requires name look-up in the names attribute (see below).\nNULL and removing list elements\n\nNULL is the null value in R; it is special in the sense that it doesn’t work in simple comparisons:\n\n3 == NULL # not FALSE!\n# logical(0)\nNULL == NULL # not even TRUE!\n# logical(0)\n\nbut has to be treated specially, using is.null:\n\nis.null(NULL)\n# [1] TRUE\n\nWhen we want to remove one or more list elements, we can do so by creating a new list that does not contain the elements that needed removal, as in\n\nm &lt;- l[c(1,3)] # remove second, implicitly\nm\n# $first\n# [1] 3\n# \n# $third\n# [1] \"foo\"\n\nbut we can also assign NULL to the element we want to eliminate:\n\nl$second &lt;- NULL\nl\n# $first\n# [1] 3\n# \n# $third\n# [1] \"foo\"\n\nAttributes\n\nWe can glue arbitrary metadata objects to data objects, as in\n\na &lt;- 1:3\nattr(a, \"some_meta_data\") = \"foo\"\na\n# [1] 1 2 3\n# attr(,\"some_meta_data\")\n# [1] \"foo\"\n\nand this can be retrieved, or replaced by\n\nattr(a, \"some_meta_data\")\n# [1] \"foo\"\nattr(a, \"some_meta_data\") &lt;- \"bar\"\nattr(a, \"some_meta_data\")\n# [1] \"bar\"\n\nIn essence, the attribute of an object is a named list, and we can get or set the complete list by\n\nattributes(a)\n# $some_meta_data\n# [1] \"bar\"\nattributes(a) = list(some_meta_data = \"foo\")\nattributes(a)\n# $some_meta_data\n# [1] \"foo\"\n\nA number of attributes are treated specially by R, see ?attributes for full details. Some of the special attributes will now be explained.\nObject class and class attribute\n \nEvery object in R “has a class”, meaning that class(obj) returns a character vector with the class of obj. Some objects have an implicit class, such as basic vectors\n\nclass(1:3)\n# [1] \"integer\"\nclass(c(TRUE, FALSE))\n# [1] \"logical\"\nclass(c(\"TRUE\", \"FALSE\"))\n# [1] \"character\"\n\nbut we can also set the class explicitly, either by using attr or by using class in the left-hand side of an expression:\n\na &lt;- 1:3\nclass(a) &lt;- \"foo\"\na\n# [1] 1 2 3\n# attr(,\"class\")\n# [1] \"foo\"\nclass(a)\n# [1] \"foo\"\nattributes(a)\n# $class\n# [1] \"foo\"\n\nin which case the newly set class overrides the earlier implicit class. This way, we can add methods for class foo by appending the class name to the method name:\n\nprint.foo &lt;- function(x, ...) { \n    print(paste(\"an object of class foo with length\", length(x)))\n}\na # when not assigned, a is printed:\n# [1] \"an object of class foo with length 3\"\n\nProviding such methods is generally intended to create more usable software, but at the same time they may make the objects more opaque. It is sometimes useful to see what an object “is made of” by printing it after the class attribute is removed, as in\n\nunclass(a)\n# [1] 1 2 3\n\nAs a more elaborate example, consider the case where a polygon is made using package sf:\n\nlibrary(sf) |&gt; suppressPackageStartupMessages()\np &lt;- st_polygon(list(rbind(c(0,0), c(1,0), c(1,1), c(0,0))))\np\n# POLYGON ((0 0, 1 0, 1 1, 0 0))\n\nwhich prints the well-known-text form; to understand what the data structure is like, we can use\n\nunclass(p)\n# [[1]]\n#      [,1] [,2]\n# [1,]    0    0\n# [2,]    1    0\n# [3,]    1    1\n# [4,]    0    0\n\nThe dim attribute\n \nThe dim attribute sets the matrix or array dimensions:\n\na &lt;- 1:8\nclass(a)\n# [1] \"integer\"\nattr(a, \"dim\") &lt;- c(2,4) # or: dim(a) = c(2,4)\nclass(a)\n# [1] \"matrix\" \"array\"\na\n#      [,1] [,2] [,3] [,4]\n# [1,]    1    3    5    7\n# [2,]    2    4    6    8\nattr(a, \"dim\") &lt;- c(2,2,2) # or: dim(a) = c(2,2,2)\nclass(a)\n# [1] \"array\"\na\n# , , 1\n# \n#      [,1] [,2]\n# [1,]    1    3\n# [2,]    2    4\n# \n# , , 2\n# \n#      [,1] [,2]\n# [1,]    5    7\n# [2,]    6    8\n\nThe names attributes\nNamed vectors carry their names in a names attribute. We saw examples for lists above, an example for a numeric vector is:\n\na &lt;- c(first = 3, second = 4, last = 5)\na[\"second\"]\n# second \n#      4\nattributes(a)\n# $names\n# [1] \"first\"  \"second\" \"last\"\n\nOther name attributes include dimnames for matrix or array, which not only names dimensions but also the labels associated values of each of the dimensions:\n\na &lt;- matrix(1:4, 2, 2)\ndimnames(a) &lt;- list(rows = c(\"row1\", \"row2\"),\n                    cols = c(\"col1\", \"col2\"))\na\n#       cols\n# rows   col1 col2\n#   row1    1    3\n#   row2    2    4\nattributes(a)\n# $dim\n# [1] 2 2\n# \n# $dimnames\n# $dimnames$rows\n# [1] \"row1\" \"row2\"\n# \n# $dimnames$cols\n# [1] \"col1\" \"col2\"\n\n\nData.frame objects have rows and columns, and each has names:\n\ndf &lt;- data.frame(a = 1:3, b = c(TRUE, FALSE, TRUE))\nattributes(df)\n# $names\n# [1] \"a\" \"b\"\n# \n# $class\n# [1] \"data.frame\"\n# \n# $row.names\n# [1] 1 2 3\n\nUsing structure\n\n\nWhen programming, the pattern of adding or modifying attributes before returning an object is extremely common, an example being:\n\nf &lt;- function(x) {\n   a &lt;- create_obj(x) # call some other function\n   attributes(a) &lt;- list(class = \"foo\", meta = 33)\n   a\n}\n\nThe last two statements can be contracted in\n\nf &lt;- function(x) {\n   a &lt;- create_obj(x) # call some other function\n   structure(a, class = \"foo\", meta = 33)\n}\n\nwhere function structure adds, replaces, or (in case of value NULL) removes attributes from the object in its first argument.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "rbasics.html#sec-dissecting",
    "href": "rbasics.html#sec-dissecting",
    "title": "Appendix B — R Basics",
    "section": "\nB.3 Dissecting a MULTIPOLYGON\n",
    "text": "B.3 Dissecting a MULTIPOLYGON\n\nWe can use the above examples to dissect an sf object with MULTIPOLYGONs into pieces. Suppose we use the nc dataset,\n\nsystem.file(\"gpkg/nc.gpkg\", package = \"sf\") %&gt;%\n    read_sf() -&gt; nc\n\nwe can see from the attributes of nc,\n\nattributes(nc)\n# $names\n#  [1] \"AREA\"      \"PERIMETER\" \"CNTY_\"     \"CNTY_ID\"   \"NAME\"     \n#  [6] \"FIPS\"      \"FIPSNO\"    \"CRESS_ID\"  \"BIR74\"     \"SID74\"    \n# [11] \"NWBIR74\"   \"BIR79\"     \"SID79\"     \"NWBIR79\"   \"geom\"     \n# \n# $row.names\n#   [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15\n#  [16]  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30\n#  [31]  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45\n#  [46]  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60\n#  [61]  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75\n#  [76]  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n#  [91]  91  92  93  94  95  96  97  98  99 100\n# \n# $class\n# [1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n# \n# $sf_column\n# [1] \"geom\"\n# \n# $agr\n#      AREA PERIMETER     CNTY_   CNTY_ID      NAME      FIPS \n#      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt; \n#    FIPSNO  CRESS_ID     BIR74     SID74   NWBIR74     BIR79 \n#      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt; \n#     SID79   NWBIR79 \n#      &lt;NA&gt;      &lt;NA&gt; \n# Levels: constant aggregate identity\n\nthat the geometry column is named geom. When we take out this column,\n\nnc$geom\n# Geometry set for 100 features \n# Geometry type: MULTIPOLYGON\n# Dimension:     XY\n# Bounding box:  xmin: -84.3 ymin: 33.9 xmax: -75.5 ymax: 36.6\n# Geodetic CRS:  NAD27\n# First 5 geometries:\n# MULTIPOLYGON (((-81.5 36.2, -81.5 36.3, -81.6 3...\n# MULTIPOLYGON (((-81.2 36.4, -81.2 36.4, -81.3 3...\n# MULTIPOLYGON (((-80.5 36.2, -80.5 36.3, -80.5 3...\n# MULTIPOLYGON (((-76 36.3, -76 36.3, -76 36.3, -...\n# MULTIPOLYGON (((-77.2 36.2, -77.2 36.2, -77.3 3...\n\nwe see an object that has the following attributes\n\nattributes(nc$geom)\n# $n_empty\n# [1] 0\n# \n# $crs\n# Coordinate Reference System:\n#   User input: NAD27 \n#   wkt:\n# GEOGCRS[\"NAD27\",\n#     DATUM[\"North American Datum 1927\",\n#         ELLIPSOID[\"Clarke 1866\",6378206.4,294.978698213898,\n#             LENGTHUNIT[\"metre\",1]]],\n#     PRIMEM[\"Greenwich\",0,\n#         ANGLEUNIT[\"degree\",0.0174532925199433]],\n#     CS[ellipsoidal,2],\n#         AXIS[\"geodetic latitude (Lat)\",north,\n#             ORDER[1],\n#             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#         AXIS[\"geodetic longitude (Lon)\",east,\n#             ORDER[2],\n#             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#     USAGE[\n#         SCOPE[\"Geodesy.\"],\n#         AREA[\"North and central America: Antigua and Barbuda - onshore. Bahamas - onshore plus offshore over internal continental shelf only. Belize - onshore. British Virgin Islands - onshore. Canada onshore - Alberta, British Columbia, Manitoba, New Brunswick, Newfoundland and Labrador, Northwest Territories, Nova Scotia, Nunavut, Ontario, Prince Edward Island, Quebec, Saskatchewan and Yukon - plus offshore east coast. Cuba - onshore and offshore. El Salvador - onshore. Guatemala - onshore. Honduras - onshore. Panama - onshore. Puerto Rico - onshore. Mexico - onshore plus offshore east coast. Nicaragua - onshore. United States (USA) onshore and offshore - Alabama, Alaska, Arizona, Arkansas, California, Colorado, Connecticut, Delaware, Florida, Georgia, Idaho, Illinois, Indiana, Iowa, Kansas, Kentucky, Louisiana, Maine, Maryland, Massachusetts, Michigan, Minnesota, Mississippi, Missouri, Montana, Nebraska, Nevada, New Hampshire, New Jersey, New Mexico, New York, North Carolina, North Dakota, Ohio, Oklahoma, Oregon, Pennsylvania, Rhode Island, South Carolina, South Dakota, Tennessee, Texas, Utah, Vermont, Virginia, Washington, West Virginia, Wisconsin and Wyoming - plus offshore . US Virgin Islands - onshore.\"],\n#         BBOX[7.15,167.65,83.17,-47.74]],\n#     ID[\"EPSG\",4267]]\n# \n# $class\n# [1] \"sfc_MULTIPOLYGON\" \"sfc\"             \n# \n# $precision\n# [1] 0\n# \n# $bbox\n#  xmin  ymin  xmax  ymax \n# -84.3  33.9 -75.5  36.6\n\nWhen we take the contents of the fourth list element, we obtain\n\nnc$geom[[4]] |&gt; format(width = 60, digits = 5)\n# [1] \"MULTIPOLYGON (((-76.009 36.32, -76.017 36.338, -76.033 36...\"\n\nwhich is a (classed) list,\n\ntypeof(nc$geom[[4]])\n# [1] \"list\"\n\nwith attributes\n\nattributes(nc$geom[[4]])\n# $class\n# [1] \"XY\"           \"MULTIPOLYGON\" \"sfg\"\n\nand length\n\nlength(nc$geom[[4]])\n# [1] 3\n\nThe length indicates the number of outer rings: a multi-polygon can consist of more than one polygon. We see that most counties only have a single polygon:\n\nlengths(nc$geom)\n#   [1] 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#  [32] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 2 1 1 1 1 1\n#  [63] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1\n#  [94] 1 2 1 1 1 1 1\n\nA multi-polygon is a list with polygons,\n\ntypeof(nc$geom[[4]])\n# [1] \"list\"\n\nand the first polygon of the fourth multi-polygon is again a list, because polygons have an outer ring possibly followed by multiple inner rings (holes)\n\ntypeof(nc$geom[[4]][[1]])\n# [1] \"list\"\n\nwe see that it contains only one ring, the exterior ring:\n\nlength(nc$geom[[4]][[1]])\n# [1] 1\n\nand we can print type, the dimension and the first set of coordinates by\n\ntypeof(nc$geom[[4]][[1]][[1]])\n# [1] \"double\"\ndim(nc$geom[[4]][[1]][[1]])\n# [1] 26  2\nhead(nc$geom[[4]][[1]][[1]])\n#       [,1] [,2]\n# [1,] -76.0 36.3\n# [2,] -76.0 36.3\n# [3,] -76.0 36.3\n# [4,] -76.0 36.4\n# [5,] -76.1 36.3\n# [6,] -76.2 36.4\n\nand we can now for instance change the latitude of the third coordinate by\n\nnc$geom[[4]][[1]][[1]][3,2] &lt;- 36.5\n\n\n\n\n\n\n\nChambers, John. 2016. Extending R. CRC Press.\n\n\nWickham, Hadley. 2014. Advanced R, Second Edition. CRC Press.https://adv-r.hadley.nz/ .",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "ERRATA.html",
    "href": "ERRATA.html",
    "title": "Appendix C — Errata to the printed ed.",
    "section": "",
    "text": "ch 7, exercise 8, the final exact=FALSE should be exact=TRUE",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Errata to the printed ed.</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Alam, Moudud, Lars Ronnegard, and Xia Shen. 2019. Hglm: Hierarchical\nGeneralized Linear Models. https://CRAN.R-project.org/package=hglm.\n\n\nAlam, Moudud, Lars Rönnegård, and Xia Shen. 2015. “Fitting\nConditional and Simultaneous Autoregressive Spatial Models in\nHglm.” The R Journal 7 (2): 5–18. https://doi.org/10.32614/RJ-2015-017.\n\n\nAnselin, Luc. 1988. Spatial Econometrics: Methods and Models.\nKluwer Academic Publishers.\n\n\n———. 1995. “Local indicators of spatial\nassociation - LISA.” Geographical\nAnalysis 27 (2): 93–115.\n\n\n———. 1996. “The Moran Scatterplot as an\nESDA Tool to Assess Local Instability in Spatial\nAssociation.” In Spatial Analytical Perspectives on\nGIS, edited by M. M. Fischer, H. J. Scholten, and D.\nUnwin, 111–25. London: Taylor & Francis.\n\n\n———. 2019. “A Local Indicator of Multivariate Spatial Association:\nExtending Geary’s c.” Geographical Analysis\n51 (2): 133–50. https://doi.org/10.1111/gean.12164.\n\n\nAnselin, Luc, Xun Li, and Julia Koschinsky. 2021.\n“GeoDa, from the Desktop to an Ecosystem for\nExploring Spatial Data.” Geographical Analysis. https://doi.org/10.1111/gean.12311.\n\n\nAppel, Marius. 2023. Gdalcubes: Earth Observation Data Cubes from\nSatellite Image Collections. https://github.com/appelmar/gdalcubes_R.\n\n\nAppel, Marius, and Edzer Pebesma. 2019. “On-Demand Processing of\nData Cubes from Satellite Image Collections with the Gdalcubes\nLibrary.” Data 4 (3): 92. https://www.mdpi.com/2306-5729/4/3/92.\n\n\nAppel, Marius, Edzer Pebesma, and Matthias Mohr. 2021. Cloud-Based\nProcessing of Satellite Image Collections in R Using\nSTAC, COGs, and on-Demand Data Cubes. https://r-spatial.org/r/2021/04/23/cloud-based-cubes.html.\n\n\nAppelhans, Tim, Florian Detsch, Christoph Reudenbach, and Stefan\nWoellauer. 2022. Mapview: Interactive Viewing of Spatial Data in\nr. https://github.com/r-spatial/mapview.\n\n\nAssunção, R. M., and E. A. Reis. 1999. “A New Proposal to Adjust\nMoran’s I for Population\nDensity.” Statistics in Medicine 18: 2147–62.\n\n\nAvis, D., and J. Horton. 1985. “Remarks on the Sphere of Influence\nGraph.” In Discrete Geometry and Convexity, edited by J.\nE. Goodman, 323–27. New York: New York Academy of Sciences, New York.\n\n\nAybar, Cesar. 2022. Rgee: R Bindings for Calling the Earth Engine\nAPI. https://CRAN.R-project.org/package=rgee.\n\n\nBaddeley, Adrian, Ege Rubak, and Rolf Turner. 2015. Spatial Point\nPatterns: Methodology and Applications with R. Chapman\n& Hall/CRC.\n\n\nBaddeley, Adrian, Rolf Turner, and Ege Rubak. 2022. Spatstat:\nSpatial Point Pattern Analysis, Model-Fitting, Simulation, Tests.\nhttp://spatstat.org/.\n\n\nBates, Douglas, Martin Maechler, Ben Bolker, and Steven Walker. 2022.\nLme4: Linear Mixed-Effects Models Using Eigen and S4. https://github.com/lme4/lme4/.\n\n\nBates, Douglas, Martin Maechler, and Mikael Jagan. 2022. Matrix:\nSparse and Dense Matrix Classes and Methods. https://CRAN.R-project.org/package=Matrix.\n\n\nBaumann, Peter, Eric Hirschorn, and Joan Masó. 2017. “OGC Coverage\nImplementation Schema.” OGC Implementation Standard. https://docs.opengeospatial.org/is/09-146r6/09-146r6.html.\n\n\nBavaud, F. 1998. “Models for Spatial Weights: A Systematic\nLook.” Geographical Analysis 30: 153–71. https://doi.org/10.1111/j.1538-4632.1998.tb00394.x.\n\n\nBecker, Marc, and Patrick Schratz. 2022. Mlr3spatial: Support for\nSpatial Objects Within the Mlr3 Ecosystem. https://CRAN.R-project.org/package=mlr3spatial.\n\n\nBenjamin, Daniel J., James O. Berger, Johannesson Magnus, Brian A.\nNosek, Wagenmakers E-J, Richard Berk, Kenneth A. Bollen, et al. 2018.\n“Redefine Statistical Significance.” Nature Human\nBehaviour 2 (1): 6–10.\n\n\nBenjamini, Yoav, and Yosef Hochberg. 1995. “Controlling the False\nDiscovery Rate: A Practical and Powerful Approach to Multiple\nTesting.” Journal of the Royal Statistical Society. Series B\n(Methodological) 57 (1): 289–300. https://doi.org/10.1111/j.2517-6161.1995.tb02031.x.\n\n\nBenjamini, Yoav, and Daniel Yekutieli. 2001. “The control of the false discovery rate in multiple\ntesting under dependency.” The Annals of\nStatistics 29 (4): 1165–88. https://doi.org/10.1214/aos/1013699998.\n\n\nBesag, Julian. 1974. “Spatial Interaction and the Statistical\nAnalysis of Lattice Systems.” Journal of the Royal\nStatistical Society. Series B (Methodological) 36: pp. 192–236.\n\n\nBIPM, IEC, ILAC IFCC, IUPAP IUPAC, and OIML ISO. 2012. “The\nInternational Vocabulary of Metrology–Basic and General Concepts and\nAssociated Terms (VIM), 3rd Edn. JCGM 200: 2012.” JCGM (Joint\nCommittee for Guides in Metrology). https://www.bipm.org/en/publications/guides/.\n\n\nBivand, Roger. 2002. “Spatial Econometrics Functions in\nR: Classes and Methods.” Journal of Geographical\nSystems 4: 405–21.\n\n\n———. 2012. “After ’Raising the Bar’: Applied\nMaximum Likelihood Estimation of Families of Models in Spatial\nEconometrics.” Estadística Española 54: 71–88.\n\n\n———. 2017. “Revisiting the Boston Data Set — Changing\nthe Units of Observation Affects Estimated Willingness to Pay for Clean\nAir.” REGION 4 (1): 109–27. https://doi.org/10.18335/region.v4i1.107.\n\n\n———. 2020. Why Have CRS, Projections and Transformations\nChanged?https://rgdal.r-forge.r-project.org/articles/CRS_projections_transformations.html\n.\n\n\n———. 2022a. classInt: Choose Univariate Class Intervals. https://CRAN.R-project.org/package=classInt.\n\n\n———. 2022b. “R Packages for Analyzing Spatial Data: A Comparative\nCase Study with Areal Data.” Geographical Analysis 54\n(3): 488–518. https://doi.org/10.1111/gean.12319.\n\n\n———. 2022c. Spdep: Spatial Dependence: Weighting Schemes,\nStatistics.\n\n\nBivand, Roger, and Virgilio Gómez-Rubio. 2021. “Spatial Survival\nModelling of Business Re-Opening After Katrina: Survival Modelling\nCompared to Spatial Probit Modelling of Re-Opening Within 3, 6 or 12\nMonths.” Statistical Modelling 21 (1-2): 137–60. https://doi.org/10.1177/1471082X20967158.\n\n\nBivand, Roger, Virgilio Gómez-Rubio, and Håvard Rue. 2015.\n“Spatial Data Analysis with r-INLA with Some Extensions.”\nJournal of Statistical Software, Articles 63 (20): 1–31. https://doi.org/10.18637/jss.v063.i20.\n\n\nBivand, Roger, Giovanni Millo, and Gianfranco Piras. 2021. “A\nReview of Software for Spatial Econometrics in R.”\nMathematics 9 (11). https://doi.org/10.3390/math9111276.\n\n\nBivand, Roger, W. Müller, and M. Reder. 2009. “Power Calculations\nfor Global and Local Moran’s I.” Computational\nStatistics and Data Analysis 53: 2859–72.\n\n\nBivand, Roger, Jakub Nowosad, and Robin Lovelace. 2022. spData:\nDatasets for Spatial Analysis. https://jakubnowosad.com/spData/.\n\n\nBivand, Roger, Edzer Pebesma, and Virgilio Gómez-Rubio. 2013.\nApplied Spatial Data Analysis with R, Second\nEdition. Springer, NY. http://www.asdar-book.org/.\n\n\nBivand, Roger, and Gianfranco Piras. 2015. “Comparing\nImplementations of Estimation Methods for Spatial Econometrics.”\nJournal of Statistical Software 63 (1): 1–36. https://doi.org/10.18637/jss.v063.i18.\n\n\n———. 2022. Spatialreg: Spatial Regression Analysis. https://CRAN.R-project.org/package=spatialreg.\n\n\nBivand, Roger, and B. A. Portnov. 2004. “Exploring Spatial Data\nAnalysis Techniques Using R: The Case of Observations with\nNo Neighbours.” In Advances in Spatial Econometrics:\nMethodology, Tools, Applications, edited by Luc Anselin, Raymond J.\nG. M. Florax, and S. J. Rey, 121–42. Berlin: Springer.\n\n\nBivand, Roger, Zhe Sha, Liv Osland, and Ingrid Sandvig Thorsen. 2017.\n“A Comparison of Estimation Methods for Multilevel Models of\nSpatially Structured Data.” Spatial Statistics. https://doi.org/10.1016/j.spasta.2017.01.002.\n\n\nBivand, Roger, and David W. S. Wong. 2018. “Comparing\nImplementations of Global and Local Indicators of Spatial\nAssociation.” TEST 27 (3): 716–48. https://doi.org/10.1007/s11749-018-0599-x.\n\n\nBlangiardo, Marta, and Michela Cameletti. 2015. Spatial and\nSpatio-Temporal Bayesian Models with r-INLA. John Wiley & Sons.\n\n\nBoots, B., and A. Okabe. 2007. “Local Statistical Spatial\nAnalysis: Inventory and Prospect.” International Journal of\nGeographical Information Science 21 (4): 355–75. https://doi.org/10.1080/13658810601034267.\n\n\nBreidt, F Jay, Jean D Opsomer, et al. 2017. “Model-Assisted Survey\nEstimation with Modern Prediction Techniques.” Statistical\nScience 32 (2): 190–205.\n\n\nBrody, Howard, Michael Russell Rip, Peter Vinten-Johansen, Nigel Paneth,\nand Stephen Rachman. 2000. “Map-Making and Myth-Making in\nBroad Street: The London Cholera\nEpidemic, 1854.” The Lancet 356 (9223): 64–68. https://doi.org/10.1016/S0140-6736(00)02442-9.\n\n\nBrooks, Mollie E., Kasper Kristensen, Koen J. van Benthem, Arni\nMagnusson, Casper W. Berg, Anders Nielsen, Hans J. Skaug, Martin\nMaechler, and Benjamin M. Bolker. 2017. “glmmTMB Balances Speed and Flexibility Among\nPackages for Zero-Inflated Generalized Linear Mixed Modeling.”\nThe R Journal 9 (2): 378–400. https://journal.r-project.org/archive/2017/RJ-2017-066/index.html.\n\n\nBrown, C. F., Steven P Brumby, Brookie Guzder-Williams, Tanya Birch,\nSamantha Brooks Hyde, Joseph Mazzariello, Wanda Czerwinski, et al. 2022.\n“Dynamic World, Near Real-Time Global 10 m Land Use Land Cover\nMapping.” Scientific Data 9 (1): 1–17. https://doi.org/10.1038/s41597-022-01307-4.\n\n\nBrown, P. G. 2010. “Overview of SciDB: Large Scale\nArray Storage, Processing and Analysis.” In Proceedings of\nthe 2010 ACM SIGMOD International Conference\non Management of Data, 963–68. ACM.\n\n\nBrus, Dick J. 2021a. “Statistical Approaches for Spatial Sample\nSurvey: Persistent Misconceptions and New Developments.”\nEuropean Journal of Soil Science 72 (2): 686–703. https://doi.org/10.1111/ejss.12988.\n\n\n———. 2021b. “Statistical Approaches for Spatial Sample Survey:\nPersistent Misconceptions and New Developments.” European\nJournal of Soil Science 72 (2): 686–703. https://doi.org/10.1111/ejss.12988.\n\n\nBureau International des Poids et Mesures. 2006. The International\nSystem of Units (SI), 8th Edition. Organisation\nIntergouvernementale de la Convention du Mètre. https://www.bipm.org/en/publications/si-brochure/download.html.\n\n\nButler, H., M. Daly, A. Doyl, S. Gillies, S. Hagen, and T. Schaub. 2016.\n“The GeoJSON Format.” Vol. Request for Comments: 7946.\nInternet Engineering Task Force (IETF). https://tools.ietf.org/html/rfc7946.\n\n\nCaldas de Castro, Marcia, and Burton H. Singer. 2006. “Controlling\nthe False Discovery Rate: A New Application to Account for Multiple and\nDependent Tests in Local Statistics of Spatial Association.”\nGeographical Analysis 38 (2): 180–208. https://doi.org/10.1111/j.0016-7363.2006.00682.x.\n\n\nChambers, John. 2016. Extending R. CRC Press.\n\n\nChrisman, Nicholas. 2012. “A Deflationary Approach to Fundamental\nPrinciples in GIScience.” In Francis Harvey\n(Ed.) Are There Fundamental Principles in Geographic Information\nScience?, 42–64. CreateSpace, United States.\n\n\nClementini, Eliseo, Paolino Di Felice, and Peter van Oosterom. 1993.\n“A Small Set of Formal Topological Relationships Suitable for\nEnd-User Interaction.” In Advances in Spatial Databases,\nedited by David Abel and Beng Chin Ooi, 277–95. Berlin, Heidelberg:\nSpringer Berlin Heidelberg.\n\n\nCliff, A. D., and J. K. Ord. 1972. “Testing for Spatial\nAutocorrelation Among Regression Residuals.” Geographical\nAnalysis 4: 267–84.\n\n\n———. 1973. Spatial Autocorrelation. London: Pion.\n\n\n———. 1981. Spatial Processes. London: Pion.\n\n\nCobb, George W., and David S. Moore. 1997. “Mathematics,\nStatistics and Teaching.” The American Mathematical\nMonthly 104: 801–23. https://www.jstor.org/stable/2975286.\n\n\nCollins, Sarah N., Robert S. James, Pallav Ray, Katherine Chen, Angie\nLassman, and James Brownlee. 2013. “Grids in Numerical Weather and\nClimate Models.” In Climate Change and Regional/Local\nResponses, edited by Yuanzhi Zhang and Pallav Ray. Rijeka:\nIntechOpen. https://doi.org/10.5772/55922.\n\n\nCressie, N. A. C. 1993. Statistics for Spatial Data. New York:\nWiley.\n\n\nCsardi, Gabor, and Tamas Nepusz. 2006. “The Igraph Software\nPackage for Complex Network Research.” InterJournal\nComplex Systems: 1695. https://igraph.org.\n\n\nDavies, Tilman, and David Bryant. 2013. “On Circulant Embedding\nfor Gaussian Random Fields in R.” Journal of\nStatistical Software, Articles 55 (9): 1–21. https://doi.org/10.18637/jss.v055.i09.\n\n\nDe Gruijter, J. J., Dick J. Brus, Marc F. P. Bierkens, and Martin\nKnotters. 2006. Sampling for Natural Resource Monitoring.\nSpringer Science & Business Media.\n\n\nDe Gruijter, J. J., and C. J. F. Ter Braak. 1990. “Model-Free\nEstimation from Spatial Samples: A Reappraisal of Classical Sampling\nTheory.” Mathematical Geology 22 (4): 407–15.\n\n\nDiggle, P. J., and P. J. Ribeiro Jr. 2007. Model-Based\nGeostatistics. New York: Springer.\n\n\nDiggle, P. J., J. A. Tawn, and R. A. Moyeed. 1998. “Model-Based\nGeostatistics.” Applied Statistics, 299–350.\n\n\nDo, Van Huyen, Thibault Laurent, and Anne Vanhems. 2021.\n“Guidelines on Areal Interpolation Methods.” In\nAdvances in Contemporary Statistics and Econometrics: Festschrift in\nHonor of Christine Thomas-Agnan, edited by Abdelaati Daouia and\nAnne Ruiz-Gazen, 385–407. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-73249-3_20.\n\n\nDo, Van Huyen, Christine Thomas-Agnan, and Anne Vanhems. 2015a.\n“Accuracy of Areal Interpolation Methods for Count Data.”\nSpatial Statistics 14: 412–38. https://doi.org/10.1016/j.spasta.2015.07.005.\n\n\n———. 2015b. “Spatial Reallocation of Areal Data: A Review.”\nRev. Econ. Rég. Urbaine 1/2: 27–58. https://www.tse-fr.eu/sites/default/files/medias/doc/wp/mad/wp_tse_397_v2.pdf.\n\n\nDuncan, O. D., R. P. Cuzzort, and B. Duncan. 1961. Statistical\nGeography: Problems in Analyzing Areal Data. Glencoe, IL: Free\nPress.\n\n\nDunnington, Dewey. 2022. Ggspatial: Spatial Data Framework for\nGgplot2. https://CRAN.R-project.org/package=ggspatial.\n\n\nDunnington, Dewey, Edzer Pebesma, and Ege Rubak. 2023. S2: Spherical\nGeometry Operators Using the S2 Geometry Library. https://CRAN.R-project.org/package=s2.\n\n\nEaton, Brian, Jonathan Gregory, Bob Drach, Karl Taylor, Steve Hankin,\nJon Blower, John Caron, et al. 2022. NetCDF Climate and Forecast\n(CF) Metadata Conventions, Version 1.10. https://cfconventions.org/.\n\n\nEddelbuettel, Dirk. 2013. Seamless R and\nC++ Integration with Rcpp. Springer.\n\n\nEgenhofer, Max J., and Robert D. Franzosa. 1991. “Point-Set\nTopological Spatial Relations.” International Journal of\nGeographical Information Systems 5 (2): 161–74. https://doi.org/10.1080/02693799108927841.\n\n\nElhorst, J. Paul. 2010. “Applied Spatial Econometrics: Raising the\nBar.” Spatial Economic Analysis 5: 9–28.\n\n\nEvenden, Gerald I. 1990. Cartographic Projection Procedures for the\nUNIX Environment — a User’s Manual. http://download.osgeo.org/proj/OF90-284.pdf.\n\n\nEvers, Kristian, and Thomas Knudsen. 2017. Transformation Pipelines\nfor PROJ.4. https://www.fig.net/resources/proceedings/fig_proceedings/fig2017/papers/iss6b/ISS6B_evers_knudsen_9156.pdf.\n\n\nFellows, Ian, and using the JMapViewer library by Jan Peter Stotz. 2019.\nOpenStreetMap: Access to Open Street Map Raster Images. https://CRAN.R-project.org/package=OpenStreetMap.\n\n\nFingleton, B. 1999. “Spurious spatial\nregression: Some Monte Carlo results with a spatial unit root and\nspatial cointegration.” Journal of Regional\nScience 9: 1–19.\n\n\nFlorax, Raymond J. G. M., Hendrik Folmer, and Sergio J. Rey. 2006.\n“A Comment on Specification Searches in Spatial Econometrics: The\nRelevance of Hendry’s Methodology: A Reply.” Regional Science\nand Urban Economics 36 (2): 300–308. https://doi.org/10.1016/j.regsciurbeco.2005.10.002.\n\n\nFlorax, Raymond J. G. M, Hendrik Folmer, and Sergio J Rey. 2003.\n“Specification Searches in Spatial Econometrics: The Relevance of\nHendry’s Methodology.” Regional Science and Urban\nEconomics 33 (5): 557–79. https://doi.org/10.1016/S0166-0462(03)00002-4.\n\n\nFreni-Sterrantino, Anna, Massimo Ventrucci, and Håvard Rue. 2018.\n“A Note on Intrinsic Conditional Autoregressive Models for\nDisconnected Graphs.” Spatial and Spatio-Temporal\nEpidemiology 26: 25–34. https://doi.org/10.1016/j.sste.2018.04.002.\n\n\nGabriel, Edith, Peter J Diggle, Barry Rowlingson, and Francisco J\nRodriguez-Cortes. 2022. Stpp: Space-Time Point Pattern Simulation,\nVisualisation and Analysis. https://CRAN.R-project.org/package=stpp.\n\n\nGabriel, Edith, Barry Rowlingson, and Peter Diggle. 2013. “Stpp:\nAn R Package for Plotting, Simulating and Analyzing\nSpatio-Temporal Point Patterns.” Journal of Statistical\nSoftware, Articles 53 (2): 1–29. https://doi.org/10.18637/jss.v053.i02.\n\n\nGaetan, Carlo, and Xavier Guyon. 2010. Spatial Statistics and\nModeling. New York: Springer.\n\n\nGalton, A. 2004. “Fields and Objects in Space, Time and\nSpace-Time.” Spatial Cognition and Computation 4.\n\n\nGarnier, Simon. 2021. Viridis: Colorblind-Friendly Color Maps for\nr. https://CRAN.R-project.org/package=viridis.\n\n\nGeary, R. C. 1954. “The Contiguity Ratio and Statistical\nMapping.” The Incorporated Statistician 5: 115–45.\n\n\nGerber, Florian, and Reinhard Furrer. 2015. “Pitfalls in the\nImplementation of Bayesian Hierarchical Modeling of Areal Count Data: An\nIllustration Using BYM and Leroux Models.” Journal of\nStatistical Software, Code Snippets 63 (1): 1–32. https://doi.org/10.18637/jss.v063.c01.\n\n\nGerber, Florian, Rogier de Jong, Michael E Schaepman, Gabriela\nSchaepman-Strub, and Reinhard Furrer. 2018. “Predicting Missing\nValues in Spatio-Temporal Remote Sensing Data.” IEEE\nTransactions on Geoscience and Remote Sensing 56 (5): 2841–53.\n\n\nGetis, A., and J. K. Ord. 1992. “The Analysis of Spatial\nAssociation by the Use of Distance Statistics.” Geographical\nAnalysis 24 (2): 189–206.\n\n\n———. 1996. “Local Spatial Statistics: An Overview.” In\nSpatial Analysis: Modelling in a GIS Environment, edited by P.\nLongley and M Batty, 261–77. Cambridge: GeoInformation International.\n\n\nGiraud, Timothée. 2022. Mapsf: Thematic Cartography. https://riatelab.github.io/mapsf/.\n\n\nGómez-Rubio, Virgilio. 2019. “Spatial Data Analysis with INLA.\nCoding Club UC3M Tutorial Series. Universidad Carlos III de\nMadrid.” https://codingclubuc3m.rbind.io/talk/2019-11-05/.\n\n\n———. 2020. Bayesian Inference with INLA. Boca Raton, FL: CRC\nPress.\n\n\nGómez-Rubio, Virgilio, Roger Bivand, and Håvard Rue. 2015. “A New\nLatent Class to Fit Spatial Econometrics Models with Integrated Nested\nLaplace Approximations.” Procedia Environmental Sciences\n27: 116–18. https://doi.org/10.1016/j.proenv.2015.07.119.\n\n\nGonzález, Álvaro. 2010. “Measurement of Areas on a Sphere Using\nFibonacci and Latitude–Longitude Lattices.”\nMathematical Geosciences 42 (1): 49–64. https://arxiv.org/pdf/0912.4540.pdf.\n\n\nGoodchild, Michael F, and Nina Siu Ngan Lam. 1980. Areal\nInterpolation: A Variant of the Traditional Spatial Problem.\nDepartment of Geography, University of Western Ontario London, ON,\nCanada.\n\n\nGorelick, Noel, Matt Hancher, Mike Dixon, Simon Ilyushchenko, David\nThau, and Rebecca Moore. 2017. “Google Earth Engine:\nPlanetary-Scale Geospatial Analysis for Everyone.” Remote\nSensing of Environment 202: 18–27. https://doi.org/10.1016/j.rse.2017.06.031.\n\n\nGoulard, Michel, Thibault Laurent, and Christine Thomas-Agnan. 2017.\n“About Predictions in Spatial Autoregressive Models: Optimal and\nAlmost Optimal Strategies.” Spatial Economic Analysis 12\n(2-3): 304–25. https://doi.org/10.1080/17421772.2017.1300679.\n\n\nGräler, Benedikt, Edzer Pebesma, and Gerard Heuvelink. 2016.\n“Spatio-Temporal Interpolation using\ngstat.” The R Journal 8 (1): 204–18.\nhttps://doi.org/10.32614/RJ-2016-014.\n\n\nHahsler, Michael, and Matthew Piekenbrock. 2022. Dbscan:\nDensity-Based Spatial Clustering of Applications with Noise (DBSCAN) and\nRelated Algorithms. https://github.com/mhahsler/dbscan.\n\n\nHalleck Vega, Solmaria, and J. Paul Elhorst. 2015. “The\nSLX Model.” Journal of Regional Science 55\n(3): 339–63. https://doi.org/10.1111/jors.12188.\n\n\nHaltiner, G. J., and R. T. Williams. 1980. Numerical Prediction and\nDynamic Meteorology. New York: John Wiley; Sons.\n\n\nHand, David J. 2004. Measurement Theory and Practice. Arnold,\nLondon.\n\n\nHealy, Kieran. 2018. Data Visualization, a Practical\nIntroduction. Princeton University Press. http://socviz.co/index.html.\n\n\nHeaton, Matthew J., Abhirup Datta, Andrew O. Finley, Reinhard Furrer,\nJoseph Guinness, Rajarshi Guhaniyogi, Florian Gerber, et al. 2018.\n“A Case Study Competition Among Methods for Analyzing Large\nSpatial Data.” Journal of Agricultural, Biological and\nEnvironmental Statistics, December. https://doi.org/10.1007/s13253-018-00348-w.\n\n\nHendry, David F. 2006. “A Comment on ‘Specification Searches\nin Spatial Econometrics: The Relevance of Hendry’s\nMethodology’.” Regional Science and Urban\nEconomics 36 (2): 309–12. https://doi.org/10.1016/j.regsciurbeco.2005.10.001.\n\n\nHepple, Leslie W. 1976. “A Maximum Likelihood Model for\nEconometric Estimation with Spatial Series.” In Theory and\nPractice in Regional Science, edited by I. Masser, 90–104. London\nPapers in Regional Science. London: Pion.\n\n\nHerring, J. R. 2010. “OpenGIS Implementation Standard for\nGeographic Information-Simple Feature Access-Part 2: SQL Option.”\nOpen Geospatial Consortium Inc. http://portal.opengeospatial.org/files/?artifact_id=25354.\n\n\n———. 2011. “OpenGIS Implementation Standard for Geographic\nInformation-Simple Feature Access-Part 1: Common Architecture.”\nOpen Geospatial Consortium Inc, 111. http://portal.opengeospatial.org/files/?artifact_id=25355.\n\n\nHerring, J. R. et al. 2011. “Opengis Implementation\nStandard for Geographic Information-Simple Feature Access-Part 1: Common\nArchitecture [Corrigendum].”\n\n\nHersbach, Hans, Bill Bell, Paul Berrisford, Shoji Hirahara, András\nHorányi, Joaquín Muñoz-Sabater, Julien Nicolas, et al. 2020. “The\nERA5 Global Reanalysis.” Quarterly Journal of the Royal\nMeteorological Society 146 (730): 1999–2049. https://doi.org/10.1002/qj.3803.\n\n\nHijmans, Robert J. 2023a. Raster: Geographic Data Analysis and\nModeling. https://rspatial.org/raster.\n\n\n———. 2023b. Terra: Spatial Data Analysis. https://rspatial.org/terra/.\n\n\nHufkens, Koen. 2023. Ecmwfr: Interface to ECMWF and CDS Data Web\nServices. https://github.com/bluegreen-labs/ecmwfr.\n\n\nIhaka, Ross, Paul Murrell, Kurt Hornik, Jason C. Fisher, Reto Stauffer,\nClaus O. Wilke, Claire D. McWhite, and Achim Zeileis. 2023.\nColorspace: A Toolbox for Manipulating and Assessing Colors and\nPalettes. https://CRAN.R-project.org/package=colorspace.\n\n\nIliffe, Jonathan, and Roger Lott. 2008. Datums and Map Projections\nfor Remote Sensing, GIS, and Surveying. Whittles Pub. CRC Press,\nScotland, UK.\n\n\nISO. 2004. Geographic Information – Simple Feature Access – Part 1:\nCommon Architecture.https://www.iso.org/standard/40114.html\n.\n\n\nJones, Philip W. 1999. “First- and Second-Order Conservative\nRemapping Schemes for Grids in Spherical Coordinates.” Mon.\nWea. Rev. 127: 2204–10. https://doi.org/\n10.1175/1520-0493(1999) .\n\n\nJoo, Rocío, Matthew E. Boone, Thomas A. Clay, Samantha C. Patrick,\nSusana Clusella-Trullas, and Mathieu Basille. 2020. “Navigating\nThrough the R Packages for Movement.” Journal of\nAnimal Ecology 89 (1): 248–67. https://doi.org/10.1111/1365-2656.13116.\n\n\nJournel, Andre G., and Charles J. Huijbregts. 1978. Mining\nGeostatistics. Academic Press, London.\n\n\nKarney, Charles F. F. 2013. “Algorithms for Geodesics.”\nJournal of Geodesy 87 (1): 43–55. https://link.springer.com/content/pdf/10.1007/s00190-012-0578-z.pdf.\n\n\nKelejian, Harry, and Gianfranco Piras. 2017. Spatial\nEconometrics. London: Academic Press.\n\n\nKitanidis, Peter K., and Robert W. Lane. 1985. “Maximum Likelihood\nParameter Estimation of Hydrologic Spatial Processes by the Gauss-Newton\nMethod.” Journal of Hydrology 79 (1-2): 53–71. https://doi.org/10.1016/0022-1694(85)90181-7.\n\n\nKnudsen, Thomas, and Kristian Evers. 2017. Transformation Pipelines\nfor PROJ.4. https://meetingorganizer.copernicus.org/EGU2017/EGU2017-8050.pdf.\n\n\nKuhn, Max. 2022. Caret: Classification and Regression Training.\nhttps://github.com/topepo/caret/.\n\n\nKuhn, Max, and Hadley Wickham. 2022. Tidymodels: Easily Install and\nLoad the Tidymodels Packages. https://CRAN.R-project.org/package=tidymodels.\n\n\nKyriakidis, P. C. 2004. “A Geostatistical Framework for\nAreal-to-Point Spatial Interpolation.” Geographical\nAnalysis 36: 259–89.\n\n\nLaurent, Thibault, and Paula Margaretic. 2021. “Predictions in\nSpatial Econometric Models: Application to Unemployment Data.” In\nAdvances in Contemporary Statistics and Econometrics: Festschrift in\nHonor of Christine Thomas-Agnan, edited by Abdelaati Daouia and\nAnne Ruiz-Gazen, 409–26. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-73249-3_21.\n\n\nLeSage, James P. 2014. “What Regional Scientists Need to Know\nabout Spatial Econometrics.” Review of Regional Studies\n44: 13–32. https://journal.srsa.org/ojs/index.php/RRS/article/view/44.1.2.\n\n\nLeSage, James P., and Kelley R. Pace. 2009. Introduction to Spatial\nEconometrics. Boca Raton, FL: CRC Press.\n\n\nLi, Xun, and Luc Anselin. 2021. Rgeoda: R Library for\nSpatial Data Analysis. https://CRAN.R-project.org/package=rgeoda.\n\n\n———. 2022. Rgeoda: R Library for Spatial Data Analysis. https://CRAN.R-project.org/package=rgeoda.\n\n\nLott, Roger. 2015. “Geographic Information-Well-Known Text\nRepresentation of Coordinate Reference Systems.” Open Geospatial\nConsortium.http://docs.opengeospatial.org/is/12-063r5/12-063r5.html\n.\n\n\nLovelace, Robin, Richard Ellison, and Malcolm Morgan. 2022. Stplanr:\nSustainable Transport Planning. https://CRAN.R-project.org/package=stplanr.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019.\nGeocomputation with R. Chapman & Hall/CRC.https://geocompr.robinlovelace.net/\n.\n\n\nLu, Meng, Marius Appel, and Edzer Pebesma. 2018. “Multidimensional\nArrays for Analysing Geoscientific Data.” ISPRS International\nJournal of Geo-Information 7 (8): 313.\n\n\nLu, Meng, Edzer Pebesma, Alber Sanchez, and Jan Verbesselt. 2016.\n“Spatio-Temporal Change Detection from Multidimensional Arrays:\nDetecting Deforestation from MODIS Time Series.” ISPRS\nJournal of Photogrammetry and Remote Sensing 117: 227–36. https://doi.org/10.1016/j.isprsjprs.2016.03.007.\n\n\nMark Padgham, Bob Rudis, Robin Lovelace, and Maëlle Salmon. 2017.\nOsmdata. The Journal of Open Source Software. Vol. 2.\nThe Open Journal. https://doi.org/10.21105/joss.00305.\n\n\nMartin, D. 1989. “Mapping Population Data from Zone Centroid\nLocations.” Transactions of the Institute of British\nGeographers, New Series 14: 90–97.\n\n\nMartinetti, Davide, and Ghislain Geniaux. 2017. “Approximate\nLikelihood Estimation of Spatial Probit Models.” Regional\nScience and Urban Economics 64: 30–45. https://doi.org/10.1016/j.regsciurbeco.2017.02.002.\n\n\nMcCulloch, Charles E., and Shayle R. Searle. 2001. Generalized,\nLinear, and Mixed Models. New York: Wiley.\n\n\nMcMillen, Daniel P. 2003. “Spatial Autocorrelation or Model\nMisspecification?” International Regional Science Review\n26: 208–17.\n\n\n———. 2013. Quantile Regression for Spatial Data. Heidelberg:\nSpringer-Verlag.\n\n\nMennis, Jeremy. 2003. “Generating Surface Models of Population\nUsing Dasymetric Mapping.” The Professional Geographer\n55 (1): 31–42.\n\n\nMeyer, Hanna, Carles Milà, and Marvin Ludwig. 2023. CAST: Caret\nApplications for Spatial-Temporal Models. https://CRAN.R-project.org/package=CAST.\n\n\nMeyer, Hanna, and Edzer Pebesma. 2021. “Predicting into Unknown\nSpace? Estimating the Area of Applicability of Spatial Prediction\nModels.” Methods in Ecology and Evolution 12 (9):\n1620–33. https://doi.org/10.1111/2041-210X.13650.\n\n\n———. 2022. “Machine Learning-Based Global Maps of Ecological\nVariables and the Challenge of Assessing Them.” Nature\nCommunincations 13.https://doi.org/10.1038/s41467-022-29838-9\n.\n\n\nMila, Carles, Jorge Mateu, Edzer Pebesma, and Hanna Meyer. 2022.\n“Nearest Neighbour Distance Matching Leave-One-Out\nCross-Validation for Map Validation.” Methods in Ecology and\nEvolution 13 (6): 1304–16. https://doi.org/10.1111/2041-210X.13851.\n\n\nMillo, Giovanni, and Gianfranco Piras. 2012. “splm: Spatial Panel Data Models in\nR.” Journal of Statistical Software 47 (1):\n1–38.\n\n\nMoran, P. A. P. 1948. “The Interpretation of Statistical\nMaps.” Journal of the Royal Statistical Society, Series B\n(Methodological) 10 (2): 243–51.\n\n\nMoreno, Mel, and Mathieu Basille. 2018. Drawing Beautiful Maps\nProgrammatically with r, Sf and Ggplot2 — Part 1: Basics. https://www.r-spatial.org/r/2018/10/25/ggplot2-sf.html.\n\n\nMur, Jesús, and Ana Angulo. 2006. “The Spatial Durbin Model and\nthe Common Factor Tests.” Spatial Economic Analysis 1\n(2): 207–26. https://doi.org/10.1080/17421770601009841.\n\n\nNepusz, Tamás. 2022. Igraph: Network Analysis and\nVisualization. https://CRAN.R-project.org/package=igraph.\n\n\nNeuwirth, Erich. 2022. RColorBrewer: ColorBrewer Palettes. https://CRAN.R-project.org/package=RColorBrewer.\n\n\nO’Brien, Joshua. 2022. gdalUtilities: Wrappers for GDAL Utilities\nExecutables. https://github.com/JoshOBrien/gdalUtilities/.\n\n\nObe, Regina O., and Leo S. Hsu. 2015. PostGIS in Action.\nManning Publications Co.\n\n\nOkabe, A., T. Satoh, T. Furuta, A. Suzuki, and K. Okano. 2008.\n“Generalized Network Voronoi Diagrams: Concepts, Computational\nMethods, and Applications.” International Journal of\nGeographical Information Science 22 (9): 965–94. https://doi.org/10.1080/13658810701587891.\n\n\nOlsson, Gunnar. 1970. “Explanation, Prediction, and Meaning\nVariance: An Assessment of Distance Interaction Models.”\nEconomic Geography 46: 223–33. https://doi.org/10.2307/143140.\n\n\nOrd, J. K. 1975. “Estimation Methods for\nModels of Spatial Interaction.” Journal of the\nAmerican Statistical Association 70 (349): 120–26.\n\n\nOrd, J. K., and A. Getis. 2001. “Testing for Local Spatial\nAutocorrelation in the Presence of Global Autocorrelation.”\nJournal of Regional Science 41 (3): 411–32.\n\n\nPace, R. K., and James P. LeSage. 2008. “A Spatial\nHausman Test.” Economics Letters 101:\n282–84.\n\n\nPapadopoulos, Stavros, Kushal Datta, Samuel Madden, and Timothy Mattson.\n2016. “The Tiledb Array Data Storage Manager.”\nProceedings of the VLDB Endowment 10 (4): 349–60.\n\n\nPebesma, Edzer. 2004. “Multivariable Geostatistics in\nS: The Gstat Package.” Computers &\nGeosciences 30: 683–91.\n\n\n———. 2012. “spacetime: Spatio-Temporal\nData in R.” Journal of Statistical Software\n51 (7): 1–30. https://www.jstatsoft.org/v51/i07/.\n\n\n———. 2018. “Simple Features for R:\nStandardized Support for Spatial Vector Data.”\nThe R Journal 10 (1): 439–46. https://doi.org/10.32614/RJ-2018-009.\n\n\n———. 2022a. Reading Zarr Files with r Package Stars. https://r-spatial.org/r/2022/09/13/zarr.html.\n\n\n———. 2022b. Sf: Simple Features for r. https://CRAN.R-project.org/package=sf.\n\n\n———. 2022c. Spacetime: Classes and Methods for Spatio-Temporal\nData. https://github.com/edzer/spacetime.\n\n\n———. 2022d. Stars: Spatiotemporal Arrays, Raster and Vector Data\nCubes. https://CRAN.R-project.org/package=stars.\n\n\n———. 2023. Lwgeom: Bindings to Selected Liblwgeom Functions for\nSimple Features. https://github.com/r-spatial/lwgeom/.\n\n\nPebesma, Edzer, and Benedikt Graeler. 2022. Gstat: Spatial and\nSpatio-Temporal Geostatistical Modelling, Prediction and\nSimulation. https://github.com/r-spatial/gstat/.\n\n\nPebesma, Edzer, Thomas Mailund, and James Hiebert. 2016.\n“Measurement Units in R.” The R\nJournal 8 (2): 486–94. https://doi.org/10.32614/RJ-2016-061.\n\n\nPebesma, Edzer, Thomas Mailund, Tomasz Kalinowski, and Iñaki Ucar. 2022.\nUnits: Measurement Units for R Vectors. https://github.com/r-quantities/units.\n\n\nPinheiro, Jose C., and Douglas M. Bates. 2000. Mixed-Effects Models\nin S and S-Plus. New York: Springer.\n\n\nPiras, Gianfranco, and Ingmar R. Prucha. 2014. “On the Finite\nSample Properties of Pre-Test Estimators of Spatial Models.”\nRegional Science and Urban Economics 46: 103–15. https://doi.org/10.1016/j.regsciurbeco.2014.03.002.\n\n\nPlate, Tony, and Richard Heiberger. 2016. Abind: Combine\nMultidimensional Arrays. https://CRAN.R-project.org/package=abind.\n\n\nPloton, Pierre, Frédéric Mortier, Maxime Réjou-Méchain, Nicolas Barbier,\nNicolas Picard, Vivien Rossi, Carsten Dormann, et al. 2020.\n“Spatial Validation Reveals Poor Predictive Performance of\nLarge-Scale Ecological Mapping Models.” Nature\nCommunications 11 (1): 4540. https://www.nature.com/articles/s41467-020-18321-y.\n\n\nRaim, A. M., S. H. Holan, J. R. Bradley, and C. K. Wikle. 2021.\n“Spatio-Temporal Change of Support Modeling with r.”\nComputational Statistics 36: 749–80. https://doi.org/\n10.1007/s00180-020-01029-4 .\n\n\nRaoult, Baudouin, Cedric Bergeron, Angel López Alós, Jean-Noël Thépaut,\nand Dick Dee. 2017. “Climate Service Develops User-Friendly Data\nStore.” ECMWF Newsletter 151: 22–27.\n\n\nRipley, B. D. 1981. Spatial Statistics. New York: Wiley.\n\n\n———. 1988. Statistical Inference for Spatial Processes.\nCambridge: Cambridge University Press.\n\n\nRue, Havard, Finn Lindgren, and Elias Teixeira Krainski. 2022. INLA:\nFull Bayesian Analysis of Latent Gaussian Models Using Integrated Nested\nLaplace Approximations.\n\n\nSarrias, Mauricio, and Gianfranco Piras. 2022. Spldv: Spatial Models\nfor Limited Dependent Variables. https://CRAN.R-project.org/package=spldv.\n\n\nSauer, Jeffery, Taylor Oshan, Sergio Rey, and Levi John Wolf. 2021.\n“The Importance of Null Hypotheses: Understanding Differences in\nLocal Moran’s Ii Under\nHeteroskedasticity.” Geographical Analysis. https://doi.org/10.1111/gean.12304.\n\n\nSchabenberger, O., and C. A. Gotway. 2005. Statistical Methods for\nSpatial Data Analysis. Boca Raton/London: Chapman & Hall/CRC.\n\n\nScheider, Simon, Benedikt Gräler, Edzer Pebesma, and Christoph Stasch.\n2016. “Modeling Spatiotemporal Information Generation.”\nInternational Journal of Geographical Information Science 30\n(10): 1980–2008. https://doi.org/10.1080/13658816.2016.1151520.\n\n\nSchlather, Martin. 2011. “Construction of Covariance Functions and\nUnconditional Simulation of Random Fields.” In Space-Time\nProcesses and Challenges Related to Environmental Problems, edited\nby E. Porcu, Montero J. M., and M. Schlather. New York: Springer.\n\n\nSchlesinger, Thomas, and Manuel J. A. Eugster. 2013. Osmar:\nOpenStreetMap and r. http://osmar.r-forge.r-project.org/.\n\n\nSchramm, Matthias, Edzer Pebesma, Milutin Milenković, Luca Foresta,\nJeroen Dries, Alexander Jacob, Wolfgang Wagner, et al. 2021. “The\nopenEO API–Harmonising the Use\nof Earth Observation Cloud Services Using Virtual Data Cube\nFunctionalities.” Remote Sensing 13 (6). https://doi.org/10.3390/rs13061125.\n\n\nSchratz, Patrick, and Marc Becker. 2022. Mlr3spatiotempcv:\nSpatiotemporal Resampling Methods for Mlr3. https://CRAN.R-project.org/package=mlr3spatiotempcv.\n\n\nShe, Bing, Xinyan Zhu, Xinyue Ye, Wei Guo, Kehua Su, and Jay Lee. 2015.\n“Weighted Network Voronoi Diagrams for Local Spatial\nAnalysis.” Computers, Environment and Urban Systems 52:\n70–80. https://doi.org/10.1016/j.compenvurbsys.2015.03.005.\n\n\nSilge, Julia, and Michael Mahoney. 2023. Spatialsample: Spatial\nResampling Infrastructure. https://CRAN.R-project.org/package=spatialsample.\n\n\nSimoes, Rolf, Gilberto Camara, Gilberto Queiroz, Felipe Souza, Pedro R.\nAndrade, Lorena Santos, Alexandre Carvalho, and Karine Ferreira. 2021.\n“Satellite Image Time Series Analysis for Big Earth Observation\nData.” Remote Sensing 13 (13). https://doi.org/10.3390/rs13132428.\n\n\nSimoes, Rolf, Felipe Carvalho, and Brazil Data Cube Team. 2023.\nRstac: Client Library for SpatioTemporal Asset Catalog. https://brazil-data-cube.github.io/rstac/.\n\n\nSkøien, Jon O, Günter Blöschl, Gregor Laaha, E Pebesma, Juraj Parajka,\nand Alberto Viglione. 2014. “Rtop: An R Package for\nInterpolation of Data with a Variable Spatial Support, with an Example\nfrom River Networks.” Computers & Geosciences 67:\n180–90.\n\n\nSmith, Tony E. 2009. “Estimation Bias in Spatial Models with\nStrongly Connected Weight Matrices.” Geographical\nAnalysis 41 (3): 307–32. https://doi.org/10.1111/j.1538-4632.2009.00758.x.\n\n\nSmith, Tony E., and K. L. Lee. 2012. “The\neffects of spatial autoregressive dependencies on inference in ordinary\nleast squares: a geometric approach.” Journal of\nGeographical Systems 14 (January): 91–124. https://doi.org/10.1007/s10109-011-0152-x.\n\n\nSokal, R. R, N. L. Oden, and B. A. Thomson. 1998. “Local Spatial\nAutocorrelation in a Biological Model.” Geographical\nAnalysis 30: 331–54.\n\n\nStasch, Christoph, Simon Scheider, Edzer Pebesma, and Werner Kuhn. 2014.\n“Meaningful Spatial Prediction and Aggregation.”\nEnvironmental Modelling & Software 51: 149–65. https://doi.org/10.1016/j.envsoft.2013.09.006.\n\n\nStoyan, Dietrich, Francisco J. Rodríguez-Cortés, Jorge Mateu, and\nWilfried Gille. 2017. “Mark Variograms for Spatio-Temporal Point\nProcesses.” Spatial Statistics 20: 125–47. https://doi.org/10.1016/j.spasta.2017.02.006.\n\n\nSuesse, Thomas. 2018. “Marginal Maximum Likelihood Estimation of\nSAR Models with Missing Data.” Computational Statistics &\nData Analysis 120: 98–110. https://doi.org/10.1016/j.csda.2017.11.004.\n\n\nTeickner, Henning, Edzer Pebesma, and Benedikt Graeler. 2022.\nSftime: Classes and Methods for Simple Feature Objects That Have a\nTime Column. https://CRAN.R-project.org/package=sftime.\n\n\nTennekes, Martijn. 2018. “tmap:\nThematic Maps in R.” Journal of Statistical\nSoftware 84 (6): 1–39. https://doi.org/10.18637/jss.v084.i06.\n\n\n———. 2022. Tmap: Thematic Maps. https://github.com/r-tmap/tmap.\n\n\nTiefelsdorf, M. 2002. “The Saddlepoint Approximation of\nMoran’s I and Local Moran’s Ii Reference\nDistributions and Their Numerical Evaluation.” Geographical\nAnalysis 34: 187–206.\n\n\nTobler, W. R. 1970. “A Computer Movie Simulating Urban Growth in\nthe Detroit Region.” Economic Geography 46: 234–40. https://doi.org/10.2307/143141.\n\n\n———. 1979. “Smooth Pycnophylactic Interpolation for Geographical\nRegions.” Journal of the American Statistical\nAssociation 74: 519–30.\n\n\nUCAR. 2014. UDUNITS 2.2.26 Manual. https://www.unidata.ucar.edu/software/udunits/udunits-current/doc/udunits/udunits2.html.\n\n\n———. 2020. The NetCDF User’s Guide. https://www.unidata.ucar.edu/software/netcdf/docs/user_guide.html.\n\n\nUmlauf, Nikolaus, Daniel Adler, Thomas Kneib, Stefan Lang, and Achim\nZeileis. 2015. “Structured Additive Regression Models: An\nR Interface to BayesX.” Journal of\nStatistical Software 63 (21): 1–46. http://www.jstatsoft.org/v63/i21/.\n\n\nUmlauf, Nikolaus, Thomas Kneib, Stefan Lang, and Achim Zeileis. 2022.\nR2BayesX: Estimate Structured Additive Regression Models with\nBayesX. https://CRAN.R-project.org/package=R2BayesX.\n\n\nUpton, G., and B. Fingleton. 1985. Spatial Data Analysis by Example:\nPoint Pattern and Qualitative Data. New York: Wiley.\n\n\nvan der Meer, Lucas, Lorena Abad, Andrea Gilardi, and Robin Lovelace.\n2022. Sfnetworks: Tidy Geospatial Networks. https://CRAN.R-project.org/package=sfnetworks.\n\n\nVan Lieshout, M. N. M. 2019. Theory of Spatial Statistics. Boca\nRaton, FL: Chapman & Hall/CRC.\n\n\nVeach, Eric, Jesse Rosenstock, Eric Engle, Robert Snedegar, Julien\nBasch, and Tom Manshreck. 2020. “S2 Geometry.”\nWebsite. https://s2geometry.io/.\n\n\nVer Hoef, Jay M, and Noel Cressie. 1993. “Multivariable Spatial\nPrediction.” Mathematical Geology 25 (2): 219–40.\n\n\nVerbesselt, Jan, Rob Hyndman, Glenn Newnham, and Darius Culvenor. 2010.\n“Detecting Trend and Seasonal Changes in Satellite Image Time\nSeries.” Remote Sensing of Environment 114 (1): 106–15.\nhttps://doi.org/10.1016/j.rse.2009.08.014.\n\n\nVranckx, M., T. Neyens, and C. Faes. 2019. “Comparison of\nDifferent Software Implementations for Spatial Disease Mapping.”\nSpatial and Spatio-Temporal Epidemiology 31: 100302. https://doi.org/10.1016/j.sste.2019.100302.\n\n\nWagner, Martin, and Achim Zeileis. 2019. “Heterogeneity and\nSpatial Dependence of Regional Growth in the EU: A\nRecursive Partitioning Approach.” German Economic Review\n20 (1): 67–82. https://doi.org/10.1111/geer.12146.\n\n\nWall, M. M. 2004. “A Close Look at the Spatial Structure Implied\nby the CAR and SAR Models.” Journal\nof Statistical Planning and Inference 121: 311–24.\n\n\nWaller, Lance A., and Carol A. Gotway. 2004. Applied Spatial\nStatistics for Public Health Data. Hoboken, NJ: John Wiley &\nSons.\n\n\nWang, Earo, Di Cook, Rob Hyndman, and Mitchell O’Hara-Wild. 2022.\nTsibble: Tidy Temporal Data Frames and Tools. https://tsibble.tidyverts.org.\n\n\nWhittle, P. 1954. “On Stationary Processes in\nthe Plane.” Biometrika 41 (3-4): 434–49. https://doi.org/10.1093/biomet/41.3-4.434.\n\n\nWickham, Hadley. 2014a. Advanced R, Second\nEdition. CRC Press.https://adv-r.hadley.nz/ .\n\n\n———. 2014b. “Tidy Data.” Journal of Statistical\nSoftware 59 (1).https://www.jstatsoft.org/article/view/v059i10\n.\n\n\n———. 2016. Ggplot2: Elegant Graphics for Data Analysis.\nSpringer.\n\n\n———. 2022. Tidyverse: Easily Install and Load the Tidyverse. https://CRAN.R-project.org/package=tidyverse.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the Tidyverse.” Journal of Open Source\nSoftware 4 (43): 1686. https://joss.theoj.org/papers/10.21105/joss.01686.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen,\nKohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, and Dewey\nDunnington. 2022. Ggplot2: Create Elegant Data Visualisations Using\nthe Grammar of Graphics. https://CRAN.R-project.org/package=ggplot2.\n\n\nWickham, Hadley, and Garret Grolemund. 2017. R for Data\nScience. O’Reilly. http://r4ds.had.co.nz/.\n\n\nWikle, Christopher K, Andrew Zammit-Mangion, and Noel Cressie. 2019.\nSpatio-Temporal Statistics with R. CRC Press.\n\n\nWilhelm, Stefan, and Miguel Godinho de Matos. 2013. “Estimating Spatial Probit Models in R.”\nThe R Journal 5 (1): 130–43. https://doi.org/10.32614/RJ-2013-013.\n\n\nWilke, Claus O. 2019. Fundamentals of Data Visualization.\nO’Reilly Media, Inc. https://serialmentor.com/dataviz/.\n\n\nWood, S. N. 2017. Generalized Additive Models: An Introduction with\nR. 2nd ed. Chapman & Hall/CRC.\n\n\n———. 2022. Mgcv: Mixed GAM Computation Vehicle with Automatic\nSmoothness Estimation. https://CRAN.R-project.org/package=mgcv.\n\n\nZeileis, Achim, Jason C. Fisher, Kurt Hornik, Ross Ihaka, Claire D.\nMcWhite, Paul Murrell, Reto Stauffer, and Claus O. Wilke. 2020.\n“colorspace: A Toolbox for\nManipulating and Assessing Colors and Palettes.” Journal of\nStatistical Software 96 (1): 1–49. https://doi.org/10.18637/jss.v096.i01.",
    "crumbs": [
      "Appendices",
      "References"
    ]
  }
]