\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{hyperref}
\hypersetup{
            pdftitle={Spatial Data Science},
            pdfauthor={Edzer Pebesma, Roger Bivand},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{longtable}}{}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Spatial Data Science}
\providecommand{\subtitle}[1]{}
\subtitle{with applications in R}
\author{Edzer Pebesma, Roger Bivand}
\date{2021-07-13}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\begin{verbatim}
# Warning in utils::citation(..., lib.loc = lib.loc): no date field
# in DESCRIPTION file of package 'gstat'
# Warning in utils::citation(..., lib.loc = lib.loc): could not
# determine year for 'gstat' from package DESCRIPTION file
# Warning in utils::citation(..., lib.loc = lib.loc): could not
# determine year for 'INLA' from package DESCRIPTION file
# Warning in utils::citation(..., lib.loc = lib.loc): no date field
# in DESCRIPTION file of package 'sf'
# Warning in utils::citation(..., lib.loc = lib.loc): could not
# determine year for 'sf' from package DESCRIPTION file
# Warning in utils::citation(..., lib.loc = lib.loc): no date field
# in DESCRIPTION file of package 'stars'
# Warning in utils::citation(..., lib.loc = lib.loc): could not
# determine year for 'stars' from package DESCRIPTION file
# Warning in utils::citation(..., lib.loc = lib.loc): no date field
# in DESCRIPTION file of package 'units'
\end{verbatim}

\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

Data science is concerned with finding answers to questions on the
basis of available data, and communicating that effort. Besides
showing the results, this communication involves sharing the data
used, but also exposing the path that led to the answers in a
comprehensive and reproducible way. It also acknowledges the fact
that available data may not be sufficient to answer questions, and
that any answers are conditional on the data collection or sampling
protocols employed.

This book introduces and explains the concepts underlying
\emph{spatial} data: points, lines, polygons, rasters, coverages, geometry
attributes, data cubes, reference systems, as well as higher-level
concepts including how attributes relate to geometries and how
this affects analysis. The relationship of attributes to geometries
is known as support, and changing support also changes the
characteristics of attributes. Some data generation processes are
continuous in space, and may be observed everywhere. Others are
discrete, observed in tesselated containers. In modern spatial data
analysis, tesellated methods are often used for all data, extending
across the legacy partition into point process, geostatistical and
lattice models. It is support (and the understanding of support) that
underlies the importance of spatial representation. The book aims
at data scientists who want to get a grip on using spatial data
in their analysis. To exemplify how to do things, it uses R.

It is often thought that spatial data boils down to having
observations' longitude and latitude in a dataset, and treating these
just like any other variable. This carries the risk of missed
opportunities and meaningless analyses. For instance,

\begin{itemize}
\tightlist
\item
  coordinate pairs really are pairs, and lose much of their meaning
  when treated independently
\item
  rather than having point locations, observations are often
  associated with spatial lines, areas, or grid cells
\item
  spatial distances between observations are often not well
  represented by straight-line distances, but by great circle
  distances, distances through networks, or by measuring the effort
  it takes getting from A to B
\end{itemize}

We introduce the concepts behind spatial data, coordinate reference
systems, spatial analysis, and introduce a number of packages,
including \texttt{sf} \citep{rjsf, R-sf}, \texttt{stars} \citep{R-stars}, \texttt{s2} \citep{R-s2}
and \texttt{lwgeom} \citep{R-lwgeom},
as well as a number of \texttt{tidyverse} \citep{R-tidyverse} extensions, and a number of
spatial analysis and visualisation packages that can be used with these packages,
including \texttt{gstat} \citep{R-gstat}, \texttt{spdep} \citep{R-spdep}, \texttt{spatialreg} \citep{R-spatialreg}, \texttt{spatstat} \citep{R-spatstat},
\texttt{tmap} \citep{R-tmap} and \texttt{mapview} \citep{R-mapview}.

The first part of this book introduces concepts of spatial data science,
and uses R only to generate text output or figures. The R code used for
this is not shown, as it would distract from the message. The online
version of this book contains the R sections, which can be unfolded
on demand and copied into the clipboard for execution and experimenting.
The second part of this book explains how the concepts introduced in
part I are dealt with using R, and deals with basic handling and plotting
of spatial and spatiotemporal data. Part III is dedicated to statistical
modelling of spatial data.

\hypertarget{acknowledgements}{%
\section{Acknowledgements}\label{acknowledgements}}

all GitHub contributors (t.b.d.),
Claus Wilke,
Jakub Nowosad,
SDSWR class summer 2021,
all sf and stars authors,

\hypertarget{part-spatial-data}{%
\part{Spatial Data}\label{part-spatial-data}}

Text introducing Part I

\hypertarget{intro}{%
\chapter{Getting Started}\label{intro}}

This chapter introduces a number of concepts associated with
handling spatial data, and points forward to later sections where
they are discussed in more detail.

\hypertarget{a-first-map}{%
\section{A first map}\label{a-first-map}}

The typical way to graph spatial data is by creating a map. Let us consider
a simple map, shown in figure \ref{fig:first-map}.

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/first-map-1} 

}

\caption{a first map}\label{fig:first-map}
\end{figure}

A number of graphical elements are present here, in this case:

\begin{itemize}
\tightlist
\item
  polygons are drawn with a black outline and filled with colors chosen according to a variable \texttt{BIR74}, whose name is in the title
\item
  a legend key explains the meaning of the colors, and has a certain \emph{color palette} and \emph{color
  breaks}, values at which color changes
\item
  the background of the map shows curved lines with constant latitude or longitude (graticule)
\item
  the axis ticks show the latitude and longitude values
\end{itemize}

\emph{Polygons} are a particular form of \emph{geometry}; spatial geometries
(points, lines, polygons, pixels) are discussed in detail in
chapter \ref{geometries}. Polygons consist of sequences of points,
connected by straight lines. How point locations of spatial data are
expressed, or measured, is discussed in chapter \ref{cs}. As can
be seen from figure \ref{fig:first-map}, lines of equal latitude
and longitude do not form straight lines, indicating that some
form of projection took place before plotting; projections are also
discussed in chapter \ref{cs} and section \ref{transform}.

The color values in figure \ref{fig:first-map} are derived
from numeric values of a variable, \texttt{BIR74}, which has a
single value associated with each geometry or \emph{feature}. Chapter
\ref{featureattributes} discusses such feature attributes, and the
way they can relate to feature geometries. In this case, \texttt{BIR74}
refers to birth counts, meaning counts \emph{over the region}. This
implies that the count does not refer to a value associated with
every point inside the polygon, which the continuous color might
suggest, but rather measures an integral (sum) over the polygon.

Before plotting figure \ref{fig:first-map} we had to read the data,
in this case from a file (section \ref{sfintro}). Printing a data
summary for the first three records of three attribute variables
shows:

\begin{verbatim}
# Simple feature collection with 100 features and 3 fields
# Geometry type: MULTIPOLYGON
# Dimension:     XY
# Bounding box:  xmin: -84.3 ymin: 33.9 xmax: -75.5 ymax: 36.6
# Geodetic CRS:  NAD27
# # A tibble: 100 x 4
#    AREA BIR74 SID74                                           geom
#   <dbl> <dbl> <dbl>                             <MULTIPOLYGON [Â°]>
# 1 0.114  1091     1 (((-81.5 36.2, -81.5 36.3, -81.6 36.3, -81.6 ~
# 2 0.061   487     0 (((-81.2 36.4, -81.2 36.4, -81.3 36.4, -81.3 ~
# 3 0.143  3188     5 (((-80.5 36.2, -80.5 36.3, -80.5 36.3, -80.5 ~
# # ... with 97 more rows
\end{verbatim}

The printed output shows:

\begin{itemize}
\tightlist
\item
  the (selected) dataset has 100 features (records) and 3 fields (attributes)
\item
  the geometry type is \texttt{MULTIPOLYGON} (chapter \ref{geometries})
\item
  it has dimension \texttt{XY}, indicating that each point will consist of 2 coordinate values
\item
  the range of x and y values of the geometry
\item
  the coordinate reference system (CRS) is geodetic, with coordinates in degrees longitude and latitude associated to the \texttt{NAD27} datum (chapter \ref{cs})
\item
  the three selected attribute variables are followed by a variable \texttt{geom} of type \texttt{MULTIPOLYGON} with unit degrees that contains the polygon information
\end{itemize}

More complicated plots can involve facet plots with a map in each
facet, as shown in figure \ref{fig:firstgather}.

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/firstgather-1} 

}

\caption{ggplot with facet maps}\label{fig:firstgather}
\end{figure}

An interactive, leaflet-based map is obtained in figure \ref{fig:mapviewfigure}.

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/mapviewfigure-1} 

}

\caption{Interactive map created with mapview.}\label{fig:mapviewfigure}
\end{figure}

\hypertarget{coordinate-reference-systems}{%
\section{Coordinate reference systems}\label{coordinate-reference-systems}}

In figure \ref{fig:first-map}, the grey lines denote the
\emph{graticule}, a grid with lines along constant latitude
or longitude. Clearly, these lines are not straight, which
indicates that a \emph{projection} of the data was used for which the
x and y axis do not align with longitude and latitude. In figure
\ref{fig:mapviewfigure} we see that the north boundary of North
Carolina is plotted as a straight line again, indicating that
another projection was used.

The ellipsoidal coordinates of the graticule of figure
\ref{fig:first-map} are associated with a particular \emph{datum}
(here: NAD27), which implicates a set of rules what the shape of the
Earth is and how it is attached to the Earth (to which point of the
Earth is the origin associated, and how is it directed). If one
would measure coordinates with a GPS device (e.g.~a mobile phone)
it would typically report coordinates associated with the WGS84
datum, which can be around 30 m different from the identical
coordinate values when associated with NAD27.

Projections describe how we go back and forth between

\begin{itemize}
\item
  \textbf{ellipsoidal coordinates} which are expressed as degrees
  latitude and longitude, pointing to locations on a shape
  approximating the Earth's shape (an ellipsoid or spheroid), and
\item
  \textbf{projected coordinates} which are coordinates on a flat,
  two-dimensional coordinate system, used when plotting maps.
\end{itemize}

Datums transformations are associated with moving from one datum
to another. Both topics are covered by \emph{spatial reference systems}
are described in more detail in chapter \ref{cs}.

\hypertarget{rasterize}{%
\section{Raster and vector data}\label{rasterize}}

Polygon, point and line geometries are examples of \emph{vector} data:
point coordinates describe the ``exact'' locations that can be
anywhere. Raster data on the other hand describe data where values
are aligned on a \emph{raster}, meaning on a regularly laid out lattice of
usually square pixels. An example is shown in figure \ref{fig:ras}.

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/ras-1} 

}

\caption{raster maps: Landsat-7 blue band, with color values derived from data values (a), the top-left 10x10 sub-image from (a) with numeric values shown (b), and overlayed by two different types of vector data: three sample points (c), and a 500m radius around the points represented as polygons (d)}\label{fig:ras}
\end{figure}

Vector and raster data can be combined in different ways; for instance we can query the raster at the three points of figure \ref{fig:ras}(c),

\begin{verbatim}
# Simple feature collection with 3 features and 1 field
# Geometry type: POINT
# Dimension:     XY
# Bounding box:  xmin: 290000 ymin: 9110000 xmax: 292000 ymax: 9120000
# Projected CRS: UTM Zone 25, Southern Hemisphere
#   L7_ETMs.tif               geometry
# 1          80 POINT (290830 9114499)
# 2          58 POINT (290019 9119219)
# 3          63 POINT (291693 9116038)
\end{verbatim}

or compute an aggregate, such as the average, over arbitrary regions such as the circles shown in figure \ref{fig:ras}(d):

\begin{verbatim}
# Simple feature collection with 3 features and 1 field
# Geometry type: POLYGON
# Dimension:     XY
# Bounding box:  xmin: 290000 ymin: 9110000 xmax: 292000 ymax: 9120000
# Projected CRS: UTM Zone 25, Southern Hemisphere
#     V1                       geometry
# 1 77.2 POLYGON ((291330 9114499, 2...
# 2 60.1 POLYGON ((290519 9119219, 2...
# 3 71.6 POLYGON ((292193 9116038, 2...
\end{verbatim}

Other raster-to-vector conversions are discussed in \ref{raster-to-vector} and include:

\begin{itemize}
\tightlist
\item
  converting raster pixels into point values
\item
  converting raster pixels into small polygons, possibly merging polygons with identical values (``polygonize'')
\item
  generating lines or polygons that delineate continuous pixel areas with a certain value \emph{range} (``contour'')
\end{itemize}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/vectoras-1} 

}

\caption{The map obtained by rasterizing county total number of births for the period 1974-1979  shown in figure 1.1}\label{fig:vectoras}
\end{figure}

Vector-to-raster conversions can be as simple as rasterizing
polygons, as shown in figure \ref{fig:vectoras}. Other, more
general vector-to-raster conversions that may involve statistical
modelling include:

\begin{itemize}
\tightlist
\item
  interpolation of point values to points on a regular grid (chapter \ref{interpolation})
\item
  estimating densities of points over a regular grid (chapter \ref{pointpatterns})
\item
  area-weighted interpolation of polygon values to grid cells (section \ref{area-weighted})
\item
  direct rasterization of points, lines or polygons (section \ref{raster-to-vector})
\end{itemize}

\hypertarget{raster-types}{%
\section{Raster types}\label{raster-types}}

Raster dimensions describe how the rows and columns relate to
spatial coordinates. Figure \ref{fig:rastertypes01} shows a number
of different possibilities.

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/rastertypes01-1} 

}

\caption{various raster types}\label{fig:rastertypes01}
\end{figure}

Regular rasters like shown in figure \ref{fig:rastertypes01} have a constant,
not necessarily square cell size and axes aligned with the x and y
(Easting and Northing) axes. Other raster types include those
where the axes are no longer aligned with x and y (\emph{rotated}),
where axes are no longer perpendicular (\emph{sheared}), or where cell
size varies along a dimension (\emph{rectilinear}). Finally, \emph{curvilinear}
rasters have cell size and/or direction properties that are no longer
independent from the other raster dimension.

When a raster that is regular in a given coordinate reference
system is projected to another raster while keeping each raster
cell in tact, it changes shape and may become rectilinear
(e.g.~when going from ellipsoidal coordinates to Mercator, as in
figure \ref{fig:mapviewfigure}) or curvilinear (e.g.~when going from
ellipsoidal coordinates to Lambert Conic Conformal, as in figure
\ref{fig:first-map}). When reverting this procedure, one can recover
the exact original raster.

Creating a new, regular grid in the new projection is called raster
(or image) \emph{reprojection} or \emph{warping} (section \ref{warp}). This
process is lossy, irreversible, and may need to be informed whether
raster cells should be interpolated, averaged or summed, whether
they denote categorical variables, or whether resampling using
nearest neighbours should be used; see also section \ref{support}.

\hypertarget{time-series-arrays-data-cubes}{%
\section{Time series, arrays, data cubes}\label{time-series-arrays-data-cubes}}

A lot of spatial data is not \emph{just} spatial, but in addition temporal. Just like any observation is associated with an observation location, it is associated with an observation time or period. The dataset on the North Carolina counties shown above contains disease cases counted over two time periods, shown in figure \ref{fig:firstgather}. Although the original dataset has these variables in two different columns, for plotting them these columns had to be stacked first, while repeating the associated geometries - a form called \emph{tidy} by \citep{tidy}. When we have longer time series associated with geometries, neither option - distributing time over multiple columns, or stacking columns while repeating geometries - works well, and a more effective way of storing such data would be a matrix or array, where one dimension refers to time, and the other(s) to space. The natural way for image or raster data is already to store them in matrices; time series of rasters then lead to a three-dimensional array. The general term for such data is a (spatiotemporal) \textbf{data cube}, where cube refers to arrays with any number of dimensions. Data cubes can refer to both raster and vector data, examples are given in chapter \ref{datacube}.

\hypertarget{support}{%
\section{Support}\label{support}}

When we have spatial data with geometries that are not points but
collections of points (multi-points, lines, polygons, pixels),
then the attributes associated with these geometries has one
of several different relationships to them. Attributes can have:

\begin{itemize}
\tightlist
\item
  a \textbf{constant} value for every point of the geometry
\item
  a value that is unique to only this geometry, describing its \textbf{identity}
\item
  a single value that is an \textbf{aggregate} over all points of the geometry
\end{itemize}

An example of a constant is land use or bedrock type of a polygon. An
example of an identity is a county name. An example of an aggregate
is the number of births over a given period of time, of a county.

The area with to which an attribute value refers to is called its
\textbf{support}: aggregate properties have ``block'' (or polygon, or line)
support, constant properties have ``point'' support (they apply to
every point). Support matters when we manipulate the data. For
instance, figure \ref{fig:vectoras} was derived from a variable that has
polygon support: the number of births per county. Rasterizing these
values gives pixels with values that are associated to counties.
The result of the rasterization is a meaningless map: the numeric
values (``birth totals'') are not associated with the raster cells,
and the county boundaries are no longer present. Totals of birth for
the whole state can no longer be recovered from the pixel values.
Ignoring support can easily lead to meaningless results. Chapter
\ref{featureattributes} discusses this further.

Raster cell values may have point support, e.g.~when the cell records
the elevation of the point at the cell centre in a digital elevation
model, or cell support, e.g.~when a satellite image pixel gives the
color value averaged over (an area similar to the) pixel. Most file
formats do not provide this information, yet it may be important
to know when aggregating, regridding or warping rasters (section
\ref{warp}).

\hypertarget{spatial-data-science-software}{%
\section{Spatial data science software}\label{spatial-data-science-software}}

Although this book largely uses R and R packages for spatial data
science, a number of these packages use software libraries that were
not developed for R specifically. As an example, the dependency
of R package \texttt{sf} on other R packages and system libraries is shown
in figure \ref{fig:gdal-fig-nodetails}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/sf_deps} 

}

\caption{sf and its dependencies; arrows indicate strong dependency, dashed arrows weak dependency}\label{fig:gdal-fig-nodetails}
\end{figure}

The C or C++ libraries used (GDAL, GEOS, PROJ, liblwgeom, s2geometry,
NetCDF, udunits2) are all developed, maintained and used by (spatial)
data science communities that are large and mostly different from
the R community. By using these libraries, R users share how we
understand what we are doing with these other communities. Because R,
Python and Julia provide interactive interfaces to this software,
many users get closer to these libraries than do users of other
software based on these libraries. The first part of this book
describes many of the concepts implemented in these libraries,
which is relevant to spatial data science in general.

\hypertarget{gdal}{%
\subsection{GDAL}\label{gdal}}

GDAL (``Geospatial Data Abstraction Library'') can be seen as the
Swiss army knife of spatial data; besides for R it is being
used in Python, QGIS, PostGIS, \href{https://trac.osgeo.org/gdal/wiki/SoftwareUsingGdal}{and more than 100 other software
projects}.

GDAL is a ``library of libraries'' -- in order to read all these data
sources it needs a large number of other libraries. It typically
links to over 100 other libraries, each of which provides access to
e.g.~a particular data file format, database access, or web service.

Binary R packages distributed by CRAN contain only statically linked
code: CRAN does not want to make any assumptions about presence
of third-party libraries on the host system. As a consequence,
when the \texttt{sf} package is installed in binary form from CRAN, it
includes a copy of all the required external libraries as well as
their dependencies, which may amount to 100 Mb.

\hypertarget{proj}{%
\subsection{PROJ}\label{proj}}

PROJ (or PR\(\phi\)J) is a library for cartographic projections
and datum transformations: it converts spatial coordinates
from one coordinate reference system to another. It comes with
a large database of known projections and access to datum grids
(high-precision pre-calculated values for datum transformations). It
aligns with an international standard for coordinate reference
systems \citep{lott2015}. Chapter \ref{cs} deals with coordinate
systems, and PROJ.

\hypertarget{geos-and-s2geometry}{%
\subsection{GEOS and s2geometry}\label{geos-and-s2geometry}}

GEOS (``Geometry Engine Open Source'') and s2geometry are two libraries
for geometric operations. They are used to find measures (length,
area, distance), and calculate predicates (do two geometries have
any points in common?) or new geometries (which points do these two
geometries have in common?). GEOS does this for flat, two-dimensional
space (indicated by \(R^2\)), s2geometry does this for geometries on
the sphere (indicated by \(S^2\)). Chapter \ref{cs} introduces
coordinate reference systems, and chapter \ref{spherical} discusses
more about the differences between working with these two spaces.

\hypertarget{netcdf-udunits2-liblwgeom}{%
\subsection{NetCDF, udunits2, liblwgeom}\label{netcdf-udunits2-liblwgeom}}

NetCDF \citep{netcdf} refers to a file format as well as a C library
for reading and writing NetCDF files. It allows the definition of
arrays of any dimensionality, and is widely used for spatial and
spatiotemporal information, especially in the (climate) modelling
communities. Udunits2 \citep{udunits2, R-units} is a database and software
library for units of measurement that allows the conversion of
units, handles derived units, and supports user-defined units. The
liblwgeom ``library'' is a software component of PostGIS \citep{postgis}
that contains several routines missing from GDAL or GEOS, including
convenient access to GeographicLib routines \citep{karney2013algorithms}
that ship with PROJ.

\hypertarget{exercises}{%
\section{Exercises}\label{exercises}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  List five differences between raster and vector data.
\item
  In addition to those listed below figure \ref{fig:first-map}, list five further graphical components that are often found on a map.
\item
  In your own words, why is the numeric information shown in figure \ref{fig:vectoras} misleading (or meaningless)?
\item
  Under which conditions would you expect strong differences when doing geometrical operations on \(S^2\), compared to doing them on \(R^2\)?
\end{enumerate}

\hypertarget{cs}{%
\chapter{Coordinates}\label{cs}}

``\emph{Data are not just numbers, they are numbers with a context}'';
``\emph{In data analysis, context provides meaning}'' \citep{cobbmoore}

Before we can try to understand geometries like points, lines,
polygons, coverage and grids, it is useful to review coordinate
systems so that we have an idea what exactly coordinates of a
point reflect. For spatial data, the location of observations
are characterized by coordinates, and coordinates are defined in
a coordinate system. Different coordinate systems can be used for
this, and the most important difference is whether coordinates are
defined over a 2-dimensional or 3-dimensional space referenced
to orthogonal axes (Cartesian coordinates), or using distance
and directions (polar coordinates, spherical and ellipsoidal
coordinates). Besides a location of observation, all observations
are associated with time of observation, and so time coordinate
systems are also briefly discussed. First we will briefly review
\emph{quantities}, to learn what units and datum are.

\hypertarget{units}{%
\section{Quantities, units, datum}\label{units}}

The VIM (``International Vocabulary of Metrology'', \citet{vim})
defines a \emph{quantity} as a ``property of a phenomenon, body, or
substance, where the property has a magnitude that can be expressed
as a number and a reference'', where ``{[}a{]} reference can be a measurement
unit, a measurement procedure, a reference material, or a combination
of such.''

Although one could argue whether all data is constituted of
quantities, there is no need to argue that proper data handling
requires that numbers (or symbols) are accompanied by information
on what they mean, in particular what they refer to.

A measurement system consist of \emph{base units} for base quantities, and
\emph{derived units} for derived quantities. For instance, the SI system
of units \citep{SI} consist of the seven base units length (metre,
m), mass (kilogram, kg), time (second, s), electric current
(ampere, A), thermodynamic temperature (Kelvin, K), amount of
substance (mole, mol), and luminous intensity (candela, cd).
Derived units are composed of products of integer powers of base
units; examples are speed (\(\mbox{m}~\mbox{s}^{-1}\)), density
(\(\mbox{kg}~\mbox{m}^{-3}\)) and area (\(\mbox{m}^2\)).

The special case of unitless measures can refer to either cases
where units cancel out (e.g.~mass fraction: kg/kg, or angle measured
in rad: m/m) or to cases where objects or events were counted
(e.g.~5 apples). Adding an angle to a count of apples would not
make sense; adding 5 apples to 3 oranges may make sense if the
result is reinterpreted as a superclass, e.g.~as \emph{pieces of fruit}.
Many data variables have units that are not expressible as SI base
units or derived units. \citet{hand} discusses many such measurement scales,
e.g.~those used to measure intelligence in social sciences, in the
context of measurement units.

For many quantities, the natural origin of values is zero. This
works for amounts, where differences between amounts result in
meaningful negative values. For locations and times, differences
have a natural zero interpretation: distance and duration. Absolute
location (position) and time need a fixed origin, from which we
can meaningfully measure other absolute space-time points: we
call this \textbf{a datum}.
For space, a datum involves more than one dimension. The combination
of a datum and a measurement unit (scale) is a \emph{reference system}.

We will now elaborate how spatial locations can be expressed as
either ellipsoidal or Cartesian coordinates. The next sections will
deal with temporal and spatial reference systems, and how they are
handled in R.

\hypertarget{ellipsoidal-coordinates}{%
\section{Ellipsoidal coordinates}\label{ellipsoidal-coordinates}}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{sds_files/figure-latex/polar-1} 

}

\caption{Two-dimensional polar (red) and Cartesian (blue) coordinates}\label{fig:polar}
\end{figure}

Figure \ref{fig:polar} shows both polar and Cartesian coordinates
for a two-dimensional situation. In Cartesian coordinates,
the point shown is \((x,y) = (3,4)\), for polar coordinates it is
\((r,\phi) = (5, \mbox{arctan}(4/3))\), where \(\mbox{arctan}(4/3)\) is
approximately \(0.93\) radians, or \(53^{\circ}\). Note that \(x\), \(y\)
and \(r\) all have length units, where \(\phi\) is an angle (a unitless
length/length ratio). Converting back and forth between Cartesian
and polar coordinates is trivial, as
\[x = r~\mbox{cos} \phi,\]
\[y = r~\mbox{sin} \phi,\]
\[r = \sqrt{x^2 + y^2}, \ \mbox{and}\]
\[\phi = \mbox{atan2}(y, x)\]
where \(\mbox{atan2}\) is used in favor of \(\mbox{atan}(y/x)\) to take care
of the right quadrant.

\hypertarget{ellipsoidal-coordinates-1}{%
\subsection{Ellipsoidal coordinates}\label{ellipsoidal-coordinates-1}}

In three dimensions, where Cartesian coordinates are expressed as
\((x,y,z)\), spherical coordinates are the three-dimensional equivalent
of polar coordinates and can be expressed as \((r,\lambda,\phi)\), where:

\begin{itemize}
\tightlist
\item
  \(r\) is the radius of the sphere,
\item
  \(\lambda\) is the longitude, measured in the \((x,y)\) plane counter-clockwise from positive \(x\), and
\item
  \(\phi\) is the latitude, the angle between the vector and the \((x,y)\) plane.
\end{itemize}

Figure \ref{fig:sphere} illustrates Cartesian geocentric and
ellipsoidal coordinates.

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/sphere-1} 

}

\caption{Cartesian geocentric coordinates (left) measure three distances, ellipsoidal coordinates (right) measure two angles, and possibly an ellipsoidal height}\label{fig:sphere}
\end{figure}

\(\lambda\) typically varies between \(-180^{\circ}\) and \(180^{\circ}\)
(or alternatively from \(0^{\circ}\) to \(360^{\circ}\)), \(\phi\) from
\(-90^{\circ}\) to \(90^{\circ}\). When we are only interested in points
\emph{on} a sphere with given radius, we can drop \(r\): \((\lambda,\phi)\)
now suffice to identify any point.

It should be noted that this is just \emph{a} definition, one could for
instance also choose to measure polar angle, the angle between
the vector and \(z\), instead of latitude. There is also a long
tradition of specifying points as \((\phi,\lambda)\) but throughout
this book we will stick to longitude-latitude, \((\lambda,\phi)\).
The point denoted in figure \ref{fig:sphere} has \((\lambda,\phi)\)
or ellipsoidal coordinates with values

\begin{verbatim}
# POINT (60 47)
\end{verbatim}

with angles measured in degrees, and geocentric coordinates

\begin{verbatim}
# POINT Z (2178844 3773868 4641765)
\end{verbatim}

with unit metres.

For points on an ellipse, there are two ways in which angle can be
expressed (figure \ref{fig:ellipse}): measured from the center of
the ellipse (\(\psi\)), or measured perpendicular to the tangent on
the ellipse at the target point (\(\phi\)).

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{sds_files/figure-latex/ellipse-1} 

}

\caption{Angles on an ellipse: geodetic (blue) and geocentric (red) latitude}\label{fig:ellipse}
\end{figure}

The most commonly used parametric model for the Earth is \emph{an
ellipsoid of revolution}, an ellipsoid with two equal semi-axes
\citep{iliffelott}. In effect, this is a flattened sphere (or spheroid):
the distance between the poles is (slightly: about 0.33\%) smaller
than the distance between two opposite points on the equator. Under
this model, longitude is always measured along a circle (as in
figure \ref{fig:sphere}), and latitude along an ellipse (as in
figure \ref{fig:ellipse}). If we
think of figure \ref{fig:ellipse} as a cross section of the Earth
passing through the poles, the \emph{geodetic} latitude measure \(\phi\)
is the one used when no further specification is given. The latitude
measure \(\psi\) is called the \emph{geocentric latitude}.

In addition to longitude and latitude we can add \emph{altitude}
or elevation to define points that are not on the ellipsoid, and
obtain a three dimensional space again. When defining altitude,
we need to choose:

\begin{itemize}
\tightlist
\item
  where zero altitude is: on the ellipsoid, or relative to the surface approximating mean sea level (the geoid)?
\item
  which direction is positive, and
\item
  which direction is ``straight up'': perpendicular to the ellipsoid surface,
  or in the direction perpendicular to the surface of the geoid?
\end{itemize}

All these choices may matter, depending on the application area
and required measurement accuracies.

The shape of the Earth is not a perfect ellipsoid. As a consequence,
several ellipsoids with different shape parameters and bound to
the Earth in different ways are being used. Such ellipsoids are called
\emph{datums}, and are briefly discussed in section \ref{crs}, along
with \emph{coordinate reference systems}.

\hypertarget{projections}{%
\subsection{Projected coordinates, distances}\label{projections}}

Because paper maps and computer screens are much more abundant
and practical than globes, most of the time we look at spatial
data we see it \emph{projected}: drawn on a flat, two-dimensional
surface. Computing the locations in a two-dimensional space means
that we work with \emph{projected} coordinates. Projecting ellipsoidal
coordinates means that shapes, directions, areas, or even all three,
are distorted \citep{iliffelott}.

Distances between two points \(p_i\) and \(p_j\) in Cartesian coordinates are computed
as Euclidean distances, in two dimensions by
\[d_{ij} = \sqrt{(x_i-x_j)^2+(y_i-y_j)^2}\]
with \(p_i = (x_i,y_i)\)
and in three dimensions by
\[d_{ij} = \sqrt{(x_i-x_j)^2+(y_i-y_j)^2+(z_i-z_j)^2}\]
with \(p_i = (x_i,y_i,z_i).\)
These distances represent the length of a \emph{straight} line between
two points \(i\) and \(j\).

For two points on a circle, the length of the arc of two points \(c_1 = (r,{\phi}_i)\) and
\(c_2 = (r, \phi_2)\) is
\[s_{ij}=r~|\phi_1-\phi_2| = r ~\theta\]
with \(\theta\) the angle between \(\phi_1\) and \(\phi_2\) in radians.
For very small values of \(\theta\), we will have \(s_{ij} \approx d_{ij}\),
because a small arc segment is nearly straight.

For two points \(p_1 = (\lambda_1,\phi_1)\) and \(p_2 = (\lambda_2,\phi_2)\) on a sphere with radius \(r'\), the \emph{great circle
distance} is the arc length between \(p_1\) and \(p_2\) on the circle
that passes through \(p_1\) and \(p_2\) and has the center of the sphere as its center, and
is given by \(s_{12} = r ~ \theta_{12}\) with
\[\theta_{12} = \arccos(\sin \phi_1 \cdot \sin \phi_2 + \cos \phi_1 \cdot \cos \phi_2 \cdot \cos(|\lambda_1-\lambda_2|))\]
the angle between \(p_1\) and \(p_2\), in radians.

Arc distances between two points on a spheroid are more complicated
to compute; a good discussion on the topic and an explanation of
the method implemented in GeographicLib (part of PROJ) is given
in \citet{karney2013algorithms}.

To show that these distance measures actually give different
values, we computed them for the distance Berlin - Paris. Here,
\texttt{gc\_} refers to ellipsoidal and spherical great circle distances,
\texttt{straight\_} refers to straight line, Euclidean distances between
Cartesian geocentric coordinates associated on the WGS84 ellipse
and sphere:

\begin{verbatim}
# Spherical geometry (s2) switched off
# Spherical geometry (s2) switched on
# Units: [km]
#       gc_ellipse straight_ellipse        gc_sphere 
#           879.70           879.00           877.46 
#  straight_sphere 
#           876.77
\end{verbatim}

\hypertarget{bounded}{%
\subsection{Bounded and unbounded spaces}\label{bounded}}

Two-dimensional and three-dimensional Euclidean spaces (\(R^2\) and
\(R^3\)) are unbounded: every line in this space has infinite length,
distances, areas or volumes are unbounded. In contrast, spaces
defined on a circle (\(S^1\)) or sphere (\(S^2\)) define a bounded set:
there may be infinitely many points but the length and area of the
circle and the radius, area and volume of a sphere are bound.

This may sound trivial, but leads to some interesting findings
when handling spatial data. A polygon on \(R^2\) has unambiguously an
inside and an outside. On a sphere, \(S^2\), any polygon divides
the sphere in two parts, and which of these two is to be considered
inside and which outside is ambiguous and needs to be defined
e.g.~by the traversal direction. Chapter \ref{spherical} will
further discuss consequences when working with geometries on \(S^2\).

\hypertarget{crs}{%
\section{Coordinate Reference Systems}\label{crs}}

We follow \citet{lott2015} when defining the following concepts (italics indicate literal quoting):

\begin{itemize}
\tightlist
\item
  a \textbf{coordinate system} is a \emph{set of mathematical rules for specifying how coordinates are to be assigned to points},
\item
  a \textbf{datum} is a \emph{parameter or set of parameters that define the position of the origin, the scale, and the orientation of a coordinate system},
\item
  a \textbf{geodetic datum} is a \emph{datum describing the relationship of a two- or three-dimensional coordinate system to the Earth}, and
\item
  a \textbf{coordinate reference system} is a \emph{coordinate system that is related to an object by a datum; for geodetic and vertical datums, the object will be the Earth.}
\end{itemize}

A readable text that further explains these concepts is \citet{iliffelott}.

The Earth does not follow a regular shape. The topography of the
Earth is of course known to vary strongly, but also the surface
formed by constant gravity at mean sea level, the geoid, is
irregular. A commonly used model that is fit to the geoid is an
ellipsoid of revolution, which is an ellipse with two identical
minor axes. Fitting such an ellipsoid to the Earth gives a datum.
However, fitting it to different areas, or based on different sets of
reference points gives different fits, and hence different datums:
a datum can for instance be fixed to a particular tectonic plate
(like ETRS89), others can be globally fit (like WGS84). More local
fits lead to smaller approximation errors.

The definitions above imply that coordinates in degrees longitude
and latitude only have a meaning, i.e.~can only be interpreted
unambiguously as Earth coordinates, when the datum they are
associated with is given.

Note that for projected data, the data that \emph{were} projected
are associated with a reference ellipsoid (datum). Going from
one projection to another \emph{without} changing datum is called
\emph{coordinate conversion}, and passes through the ellipsoidal
coordinates associated with the datum involved. This process is
lossless and invertible: the parameters and equations associated
with a \emph{conversion} are not empirical. Recomputing coordinates in a
new datum is called \emph{coordinate transformation}, and is approximate:
because datums are a result of model fitting, transformations
between datums are models too that have been fit; the equations
involved are empirical, and multiple transformation paths, based
on different model fits and associated with different accuracies,
are possible.

Plate tectonics imply that within a global datum, fixed objects may
have coordinates that change over time, and that transformations
from one datum to another may be time-dependent. Earthquakes are a
cause of more local and sudden changes in coordinates.

\hypertarget{projlib}{%
\section{PROJ and mapping accuracy}\label{projlib}}

Very few living people active in open source geospatial software
can remember the time before PROJ. PROJ \citep{evenden:90} started in the
1970s as a Fortran project, and was released in 1985 as a C library
for cartographic projections. It came with command line tools for
direct and inverse projections, and could be linked to software
to let it support (re)projection directly. Originally, datums were
considered implicit, and no datum transformations were allowed.

In the early 2000s, PROJ was known as PROJ.4, after its never changing
major version number. Amongst others motivated by the rise of GPS,
the need for datum transformations increased and PROJ.4 was extended
with rudimentary datum support. PROJ definitions for coordinate
reference systems would look like this:

\begin{verbatim}
+proj=utm +zone=33 +datum=WGS84 +units=m +no_defs
\end{verbatim}

where \emph{key}=\emph{value} pairs are preceded by a \texttt{+} and separated by
a space. This form came to be known as ``PROJ.4 string'', since the
PROJ project stayed at version 4.x for several decades. Other datums
would come with fields like:

\begin{verbatim}
+ellps=bessel +towgs84=565.4,50.3,465.6,-0.399,0.344,-1.877,4.072
\end{verbatim}

indicating another ellipse, as well as the seven (or three)
parameters for transforming from this ellipse to WGS84 (the ``World
Geodetic System 1984'' global datum once popularized by GPS),
effectively defining the datum in terms of a transformation to WGS84.

Along with PROJ.4 came a set of databases with known (registered)
projections, from which the best known is the EPSG registry.
National mapping agencies would provide (and update over time)
their best guesses of \texttt{+towgs84=} parameters for national coordinate
reference systems, and distribute it through the EPSG registry,
which was part of PROJ distributions.
For some transformations, \emph{datum grids} were available and
distributed as part of PROJ.4: such grids are raster maps that provide
for every location pre-computed values for the shift in longitude
and latitude, or elevation, for a particular datum transformation.

\begin{verbatim}
# downsample set to c(3,3,1)
\end{verbatim}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/horizontalgrid-1} 

}

\caption{UK horizontal datum grid, from datum OSGB 1936 (EPSG:4277) to datum ETRS89 (EPSG:4258); units arc-seconds}\label{fig:horizontalgrid}
\end{figure}

\begin{verbatim}
# downsample set to c(2,2)
\end{verbatim}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/verticalgrid-1} 

}

\caption{UK vertical datum grid, from ETRS89 (EPSG:4937) to ODN height (EPSG:5701), units m}\label{fig:verticalgrid}
\end{figure}

In PROJ.4, every coordinate transformation had to go through a
conversion to and from WGS84; even reprojecting data associated with
a datum different from WGS84 had to go through a transformation to
and from WGS84. The associated errors of up to 100 m were acceptable
for mapping purposes for not too small areas, but applications that
need high accuracy transformations, e.g.~precision agriculture,
planning flights of UAV's, or object tracking are often more
demanding in terms of accuracy.

In 2018, after a successful ``GDAL Coordinate System Barn Raising''
initiative, a number of companies profiting from the open source
geospatial software stack supported the development of a more modern,
mature coordinate transformation system in PROJ. Over a few years,
PROJ.4 evolved through versions 5, 6, 7 and 8 and was hence renamed
into PROJ (or PR\(\phi\)J).

The most notable changes include:

\begin{itemize}
\tightlist
\item
  although PROJ.4 strings can still be used to initialize certain
  coordinate reference systems, they are no longer sufficient to
  represent all of them; a new format, WKT2 (described in next section)
  replaces it
\item
  WGS84 as a hub datum is dropped: coordinate transformation no longer
  need to go through a particular datum
\item
  multiple conversion or transformation paths (so-called pipelines)
  to go from CRS A to CRS B are possible, and can be reported along
  with the associated accuracy; PROJ will by default use the most accurate
  one but user control is possible
\item
  transformation pipelines can chain an arbitrary number of
  elementary transformation operations, including swapping of axes
  and unit transformations
\item
  datum grids, of which there are now \emph{many} more, are no longer distributed
  with the library but are accessible from a content delivery network (CDN); PROJ allows to enabling and
  disabling network access to access these grids, and only downloads the section
  of the grid actually needed, storing it in a cache on the user's machine for
  future use
\item
  coordinate transformations receive support for epochs, time-dependent
  transformations (and hence: four-dimensional coordinates, including the
  source and target time)
\item
  the set of files with registered coordinate reference systems is handled
  in an SQLite database
\item
  instead of always handling axis order (longitude, latitude),
  when the authority defines differently this is now obeyed (with the
  most notable example: EPSG:4326 defines axis order to be latitude,
  longitude.)
\end{itemize}

All these points sound like massive improvements, and accuracies
of transformation can be below 1 metre. An interesting point
is the last: Where we could safely assume for many decades that
spatial data with ellipsoidal coordinates would have axis order
(longitude, latitude), this is no longer the case. We will see in
section \ref{axisorder} how to deal with this.

Examples of a horizontal datum grids, downloaded from cdn.proj.org,
are shown in figure \ref{fig:horizontalgrid} and for a vertical
datum grid in figure \ref{fig:verticalgrid}. Datum grids may carry
per-pixel accuracy values.

\hypertarget{wkt2}{%
\section{WKT-2}\label{wkt2}}

\citet{lott2015} describes a standard for encoding coordinate reference
systems, as well as transformations between them using \emph{well known
text}; the standard (and format) is referred to informally as
WKT-2. As mentioned above, GDAL and PROJ fully support this encoding.
An example of WKT2 for CRS \texttt{OGC:CRS84} is:

\begin{verbatim}
GEOGCRS["WGS 84",
    DATUM["World Geodetic System 1984",
        ELLIPSOID["WGS 84",6378137,298.257223563,
            LENGTHUNIT["metre",1]],
        ID["EPSG",6326]],
    PRIMEM["Greenwich",0,
        ANGLEUNIT["degree",0.0174532925199433],
        ID["EPSG",8901]],
    CS[ellipsoidal,2],
        AXIS["longitude",east,
            ORDER[1],
            ANGLEUNIT["degree",0.0174532925199433,
                ID["EPSG",9122]]],
        AXIS["latitude",north,
            ORDER[2],
            ANGLEUNIT["degree",0.0174532925199433,
                ID["EPSG",9122]]]]
\end{verbatim}

This shows a WGS84 ellipsoid, and a coordinate system with the axis
order (longitude, latitude) that can be used to replace \texttt{EPSG:4326}
when one wants unambiguously ``traditional'' (GIS) axis order.

A longer introduction on the history and recent changes in PROJ is
given in \citet{rogerCRS}, building upon the work of \citet{knudsen+evers17} and
\citet{evers+knudsen17}.

\hypertarget{exercises-1}{%
\section{Exercises}\label{exercises-1}}

Try to solve the following exercises with R (without loading packages); try to use functions where appropriate:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  list three \emph{geographic} measures that do not have a natural zero origin
\item
  convert the \((x,y)\) points \((10,2)\), \((-10,-2)\), \((10,-2)\) and \((0,10)\) to polar coordinates
\item
  convert the polar \((r,\phi)\) points \((10,45^{\circ})\), \((0,100^{\circ})\) and \((5,359^{\circ})\) to Cartesian coordinates
\item
  assuming the Earth is a sphere with a radius of 6371 km, compute for \((\lambda,\phi)\) points the great circle distance between \((10,10)\) and \((11,10)\), between \((10,80)\) and \((11,80)\), between \((10,10)\) and \((10,11)\) and between \((10,80)\) and \((10,81)\) (units: degree). What are the distance units?
\end{enumerate}

\hypertarget{geometries}{%
\chapter{Geometries}\label{geometries}}

Having learned how we represent coordinates systems, we can define
how geometries can be described using these coordinate systems. This
chapter will explain:

\begin{itemize}
\tightlist
\item
  \emph{simple features}, a standard that describes point, line and polygon
  geometries along with operations on them,
\item
  operations on geometries
\item
  coverages, subdivisions of larger regions into sub-regions
\item
  networks
\end{itemize}

Geometries on the sphere are discussed in chapter \ref{spherical},
rasters and other rectangular subdivisions of space are discussed
in chapter \ref{datacube}.

\hypertarget{simplefeatures}{%
\section{Simple feature geometries}\label{simplefeatures}}

Simple feature geometries are a way to describe the geometries of
\emph{features}. By \emph{features} we mean \emph{things} that have a geometry,
potentially some time properties, and other attributes that could
include a label describing the thing and quantitative measures of it.
The main application of simple feature geometries is to describe
geometries in two-dimensional space by points, lines, or polygons. The
``simple'' adjective refers to the fact that the line or polygon
geometries are represented by sequences of points connected with
straight lines that do not self-intersect.

\emph{Simple features access} is a standard \citep{sfa, sfa2, iso} for
describing simple feature geometries that includes:

\begin{itemize}
\tightlist
\item
  a class hierarchy
\item
  a set of operations
\item
  binary and text encodings
\end{itemize}

We will first discuss the seven most common simple feature geometry
types.

\hypertarget{seven}{%
\subsection{The big seven}\label{seven}}

The most commonly used simple features geometries, used to represent a \emph{single} feature are:

\begin{longtable}[]{@{}ll@{}}
\toprule
\begin{minipage}[b]{0.25\columnwidth}\raggedright
type\strut
\end{minipage} & \begin{minipage}[b]{0.69\columnwidth}\raggedright
description\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.25\columnwidth}\raggedright
\texttt{POINT}\strut
\end{minipage} & \begin{minipage}[t]{0.69\columnwidth}\raggedright
single point geometry\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.25\columnwidth}\raggedright
\texttt{MULTIPOINT}\strut
\end{minipage} & \begin{minipage}[t]{0.69\columnwidth}\raggedright
set of points\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.25\columnwidth}\raggedright
\texttt{LINESTRING}\strut
\end{minipage} & \begin{minipage}[t]{0.69\columnwidth}\raggedright
single linestring (two or more points connected by straight lines)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.25\columnwidth}\raggedright
\texttt{MULTILINESTRING}\strut
\end{minipage} & \begin{minipage}[t]{0.69\columnwidth}\raggedright
set of linestrings\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.25\columnwidth}\raggedright
\texttt{POLYGON}\strut
\end{minipage} & \begin{minipage}[t]{0.69\columnwidth}\raggedright
exterior ring with zero or more inner rings, denoting holes\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.25\columnwidth}\raggedright
\texttt{MULTIPOLYGON}\strut
\end{minipage} & \begin{minipage}[t]{0.69\columnwidth}\raggedright
set of polygons\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.25\columnwidth}\raggedright
\texttt{GEOMETRYCOLLECTION}\strut
\end{minipage} & \begin{minipage}[t]{0.69\columnwidth}\raggedright
set of the geometries above\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/sfgeometries-1} 

}

\caption{sketches of the main simple feature geometry types}\label{fig:sfgeometries}
\end{figure}

Figure \ref{fig:sfgeometries} shows examples of these basic
geometry types. The human-readable, ``well-known-text'' (WKT) representation
of the geometries plotted are:

\begin{verbatim}
POINT (0 1)
MULTIPOINT ((1 1), (2 2), (4 1), (2 3), (1 4))
LINESTRING (1 1, 5 5, 5 6, 4 6, 3 4, 2 3)
MULTILINESTRING ((1 1, 5 5, 5 6, 4 6, 3 4, 2 3), (3 0, 4 1, 2 1))
POLYGON ((2 1, 3 1, 5 2, 6 3, 5 3, 4 4, 3 4, 1 3, 2 1),
    (2 2, 3 3, 4 3, 4 2, 2 2))
MULTIPOLYGON (((2 1, 3 1, 5 2, 6 3, 5 3, 4 4, 3 4, 1 3, 2 1),
    (2 2, 3 3, 4 3, 4 2, 2 2)), ((3 7, 4 7, 5 8, 3 9, 2 8, 3 7)))
GEOMETRYCOLLECTION (
    POLYGON ((2 1, 3 1, 5 2, 6 3, 5 3, 4 4, 3 4, 1 3, 2 1),
      (2 2 , 3 3, 4 3, 4 2, 2 2)),
    LINESTRING (1 6, 5 10, 5 11, 4 11, 3 9, 2 8),
    POINT (2 5),
    POINT (5 4)
)
\end{verbatim}

In this representation, coordinates are separated by space, and
points by commas. Sets are grouped by parentheses, and separated
by commas.

Individual points in a geometry contain at least two coordinates:
x and y, in that order. If these coordinates refer to ellipsoidal
coordinates, x and y usually refer to longitude and latitude,
respectively, although sometimes to latitude and longitude (see
sections \ref{projlib} and \ref{axisorder}).

\hypertarget{valid}{%
\subsection{Simple and valid geometries, ring direction}\label{valid}}

Linestrings are called \emph{simple} when they do not self-intersect:

\begin{verbatim}
# LINESTRING (0 0, 1 1, 2 2, 0 2, 1 1, 2 0)
# is_simple 
#     FALSE
\end{verbatim}

Valid polygons and multipolygons obey all of the following properties:

\begin{itemize}
\tightlist
\item
  polygon rings are closed (the last point equals the first)
\item
  polygon holes (inner rings) are inside their exterior ring
\item
  polygon inner rings maximally touch the exterior ring in single points, not over a line
\item
  a polygon ring does not repeat its own path
\item
  in a multipolygon, an external ring maximally touches another exterior ring in single points, not over a line
\end{itemize}

If this is not the case, the geometry concerned is not valid. Invalid
geometries typically cause errors when they are processed, but can
usually be repaired to make them valid.

A further convention is that the outer ring of a polygon is winded
counter-clockwise, while the holes are winded clockwise, but polygons
for which this is not the case are still considered valid. For polygons
on the sphere, the ``clockwise'' is not very useful: if for instance we take
the equator as polygon, is the Northern hemisphere or the Southern hemisphere
``inside''? The convention taken here is to consider the area on the left while
traversing the polygon is considered the polygon's inside.

\hypertarget{z-and-m-coordinates}{%
\subsection{Z and M coordinates}\label{z-and-m-coordinates}}

In addition to X and Y coordinates, Single points (vertices) of
simple feature geometries may have:

\begin{itemize}
\tightlist
\item
  a \texttt{Z} coordinate, denoting altitude, and/or
\item
  an \texttt{M} value, denoting some ``measure''
\end{itemize}

The \texttt{M} attribute shall be a property of the vertex. It sounds
attractive to encode a time stamp in it, e.g.~to pack movement data
(trajectories) in \texttt{LINESTRING}s. These become however invalid (or
``non-simple'') once the trajectory self-intersects, which easily
happens when only \texttt{X} and \texttt{Y} are considered for self-intersections.

Both \texttt{Z} and \texttt{M} are not found often, and software support
to do something useful with them is (still) rare. Their
WKT representation are fairly easily understood:

\begin{verbatim}
# POINT Z (1 3 2)
# POINT M (1 3 2)
# LINESTRING ZM (3 1 2 4, 4 4 2 2)
\end{verbatim}

\hypertarget{empty-geometries}{%
\subsection{Empty geometries}\label{empty-geometries}}

A very important concept in the feature geometry framework is that of the
empty geometry.
Empty geometries arise naturally when we do geometrical
operations (section \ref{opgeom}), for instance when we want to
know the intersection of \texttt{POINT\ (0\ 0)} and \texttt{POINT\ (1\ 1)}:

\begin{verbatim}
# GEOMETRYCOLLECTION EMPTY
\end{verbatim}

and it represents essentially the empty set: when combining
(unioning) an empty point it with other non-empty geometries,
it vanishes.

All geometry types have a special value representing the empty (typed) geometry:

\begin{verbatim}
# POINT EMPTY
# LINESTRING M EMPTY
\end{verbatim}

and so on, but they all point to the empty set, differing only in their
dimension (section \ref{de9im}).

\hypertarget{ten-further-geometry-types}{%
\subsection{Ten further geometry types}\label{ten-further-geometry-types}}

There are 10 more geometry types which are more rare, but increasingly find implementation:

\begin{longtable}[]{@{}ll@{}}
\toprule
\begin{minipage}[b]{0.26\columnwidth}\raggedright
type\strut
\end{minipage} & \begin{minipage}[b]{0.68\columnwidth}\raggedright
description\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.26\columnwidth}\raggedright
\texttt{CIRCULARSTRING}\strut
\end{minipage} & \begin{minipage}[t]{0.68\columnwidth}\raggedright
The CIRCULARSTRING is the basic curve type, similar to a LINESTRING in the linear world. A single segment requires three points, the start and end points (first and third) and any other point on the arc. The exception to this is for a closed circle, where the start and end points are the same. In this case the second point MUST be the center of the arc, i.e.~the opposite side of the circle. To chain arcs together, the last point of the previous arc becomes the first point of the next arc, just like in LINESTRING. This means that a valid circular string must have an odd number of points greater than 1.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.26\columnwidth}\raggedright
\texttt{COMPOUNDCURVE}\strut
\end{minipage} & \begin{minipage}[t]{0.68\columnwidth}\raggedright
A compound curve is a single, continuous curve that has both curved (circular) segments and linear segments. That means that in addition to having well-formed components, the end point of every component (except the last) must be coincident with the start point of the following component.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.26\columnwidth}\raggedright
\texttt{CURVEPOLYGON}\strut
\end{minipage} & \begin{minipage}[t]{0.68\columnwidth}\raggedright
Example compound curve in a curve polygon: CURVEPOLYGON(COMPOUNDCURVE(CIRCULARSTRING(0 0,2 0, 2 1, 2 3, 4 3),(4 3, 4 5, 1 4, 0 0)), CIRCULARSTRING(1.7 1, 1.4 0.4, 1.6 0.4, 1.6 0.5, 1.7 1) )\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.26\columnwidth}\raggedright
\texttt{MULTICURVE}\strut
\end{minipage} & \begin{minipage}[t]{0.68\columnwidth}\raggedright
A MultiCurve is a 1-dimensional GeometryCollection whose elements are Curves, it can include linear strings, circular strings or compound strings.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.26\columnwidth}\raggedright
\texttt{MULTISURFACE}\strut
\end{minipage} & \begin{minipage}[t]{0.68\columnwidth}\raggedright
A MultiSurface is a 2-dimensional GeometryCollection whose elements are Surfaces, all using coordinates from the same coordinate reference system.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.26\columnwidth}\raggedright
\texttt{CURVE}\strut
\end{minipage} & \begin{minipage}[t]{0.68\columnwidth}\raggedright
A Curve is a 1-dimensional geometric object usually stored as a sequence of Points, with the subtype of Curve specifying the form of the interpolation between Points\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.26\columnwidth}\raggedright
\texttt{SURFACE}\strut
\end{minipage} & \begin{minipage}[t]{0.68\columnwidth}\raggedright
A Surface is a 2-dimensional geometric object\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.26\columnwidth}\raggedright
\texttt{POLYHEDRALSURFACE}\strut
\end{minipage} & \begin{minipage}[t]{0.68\columnwidth}\raggedright
A PolyhedralSurface is a contiguous collection of polygons, which share common boundary segments\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.26\columnwidth}\raggedright
\texttt{TIN}\strut
\end{minipage} & \begin{minipage}[t]{0.68\columnwidth}\raggedright
A TIN (triangulated irregular network) is a PolyhedralSurface consisting only of Triangle patches.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.26\columnwidth}\raggedright
\texttt{TRIANGLE}\strut
\end{minipage} & \begin{minipage}[t]{0.68\columnwidth}\raggedright
A Triangle is a polygon with 3 distinct, non-collinear vertices and no interior boundary\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\texttt{CIRCULASTRING}, \texttt{COMPOUNDCURVE} and \texttt{CURVEPOLYGON} are not
described in the SFA standard, but in the \href{https://www.iso.org/standard/38651.html}{SQL-MM part 3
standard}. The
descriptions above were copied from the \href{http://postgis.net/docs/using_postgis_dbmanagement.html}{PostGIS
manual}.

\hypertarget{text-and-binary-encodings}{%
\subsection{Text and binary encodings}\label{text-and-binary-encodings}}

Part of the simple feature standard are two encodings: a text and
a binary encoding. The well-known text encoding, used above, is
human-readable, the well-known binary encoding is machine-readable.
Binary encodings are lossless and typically faster to work with
than text encoding (and decoding), and are used for instance in all
communications between R package \texttt{sf} and the GDAL, GEOS, liblwgeom
and s2geometry libraries (figure \ref{fig:gdal-fig-nodetails}).

\hypertarget{opgeom}{%
\section{Operations on geometries}\label{opgeom}}

Simple feature geometries can be queried for properties,
transformed into new geometries, and combinations of geometries can
be queried for properties. This section gives an overview of the
operations entirely focusing on \emph{geometrical} properties. Chapter
\ref{featureattributes} focuses on the analysis of non-geometrical
feature properties, in relationship to their geometries. Some of
the material in this section appeared in \citet{rjsf}.

We can categorize operations on geometries in terms of what they
take as input, and what they return as output. In terms of output
we have operations that return:

\begin{itemize}
\tightlist
\item
  \textbf{predicates}: a logical asserting a certain property is \texttt{TRUE}
\item
  \textbf{measures}: a quantity (e.g.~a numeric value with measurement unit)
\item
  \textbf{transformations}: newly generated geometries
\end{itemize}

and in terms of what they operate on, we distinguish operations
that are:

\begin{itemize}
\tightlist
\item
  \textbf{unary} when they work on a single geometry
\item
  \textbf{binary} when they work on pairs of geometries
\item
  \textbf{n-ary} when they work on sets of geometries
\end{itemize}

\hypertarget{unary-predicates}{%
\subsection{Unary predicates}\label{unary-predicates}}

Unary predicates describe a certain property of a geometry.
The predicates \texttt{is\_simple}, \texttt{is\_valid}, and \texttt{is\_empty} return
respectively whether a geometry is simple, valid or empty. Given a
coordinate reference system, \texttt{is\_longlat} returns whether the
coordinates are geographic or projected. \texttt{is(geometry,\ class)}
checks whether a geometry belongs to a particular class.

\hypertarget{de9im}{%
\subsection{Binary predicates and DE-9IM}\label{de9im}}

The Dimensionally Extended Nine-Intersection Model (DE-9IM,
\citet{de9im1}; \citet{de9im2}) is a model that helps describing the qualitative
relation between any two geometries in two-dimensional space
(\(R^2\)). Any geometry has a \emph{dimension} value that is:

\begin{itemize}
\tightlist
\item
  0 for points,
\item
  1 for linear geometries,
\item
  2 for polygonal geometries, and
\item
  F (false) for empty geometries
\end{itemize}

Any geometry also has an inside (I), a boundary (B) and an exterior (E); these
roles are obvious for polygons but, e.g.~for:

\begin{itemize}
\tightlist
\item
  \textbf{lines} the boundary is formed by the end points, and the interior
  by all non-end points on the line
\item
  \textbf{points} have a zero-dimensional inside but no boundary
\end{itemize}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/de9im-1} 

}

\caption{DE-9IM: intersections between the interior, boundary and exterior of a polygon (rows) and of a linestring (columns) indicated by red}\label{fig:de9im}
\end{figure}

Figure \ref{fig:de9im} shows the intersections between the I,
B and E components of a polygon and a linestring indicated by red;
the sub-plot title gives the dimension of these intersections (0,
1, 2 or F). The relationship between the two geometries is the
concatenation of these dimensions:

\begin{verbatim}
#      [,1]       
# [1,] "1020F1102"
\end{verbatim}

Using the ability to express relationships, we can also query
pairs of geometries about particular conditions expressed in a
\emph{mask string}; e.g.~the string \texttt{"*0*******"} would evaluate \texttt{TRUE}
when the second geometry has one or more boundary points in common
with the interior of the first geometry; the symbol \texttt{*} standing for
``any dimensionality'' (0, 1, 2 or F). The mask string \texttt{"T********"}
matches pairs of geometry with intersecting interiors. Here, the
symbol \texttt{T} stands for any non-empty intersection (of dimensionality
0, 1 or 2).

Binary predicates are further described using normal-language verbs,
using DE-9IM definitions. For instance, the predicate \texttt{equals}
corresponds to the relationship \texttt{"T*F**FFF*"}. If any two geometries
obey this relationship, they are (topologically) equal, but may
have a different ordering of nodes.

A list of binary predicates is:

\begin{longtable}[]{@{}lll@{}}
\toprule
\begin{minipage}[b]{0.23\columnwidth}\raggedright
predicate\strut
\end{minipage} & \begin{minipage}[b]{0.54\columnwidth}\raggedright
meaning\strut
\end{minipage} & \begin{minipage}[b]{0.13\columnwidth}\raggedright
inverse of\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{equals}\strut
\end{minipage} & \begin{minipage}[t]{0.54\columnwidth}\raggedright
Two geometries A and B are topologically identical\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{contains}\strut
\end{minipage} & \begin{minipage}[t]{0.54\columnwidth}\raggedright
None of the points of A are outside B\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedright
\texttt{within}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{contains\_properly}\strut
\end{minipage} & \begin{minipage}[t]{0.54\columnwidth}\raggedright
A contains B and B has no points in common with the boundary of A\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{covers}\strut
\end{minipage} & \begin{minipage}[t]{0.54\columnwidth}\raggedright
No points of B lie in the exterior of A\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedright
\texttt{covered\_by}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{covered\_by}\strut
\end{minipage} & \begin{minipage}[t]{0.54\columnwidth}\raggedright
Inverse of \texttt{covers}\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{crosses}\strut
\end{minipage} & \begin{minipage}[t]{0.54\columnwidth}\raggedright
A and B have some but not all interior points in common\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{disjoint}\strut
\end{minipage} & \begin{minipage}[t]{0.54\columnwidth}\raggedright
A and B have no points in common\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedright
\texttt{intersects}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{equals}\strut
\end{minipage} & \begin{minipage}[t]{0.54\columnwidth}\raggedright
A and B are geometrically equal; node order number of nodes may differ; identical to A contains B AND A within B\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{equals\_exact}\strut
\end{minipage} & \begin{minipage}[t]{0.54\columnwidth}\raggedright
A and B are geometrically equal, and have identical node order\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{intersects}\strut
\end{minipage} & \begin{minipage}[t]{0.54\columnwidth}\raggedright
A and B are not disjoint\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedright
\texttt{disjoint}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{is\_within\_distance}\strut
\end{minipage} & \begin{minipage}[t]{0.54\columnwidth}\raggedright
A is closer to B than a given distance\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{within}\strut
\end{minipage} & \begin{minipage}[t]{0.54\columnwidth}\raggedright
None of the points of B are outside A\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedright
\texttt{contains}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{touches}\strut
\end{minipage} & \begin{minipage}[t]{0.54\columnwidth}\raggedright
A and B have at least one boundary point in common, but no interior points\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{overlaps}\strut
\end{minipage} & \begin{minipage}[t]{0.54\columnwidth}\raggedright
A and B have some points in common; the dimension of these is identical to that of A and B\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{relate}\strut
\end{minipage} & \begin{minipage}[t]{0.54\columnwidth}\raggedright
given a mask pattern, return whether A and B adhere to this pattern\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

The \href{https://en.wikipedia.org/wiki/DE-9IM}{Wikipedia DE-9IM page}
provides the \texttt{relate} patterns for each of these verbs. They are
important to check out; for instance \emph{covers} and \emph{contains} (and
their inverses) are often not completely intuitive:

\begin{itemize}
\tightlist
\item
  if A \emph{contains} B, B has no points in common with the exterior \emph{or
  boundary} of A
\item
  if A \emph{covers} B, B has no points in common with the exterior of A
\end{itemize}

\hypertarget{unary-measures}{%
\subsection{Unary Measures}\label{unary-measures}}

Unary measures return a measure or quantity that describes a property of
the geometry:

\begin{longtable}[]{@{}ll@{}}
\toprule
\begin{minipage}[b]{0.24\columnwidth}\raggedright
measure\strut
\end{minipage} & \begin{minipage}[b]{0.70\columnwidth}\raggedright
returns\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.24\columnwidth}\raggedright
\texttt{dimension}\strut
\end{minipage} & \begin{minipage}[t]{0.70\columnwidth}\raggedright
0 for points, 1 for linear, 2 for polygons, possibly \texttt{NA} for empty geometries\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\raggedright
\texttt{area}\strut
\end{minipage} & \begin{minipage}[t]{0.70\columnwidth}\raggedright
the area of a geometry\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\raggedright
\texttt{length}\strut
\end{minipage} & \begin{minipage}[t]{0.70\columnwidth}\raggedright
the length of a linear geometry\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{binary-measures}{%
\subsection{Binary Measures}\label{binary-measures}}

\texttt{distance} returns the distance between pairs of geometries.
The qualitative measure \texttt{relate} (without mask) gives the relation
pattern, a description of the geometrical relationship between two
geometries explained in section \ref{de9im}.

\hypertarget{unary-transformers}{%
\subsection{Unary Transformers}\label{unary-transformers}}

Unary transformations work on a per-geometry basis, and for each geometry return a new geometry.

\begin{longtable}[]{@{}ll@{}}
\toprule
\begin{minipage}[b]{0.23\columnwidth}\raggedright
transformer\strut
\end{minipage} & \begin{minipage}[b]{0.71\columnwidth}\raggedright
returns a geometry \ldots{}\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{centroid}\strut
\end{minipage} & \begin{minipage}[t]{0.71\columnwidth}\raggedright
of type \texttt{POINT} with the geometry's centroid\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{buffer}\strut
\end{minipage} & \begin{minipage}[t]{0.71\columnwidth}\raggedright
that is this larger (or smaller) than the input geometry, depending on the buffer size\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{jitter}\strut
\end{minipage} & \begin{minipage}[t]{0.71\columnwidth}\raggedright
that was moved in space a certain amount, using a bivariate uniform distribution\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{wrap\_dateline}\strut
\end{minipage} & \begin{minipage}[t]{0.71\columnwidth}\raggedright
cut into pieces that do no longer cover the dateline\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{boundary}\strut
\end{minipage} & \begin{minipage}[t]{0.71\columnwidth}\raggedright
with the boundary of the input geometry\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{convex\_hull}\strut
\end{minipage} & \begin{minipage}[t]{0.71\columnwidth}\raggedright
that forms the convex hull of the input geometry (figure \ref{fig:vor})\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{line\_merge}\strut
\end{minipage} & \begin{minipage}[t]{0.71\columnwidth}\raggedright
after merging connecting \texttt{LINESTRING} elements of a \texttt{MULTILINESTRING} into longer \texttt{LINESTRING}s.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{make\_valid}\strut
\end{minipage} & \begin{minipage}[t]{0.71\columnwidth}\raggedright
that is valid\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{node}\strut
\end{minipage} & \begin{minipage}[t]{0.71\columnwidth}\raggedright
with added nodes to linear geometries at intersections without a node; only works on individual linear geometries\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{point\_on\_surface}\strut
\end{minipage} & \begin{minipage}[t]{0.71\columnwidth}\raggedright
with a (arbitrary) point on a surface\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{polygonize}\strut
\end{minipage} & \begin{minipage}[t]{0.71\columnwidth}\raggedright
of type polygon, created from lines that form a closed ring\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{segmentize}\strut
\end{minipage} & \begin{minipage}[t]{0.71\columnwidth}\raggedright
a (linear) geometry with nodes at a given density or minimal distance\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{simplify}\strut
\end{minipage} & \begin{minipage}[t]{0.71\columnwidth}\raggedright
simplified by removing vertices/nodes (lines or polygons)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{split}\strut
\end{minipage} & \begin{minipage}[t]{0.71\columnwidth}\raggedright
that has been split with a splitting linestring\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{transform}\strut
\end{minipage} & \begin{minipage}[t]{0.71\columnwidth}\raggedright
transformed or convert to a new coordinate reference system (chapter \ref{cs})\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{triangulate}\strut
\end{minipage} & \begin{minipage}[t]{0.71\columnwidth}\raggedright
with Delauney triangulated polygon(s) (figure \ref{fig:vor})\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{voronoi}\strut
\end{minipage} & \begin{minipage}[t]{0.71\columnwidth}\raggedright
with the Voronoi tessellation of an input geometry (figure \ref{fig:vor})\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{zm}\strut
\end{minipage} & \begin{minipage}[t]{0.71\columnwidth}\raggedright
with removed or added \texttt{Z} and/or \texttt{M} coordinates\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{collection\_extract}\strut
\end{minipage} & \begin{minipage}[t]{0.71\columnwidth}\raggedright
with subgeometries from a \texttt{GEOMETRYCOLLECTION} of a particular type\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{cast}\strut
\end{minipage} & \begin{minipage}[t]{0.71\columnwidth}\raggedright
that is converted to another type\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{+}\strut
\end{minipage} & \begin{minipage}[t]{0.71\columnwidth}\raggedright
that is shifted over a given vector\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.23\columnwidth}\raggedright
\texttt{*}\strut
\end{minipage} & \begin{minipage}[t]{0.71\columnwidth}\raggedright
that is multiplied by a scalar or matrix\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{sds_files/figure-latex/vor-1} 

}

\caption{for a set of points, left: convex hull (red); middle: Voronoi polygons; right: Delauney triangulation}\label{fig:vor}
\end{figure}

\hypertarget{bintrans}{%
\subsection{Binary Transformers}\label{bintrans}}

Binary transformers are functions that return a geometry based on
operating on a pair of geometries. They include:

\begin{longtable}[]{@{}llc@{}}
\toprule
\begin{minipage}[b]{0.19\columnwidth}\raggedright
function\strut
\end{minipage} & \begin{minipage}[b]{0.59\columnwidth}\raggedright
returns\strut
\end{minipage} & \begin{minipage}[b]{0.14\columnwidth}\centering
infix operator\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.19\columnwidth}\raggedright
\texttt{intersection}\strut
\end{minipage} & \begin{minipage}[t]{0.59\columnwidth}\raggedright
the overlapping geometries for pair of geometries\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
\texttt{\&}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\raggedright
\texttt{union}\strut
\end{minipage} & \begin{minipage}[t]{0.59\columnwidth}\raggedright
the combination of the geometries; removes internal boundaries and duplicate points, nodes or line pieces\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
\texttt{\textbar{}}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\raggedright
\texttt{difference}\strut
\end{minipage} & \begin{minipage}[t]{0.59\columnwidth}\raggedright
the geometries of the first after removing the overlap with the second geometry\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
\texttt{/}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.19\columnwidth}\raggedright
\texttt{sym\_difference}\strut
\end{minipage} & \begin{minipage}[t]{0.59\columnwidth}\raggedright
the combinations of the geometries after removing where they overlap\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\centering
\texttt{\%/\%}\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{nary}{%
\subsection{N-ary Transformers}\label{nary}}

N-ary transformers operate on sets of geometries.
\texttt{union} can be applied to a set of geometries to return its
geometrical union. Otherwise, any set of geometries can be combined
into a \texttt{MULTI}-type geometry when they have equal dimension, or
else into a \texttt{GEOMETRYCOLLECTION}. Without unioning, this may
lead to a geometry that is not valid, e.g.~because two polygon
rings have a boundary line in common.

N-ary \texttt{intersection} and \texttt{difference} take a single argument,
but operate (sequentially) on all pairs, triples, quadruples, etc.
Consider the plot in figure \ref{fig:boxes}: how do we identify
the area where all three boxes overlap? Using binary intersections
gives us intersections for all pairs: 1-1, 1-2, 1-3, 2-1, 2-2, 2-3,
3-1, 3-2, 3-3, but does not let us identify areas where more than
two geometries intersect.
Figure \ref{fig:boxes} (right) shows the n-ary intersection: the 7
unique, non-overlapping geometries originating from intersection
of one, two, \emph{or more} geometries.

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{sds_files/figure-latex/boxes-1} 

}

\caption{left: three overlapping boxes -- how do we identify the small box where all three overlap? right: unique, non-overlapping n-ary intersections}\label{fig:boxes}
\end{figure}

Similarly, one can compute an n-ary \emph{difference} from a set \(\{s_1, s_2, s_3, ...\}\) by creating differences \(\{s_1, s_2-s_1, s_3-s_2-s_1, ...\}\). This is shown in figure \ref{fig:diff}, left for the original
set, right for the set after reversing its order to make clear that
the result here depends on the ordering of the input geometries. Again,
resulting geometries do not overlap.

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{sds_files/figure-latex/diff-1} 

}

\caption{difference between subsequent boxes, left: in original order; right: in reverse order}\label{fig:diff}
\end{figure}

\hypertarget{precision}{%
\section{Precision}\label{precision}}

Geometrical operations, such as finding out whether a certain
point is on a line, may fail when coordinates are represented by
double precision floating point numbers, such as 8-byte doubles
used in R. An often chosen remedy is to limit the precision of the
coordinates before the operation. For this, a \emph{precision model}
is adopted; the most common is to choose a factor \(p\) and compute
rounded coordinates \(c'\) from original coordinates \(c\) by
\[c' = \mbox{round}(p \cdot c) / p\]

Rounding of this kind brings the coordinates to points on a
regular grid with spacing \(1/p\), which is beneficial for geometric
computations. Of course, it also affects all computations like
areas and distances, and may turn valid geometries into invalid
ones. Which precision values are best for which application is
often a matter of common sense combined with trial and error.

\hypertarget{coverages}{%
\section{Coverages: tessellations and rasters}\label{coverages}}

The Open Geospatial Consortium defines a \emph{coverage} as a ``feature
that acts as a function to return values from its range for any
direct position within its spatiotemporal domain'' \citep{ogccov}. Having
a \emph{function} implies that for every ``point'', i.e.~every combination
of spatial point and a moment in time of the spatiotemporal domain,
we have \emph{single} value for the range. This is a very common situation
for spatiotemporal phenomena, a few examples can be given:

\begin{itemize}
\tightlist
\item
  boundary disputes aside, every point in a region (domain) belongs to a single administrative unit (range)
\item
  at any given moment in time, every point in a region (domain) has a certain \emph{land cover type} (range)
\item
  every point in an area (domain) has a single elevation (range), e.g.~measured with respect to a given mean sea level surface
\item
  every spatiotemporal point in a three-dimensional body of air (domain) has single value for temperature (range)
\end{itemize}

A caveat here is that because observation or measurement always takes
time and requires space, measured values are always an average over
a spatiotemporal volume, and hence range variables can rarely be
measured for true, zero-volume ``points''; for many practical cases
however the measured volume is small enough to be considered a
``point''; for a variable like \emph{land cover type} the volume needs to
be chosen such that the types distinguished make sense with respect
to the measured units.

In the first two of the given examples the range variable is
\emph{categorical}, in the last two the range variable is \emph{continuous}.
For categorical range variables, if large connected areas have a
constant range value, an efficient way to represent these data
is by storing the boundaries of the areas with constant value, such
as country boundaries. Although this can be done (and is often done)
by a set of simple feature geometries (polygons or multipolygons),
but this brings along some challenges:

\begin{itemize}
\tightlist
\item
  it is hard to guarantee for such a set of simple feature polygons that they do not overlap, or that there are no gaps between them
\item
  simple features have no way of assigning points \emph{on} the boundary of two adjacent polygons uniquely to a single polygon, which introduces ambiguity in terms the interpretation as coverage
\end{itemize}

\hypertarget{topological-models}{%
\subsection{Topological models}\label{topological-models}}

A data model that guarantees no inadvertent gaps or overlaps of
polygonal coverages is the \emph{topological} model, examples of which
are found in geographic information systems (GIS) like GRASS GIS
or ArcGIS. Topological models store boundaries between polygons
only once, and register which polygonal area is on either side
of a boundary.

Deriving the set of (multi)polygons for each area with a constant
range value from a topological model is straightforward; the other
way around: reconstructing topology from a set of polygons typically
involves setting thresholds on errors and handling gaps or overlaps.

\hypertarget{raster-tessellations}{%
\subsection{Raster tessellations}\label{raster-tessellations}}

A tessellation is a subdivision of a space (area, volume) into
smaller elements by ways of polygons. A regular tessellation
does this with regular polygons: triangles, squares or hexagons.
Tessellations using squares are very commonly used for spatial data,
and are called \emph{raster data}. Raster data
tessellate each spatial dimension \(d\) into regular cells,
formed e.g.~by left-closed and right-open intervals \(d_i\):
\begin{equation}
d_i = d_0 + [i \times \delta, (i+1) \times \delta)
\end{equation}
with \(d_0\) an offset, \(\delta\) the interval (cell or
pixel) size, and where the cell index \(i\) is an arbitrary but
consecutive set of integers. The \(\delta\) value is often taken
negative for the \(y\)-axis (Northing), indicating that raster
row numbers increasing Southwards correspond to \(y\)-coordinates
increasing Northwards.

Where in arbitrary polygon tessellations the assignment of points
to polygons is ambiguous for points falling on a boundary shared
by two polygons, using left-closed ``{[}'' and right-open ``)'' intervals
in regular tessellations removes this ambiguity. This means that for
rasters with negative \(\delta\) values for the \(y\)-coordinate and
positive for the \(x\)-coordinate, only the top-left corner point
is part of each raster cell. An artifact resulting from this is
shown in figure \ref{fig:rasterizeline}.

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/rasterizeline-1} 

}

\caption{rasterization artifact: as only top-left corners are part of the raster cell, only cells below the diagonal line are rasterized}\label{fig:rasterizeline}
\end{figure}

Tessellating the time dimension with left-closed, right-open intervals
is very common, and reflects the implicit assumption underlying
time series software such as the \texttt{xts} package in R, where time
stamps indicate the start of time intervals. Different models can
be combined: one could use simple feature polygons to tessellate
space, and combine this with a regular tessellation of time in order
to cover a space-time \emph{vector data cube}. Raster and vector data
cubes are discussed in chapter \ref{datacube}.

As mentioned above, besides square cells the other two shapes
that can lead to regular tessellations of \(R^2\) are triangles
and hexagons. On the sphere, there are few more, including cube,
octahedron, icosahedron and dodecahedron. A spatial index that
builds on the cube is \href{https://s2geometry.io/}{s2geometry}, the
\href{https://eng.uber.com/h3/}{H3 library} uses the icosahedron and
densifies that with (mostly) hexagons. Mosaics that cover the entire
Earth are also called \emph{discrete global grids}.

\hypertarget{networks}{%
\section{Networks}\label{networks}}

Spatial networks are typically composed of linear (\texttt{LINESTRING})
elements, but possess further topological properties describing
the network coherence:

\begin{itemize}
\tightlist
\item
  start and endpoints of a linestring may be connected to other linestring
  start or end points, forming a set of nodes and edges
\item
  edges may be directed, to only allow for connection (flow,
  transport) in one way
\end{itemize}

R packages including \texttt{osmar} \citep{R-osmar}, \texttt{stplanr} \citep{R-stplanr} and
\texttt{sfnetworks} \citep{R-sfnetworks} provide functionality for constructing
network objects, and working with them, e.g.~computing shortest or
fastest routes through a network. Package \texttt{spatstat} \citep{R-spatstat, baddeley2015spatial} has infrastructure for analysing point patterns
on linear networks (chapter \citet{ref}(pointpatterns)).

\hypertarget{exercises-2}{%
\section{Exercises}\label{exercises-2}}

For the following exercises, use R where possible.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Give two examples of geometries in 2-D (flat) space that cannot be represented as simple feature geometries, and create a plot of them.
\item
  Recompute the coordinates 10.542, 0.01, 45321.6789 using precision values 1, 1e3, 1e6, and 1e-2.
\item
  Describe a practical problem for which an n-ary intersection would be needed.
\item
  How can you create a Voronoi diagram (figure \ref{fig:vor}) that has one closed polygons for every single point?
\item
  Give the unary measure \texttt{dimension} for geometries \texttt{POINT\ Z\ (0\ 1\ 1)}, \texttt{LINESTRING\ Z\ (0\ 0\ 1,1\ 1\ 2)}, and \texttt{POLYGON\ Z\ ((0\ 0\ 0,1\ 0\ 0,1\ 1\ 0,0\ 0\ 0))}
\item
  Give the DE-9IM relation between \texttt{LINESTRING(0\ 0,1\ 0)} and \texttt{LINESTRING(0.5\ 0,0.5\ 1)}; explain the individual characters.
\item
  Can a set of simple feature polygons form a coverage? If so, under which constraints?
\item
  For the \texttt{nc} counties in the dataset that comes with R package \texttt{sf}, find the points touched by four counties.
\item
  How would figure \ref{fig:rasterizeline} look like if \(\delta\) for the \(y\)-coordinate was positive?
\end{enumerate}

\hypertarget{spherical}{%
\chapter{Spherical Geometries}\label{spherical}}

``\emph{There are too many false conclusions drawn and stupid measurements
made when geographic software, built for projected Cartesian
coordinates in a local setting, is applied at the global scale}''
\citep{chrisman}

The previous chapter discussed geometries defined on the plane,
\(R^2\). This chapter discusses what changes when we consider
geometries not on the plane, but on the sphere (\(S^2\)).

Although we learned in chapter \ref{cs} that the shape of the Earth
is usually approximated by an ellipsoid, none of the libraries shown
in green in figure \ref{fig:gdal-fig-nodetails} provide access
to a comprehensive set of functions that compute on an ellipsoid.
Only the s2geometry \citep{R-s2, s2geometry} library does provide it
using a sphere rather than an ellipsoid. However, when compared
to using a flat (projected) space we did in the previous chapter,
a sphere is a much better approximation to an ellipsoid.

\hypertarget{straight-lines}{%
\section{Straight lines}\label{straight-lines}}

The basic premise of \emph{simple features} of chapter \ref{geometries}
is that geometries are represented by sequences of points \emph{connected
by straight lines}. On \(R^2\) (or any Cartesian space), this is
trivial, but on a sphere straight lines do not exist. The shortest
line connecting two points is an arc of the circle through both
points and the center of the sphere, also called a \emph{great circle
segment}. A consequence is that ``the'' shortest distance line
connecting two points on opposing sides of the sphere does not exist,
as any great circle segment connecting them has equal length.

\hypertarget{ring-direction}{%
\section{Ring direction}\label{ring-direction}}

Any polygon on the sphere divides the sphere surface in two parts
with finite area: the inside and the outside. Using the ``counter
clockwise rule'' as was done for \(R^2\) will not work, because the
direction interpretation depends on what is defined as inside. The
convention here is to define the inside as the left (or right)
side of the polygon boundary when traversing its points in
sequence. Reversal of the node order then switches inside and
outside.

\hypertarget{full-polygon}{%
\section{Full polygon}\label{full-polygon}}

In addition to empty polygons, one can define the
full polygon on a sphere, which comprises its entire surface. This is useful,
for instance for computing the oceans as the geometric difference
between the full polygon and those of the land mass.

\hypertarget{bounding-box-rectangle-and-cap}{%
\section{Bounding box, rectangle, and cap}\label{bounding-box-rectangle-and-cap}}

Where in \(R^2\) one can easily define bounding boxes as the range
of the \(x\) and \(y\) coordinates, for ellipsoidal coordinates these
ranges are not of much use when geometries cross the antimeridian
(longitude +/- 180) or one of the poles. The assumption in \(R^2\)
that lower \(x\) values are Westwards of higher ones does not hold
when crossing the antimeridian. An alternative to delineating
an area on a sphere that is more natural is the \emph{bounding cap},
defined by its center coordinates and a radius. For Antarctica,
as depicted in figure \ref{fig:antarctica} (a) and (c), the
bounding box formed by coordinate ranges is

\begin{verbatim}
#   xmin   ymin   xmax   ymax 
# -180.0  -85.2  179.6  -60.5
\end{verbatim}

which clearly does not contain the region (\texttt{ymin} being -90 and \texttt{xmax} 180).
Two geometries that do contain the region are the bounding cap:

\begin{verbatim}
#   lng lat angle
# 1   0 -90  29.5
\end{verbatim}

and the bounding \emph{rectangle}:

\begin{verbatim}
#   lng_lo lat_lo lng_hi lat_hi
# 1   -180    -90    180  -60.5
\end{verbatim}

For an area spanning the antimeridian, here the Fiji island country,
the bounding box:

\begin{verbatim}
#   xmin   ymin   xmax   ymax 
# -179.9  -21.7  180.2  -12.5
\end{verbatim}

seems to span most of the Earth, as opposed to the bounding rectangle:

\begin{verbatim}
#   lng_lo lat_lo lng_hi lat_hi
# 1    175  -21.7   -178  -12.5
\end{verbatim}

where a value \texttt{lng\_lo} \emph{larger} than \texttt{lng\_hi} indicates that the
bounding rectangle spans the antimeridian. This property could not
be inferred from the coordinate ranges.

\hypertarget{validity-on-the-sphere}{%
\section{Validity on the sphere}\label{validity-on-the-sphere}}

Many global datasets are given in ellipsoidal coordinates but are
prepared in a way that they ``work'' when interpreted on the \(R^2\)
space {[}-180,180{]} \(\times\) {[}-90,90{]}. This means that:

\begin{itemize}
\tightlist
\item
  geometries crossing the antimeridian (longitude +/- 180) are cut in
  halves, such that they no longer cross it (but nearly touch each other)
\item
  geometries including a pole, like Antarctica, are cut at +/- 180 and
  make an excursion through -180,-90 and 180,-90 (both representing the
  Geographic South Pole)
\end{itemize}

Figure \ref{fig:antarctica} shows two different representation of
Antarctica, plotted with ellipsoidal coordinates taken as \(R^2\)
(top) and in a Polar Stereographic projection (bottom), without
(left) and with (right) an excursion through the Geographic South
Pole. In the projections as plotted, polygons (b) and
(c) are valid; polygon (a) is not valid as it self-intersects,
polygon (d) is not valid because it traverses the same edge to the
South Pole twice. On the sphere (\(S^2\)), polygon (a) is valid but
(b) is not, for the same reason as (d) is not valid.

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/antarctica-1} 

}

\caption{different representations of Antarctica, (a, c): with a polygon not passing through (-180 -90); (b, d): with a polygon passing through (-180 -90) and (180 -90)}\label{fig:antarctica}
\end{figure}

\hypertarget{exercises-3}{%
\section{Exercises}\label{exercises-3}}

For the following exercises, use R where possible or relevant.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  How does the \href{https://tools.ietf.org/html/rfc7946}{GeoJSON} format define
  ``straight'' lines between ellipsoidal coordinates (section 3.1.1)?
  Using this definition of straight, how would \texttt{LINESTRING(0\ 85,180\ 85)}
  look like in a polar projection? How could this geometry be modified to
  have it cross the North Pole?
\item
  For a typical polygon on \(S^2\), how can you find out ring direction?
\item
  Are there advantages of using bounding caps over using bounding boxes? If so, list them.
\item
  Why is, for small areas, the orthographic projection centered
  at the area a good approximation of the geometry as handled on \(S^2\)?
\item
  For \texttt{rnaturalearth::ne\_countries(country\ =\ "Fiji",\ returnclass="sf")}, check whether the geometry is valid on \(R^2\),
  on an orthographic projection centered on the country, and on \(S^2\).
  How can the geometry be made valid on \texttt{S\^{}2}? Plot the resulting
  geometry back on \(R^2\). Compare the centroid of the country,
  as computed on \(R^2\) and on \(S^2\), and the distance between the two.
\end{enumerate}

\hypertarget{featureattributes}{%
\chapter{Attributes and Support}\label{featureattributes}}

Feature \emph{attributes} refer to the properties of features (``things'')
that do not describe the feature's geometry. Feature attributes can
be \emph{derived} from geometry (e.g.~length of a \texttt{LINESTRING}, area
of a \texttt{POLYGON}) but they can also refer to non-derived properties,
such as:

\begin{itemize}
\tightlist
\item
  the name of a street or a county
\item
  the number of people living in a country
\item
  the type of a road
\item
  the soil type in a polygon from a soil map
\item
  the opening hours of a shop
\item
  the body weight or heart beat rate of an animal
\item
  the NO\(_2\) concentration measured at an air quality monitoring station
\end{itemize}

In some cases, time properties can be seen as attributes of
features, e.g.~the date of birth of a person or the construction
year of a road. When an attribute such as for instance air quality
is a function of both space and time, time is best handled on
equal footing with geometry (e.g.~in a data cube, see chapter
\ref{datacube}).

Spatial data science software implementing simple features
typically organizes data in tables that contain both geometries and
attributes for features; this is true for \texttt{geopandas} in Python,
\texttt{PostGIS} tables in PostgreSQL, and \texttt{sf} objects in R. The geometric
operations described in section \ref{opgeom} operate
on geometries \emph{only}, and may occasionally yield new attributes
(predicates, measures or transformations), but do not operate on
attributes present.

When, while manipulating geometries, attribute \emph{values} are retained
unmodified, support problems may arise.
If we look into a simple case of replacing a county polygon with
the centroid of that polygon on a dataset that has attributes,
we see that R package \texttt{sf} issues a warning:

\begin{verbatim}
# Warning in st_centroid.sf(.): st_centroid assumes attributes are
# constant over geometries of x
\end{verbatim}

The reason for this is that the dataset contains variables with
values that are associated with entire polygons -- in this case:
population counts -- meaning they are not associated with a
\texttt{POINT} geometry replacing the polygon.

In section \ref{support} we already described that for non-point
geometries (lines, polygons), feature attribute values either
have \emph{point support}, meaning that the value applies to \emph{every
point}, or they have \emph{block support}, meaning that the value
\emph{summarizes all points} in the geometry. (More complex options,
e.g.~in between these two extremes, may also occur.) This chapter
will describe different ways in which an attribute may relate to
the geometry, its consequences on analysing such data, and ways to
derive attribute data for different geometries (up- and downscaling).

\hypertarget{agr}{%
\section{Attribute-geometry relationships and support}\label{agr}}

Changing the feature geometry without changing the feature attributes
does change the \emph{feature}, since the feature is characterised by
the combination of geometry and attributes. Can we, ahead of time,
predict whether the resulting feature will still meaningfully relate
to the attribute value when we replace all geometries for instance
with their convex hull or centroid? It depends.

Take the example of a road, represented by a \texttt{LINESTRING}, which has
an attribute property \emph{road width} equal to 10 m. What can we say about
the road width of an arbitrary subsection of this road? That depends
on whether the attribute road length describes, for instance the
road width \emph{everywhere}, meaning that road width is constant along the
road, or whether it describes an aggregate property, such as minimum
or average road width. In case of the minimum, for an arbitrary
subsection of the road one could still argue that the minimum
road width must be at least as large as the minimum road width for
the whole segment, but it may no longer be \emph{the minimum} for that
subsection. This gives us two ``types'' for the attribute-geometry
relationship (\textbf{AGR}):

\begin{itemize}
\tightlist
\item
  \textbf{constant} the attribute value is valid everywhere in or over the
  geometry; we can think of the feature as consisting of an infinite
  number of points that all have this attribute value; in the geostatistical
  literature this is known as a variable with \emph{point support}
\item
  \textbf{aggregate} the attribute is an aggregate, a summary value over
  the geometry; we can think of the feature as a \emph{single} observation
  with a value that is associated with the \emph{entire} geometry; this is
  also known as a variable having \emph{block support}
\end{itemize}

For polygon data, typical examples of \textbf{constant} AGR (point
support) variables are:

\begin{itemize}
\tightlist
\item
  land use for a land use polygon
\item
  rock units or geologic strata in a geological map
\item
  soil type in a soil map
\item
  elevation class in a elevation map that shows elevation as classes
\item
  climate zone in a climate zone map
\end{itemize}

A typical property of such variables is that they have geometries
that are not man-made and also not associated with a sensor device
(such as remote sensing image pixel boundaries). Instead, the geometry
follows from the variable observed.

Examples for the \textbf{aggregate} AGR (block support) variables are:

\begin{itemize}
\tightlist
\item
  population, either as number of persons or as population density
\item
  other socio-economic variables, summarised by area
\item
  average reflectance over a remote sensing pixel
\item
  total emission of pollutants by region
\item
  block mean NO\(_2\) concentrations, as e.g.~obtained by block kriging
  over square blocks or a dispersion model that predicts areal means
\end{itemize}

A typical property of such variables is that associated geometries
come for instance from legislation, observation devices or analysis
choices, but not intrinsically from the observed variable.

A third type of AGR arises when an attribute \emph{identifies} a feature
geometry; we call an attribute an \textbf{identity} variable when
the associated geometry uniquely identifies the variable's value
(there are no other geometries with the same value). An example is
county name: the name identifies the county, and is still the county
for any sub-area (point support), but for arbitrary sub-areas, the
attributes loses the \textbf{identity} property to become a \textbf{constant}
attribute. An example is:

\begin{itemize}
\tightlist
\item
  an arbitrary point (or region) inside a county is still part of the
  county and must have the same value for county name, but it does not
  longer identify the (entire) geometry corresponding to that county
\end{itemize}

The challenge here is that spatial information (ignoring time for
simplicity) belongs to different phenomena types \citep[e.g.~][]{scheider2016},
including:

\begin{itemize}
\tightlist
\item
  \textbf{fields}: where over \emph{continuous} space, every location corresponds to a single value, e.g.~elevation, air quality, or land use
\item
  \textbf{objects}: found at a \emph{discrete} set of locations, e.g.~houses or persons
\item
  \textbf{aggregates}: e.g.~sums, totals, averages of fields, counts or densities of objects, associated with lines or regions
\end{itemize}

but that different spatial geometry types (points, lines, polygons,
raster cells) have no simple mapping to these phenomena types:

\begin{itemize}
\tightlist
\item
  points may refer to sample locations of observations on fields (air quality) or to locations of objects
\item
  lines may be used for objects (roads, rivers), contours of a field, or administrative borders
\item
  raster pixels and polygons may reflect fields of a categorical
  variable such as land use (\emph{coverage}), but also aggregates such
  as population density
\end{itemize}

Properly specifying attribute-geometry relationships, and warning
against their absence or cases when change in geometry (change
of support) implies a change of information can help avoiding a
large class of common spatial data analysis mistakes \citep{stasch2014}
associated with the \emph{support} of spatial data.

\hypertarget{aggregating-and-summarising}{%
\section{Aggregating and summarising}\label{aggregating-and-summarising}}

Aggregating records in a table (or \texttt{data.frame}) involves two steps:

\begin{itemize}
\tightlist
\item
  grouping records based on a grouping predicate, and
\item
  applying an aggregation function to the attribute values of a
  group to summarize them into a single number.
\end{itemize}

In SQL, this looks for instance like

\begin{verbatim}
SELECT GroupID, SUM(population) FROM table GROUP BY GroupID;
\end{verbatim}

indicating the aggregation \emph{function} (\texttt{SUM}) and the
\emph{grouping predicate} (\texttt{GroupID}).

R package \texttt{dplyr} for instance uses two steps to accomplish this:
function \texttt{group\_by} specifies the group membership of records,
\texttt{summarize} computes data summaries (such as \texttt{sum} or \texttt{mean}) for
each of the groups. R (base) function \texttt{aggregate} does both in a
single function that takes the data table, the grouping predicate(s)
and the aggregation function.

An example for the North Carolina counties is shown in figure
\ref{fig:ncaggregation}. Here, we grouped counties by their position
(according to the quadrant in which the county centroid is with
respect to ellipsoidal coordinate \texttt{POINT(-79,\ 35.5)}) and counted
the number of disease cases per group. The result shows that
the geometries of the resulting groups have been unioned (section
\ref{bintrans}): this is necessary because the \texttt{MULTIPOLYGON}
formed by just putting all the county geometries together would
have many duplicate boundaries, and hence not be \emph{valid} (section
\ref{valid}).

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/ncaggregation-1} 

}

\caption{SID74 counts by county quadrant, with county polygons unioned by county quadrant}\label{fig:ncaggregation}
\end{figure}

Plotting collated county polygons is technically not a problem, but
for this case would raise the wrong suggestion that the group sums
relate to the counties, and not the group of counties.

One particular property of aggregation in this way is that each
record is assigned to a single group; this has the advantage that
the sum of the group-wise sums equals the sum of the ungrouped data:
for variables that reflect \emph{amount}, nothing gets lost and nothing
is added. The newly formed geometry is the result of unioning the
geometries of the records.

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/ncblocks-1} 

}

\caption{Example target blocks layed out over North Carolina counties}\label{fig:ncblocks}
\end{figure}

When we need an aggregate for a new area that is \emph{not} a union of the
geometries for a group of records, and we use a spatial predicate
then single records may be matched to multiple groups. When taking
the rectangles of figure \ref{fig:ncblocks} as the target areas,
and summing for each rectangle the disease cases of the counties
that \emph{intersect} with the rectangles of figure \ref{fig:ncblocks},
the sum of these will be much larger:

\begin{verbatim}
#   sid74_sum_counties sid74_sum_rectangles 
#                  667                 2621
\end{verbatim}

Choosing another predicate, e.g.~\emph{contains} or \emph{covers} would on
the contrary result in much smaller values, because many counties
are not contained by \emph{any} the target geometries. However, there are
a few cases where this approach might be good or satisfactory:

\begin{itemize}
\tightlist
\item
  when we want to aggregate \texttt{POINT} geometries by a set of polygons,
  and all points are contained by a single polygon. If points fall on a
  shared boundary than they are assigned to both polygons (this is
  the case for DE-9IM-based GEOS library; the s2geometry library has
  the option to define polygons as ``semi-open'', which implies that
  points are assigned to single polygons when the polygons form a
  coverage)
\item
  when aggregating many very small polygons or raster pixels over
  larger areas, e.g.~averaging altitude from 30 m resolution raster
  over North Carolina counties, the error made by multiple matches
  may be insignificant
\end{itemize}

A more comprehensive approach to aggregating spatial data associated
to areas to larger, arbitrary shaped areas is by using area-weighted
interpolation.

\hypertarget{area-weighted}{%
\section{Area-weighted interpolation}\label{area-weighted}}

When we want to combine geometries and attributes of two datasets
such that we get attribute values of a source dataset summarised for
the geometries of a target, where source and target geometries are
unrelated, area-weighted interpolation may be a simple approach. In
effect, it considers the area of overlap of the source and target
geometries, and uses that to weight the source attribute values into
the target value \citep{goodchild, thomas, do, Do2021}. Here, we follow the
notation of \citet{do}.

Area-weighted interpolation computes, for each of \(q\) spatial target
areas \(T_j\), a weighted average from the values \(Y_i\) corresponding
to the \(p\) spatial source areas \(S_i\),

\begin{equation}
\hat{Y}_j(T_j) = \sum_{i=1}^p w_{ij} Y_i(S_i)
\label{eq:aw}
\end{equation}

where the \(w_{ij}\) depend on the amount of overlap of \(T_j\) and
\(S_i\), \(A_{ij} = T_j \cap S_i\).

Different options exist for choosing weights, including methods
using external variables (e.g.~dasymetric mapping, \citet{mennis}).
Two simple approaches for computing weights that do not use external
variables arise, depending on whether the variable \(Z\) is \emph{intensive}
or \emph{extensive}.

\hypertarget{extensiveintensive}{%
\subsection{Spatially extensive and intensive variables}\label{extensiveintensive}}

An example of a extensive variable is \emph{population count}. It
is associated with an area, and if that area is cut into smaller
areas, the population count is split accordingly: not necessary
proportional to area, because population is rarely uniform, but split
in such a way that the sum of the population count for the smaller
areas equals that of the total. An example of a related variable
that is \emph{intensive} is population density. If an area is split
into smaller areas, population density is not split similarly:
the sum of the population densities for the smaller areas is a
meaningless measure, as opposed to the average of the population
densities which will be similar to the density of the total area.

Extensive variables correspond to amounts, associated with a physical
size (length, area, volume); for \emph{spatially} extensive variables,
if the area a value corresponds to is cut in parts, the values
associated with the sub-area are split accordingly. In other words:
the value is proportional to the support. Intensive variables are
variables that do not have values proportional to support: if the
area is split, values may vary but \emph{on average} remain the same.
The corresponding example of an intensive variable is \emph{population
density}: when we split an area into sub-areas, the sub-areas either
have identical population densities (in case population is uniformly
distributed) or, more realistically, have \emph{varying} population
densities that by necessity are both higher and lower than the
density of the total area.

When we assume that the extensive variable \(Y\) is uniformly
distributed over space, the value \(Y_{ij}\), derived from \(Y_i\)
for a sub-area of \(S_i\), \(A_{ij} = T_j \cap S_i\) of \(S_i\) is

\[\hat{Y}_{ij}(A_{ij}) = \frac{|A_{ij}|}{|S_i|} Y_i(S_i)\]

where \(|\cdot|\) denotes the spatial area.
For estimating \(Y_j(T_j)\) we sum all the elements over area \(T_j\):

\begin{equation}
\hat{Y}_j(T_j) = \sum_{i=1}^p \frac{|A_{ij}|}{|S_i|} Y_i(S_i)
\label{eq:awextensive}
\end{equation}

For an intensive variable, under the assumption that the variable
has a constant value over each area \(S_i\), the estimate for a
sub-area equals that of the total,

\[\hat{Y}_{ij} = Y_i(S_i)\]

and we can estimate the value of \(Y\) for a new spatial unit \(T_j\)
by an area-weighted average of the source values:

\begin{equation}
\hat{Y}_j(T_j) = \sum_{i=1}^p \frac{|A_{ij}|}{|T_j|} Y_i(S_i)
\label{eq:awintensive}
\end{equation}

\hypertarget{dasymetric-mapping}{%
\subsection{Dasymetric mapping}\label{dasymetric-mapping}}

Dasymetric mapping distributes variables, such as population,
known at a coarse spatial aggregation level over finer spatial
units by using other variables that are associated with population
distribution, such as land use, building density, or road density.
The simplest approach to dasymetric mapping is obtained for
extensive variables, where the ratio \(|A_{ij}| / |S_i|\) in
\eqref{eq:awextensive} is replaced by the ratio of another extensive
variable \(X_{ij}(S_{ij})/X_i(S_i)\), which has to be known for both
the intersecting regions \(S_{ij}\) and the source regions \(S_i\).
\citet{do} discuss several alternatives for intensive \(Y\) and/or \(X\),
and cases where \(X\) is known for other areas.

\hypertarget{support-in-file-formats}{%
\subsection{Support in file formats}\label{support-in-file-formats}}

GDAL's vector API supports reading and writing so-called field
domains, which can have a ``split policy'' and a ``merge policy''
indicating what should be done with attribute variables when
geometries are split or merged. The values of these can be
``duplicate'' for split and ``geometry weighted'' for merge, in
case of spatially intensive variables, or they can be ``geometry
ratio'' for split and ``sum'' for merge, in case of spatially
extensive variables. At the time of writing this, the \href{https://github.com/OSGeo/gdal/pull/3638}{file
formats supporting this}
are GeoPackage and FileGDB.

\hypertarget{updownscaling}{%
\section{Up- and Downscaling}\label{updownscaling}}

Up- and downscaling refers in general to obtaining high-resolution
information from low-resolution data (downscaling) or obtaining
low-resolution information from high-resolution data (upscaling).
Both are activities involve attributes' relation to geometries and
both change support. They are synonymous with aggregation (upscaling)
and disaggregation (downscaling).

The simplest form of downscaling is sampling (or extracting)
polygon, line or grid cell values at point locations. This works
well for variables with point-support (``constant'' AGR), but is at
best approximate when the values are aggregates.

Challenging applications for downscaling include high-resolution
prediction of variables obtained by low-resolution weather prediction
models or climate change models, and the high-resolution prediction
of satellite image derived variables based on the fusion of sensors
with different spatial and temporal resolutions.

The application of areal interpolation using \eqref{eq:aw} with
its realisations for extensive \eqref{eq:awextensive} and intensive
\eqref{eq:awintensive} variables allows moving information from any
source area \(S_i\) to any target area \(T_j\) as long as the two areas
have some overlap. This means that one can go arbitrarily to much
larger units (aggregation) or to much smaller units (disaggregation).
Of course this makes only sense to the extent that the assumptions
hold: over the source regions extensive variables need to be
uniformly distributed and intensive variables need to have constant
value.

The ultimate disaggregation involves retrieving (extracting)
point values from line or area data. For this, we cannot work with
equations \eqref{eq:awextensive} or \eqref{eq:awintensive} because
\(|A_{ij}| = 0\) for points, but under the assumption of having a
constant value over the geometry, for intensive variables the value
\(Y_i(S_i)\) can be assigned to points as long as all points can be
uniquely assigned to a single source area \(S_i\). For polygon data,
this implies that \(Y\) needs to be a coverage variable (section
\ref{coverages}).

In cases where values associated with areas are \textbf{aggregate} values
over the area, the assumptions made by area-weighted interpolation
or dasymetric mapping -- uniformity or constant values over the
source areas -- are highly unrealistic. In such cases, these simple
approaches still be reasonable approximations, for instance when:

\begin{itemize}
\tightlist
\item
  the source and target area are nearly identical
\item
  the variability inside source units is very small, and the variable
  is nearly uniform or constant
\end{itemize}

In other cases, results obtained using these methods are merely
consequences of unjustified assumptions. Statistical aggregation
methods that can estimate quantities for larger regions from points
or smaller regions include:

\begin{itemize}
\tightlist
\item
  design-based methods, which require that a probability sample is
  available from the target region, with known inclusion probabilities
  (\citet{brus2021}, section \ref{design}), and
\item
  model-based methods, which assume a random field model with spatially
  correlated values (block kriging, section \ref{blockkriging})
\end{itemize}

Alternative disaggregation methods include:

\begin{itemize}
\tightlist
\item
  deterministic, smoothing-based approaches such as kernel- or spline-based
  smoothing methods \citep{toblerpyc, martin89}
\item
  statistical, model-based approaches: area-to-area and area-to-point
  kriging \citep{kyriakidis04, stcos}.
\end{itemize}

\hypertarget{exercises-4}{%
\section{Exercises}\label{exercises-4}}

Where relevant, try to make the following exercises with R.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  When we add a variable to the \texttt{nc} dataset by \texttt{nc\$State\ =\ "North\ Carolina"} (i.e., all counties get assigned the same state
  name). Which value would you attach to this variable for the
  attribute-geometry relationship (agr)?
\item
  Create a new \texttt{sf} object from the geometry obtained by
  \texttt{st\_union(nc)}, and assign \texttt{"North\ Carolina"} to the variable
  \texttt{State}. Which \texttt{agr} can you now assign to this attribute variable?
\item
  Use \texttt{st\_area} to add a variable with name \texttt{area} to \texttt{nc}. Compare
  the \texttt{area} and \texttt{AREA} variables in the \texttt{nc} dataset. What are
  the units of \texttt{AREA}? Are the two linearly related? If there are
  discrepancies, what could be the cause?
\item
  Is the \texttt{area} variable intensive or extensive? Is its agr equal to
  \texttt{constant}, \texttt{identity} or \texttt{aggregate}?
\item
  Consider figure \ref{fig:awiex}; using the equations in section
  \ref{extensiveintensive}, compute the area-weighted interpolations
  for (a) the dashed cell and (b) for the square enclosing all four solid
  cells, first for the case where the four cells represent (i) an extensive
  variable, and (ii) an intensive variable. The red numbers are the
  data values of the source areas.
\end{enumerate}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/awiex-1} 

}

\caption{example data for area-weighted interpolation}\label{fig:awiex}
\end{figure}

\hypertarget{datacube}{%
\chapter{Data Cubes}\label{datacube}}

Data cubes arise naturally when we observe properties of a set of
geometries repeatedly over time. Time information may sometimes be
considered as an attribute of a feature, e.g.~when we register the year
of construction of a building, or the date of birth of a person (chapter
\ref{featureattributes}). In other cases it may refer to the time
of observing an attribute, or the time for which a prediction of an
attribute has been made. In these cases, time is on equal footing
with space, and time and space together describe the physical
dimensions over which we observe, model, make predictions or make
forecasts.

One way of considering our world is that of a four-dimensionsal
space, with three space and one time dimension. In that view,
events become ``things'' or ``objects'' that have as duration their
size on the time dimension \citep{galton}. Although such a view does
not align well with how we experience and describe the world,
from a data analytic perspective, four numbers, along with their
reference systems, suffice to describe space and time coordinates
of an observation associated with a point location and time instance.

We define data cubes as array data with one or more array dimensions
associated with space and/or time \citep{lu2018multidimensional}. This
implies that raster data, features with attributes, and time series
data are all special cases of data cubes. Since we do not restrict
to three-dimensional structures, we actually mean \emph{hypercubes}
rather than cubes, and as the cube extent of the different dimensions
does not have to be identical, or have comparable units, the better
term would be \emph{hyperrectangle}. For simplicity, we talk about data
cubes instead.

A canonical form of a data cube is shown in figure \ref{fig:cube}:
it shows in a perspective plot a set of raster layers for the same
region that were collected (observed, or modelled) at different time
steps. The three cube dimensions, longitude, latitude and time are
thought of as being orthogonal. Arbitrary two-dimensional cube
slices are obtained by fixing one of the dimensions at a particular
value, one-dimensional slices are obtained by fixing two of the
dimensions at a particular value, and a scalar is obtained by fixing
three dimensions at a particular value.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{sds_files/figure-latex/cube-1} 

}

\caption{Raster data cube with dimensions latitude, longitude and time}\label{fig:cube}
\end{figure}

\hypertarget{a-four-dimensional-data-cube}{%
\section{A four-dimensional data cube}\label{a-four-dimensional-data-cube}}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/cube4d-1} 

}

\caption{Four-dimensional raster data cube with dimensions x, y, bands and time}\label{fig:cube4d}
\end{figure}

Figure \ref{fig:cube4d} depicts a four-dimensional raster data
cube, where three-dimensional raster data cubes with a spectral
dimension (``bands'') are organised along a fourth dimension, a time
axis. Color image data always has three bands (Blue, Green, Red),
and this example has a fourth band (near infrared, NIR), which is
commonly found in spectral remote sensing data.

Figure \ref{fig:cube4d2} shows exactly the same data, but layed out
flat as a facet plot (or scatterplot matrix), where two dimensions
(\(x\) and \(y\)) are aligned with (or nested within) the dimensions
\emph{bands} and \emph{time}, respectively.

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/cube4d2-1} 

}

\caption{Four-dimensional raster data cube layed out flat over two dimensions}\label{fig:cube4d2}
\end{figure}

\hypertarget{dimensions-attributes-and-support}{%
\section{Dimensions, attributes, and support}\label{dimensions-attributes-and-support}}

Phenomena in space and time can be thought of as functions
with domain space and time, and with range one or more observed
attributes. For clearly identifiable discrete events or objects,
the range is typically discrete, and precise delineation involves
describing the precise coordinates where a thing starts or stops,
which is best suited by vector geometries. For continuous phenomena,
variables that take on a value everywhere such as air temperature
or land use type, there are infinitely many values to represent and
a common approach is to discretize space and time \emph{regularly} over
the spatiotemporal domain (extent) of interest. This leads to a
number of familiar data structures:

\begin{itemize}
\tightlist
\item
  time series, depicted as time lines for functions of time
\item
  image or raster data for two-dimensional spatial data
\item
  time sequences of images for dynamic spatial data
\end{itemize}

The third form of this, where a variable \(Z\) depends on \(x\), \(y\)
and \(t\), as in

\[Z = f(x, y, t)\]

is the archetype of a spatiotemporal array or \emph{data cube}: the shape
of the volume where points regularly discretizing the domain forms a
cube. We call the variables that form the range (here: \(x, y, t\)) the
cube \emph{dimensions}. Data cubes may have multiple attributes, as in

\[\{Z_1,Z_2,...,Z_p\} = f(x, y, t)\]

and if \(Z\) is functional, e.g.~reflectance values measured over
the electromagnetic spectrum, the spectral wavelengths \(\lambda\)
may form an additional dimension, as in \(Z = f(x,y,t,\lambda)\);
section \ref{switching} discusses the alternative of representing
color bands as attributes.

Multiple time dimensions arise for instance when making forecasts for
different times in the future \(t'\) at different times \(t\), or when
time is split into multiple dimensions (e.g.~year, day-of-year,
hour-of-day). The most general definition of a data cube is a
functional mapping from \(n\) dimensions to \(p\) attributes:

\[\{Z_1,Z_2,...,Z_p\} = f(D_1,D_2,...,D_n)\]

Here, we will consider any dataset with one or more space dimensions
and zero or more time dimensions as data cubes. That includes:

\begin{itemize}
\tightlist
\item
  simple features (section \ref{simplefeatures})
\item
  time series for sets of features
\item
  raster data
\item
  multi-spectral raster data (images)
\item
  time series of multi-spectral raster data (video)
\end{itemize}

\hypertarget{regular-dimensions-gdals-geotransform}{%
\subsection{Regular dimensions, GDAL's geotransform}\label{regular-dimensions-gdals-geotransform}}

Data cubes are usually stored in multi-dimensional arrays, and the
usual relationship between 1-based array index \(i\) and an associated
regularly discretized dimension variable \(x\) is

\[x = o_x + (i-1) d_x\]

with \(o_x\) the origin, and \(d_x\) the grid spacing for this dimension.

For more general cases like those in figure
\ref{fig:rastertypes01}b-c, the relation between \(x\) and \(y\)
and array indexes \(i\) and \(j\) is

\[x = o_x + (i-1) d_x + (j-1) a_1\]

\[y = o_y + (i-1) a_2 + (j-1) d_y\]

With two affine parameters \(a_1\) and \(a_2\); this is the so-called
\emph{geotransform} as used in GDAL. When \(a_1=a_2=0\), this
reduces to the regular raster of figure \ref{fig:rastertypes01}a
with square cells if \(d_x = d_y\).
For integer indexes, the coordinates are that of the starting \emph{edge}
of a grid cell, and the cell area (pixel) spans a range corresponding to
index values ranging from \(i\) (inclusive) to \(i+1\) (exclusive).
For most common imagery formats, \(d_y\) is negative,
indicating that image row index increases with decreasing \(y\)
values (southward). To get the \(x\)- and \(y\)-coordinate of the grid
cell \emph{center} of the top left grid cell (in case of a negative \(d_y\)),
we use \(i=1.5\) and \(j=1.5\).

For rectilinear rasters, a table that maps array index to dimension
values is needed. NetCDF files for instance always stores all values
of spatial dimension variables that correspond to the center of
spatial grid cells, and may in addition store grid cell boundaries
(which is needed to define rectlinear dimensions unambiguously).

For curvilinear rasters an array that maps every combination of \(i,j\)
into \(x,y\) pairs is needed, or a parametric function that does this
(e.g.~an projection or its inverse). NetCDF files often provide both,
when available.

\hypertarget{support-along-cube-dimensions}{%
\subsection{Support along cube dimensions}\label{support-along-cube-dimensions}}

Section \ref{agr} defined \emph{spatial} support of an attribute variable
as the size (length, area, volume) of a geometry a particular
observation or prediction is associated with. The same notion
applies to temporal support. Although time is rarely reported by
explicit time periods having a start- and end-time, in many cases
either the time stamp implies a period (e.g.~ISO-8601 indications
like ``2021'' for a full year, ``2021-01'' for full month) or the time
period is taken as the period from the time stamp of the current
record up to but not including the time stamp of the next record.

An example is MODIS satellite imagery, where vegetation indexes (NDVI
and EVI) are available as 16-day composites, meaning that over 16-day
periods all available imagery is aggregated into a single image; such
composites have temporal ``block support''. Sentinel-2 or Landsat-8
data on the other hand are ``snapshot'' images and have temporal
``point support''. When temporally aggregating data with temporal
point support e.g.~to monthly values one would select all images
falling in the target time interval. When aggregating temporal block
support imagery such as the MODIS 16-day composite, one might
weigh images, e.g.~according to the amount of overlap of the 16-day
composite period and the target period, similar to area-weighted
interpolation but over the time dimension.

\hypertarget{dcoperations}{%
\section{Operations on data cubes}\label{dcoperations}}

\hypertarget{slicing-a-cube-filter}{%
\subsection{Slicing a cube: filter}\label{slicing-a-cube-filter}}

Data cubes can be sliced into sub-cubes by fixing a dimension at a
particular value. Figure \ref{fig:cube4filter} shows the sub-cubes
obtained by doing with each of the dimensions. In this figure,
the spatial filtering does not happen by fixing a single spatial
dimension at a particular value, but by selecting a particular
subregion, which is a more common operation. Fixing \(x\) or \(y\) would
give a sub-cube along a transect of constant \(x\) or \(y\), which can
be used to show a HovmÃ¶ller diagram, where an attribute is plotted
(colored) in the space of one space and one time dimension.

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/cube4filter-1} 

}

\caption{Data cube filtering by time, band or spatially}\label{fig:cube4filter}
\end{figure}

\hypertarget{applydimension}{%
\subsection{Applying functions to dimensions}\label{applydimension}}

A common analysis involves applying a function over one or more
cube dimensions. Simple cases arise where a function such as \texttt{abs},
\texttt{sin} or \texttt{sqrt} is applied to all values in the cube, or when a function
takes all values in the cube and returns a single scalar, e.g.~when
one computes the mean or maximum value over the entire cube. Other
options include applying the function to selected dimensions,
e.g.~applying a temporal low-pass filter to every individual
(pixel/band) time series as shown in figure \ref{fig:cube4apply1},
or applying a \emph{spatial} low-pass filter to every spatial
slice (i.e.~every band/time combination), shown in figure
\ref{fig:cube4apply2}.

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/cube4apply2-1} 

}

\caption{Low pass filtering of spatial slices}\label{fig:cube4apply2}
\end{figure}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/cube4apply1-1} 

}

\caption{Low pass filtering of time series}\label{fig:cube4apply1}
\end{figure}

\hypertarget{reducedimension}{%
\subsection{Reducing dimensions}\label{reducedimension}}

When applying function \texttt{mean} to an entire data cube, all dimensions
vanish: the resulting ``data cube'' has dimensionality zero. We
can also apply functions to a limited set of dimensions such that
selected dimensions vanish, or are \emph{reduced}. We already saw that
filtering is a special case of this, but more in general we could
for instance compute the maximum of every time series, the mean over
every spatial slice, or a band index such as NDVI that summarizes
different spectral values into a single new ``band'' with the index
value. Figure \ref{fig:cube4reduce} illustrates these options.

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/cube4reduce-1} 

}

\caption{Reducing data cube dimensions}\label{fig:cube4reduce}
\end{figure}

\hypertarget{vectordatacubes}{%
\section{Aggregating raster to vector cubes}\label{vectordatacubes}}

Figure \ref{fig:cube4agg} illustrates how a four-dimensional raster
data cube can be aggregated to a three-dimensional \emph{vector data
cube}. Pixels in the raster are grouped by spatial intersection
with a set of vector geometries, and each group is then reduced
to a single value by an aggregation function such as \texttt{mean} or
\texttt{max}. In the example, the \emph{two} spatial dimensions \(x\) and \(y\)
reduce to a single dimension, the one-dimensional sequence of feature
geometries, with geometries that are defined in the space of \(x\)
and \(y\). Grouping geometries can also be \texttt{POINT} geometries, in
which case the aggregation function is obsolete as single values
at the \texttt{POINT} locations are \emph{extracted}, e.g.~by querying a pixels
value or by interpolating from the nearest pixels.

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/cube4agg-1} 

}

\caption{Aggregating a raster data cube to a vector data cube}\label{fig:cube4agg}
\end{figure}

Further examples of vector data cubes include air quality data,
where we could have \(PM_{10}\) measurements over two dimensions:

\begin{itemize}
\tightlist
\item
  a sequence of monitoring stations, and
\item
  a sequence of time intervals
\end{itemize}

or where we consider time series of demographic or epidemiological
data, consisting of (population, disease) counts, with number
of persons

\begin{itemize}
\tightlist
\item
  by region, for a sequence of \(n\) regions
\item
  by age class, for \(m\) age classes, and
\item
  by year, for \(p\) years
\end{itemize}

which forms an array with \(n m p\) elements.

For spatial data science, support of vector and raster data cubes
is extremely useful, because many variables are both spatially
and temporaly varying, and because we often want to either change
dimensions or aggregate them out, but in a fully flexible manner
and order. Examples of changing dimensions are:

\begin{itemize}
\tightlist
\item
  interpolating air quality measurements to values on a regular grid (raster; chapter \ref{interpolation})
\item
  estimating density maps from points or lines, e.g.~with the number of flights passing by per week within a range of 1 km (chapter \ref{pointpatterns})
\item
  aggregating climate model predictions to summary indicators for administrative regions
\item
  combining Earth observation data from different sensors, e.g.~MODIS (250 m pixels, every 16 days) with Sentinel-2 (10 m, every 5 days)
\end{itemize}

Examples of aggregating one ore more full dimensions are assessments of:

\begin{itemize}
\tightlist
\item
  which air quality monitoring stations indicate unhealthy conditions (time)
\item
  which region has the highest increase in disease incidence (space, time)
\item
  global warming (e.g.~in degrees per year)
\end{itemize}

\hypertarget{switching}{%
\section{Switching dimension with attributes}\label{switching}}

When we accept that a dimension can also reflect an unordered,
categorical variable, then one can easily swap a set of attributes
for a single dimension, by replacing

\[\{Z_1,Z_2,...,Z_p\} = f(D_1,D_2,...,D_n)\]

with

\[Z = f(D_1,D_2,...,D_n, D_{n+1})\]

where \(D_{n+1}\) has cardinality \(p\) and has as labels (the names
of) \(Z_1,Z_2,...,Z_p\). Figure \ref{fig:aqdc} shows a vector data
cube for air quality stations where one cube dimension reflects
air quality parameters. When the \(Z_i\) have incompatible measurement
units, as in figure \ref{fig:aqdc}, one would have to take care
when reducing the ``parameter'' dimension \(D_{n+1}\): numeric functions
like \texttt{mean} or \texttt{max} would be meaningless. Counting the number of
variables that exceed their respective threshold values may however
be meaningful.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{sds_files/figure-latex/aqdc-1} 

}

\caption{Vector data cube with air quality time series}\label{fig:aqdc}
\end{figure}

Being able to swap dimensions to attributes flexibly and vice-versa
leads to extremely flexible analysis possibilities, as e.g.~shown
by the array database SciDB \citep{brown2010overview}.

\hypertarget{otherdynamic}{%
\section{Other dynamic spatial data}\label{otherdynamic}}

We have seen several dynamic raster and vector data examples
that match the data cube structure well. Other data examples
do less so: in particular spatiotemporal point patterns (chapter
\ref{pointpatterns}) and trajectories (movement data; for a recent review, see \citet{https://doi.org/10.1111/1365-2656.13116}) are often more
straightforward to not handle as a data cube. Spatiotemporal point
patterns are the sets of spatiotemporal coordinates of events or
objects: accidents, disease cases, traffic jams, lightning strikes,
and so on. Trajectory data are time sequences of spatial locations of
moving objects (persons, cars, satellites, animals). For such data,
the primary information is in the coordinates, and shifting these
to a limited set of regularly discretized grid cells covering the
space may help some analysis, e.g.~to quickly explore patterns in
areas of higher densities, but the loss of the exact coordinates
also hinders a number of analysis approaches involving distance,
direction or speed calculations. Nevertheless, for such data often
the first computational steps involves generation of data cube
representations by aggregating to a time-fixed spatial and/or
space-fixed temporal discretization.

Using sparse array representations of data cubes to represent
point pattern or trajectory data, as e.g.~offered by SciDB
\citep{brown2010overview} or TileDB \citep{tiledb}, may strongly limit the
loss of coordinate accuracy by choosing dimensions that represent
an extremely dense grid, and storing only those grid cell that
contain data points. For trajectory data, such representations
would need to add a grouping dimension to identify individuals,
or individual sequences of consecutive movement observations.

\hypertarget{exercises-5}{%
\section{Exercises}\label{exercises-5}}

Use words to solve the following exercises, if needed or relevant
use R code to illustrate the argument(s).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Why is it difficult to represent trajectories, sequences of
  \((x,y,t)\) obtained by tracking moving objects, by data cubes as
  described in this chapter?
\item
  In a socio-economic vector data cube with variables population,
  life expectancy, and gross domestic product ordered by dimensions country and
  year, which variables have block support for the spatial dimension,
  and which have block support for the temporal dimension?
\item
  The Sentinel-2 satellites collect images in 12 spectral bands;
  list advantages and disadvantages to represent them as (i) different
  data cubes, (ii) a data cube with 12 attributes, one for each band,
  and (iii) a single attribute data cube with a spectral dimension.
\item
  Explain why a curvilinear raster as shown in figure
  \ref{fig:rastertypes01} can be considered a special case of a
  data cube.
\item
  Explain how the following problems can be solved with data cube
  operations \texttt{filter}, \texttt{apply}, \texttt{reduce} and/or \texttt{aggregate}, and
  in which order. Also mention for each which function is applied,
  and what the dimensionality of the resulting data cube is (if any):

  \begin{itemize}
  \tightlist
  \item
    from hourly \(PM_{10}\) measurements for a set of air quality
    monitoring stations, compute per station the amount of days
    per year that the average daily \(PM_{10}\) value exceeds
    50 \(\mu g/m^3\)
  \item
    for a sequence of aerial images of an oil spill, find the
    time at which the oil spill had its largest extent, and the
    corresponding extent
  \item
    from a 10-year period with global daily sea surface temperature
    (SST) raster maps, find the area with the 10\% largest and 10\%
    smallest temporal trends in SST values.
  \end{itemize}
\end{enumerate}

\hypertarget{part-r-for-spatial-data-science}{%
\part{R for Spatial Data Science}\label{part-r-for-spatial-data-science}}

\hypertarget{sf}{%
\chapter{Introduction to sf and stars}\label{sf}}

This chapter introduces R packages \texttt{sf} and \texttt{stars}. \texttt{sf} provides
a table format for simple features, where feature geometries are
carried in a list-column. R package \texttt{stars} was written to support
raster and vector datacubes (Chapter \ref{datacube}), and has
raster data stacks and feature time series as special cases. \texttt{sf}
first appeared on CRAN in 2016, \texttt{stars} in 2018. Development of both
packages received support from the R Consortium as well as strong
community engagement. The packages were designed to work together.

All functions operating on \texttt{sf} or \texttt{stars} objects start with \texttt{st\_},
making it easy to recognize them or to search for them when using
command line completion.

\hypertarget{sfintro}{%
\section{\texorpdfstring{Package \texttt{sf}}{Package sf}}\label{sfintro}}

Intended to succeed and replace R packages \texttt{sp}, \texttt{rgeos} and the
vector parts of \texttt{rgdal}, R Package \texttt{sf} \citep{rjsf} was developed to
move spatial data analysis in R closer to standards-based approaches
seen in the industry and open source projects, to build upon more
modern versions of the open source geospatial software stack (figure
\ref{fig:gdal-fig-nodetails}), and to allow for integration of R
spatial software with the tidyverse if desired.

To do so, R package \texttt{sf} provides simple features access
\citep{herring2011opengis}, natively, to R. It provides an interface
to several \texttt{tidyverse} packages, in particular to \texttt{ggplot2},
\texttt{dplyr} and \texttt{tidyr}. It can read and write data through GDAL, execute
geometrical operations using GEOS (for projected coordinates)
or s2geometry (for ellipsoidal coordinates), and carry out
coordinate transformations or conversions using PROJ. External C++
libraries are interfaced using \texttt{Rcpp} \citep{eddelbuettel2013seamless}.

Package \texttt{sf} represents sets of simple features in \texttt{sf} objects,
a sub-class of a \texttt{data.frame} or tibble. \texttt{sf} objects contain at
least one \emph{geometry list-column} of class \texttt{sfc}, which for each
element contains the geometry as an R object of class \texttt{sfg}.
A geometry list-column acts as a variable in a \texttt{data.frame}
or tibble, but has a more complex structure than e.g.~numeric or
character variables. Following the convention of \texttt{PostGIS}, all
operations (functions, method) that operate on \texttt{sf} objects or
related start with \texttt{st\_}.

An \texttt{sf} object has the following meta-data:

\begin{itemize}
\tightlist
\item
  the name of the (active) geometry column, held in attribute \texttt{sf\_column}
\item
  for each non-geometry variable, the attribute-geometry
  relationships (section \ref{agr}), held in attribute \texttt{agr}
\end{itemize}

An \texttt{sfc} geometry list-column has the following meta-data:

\begin{itemize}
\tightlist
\item
  the coordinate reference system held in attribute \texttt{crs}
\item
  the bounding box held in attribute \texttt{bbox}
\item
  the precision held in attribute \texttt{precision}
\item
  the number of empty geometries held in attribute \texttt{n\_empty}
\end{itemize}

These attributes may best be accessed or set by using
functions like \texttt{st\_bbox}, \texttt{st\_crs}, \texttt{st\_set\_crs}, \texttt{st\_agr},
\texttt{st\_set\_agr}, \texttt{st\_precision}, and \texttt{st\_set\_precision}.

\hypertarget{creation}{%
\subsection{Creation}\label{creation}}

One could create an \texttt{sf} object from scratch e.g.~by

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(sf)}
\NormalTok{p1 =}\StringTok{ }\KeywordTok{st_point}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\FloatTok{7.35}\NormalTok{, }\FloatTok{52.42}\NormalTok{))}
\NormalTok{p2 =}\StringTok{ }\KeywordTok{st_point}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\FloatTok{7.22}\NormalTok{, }\FloatTok{52.18}\NormalTok{))}
\NormalTok{p3 =}\StringTok{ }\KeywordTok{st_point}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\FloatTok{7.44}\NormalTok{, }\FloatTok{52.19}\NormalTok{))}
\NormalTok{sfc =}\StringTok{ }\KeywordTok{st_sfc}\NormalTok{(}\KeywordTok{list}\NormalTok{(p1, p2, p3), }\DataTypeTok{crs =} \StringTok{'OGC:CRS84'}\NormalTok{)}
\KeywordTok{st_sf}\NormalTok{(}\DataTypeTok{elev =} \KeywordTok{c}\NormalTok{(}\FloatTok{33.2}\NormalTok{, }\FloatTok{52.1}\NormalTok{, }\FloatTok{81.2}\NormalTok{), }\DataTypeTok{marker =} \KeywordTok{c}\NormalTok{(}\StringTok{"Id01"}\NormalTok{, }\StringTok{"Id02"}\NormalTok{, }\StringTok{"Id03"}\NormalTok{),}
      \DataTypeTok{geom =}\NormalTok{ sfc)}
\CommentTok{# Simple feature collection with 3 features and 2 fields}
\CommentTok{# Geometry type: POINT}
\CommentTok{# Dimension:     XY}
\CommentTok{# Bounding box:  xmin: 7.22 ymin: 52.2 xmax: 7.44 ymax: 52.4}
\CommentTok{# Geodetic CRS:  WGS 84}
\CommentTok{#   elev marker              geom}
\CommentTok{# 1 33.2   Id01 POINT (7.35 52.4)}
\CommentTok{# 2 52.1   Id02 POINT (7.22 52.2)}
\CommentTok{# 3 81.2   Id03 POINT (7.44 52.2)}
\end{Highlighting}
\end{Shaded}



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/sf_obj} 

}

\caption{components of an \texttt{sf} object}\label{fig:sfobj}
\end{figure}

Figure \ref{fig:sfobj} gives an explanation of the components
printed. Rather than creating objects from scratch, spatial data
in R are typically read from an external source, which can be

\begin{itemize}
\tightlist
\item
  an external file,
\item
  a request to a web service, or
\item
  a dataset held in some form in another R package.
\end{itemize}

The next section introduces reading from files; section
\ref{largesf} discusses handling of datasets too large to fit into
working memory.

\hypertarget{reading-and-writing}{%
\subsection{Reading and writing}\label{reading-and-writing}}

Reading datasets from an external ``data source'' (file, web service,
or even string) is done using \texttt{st\_read}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(sf)}
\NormalTok{(}\DataTypeTok{file =} \KeywordTok{system.file}\NormalTok{(}\StringTok{"gpkg/nc.gpkg"}\NormalTok{, }\DataTypeTok{package=}\StringTok{"sf"}\NormalTok{))}
\CommentTok{# [1] "/home/edzer/R/x86_64-pc-linux-gnu-library/4.0/sf/gpkg/nc.gpkg"}
\NormalTok{nc =}\StringTok{ }\KeywordTok{st_read}\NormalTok{(file)}
\CommentTok{# Reading layer `nc.gpkg' from data source }
\CommentTok{#   `/home/edzer/R/x86_64-pc-linux-gnu-library/4.0/sf/gpkg/nc.gpkg' }
\CommentTok{#   using driver `GPKG'}
\CommentTok{# Simple feature collection with 100 features and 14 fields}
\CommentTok{# Geometry type: MULTIPOLYGON}
\CommentTok{# Dimension:     XY}
\CommentTok{# Bounding box:  xmin: -84.3 ymin: 33.9 xmax: -75.5 ymax: 36.6}
\CommentTok{# Geodetic CRS:  NAD27}
\end{Highlighting}
\end{Shaded}

Here, the file name and path \texttt{file} is read from the \texttt{sf} package,
which has a different path on every machine, and hence is guaranteed
to be present on every \texttt{sf} installation.

Command \texttt{st\_read} has two arguments: the \emph{data set name} (\texttt{dsn})
and the \emph{layer}. In the example above, the \emph{geopackage} file
contains only a single layer that is being read. If it had contained
multiple layers, then the first layer would have been read and a
warning would have been emitted. The available layers of a data
set can be queried by

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{st_layers}\NormalTok{(file)}
\CommentTok{# Driver: GPKG }
\CommentTok{# Available layers:}
\CommentTok{#   layer_name geometry_type features fields}
\CommentTok{# 1    nc.gpkg Multi Polygon      100     14}
\end{Highlighting}
\end{Shaded}

Simple feature objects can be written with \texttt{st\_write}, as in

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\DataTypeTok{file =} \KeywordTok{tempfile}\NormalTok{(}\DataTypeTok{fileext =} \StringTok{".gpkg"}\NormalTok{))}
\CommentTok{# [1] "/tmp/Rtmp2fxnVb/filefaf227111bc79.gpkg"}
\KeywordTok{st_write}\NormalTok{(nc, file, }\DataTypeTok{layer =} \StringTok{"layer_nc"}\NormalTok{)}
\CommentTok{# Writing layer `layer_nc' to data source }
\CommentTok{#   `/tmp/Rtmp2fxnVb/filefaf227111bc79.gpkg' using driver `GPKG'}
\CommentTok{# Writing 100 features with 14 fields and geometry type Multi Polygon.}
\end{Highlighting}
\end{Shaded}

where the file format (GPKG) is derived from the file name extension.

\hypertarget{subsetting}{%
\subsection{Subsetting}\label{subsetting}}

A very common operation is to subset objects; base R can use \texttt{{[}}
for this. The rules that apply to \texttt{data.frame} objects also apply to
\texttt{sf} objects, e.g.~that records 2-5 and columns 3-7 are selected by

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nc[}\DecValTok{2}\OperatorTok{:}\DecValTok{5}\NormalTok{, }\DecValTok{3}\OperatorTok{:}\DecValTok{7}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

but with a few additional features, in particular

\begin{itemize}
\tightlist
\item
  the \texttt{drop} argument is by default \texttt{FALSE} meaning that the
  geometry column is \emph{always} selected, and an \texttt{sf} object is
  returned; when it is set to \texttt{TRUE} and the geometry column \emph{not} selected,
  it is dropped and a \texttt{data.frame} is returned;
\item
  selection with a spatial (\texttt{sf}, \texttt{sfc} or \texttt{sfg}) object as first argument leads to
  selection of the features that spatially \emph{intersect} with that
  object (see next section); other predicates than \emph{intersects} can be chosen by setting
  parameter \texttt{op} to a function such as \texttt{st\_covers} or or any other
  binary predicate function listed in section \ref{de9im}
\end{itemize}

\hypertarget{binary-predicates}{%
\subsection{Binary predicates}\label{binary-predicates}}

Binary predicates like \texttt{st\_intersects}, \texttt{st\_covers} etc (section
\ref{de9im}) take two sets of features or feature geometries and return for
all pairs whether the predicate is \texttt{TRUE} or \texttt{FALSE}. For large
sets this would potentially result in a huge matrix, typically filled
mostly with \texttt{FALSE} values and for that reason a sparse representation
is returned by default:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nc5 =}\StringTok{ }\NormalTok{nc[}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{, ]}
\NormalTok{nc7 =}\StringTok{ }\NormalTok{nc[}\DecValTok{1}\OperatorTok{:}\DecValTok{7}\NormalTok{, ]}
\NormalTok{(}\DataTypeTok{i =} \KeywordTok{st_intersects}\NormalTok{(nc5, nc7))}
\CommentTok{# Sparse geometry binary predicate list of length 5, where}
\CommentTok{# the predicate was `intersects'}
\CommentTok{#  1: 1, 2}
\CommentTok{#  2: 1, 2, 3}
\CommentTok{#  3: 2, 3}
\CommentTok{#  4: 4, 7}
\CommentTok{#  5: 5, 6}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/fig57-1} 

}

\caption{First seven North Carolina counties}\label{fig:fig57}
\end{figure}

Figure \ref{fig:fig57} shows how the intersections of the first
five with the first seven counties can be understood.
We can transform the sparse logical matrix into a dense matrix by

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{as.matrix}\NormalTok{(i)}
\CommentTok{#       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]}
\CommentTok{# [1,]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE}
\CommentTok{# [2,]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE}
\CommentTok{# [3,] FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE}
\CommentTok{# [4,] FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE}
\CommentTok{# [5,] FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE}
\end{Highlighting}
\end{Shaded}

The number of countries that each of \texttt{nc5} intersects with is

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{lengths}\NormalTok{(i)}
\CommentTok{# [1] 2 3 2 2 2}
\end{Highlighting}
\end{Shaded}

and the other way around, the number of counties in \texttt{nc5} that
intersect with each of the counties in \texttt{nc7} is

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{lengths}\NormalTok{(}\KeywordTok{t}\NormalTok{(i))}
\CommentTok{# [1] 2 3 2 1 1 1 1}
\end{Highlighting}
\end{Shaded}

The object \texttt{i} is of class \texttt{sgbp} (sparse geometrical binary
predicate), and is a list of integer vectors, with each element
representing a row in the logical predicate matrix holding the column
indices of the \texttt{TRUE} values for that row. It further holds some
metadata like the predicate used, and the total number of columns.
Methods available for \texttt{sgbp} objects include

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{methods}\NormalTok{(}\DataTypeTok{class =} \StringTok{"sgbp"}\NormalTok{)}
\CommentTok{# [1] as.data.frame as.matrix     dim           Ops          }
\CommentTok{# [5] print         t            }
\CommentTok{# see '?methods' for accessing help and source code}
\end{Highlighting}
\end{Shaded}

where the only \texttt{Ops} method available is \texttt{!}, the negation operation.

\hypertarget{tidyverse}{%
\subsection{tidyverse}\label{tidyverse}}

The tidyverse is a collection of data science packages that work
together, described e.g.~in \citep{r4ds, welcome}. Package \texttt{sf} has
tidyverse-style read and write functions, \texttt{read\_sf} and \texttt{write\_sf},
which return a tibble rather than a \texttt{data.frame}, do not print any
output, and overwrite existing data by default.

Further \texttt{tidyverse} generics with methods for \texttt{sf} objects include
\texttt{filter}, \texttt{select}, \texttt{group\_by}, \texttt{ungroup}, \texttt{mutate}, \texttt{transmute},
\texttt{rowwise}, \texttt{rename}, \texttt{slice}, \texttt{summarise}, \texttt{distinct}, \texttt{gather}, \texttt{pivot\_longer},
\texttt{spread}, \texttt{nest}, \texttt{unnest}, \texttt{unite}, \texttt{separate}, \texttt{separate\_rows},
\texttt{sample\_n}, and \texttt{sample\_frac}. Most of these methods simply manage
the metadata of \texttt{sf} objects, and make sure the geometry remains
present. In case a user wants the geometry to be removed, one can
use \texttt{st\_set\_geometry(nc,\ NULL)} or simply coerce to a \texttt{tibble} or
\texttt{data.frame} before selecting:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\NormalTok{nc }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as_tibble}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(BIR74) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{head}\NormalTok{(}\DecValTok{3}\NormalTok{)}
\CommentTok{# # A tibble: 3 x 1}
\CommentTok{#   BIR74}
\CommentTok{#   <dbl>}
\CommentTok{# 1  1091}
\CommentTok{# 2   487}
\CommentTok{# 3  3188}
\end{Highlighting}
\end{Shaded}

The \texttt{summarise} method for \texttt{sf} objects has two special arguments:

\begin{itemize}
\tightlist
\item
  \texttt{do\_union} (default \texttt{TRUE}) determines whether grouped geometries are unioned on return, so that they form a valid geometry;
\item
  \texttt{is\_coverage} (default \texttt{FALSE}) in case the geometries grouped form a coverage (do not have overlaps), setting this to \texttt{TRUE} speeds up the unioning
\end{itemize}

The \texttt{distinct} method selects distinct records, where \texttt{st\_equals}
is used to evaluate distinctness of geometries.

\texttt{filter} can be used with the usual predicates; when one wants to
use it with a spatial predicate, e.g.~to select all counties less
than 50 km away from Orange county, one could use

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{orange <-}\StringTok{ }\NormalTok{nc }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(NAME }\OperatorTok{==}\StringTok{ "Orange"}\NormalTok{)}
\NormalTok{wd =}\StringTok{ }\KeywordTok{st_is_within_distance}\NormalTok{(nc, orange, units}\OperatorTok{::}\KeywordTok{set_units}\NormalTok{(}\DecValTok{50}\NormalTok{, km))}
\NormalTok{o50 <-}\StringTok{ }\NormalTok{nc }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(}\KeywordTok{lengths}\NormalTok{(wd) }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{)}
\KeywordTok{nrow}\NormalTok{(o50)}
\CommentTok{# [1] 17}
\end{Highlighting}
\end{Shaded}

Figure \ref{fig:orangebuffer} shows the results of this analysis,
and in addition a buffer around the county borders; note that this buffer
serves for illustration, it was \emph{not} used to select the counties.

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/orangebuffer-1} 

}

\caption{Orange county (orange), counties within a 50 km radius (black), a 50 km buffer around Orange county (brown), and remaining counties (grey)}\label{fig:orangebuffer}
\end{figure}

\hypertarget{spatial-joins}{%
\section{Spatial joins}\label{spatial-joins}}

In regular (left, right or inner) joins, \emph{joined} records from a
pair of tables are reported when one or more selected attributes
match (are identical) in both tables. A spatial join is similar,
but the criterion to join records is not equality of attributes but
a spatial predicate. This leaves a wide variety of options in order
to define \emph{spatially} matching records, using binary predicates
listed in section \ref{de9im}. The concepts of ``left'', ``right'',
``inner'' or ``full'' joins remain identical to the non-spatial join
as the options for handling records that have no spatial match.

When using spatial joins, each record may have several matched
records, yielding a large result table. A way to reduce this
complexity may be to select from the matching records the one with
the largest overlap with the target geometry. An example of this
is shown (visually) in figure \ref{fig:largest}; this is done using
\texttt{st\_join} with argument \texttt{largest\ =\ TRUE}.



\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{sds_files/figure-latex/largest-1} 

}

\caption{example of \texttt{st\_join} with \texttt{largest\ =\ TRUE}: the label of the polygon in the top figure with the largest intersection with polygons in the bottom figure is assigned to the polygons of the bottom figure.}\label{fig:largest}
\end{figure}

Another way to reduce the result set is to use \texttt{aggregate} after a
join, all matching records, and union their geometries; see Section
\ref{updownscaling}.

\hypertarget{sampling-gridding-interpolating}{%
\subsection{sampling, gridding, interpolating}\label{sampling-gridding-interpolating}}

Several convenience functions are available in package \texttt{sf}, some
of which will be discussed here. Function \texttt{st\_sample} generates
a sample of points randomly sampled from target geometries, where
target geometries can be point, line or polygon geometries. Sampling
strategies can be (completely) random, regular or (with polygons)
triangular. Chapter \ref{pointpatterns} explains how spatial sampling
(or point pattern simulation) methods available in package \texttt{spatstat}
are interfaced through \texttt{st\_sample}.

Function \texttt{st\_make\_grid} creates a square, rectangular or hexagonal
grid over a region, or points with the grid centers or corners. It
was used to create the rectangular grid in figure \ref{fig:largest}.

Function \texttt{st\_interpolate\_aw} ``interpolates'' area values to new areas,
as explained in section \ref{area-weighted}, both for intensive
and extensive variables.

\hypertarget{package-stars}{%
\section{\texorpdfstring{Package \texttt{stars}}{Package stars}}\label{package-stars}}

Athough package \texttt{sp} has always had limited support for raster data,
over the last decade R package \texttt{raster} \citep{R-raster} has clearly been
dominant as the prime package for powerful, flexible and scalable
raster analysis. The raster data model of package \texttt{raster} (and
its successor, \texttt{terra} \citep{R-terra}) is that of a 2D regular raster,
or a set of raster layers (a ``raster stack''). This aligns with
the classical static ``GIS world view'', where the world is modelled
as a set of layers, each representing a different theme. A lot of
data available today however is dynamic, and comes as time series
of rasters or raster stacks. A raster stack does not meaningfully
reflect this, requiring the user to keep a register of which layer
represents what.

Also, the \texttt{raster} package, and its successer \texttt{terra} do an excellent
job in scaling computations up to datasizes no larger than the local
storage (the computer's hard drives), and doing this fast. Recent
datasets however, including satellite imagery, climate model or
weather forecasting data, often no longer fit in local storage
(chapter \ref{large}). Package \texttt{spacetime} \citep{spacetime2012}
addresses the analysis of time series of vector geometries or raster
grid cells, but does not extend to higher-dimensional arrays.

Here, we introduce package \texttt{stars} for analysing raster and vector
data cubes. The package

\begin{itemize}
\tightlist
\item
  allows for representing dynamic (time varying) raster stacks,
\item
  aims at being scalable, also beyond local disk size,
\item
  provides a strong integration of raster functions in the GDAL
  library
\item
  in addition to regular grids handles rotated, sheared, rectilinear and curvilinear rasters (figure \ref{fig:rastertypes01}),
\item
  provides a tight integration with package \texttt{sf},
\item
  also handles array data with non-raster spatial dimensions, the \emph{vector data cubes}, and
\item
  follows the tidyverse design principles.
\end{itemize}

Vector data cubes include for instance time series for simple
features, or spatial graph data such as potentially dynamic
origin-destination matrices. The concept of spatial vector and
raster data cubes was explained in chapter \ref{datacube}.

\hypertarget{reading-and-writing-raster-data}{%
\subsection{Reading and writing raster data}\label{reading-and-writing-raster-data}}

Raster data typically are read from a file. We use a dataset
containing a section of Landsat 7 scene, with the 6 30m-resolution
bands (bands 1-5 and 7) for a region covering the city of Olinda,
Brazil. We can read the example GeoTIFF file holding a regular,
non-rotated grid from the package \texttt{stars}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tif =}\StringTok{ }\KeywordTok{system.file}\NormalTok{(}\StringTok{"tif/L7_ETMs.tif"}\NormalTok{, }\DataTypeTok{package =} \StringTok{"stars"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(stars)}
\NormalTok{(}\DataTypeTok{r =} \KeywordTok{read_stars}\NormalTok{(tif))}
\CommentTok{# stars object with 3 dimensions and 1 attribute}
\CommentTok{# attribute(s):}
\CommentTok{#              Min. 1st Qu. Median Mean 3rd Qu. Max.}
\CommentTok{# L7_ETMs.tif     1      54     69 68.9      86  255}
\CommentTok{# dimension(s):}
\CommentTok{#      from  to  offset delta            refsys point values x/y}
\CommentTok{# x       1 349  288776  28.5 UTM Zone 25, S... FALSE   NULL [x]}
\CommentTok{# y       1 352 9120761 -28.5 UTM Zone 25, S... FALSE   NULL [y]}
\CommentTok{# band    1   6      NA    NA                NA    NA   NULL}
\end{Highlighting}
\end{Shaded}

where we see the offset, cellsize, coordinate reference system,
and dimensions. The dimension table reports the following for
each dimension:

\begin{itemize}
\tightlist
\item
  \texttt{from}: the starting index
\item
  \texttt{to}: the ending index
\item
  \texttt{offset}: the dimension value at the start (edge) of the first pixel
\item
  \texttt{delta}: the cell size; negative \texttt{delta} values indicate that pixel index increases with decreasing dimension values
\item
  \texttt{refsys}: the reference system
\item
  \texttt{point}: boolean, indicating whether cell values have point support or cell support
\item
  \texttt{values}: a list with values or labels associated with each of the dimension's values
\item
  \texttt{x/y}: an indicator whether a dimension is associated with a spatial raster x- or y-axis
\end{itemize}

For regular, rotated or sheared grids or other regularly discretized
dimensions (e.g.~time), \texttt{offset} and \texttt{delta} are not \texttt{NA}
and \texttt{values} is \texttt{NULL}; for other cases, \texttt{offset} and \texttt{delta} are
\texttt{NA} and \texttt{values} contains one of:

\begin{itemize}
\tightlist
\item
  the sequence of values, or intervals, in case of a rectlinear spatial raster or irregular time dimension
\item
  in case of a vector data cube, geometries associated with the spatial dimension
\item
  in case of a curvilinear raster, the matrix with coordinate values for each raster cell
\item
  in case of a discrete dimension, the band names or labels associated with the dimension values
\end{itemize}

The object \texttt{r} is of class \texttt{stars} and is a simple list of length
one, holding a three-dimensional array:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{length}\NormalTok{(r)}
\CommentTok{# [1] 1}
\KeywordTok{class}\NormalTok{(r[[}\DecValTok{1}\NormalTok{]])}
\CommentTok{# [1] "array"}
\KeywordTok{dim}\NormalTok{(r[[}\DecValTok{1}\NormalTok{]])}
\CommentTok{#    x    y band }
\CommentTok{#  349  352    6}
\end{Highlighting}
\end{Shaded}

and in addition holds an attribute with a dimensions table with all the metadata
required to know what the array dimensions refer to, obtained by

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{st_dimensions}\NormalTok{(r)}
\end{Highlighting}
\end{Shaded}

We can get the spatial extent of the array by

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{st_bbox}\NormalTok{(r)}
\CommentTok{#    xmin    ymin    xmax    ymax }
\CommentTok{#  288776 9110729  298723 9120761}
\end{Highlighting}
\end{Shaded}

Raster data can be written to local disk using \texttt{write\_stars}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tf =}\StringTok{ }\KeywordTok{tempfile}\NormalTok{(}\DataTypeTok{fileext =} \StringTok{".tif"}\NormalTok{)}
\KeywordTok{write_stars}\NormalTok{(r, tf)}
\end{Highlighting}
\end{Shaded}

where again the data format (in this case, GeoTIFF) is derived from the file
extension. As for simple features, reading and writing uses the GDAL
library; the list of available drivers for raster data is obtained
by

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{st_drivers}\NormalTok{(}\StringTok{"raster"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{subsetting-stars-data-cubes}{%
\subsection{\texorpdfstring{Subsetting \texttt{stars} data cubes}{Subsetting stars data cubes}}\label{subsetting-stars-data-cubes}}

Data cubes can be subsetted using \texttt{{[}}, or using tidyverse verbs. The
first options, \texttt{{[}} uses for the arguments: attributes first,
followed by dimension. This means that \texttt{r{[}1:2,\ 101:200,\ ,\ 5:10{]}}
selects from \texttt{r} attributes 1-2, index 101-200 for dimension 1, and index
5-10 for dimension 3; omitting dimension 2 means that no subsetting
takes place. For attributes, attributes names or logical vectors
can be used. For dimensions, logical vectors are not supported;
Selecting discontinuous ranges supported when it it is a regular
sequence. By default, \texttt{drop} is FALSE, when set to \texttt{TRUE} dimensions
with a single value are dropped:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{r[,}\DecValTok{1}\OperatorTok{:}\DecValTok{100}\NormalTok{, }\KeywordTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{250}\NormalTok{, }\DecValTok{5}\NormalTok{), }\DecValTok{4}\NormalTok{] }\OperatorTok{%>%}\StringTok{ }\KeywordTok{dim}\NormalTok{()}
\CommentTok{#    x    y band }
\CommentTok{#  100   50    1}
\NormalTok{r[,}\DecValTok{1}\OperatorTok{:}\DecValTok{100}\NormalTok{, }\KeywordTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{250}\NormalTok{, }\DecValTok{5}\NormalTok{), }\DecValTok{4}\NormalTok{, drop =}\StringTok{ }\OtherTok{TRUE}\NormalTok{] }\OperatorTok{%>%}\StringTok{ }\KeywordTok{dim}\NormalTok{()}
\CommentTok{#   x   y }
\CommentTok{# 100  50}
\end{Highlighting}
\end{Shaded}

For selecting particular ranges of dimension \emph{values}, one can use
\texttt{filter} (after loading \texttt{dplyr}):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr, }\DataTypeTok{warn.conflicts =} \OtherTok{FALSE}\NormalTok{)}
\KeywordTok{filter}\NormalTok{(r, x }\OperatorTok{>}\StringTok{ }\DecValTok{289000}\NormalTok{, x }\OperatorTok{<}\StringTok{ }\DecValTok{290000}\NormalTok{)}
\CommentTok{# stars object with 3 dimensions and 1 attribute}
\CommentTok{# attribute(s):}
\CommentTok{#              Min. 1st Qu. Median Mean 3rd Qu. Max.}
\CommentTok{# L7_ETMs.tif     5      51     63 64.3      75  242}
\CommentTok{# dimension(s):}
\CommentTok{#      from  to  offset delta            refsys point values x/y}
\CommentTok{# x       1  35  289004  28.5 UTM Zone 25, S... FALSE   NULL [x]}
\CommentTok{# y       1 352 9120761 -28.5 UTM Zone 25, S... FALSE   NULL [y]}
\CommentTok{# band    1   6       1     1                NA    NA   NULL}
\end{Highlighting}
\end{Shaded}

which changes the offset of the \(x\) dimension. Particular cube slices
can also be obtained with \texttt{slice}, e.g.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{slice}\NormalTok{(r, band, }\DecValTok{3}\NormalTok{)}
\CommentTok{# stars object with 2 dimensions and 1 attribute}
\CommentTok{# attribute(s):}
\CommentTok{#              Min. 1st Qu. Median Mean 3rd Qu. Max.}
\CommentTok{# L7_ETMs.tif    21      49     63 64.4      77  255}
\CommentTok{# dimension(s):}
\CommentTok{#   from  to  offset delta            refsys point values x/y}
\CommentTok{# x    1 349  288776  28.5 UTM Zone 25, S... FALSE   NULL [x]}
\CommentTok{# y    1 352 9120761 -28.5 UTM Zone 25, S... FALSE   NULL [y]}
\end{Highlighting}
\end{Shaded}

which drops the singular dimension. \texttt{mutate} can be used on \texttt{stars}
objects to add new arrays as functions of existing ones, \texttt{transmute}
drops existing ones.

\hypertarget{cropping}{%
\subsection{Cropping}\label{cropping}}

Further subsetting can be done using spatial objects of class \texttt{sf}, \texttt{sfc}
or \texttt{bbox}, e.g.~when using the sample raster,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{b =}\StringTok{ }\KeywordTok{st_bbox}\NormalTok{(r) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{st_as_sfc}\NormalTok{() }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{st_centroid}\NormalTok{() }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{st_buffer}\NormalTok{(units}\OperatorTok{::}\KeywordTok{set_units}\NormalTok{(}\DecValTok{500}\NormalTok{, m))}
\NormalTok{r[b]}
\CommentTok{# stars object with 3 dimensions and 1 attribute}
\CommentTok{# attribute(s):}
\CommentTok{#              Min. 1st Qu. Median Mean 3rd Qu. Max. NA's}
\CommentTok{# L7_ETMs.tif    22      54     66 67.7    78.2  174 2184}
\CommentTok{# dimension(s):}
\CommentTok{#      from  to  offset delta            refsys point values x/y}
\CommentTok{# x     157 193  288776  28.5 UTM Zone 25, S... FALSE   NULL [x]}
\CommentTok{# y     159 194 9120761 -28.5 UTM Zone 25, S... FALSE   NULL [y]}
\CommentTok{# band    1   6      NA    NA                NA    NA   NULL}
\end{Highlighting}
\end{Shaded}

selects the circular center region with a diameter of 500 metre, for
the first band shown in figure \ref{fig:circr},

\begin{verbatim}
# downsample set to c(0,0,1)
\end{verbatim}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/circr-1} 

}

\caption{circular center region of the Landsat 7 scene (band 1)}\label{fig:circr}
\end{figure}

where we see that pixels outside the spatial object are assigned \texttt{NA} values.
This object still has dimension indexes relative to the \texttt{offset} and \texttt{delta}
values of \texttt{r}; we can reset these to a new \texttt{offset} with

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{r[b] }\OperatorTok{%>%}\StringTok{ }\KeywordTok{st_normalize}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\KeywordTok{st_dimensions}\NormalTok{()}
\CommentTok{#      from to  offset delta            refsys point values x/y}
\CommentTok{# x       1 37  293222  28.5 UTM Zone 25, S... FALSE   NULL [x]}
\CommentTok{# y       1 36 9116258 -28.5 UTM Zone 25, S... FALSE   NULL [y]}
\CommentTok{# band    1  6      NA    NA                NA    NA   NULL}
\end{Highlighting}
\end{Shaded}

By default, the resulting raster is cropped to the extent of the
selection object; an object with the same dimensions as the input
object is obtained with

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{r[b, crop =}\StringTok{ }\OtherTok{FALSE}\NormalTok{]}
\CommentTok{# stars object with 3 dimensions and 1 attribute}
\CommentTok{# attribute(s):}
\CommentTok{#              Min. 1st Qu. Median Mean 3rd Qu. Max.   NA's}
\CommentTok{# L7_ETMs.tif    22      54     66 67.7    78.2  174 731280}
\CommentTok{# dimension(s):}
\CommentTok{#      from  to  offset delta            refsys point values x/y}
\CommentTok{# x       1 349  288776  28.5 UTM Zone 25, S... FALSE   NULL [x]}
\CommentTok{# y       1 352 9120761 -28.5 UTM Zone 25, S... FALSE   NULL [y]}
\CommentTok{# band    1   6      NA    NA                NA    NA   NULL}
\end{Highlighting}
\end{Shaded}

Cropping a \texttt{stars} object can alternatively be done directly with \texttt{st\_crop}, as in

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{st_crop}\NormalTok{(r, b)}
\end{Highlighting}
\end{Shaded}

\hypertarget{redimensioning-stars-objects}{%
\subsection{\texorpdfstring{redimensioning \texttt{stars} objects}{redimensioning stars objects}}\label{redimensioning-stars-objects}}

Package \texttt{stars} uses package \texttt{abind} \citep{R-abind} for a number of
array manipulations. One of them is \texttt{aperm} which transposes an
array by permuting it. A method for \texttt{stars} objects is provided, and

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{aperm}\NormalTok{(r, }\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\CommentTok{# stars object with 3 dimensions and 1 attribute}
\CommentTok{# attribute(s):}
\CommentTok{#              Min. 1st Qu. Median Mean 3rd Qu. Max.}
\CommentTok{# L7_ETMs.tif     1      54     69 68.9      86  255}
\CommentTok{# dimension(s):}
\CommentTok{#      from  to  offset delta            refsys point values x/y}
\CommentTok{# band    1   6      NA    NA                NA    NA   NULL    }
\CommentTok{# x       1 349  288776  28.5 UTM Zone 25, S... FALSE   NULL [x]}
\CommentTok{# y       1 352 9120761 -28.5 UTM Zone 25, S... FALSE   NULL [y]}
\end{Highlighting}
\end{Shaded}

permutes the order of dimensions of the resulting object.

Attributes and dimensions can be swapped, using \texttt{merge} and \texttt{split}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\DataTypeTok{rs =} \KeywordTok{split}\NormalTok{(r))}
\CommentTok{# stars object with 2 dimensions and 6 attributes}
\CommentTok{# attribute(s):}
\CommentTok{#     Min. 1st Qu. Median Mean 3rd Qu. Max.}
\CommentTok{# X1    47      67     78 79.1      89  255}
\CommentTok{# X2    32      55     66 67.6      79  255}
\CommentTok{# X3    21      49     63 64.4      77  255}
\CommentTok{# X4     9      52     63 59.2      75  255}
\CommentTok{# X5     1      63     89 83.2     112  255}
\CommentTok{# X6     1      32     60 60.0      88  255}
\CommentTok{# dimension(s):}
\CommentTok{#   from  to  offset delta            refsys point values x/y}
\CommentTok{# x    1 349  288776  28.5 UTM Zone 25, S... FALSE   NULL [x]}
\CommentTok{# y    1 352 9120761 -28.5 UTM Zone 25, S... FALSE   NULL [y]}
\KeywordTok{merge}\NormalTok{(rs)}
\CommentTok{# stars object with 3 dimensions and 1 attribute}
\CommentTok{# attribute(s):}
\CommentTok{#    Min. 1st Qu. Median Mean 3rd Qu. Max.}
\CommentTok{# X     1      54     69 68.9      86  255}
\CommentTok{# dimension(s):}
\CommentTok{#            from  to  offset delta            refsys point}
\CommentTok{# x             1 349  288776  28.5 UTM Zone 25, S... FALSE}
\CommentTok{# y             1 352 9120761 -28.5 UTM Zone 25, S... FALSE}
\CommentTok{# attributes    1   6      NA    NA                NA    NA}
\CommentTok{#               values x/y}
\CommentTok{# x               NULL [x]}
\CommentTok{# y               NULL [y]}
\CommentTok{# attributes X1,...,X6}
\end{Highlighting}
\end{Shaded}

split distributes the \texttt{band} dimension over 6 attributes of a
2-dimensional array, \texttt{merge} reverses this operation. \texttt{st\_redimension}
can be used for more generic operations, such as splitting a single array
dimension over two new dimensions:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{st_redimension}\NormalTok{(r, }\KeywordTok{c}\NormalTok{(}\DecValTok{349}\NormalTok{, }\DecValTok{352}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\CommentTok{# stars object with 4 dimensions and 1 attribute}
\CommentTok{# attribute(s):}
\CommentTok{#              Min. 1st Qu. Median Mean 3rd Qu. Max.}
\CommentTok{# L7_ETMs.tif     1      54     69 68.9      86  255}
\CommentTok{# dimension(s):}
\CommentTok{#    from  to  offset delta            refsys point values}
\CommentTok{# X1    1 349  288776  28.5 UTM Zone 25, S... FALSE   NULL}
\CommentTok{# X2    1 352 9120761 -28.5 UTM Zone 25, S... FALSE   NULL}
\CommentTok{# X3    1   3      NA    NA                NA    NA   NULL}
\CommentTok{# X4    1   2      NA    NA                NA    NA   NULL}
\end{Highlighting}
\end{Shaded}

\hypertarget{extracting-point-samples-aggregating}{%
\subsection{extracting point samples, aggregating}\label{extracting-point-samples-aggregating}}

A very common use case for raster data cube analysis is the extraction
of values at certain locations, or computing aggregations over certain
geometries. \texttt{st\_extract} extracts point values. We will do this
for a few randomly sampled points over the bounding box of \texttt{r}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pts =}\StringTok{ }\KeywordTok{st_bbox}\NormalTok{(r) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{st_as_sfc}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\KeywordTok{st_sample}\NormalTok{(}\DecValTok{20}\NormalTok{)}
\NormalTok{(}\DataTypeTok{e =} \KeywordTok{st_extract}\NormalTok{(r, pts))}
\CommentTok{# stars object with 2 dimensions and 1 attribute}
\CommentTok{# attribute(s):}
\CommentTok{#              Min. 1st Qu. Median Mean 3rd Qu. Max.}
\CommentTok{# L7_ETMs.tif    12    41.8     63   61    80.5  145}
\CommentTok{# dimension(s):}
\CommentTok{#          from to offset delta            refsys point}
\CommentTok{# geometry    1 20     NA    NA UTM Zone 25, S...  TRUE}
\CommentTok{# band        1  6     NA    NA                NA    NA}
\CommentTok{#                                                     values}
\CommentTok{# geometry POINT (293002 9115516),...,POINT (290941 9114128)}
\CommentTok{# band                                                  NULL}
\end{Highlighting}
\end{Shaded}

which results in a vector data cube with 4 points and 6 bands.

\hypertarget{predictive-models}{%
\subsection{Predictive models}\label{predictive-models}}

The typical model prediction workflow in R works as follows:

\begin{itemize}
\tightlist
\item
  use a \texttt{data.frame} with response and predictor variables (covariates)
\item
  create a model object based on this \texttt{data.frame}
\item
  call \texttt{predict} with this model object and the \texttt{data.frame} with target predictor variable values
\end{itemize}

Package \texttt{stars} provides a \texttt{predict} method for \texttt{stars} objects
that essentially wraps the last step, by creating the \texttt{data.frame},
calling the \texttt{predict} method for that, and reconstructing a \texttt{stars}
object with the predicted values.

We will illustrate this with a trivial two-class example mapping
land from sea in the example Landsat data set, using the sample
points extracted above, shown in figure \ref{fig:rsample}.

\begin{verbatim}
# downsample set to c(0,0,1)
\end{verbatim}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/rsample-1} 

}

\caption{randomly chosen sample locations for training data; red: water, green: land}\label{fig:rsample}
\end{figure}

From this figure, we read ``by eye'' that the points 8, 14, 15, 18 and
19 are on water, the others on land. Using a linear discriminant
(``maximum likelihood'') classifier, we find model predictions as
shown in figure \ref{fig:lda}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rs =}\StringTok{ }\KeywordTok{split}\NormalTok{(r)}
\NormalTok{trn =}\StringTok{ }\KeywordTok{st_extract}\NormalTok{(rs, pts)}
\NormalTok{trn}\OperatorTok{$}\NormalTok{cls =}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\StringTok{"land"}\NormalTok{, }\DecValTok{20}\NormalTok{)}
\NormalTok{trn}\OperatorTok{$}\NormalTok{cls[}\KeywordTok{c}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{14}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{19}\NormalTok{)] =}\StringTok{ "water"}
\NormalTok{model =}\StringTok{ }\NormalTok{MASS}\OperatorTok{::}\KeywordTok{lda}\NormalTok{(cls }\OperatorTok{~}\StringTok{ }\NormalTok{., }\KeywordTok{st_set_geometry}\NormalTok{(trn, }\OtherTok{NULL}\NormalTok{))}
\NormalTok{pr =}\StringTok{ }\KeywordTok{predict}\NormalTok{(rs, model)}
\end{Highlighting}
\end{Shaded}

Here, we used the \texttt{MASS::} prefix to avoid loading \texttt{MASS}, as that
would mask \texttt{select} from \texttt{dplyr}. Another way would be to load
\texttt{MASS} and unload it later on with \texttt{detach()}.



\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/lda-1} 

}

\caption{Linear discriminant classifier for land/water, based on training data of figure \ref{fig:rsample}}\label{fig:lda}
\end{figure}

We also see that the layer plotted in figure \ref{fig:lda} is a
\texttt{factor} variable, with class labels.

\hypertarget{gdal-utils}{%
\subsection{GDAL utils}\label{gdal-utils}}

\hypertarget{plotting-raster-data}{%
\subsection{Plotting raster data}\label{plotting-raster-data}}

We can use the base \texttt{plot} method for \texttt{stars} objects, where the plot created with
\texttt{plot(r)} is shown in figure \ref{fig:firststars}.

\begin{verbatim}
# downsample set to c(2,2,1)
\end{verbatim}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/firststars-1} 

}

\caption{6 30m Landsat bands downsampled to 90m for Olinda, Br.}\label{fig:firststars}
\end{figure}

is shown in figure \ref{fig:firststars}.
The default color scale uses grey tones, and stretches these such
that color breaks correspond to data quantiles over all bands
(``histogram equalization''). A more familiar view is the RGB or
false color composite shown in figure \ref{fig:starsrgb}.

\begin{verbatim}
# downsample set to c(2,2,1)
# downsample set to c(2,2,1)
\end{verbatim}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{sds_files/figure-latex/starsrgb-1} 

}

\caption{Two color composites}\label{fig:starsrgb}
\end{figure}

Further details and options are given in Chapter \ref{plotting}.

\hypertarget{analysing-raster-data}{%
\subsection{Analysing raster data}\label{analysing-raster-data}}

Element-wise mathematical functions (section \ref{applydimension})
on \texttt{stars} objects are just passed on to the arrays. This means
that we can call functions and create expressions:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{log}\NormalTok{(r)}
\CommentTok{# stars object with 3 dimensions and 1 attribute}
\CommentTok{# attribute(s):}
\CommentTok{#              Min. 1st Qu. Median Mean 3rd Qu. Max.}
\CommentTok{# L7_ETMs.tif     0    3.99   4.23 4.12    4.45 5.54}
\CommentTok{# dimension(s):}
\CommentTok{#      from  to  offset delta            refsys point values x/y}
\CommentTok{# x       1 349  288776  28.5 UTM Zone 25, S... FALSE   NULL [x]}
\CommentTok{# y       1 352 9120761 -28.5 UTM Zone 25, S... FALSE   NULL [y]}
\CommentTok{# band    1   6      NA    NA                NA    NA   NULL}
\NormalTok{r }\OperatorTok{+}\StringTok{ }\DecValTok{2} \OperatorTok{*}\StringTok{ }\KeywordTok{log}\NormalTok{(r)}
\CommentTok{# stars object with 3 dimensions and 1 attribute}
\CommentTok{# attribute(s):}
\CommentTok{#              Min. 1st Qu. Median Mean 3rd Qu. Max.}
\CommentTok{# L7_ETMs.tif     1      62   77.5 77.1    94.9  266}
\CommentTok{# dimension(s):}
\CommentTok{#      from  to  offset delta            refsys point values x/y}
\CommentTok{# x       1 349  288776  28.5 UTM Zone 25, S... FALSE   NULL [x]}
\CommentTok{# y       1 352 9120761 -28.5 UTM Zone 25, S... FALSE   NULL [y]}
\CommentTok{# band    1   6      NA    NA                NA    NA   NULL}
\end{Highlighting}
\end{Shaded}

or even mask out certain values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{r2 =}\StringTok{ }\NormalTok{r}
\NormalTok{r2[r }\OperatorTok{<}\StringTok{ }\DecValTok{50}\NormalTok{] =}\StringTok{ }\OtherTok{NA}
\NormalTok{r2}
\CommentTok{# stars object with 3 dimensions and 1 attribute}
\CommentTok{# attribute(s):}
\CommentTok{#              Min. 1st Qu. Median Mean 3rd Qu. Max.   NA's}
\CommentTok{# L7_ETMs.tif    50      64     75   79      90  255 149170}
\CommentTok{# dimension(s):}
\CommentTok{#      from  to  offset delta            refsys point values x/y}
\CommentTok{# x       1 349  288776  28.5 UTM Zone 25, S... FALSE   NULL [x]}
\CommentTok{# y       1 352 9120761 -28.5 UTM Zone 25, S... FALSE   NULL [y]}
\CommentTok{# band    1   6      NA    NA                NA    NA   NULL}
\end{Highlighting}
\end{Shaded}

or un-mask areas:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{r2[}\KeywordTok{is.na}\NormalTok{(r2)] =}\StringTok{ }\DecValTok{0}
\NormalTok{r2}
\CommentTok{# stars object with 3 dimensions and 1 attribute}
\CommentTok{# attribute(s):}
\CommentTok{#              Min. 1st Qu. Median Mean 3rd Qu. Max.}
\CommentTok{# L7_ETMs.tif     0      54     69   63      86  255}
\CommentTok{# dimension(s):}
\CommentTok{#      from  to  offset delta            refsys point values x/y}
\CommentTok{# x       1 349  288776  28.5 UTM Zone 25, S... FALSE   NULL [x]}
\CommentTok{# y       1 352 9120761 -28.5 UTM Zone 25, S... FALSE   NULL [y]}
\CommentTok{# band    1   6      NA    NA                NA    NA   NULL}
\end{Highlighting}
\end{Shaded}

Dimension-wise, we can apply functions to selected array dimensions
(section \ref{reducedimension})
of stars objects similar to how \texttt{apply} does this to arrays. For
instance, we can compute for each pixel the mean of the 6 band values by

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{st_apply}\NormalTok{(r, }\KeywordTok{c}\NormalTok{(}\StringTok{"x"}\NormalTok{, }\StringTok{"y"}\NormalTok{), mean)}
\CommentTok{# stars object with 2 dimensions and 1 attribute}
\CommentTok{# attribute(s):}
\CommentTok{#       Min. 1st Qu. Median Mean 3rd Qu. Max.}
\CommentTok{# mean  25.5    53.3   68.3 68.9      82  255}
\CommentTok{# dimension(s):}
\CommentTok{#   from  to  offset delta            refsys point values x/y}
\CommentTok{# x    1 349  288776  28.5 UTM Zone 25, S... FALSE   NULL [x]}
\CommentTok{# y    1 352 9120761 -28.5 UTM Zone 25, S... FALSE   NULL [y]}
\end{Highlighting}
\end{Shaded}

A more meaningful function would e.g.~compute the NDVI (normalized
differenced vegetation index):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ndvi =}\StringTok{ }\ControlFlowTok{function}\NormalTok{(b1, b2, b3, b4, b5, b6) (b4 }\OperatorTok{-}\StringTok{ }\NormalTok{b3)}\OperatorTok{/}\NormalTok{(b4 }\OperatorTok{+}\StringTok{ }\NormalTok{b3)}
\KeywordTok{st_apply}\NormalTok{(r, }\KeywordTok{c}\NormalTok{(}\StringTok{"x"}\NormalTok{, }\StringTok{"y"}\NormalTok{), ndvi)}
\CommentTok{# stars object with 2 dimensions and 1 attribute}
\CommentTok{# attribute(s):}
\CommentTok{#         Min. 1st Qu.  Median    Mean 3rd Qu.  Max.}
\CommentTok{# ndvi  -0.753  -0.203 -0.0687 -0.0643   0.187 0.587}
\CommentTok{# dimension(s):}
\CommentTok{#   from  to  offset delta            refsys point values x/y}
\CommentTok{# x    1 349  288776  28.5 UTM Zone 25, S... FALSE   NULL [x]}
\CommentTok{# y    1 352 9120761 -28.5 UTM Zone 25, S... FALSE   NULL [y]}
\end{Highlighting}
\end{Shaded}

Alternatively, one could have defined

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ndvi2 =}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) (x[}\DecValTok{4}\NormalTok{]}\OperatorTok{-}\NormalTok{x[}\DecValTok{3}\NormalTok{])}\OperatorTok{/}\NormalTok{(x[}\DecValTok{4}\NormalTok{]}\OperatorTok{+}\NormalTok{x[}\DecValTok{3}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

which is more convenient if the number of bands is large,
but which is also much slower than \texttt{ndvi} as it
needs to be called for every pixel whereas \texttt{ndvi} can be called
once for all pixels, or for large chunks of pixels.
The mean for each band over the whole image is computed by

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{st_apply}\NormalTok{(r, }\KeywordTok{c}\NormalTok{(}\StringTok{"band"}\NormalTok{), mean))}
\CommentTok{#   band mean}
\CommentTok{# 1    1 79.1}
\CommentTok{# 2    2 67.6}
\CommentTok{# 3    3 64.4}
\CommentTok{# 4    4 59.2}
\CommentTok{# 5    5 83.2}
\CommentTok{# 6    6 60.0}
\end{Highlighting}
\end{Shaded}

the result of which is small enough to be printed here as a \texttt{data.frame}. In these
two examples, entire dimensions disappear. Sometimes, this does not
happen (section \ref{applydimension}); we can for instance compute the three quartiles for each band

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{st_apply}\NormalTok{(r, }\KeywordTok{c}\NormalTok{(}\StringTok{"band"}\NormalTok{), quantile, }\KeywordTok{c}\NormalTok{(.}\DecValTok{25}\NormalTok{, }\FloatTok{.5}\NormalTok{, }\FloatTok{.75}\NormalTok{))}
\CommentTok{# stars object with 2 dimensions and 1 attribute}
\CommentTok{# attribute(s):}
\CommentTok{#              Min. 1st Qu. Median Mean 3rd Qu. Max.}
\CommentTok{# L7_ETMs.tif    32    60.8   66.5 69.8    78.8  112}
\CommentTok{# dimension(s):}
\CommentTok{#          from to offset delta refsys point        values}
\CommentTok{# quantile    1  3     NA    NA     NA    NA 25%, 50%, 75%}
\CommentTok{# band        1  6     NA    NA     NA    NA          NULL}
\end{Highlighting}
\end{Shaded}

and see that this \emph{creates} a new dimension, \texttt{quantile}, with three values.
Alternatively, the three quantiles over the 6 bands for each pixel are
obtained by

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{st_apply}\NormalTok{(r, }\KeywordTok{c}\NormalTok{(}\StringTok{"x"}\NormalTok{, }\StringTok{"y"}\NormalTok{), quantile, }\KeywordTok{c}\NormalTok{(.}\DecValTok{25}\NormalTok{, }\FloatTok{.5}\NormalTok{, }\FloatTok{.75}\NormalTok{))}
\CommentTok{# stars object with 3 dimensions and 1 attribute}
\CommentTok{# attribute(s):}
\CommentTok{#              Min. 1st Qu. Median Mean 3rd Qu. Max.}
\CommentTok{# L7_ETMs.tif     4      55   69.2 67.2    81.2  255}
\CommentTok{# dimension(s):}
\CommentTok{#          from  to  offset delta            refsys point}
\CommentTok{# quantile    1   3      NA    NA                NA    NA}
\CommentTok{# x           1 349  288776  28.5 UTM Zone 25, S... FALSE}
\CommentTok{# y           1 352 9120761 -28.5 UTM Zone 25, S... FALSE}
\CommentTok{#                 values x/y}
\CommentTok{# quantile 25%, 50%, 75%    }
\CommentTok{# x                 NULL [x]}
\CommentTok{# y                 NULL [y]}
\end{Highlighting}
\end{Shaded}

\hypertarget{curvilinear-rasters}{%
\subsection{Curvilinear rasters}\label{curvilinear-rasters}}

There are several reasons why non-regular rasters occur (figure
\ref{fig:rastertypes01}). For one, when the data is Earth-bound,
a regular raster does not fit the Earth's surface, which is
curved. Other reasons are:

\begin{itemize}
\tightlist
\item
  when we convert or transform a regular raster data into another
  coordinate reference system, it will become curvilinear unless we
  resample (warp; section \ref{warp}); warping always goes at the
  cost of some loss of data and is not reversible.
\item
  observation may lead to irregular rasters; e.g.~for satellite swaths, we
  may have a regular raster in the direction of the satellite (not
  aligned with \(x\) or \(y\)), and rectilinear in the direction perpendicular to that
  (e.g.~if the sensor discretizes the viewing \emph{angle} in equal sections)
\end{itemize}

\hypertarget{vector-data-cube-examples}{%
\section{Vector data cube examples}\label{vector-data-cube-examples}}

\hypertarget{example-aggregating-air-quality-time-series}{%
\subsection{Example: aggregating air quality time series}\label{example-aggregating-air-quality-time-series}}

Air quality data from package \texttt{spacetime} were
obtained from the \href{https://www.eea.europa.eu/data-and-maps/data/aqereporting-8}{airBase European air quality data
base}.
Daily average PM\(_{10}\) values were downloaded for rural background
stations in Germany, 1998-2009.

We can create a \texttt{stars} object from the \texttt{air} matrix, the \texttt{dates}
Date vector and the \texttt{stations} \texttt{SpatialPoints} objects by

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(spacetime)}
\KeywordTok{data}\NormalTok{(air) }\CommentTok{# this loads several datasets in .GlobalEnv}
\KeywordTok{dim}\NormalTok{(air)}
\CommentTok{# space  time }
\CommentTok{#    70  4383}
\NormalTok{d =}\StringTok{ }\KeywordTok{st_dimensions}\NormalTok{(}\DataTypeTok{station =} \KeywordTok{st_as_sfc}\NormalTok{(stations), }\DataTypeTok{time =}\NormalTok{ dates)}
\NormalTok{(}\DataTypeTok{aq =} \KeywordTok{st_as_stars}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DataTypeTok{PM10 =}\NormalTok{ air), }\DataTypeTok{dimensions =}\NormalTok{ d))}
\CommentTok{# stars object with 2 dimensions and 1 attribute}
\CommentTok{# attribute(s):}
\CommentTok{#       Min. 1st Qu. Median Mean 3rd Qu. Max.   NA's}
\CommentTok{# PM10     0    9.92   14.8 17.7      22  274 157659}
\CommentTok{# dimension(s):}
\CommentTok{#         from   to     offset  delta            refsys point}
\CommentTok{# station    1   70         NA     NA +proj=longlat ...  TRUE}
\CommentTok{# time       1 4383 1998-01-01 1 days              Date FALSE}
\CommentTok{#                                          values}
\CommentTok{# station POINT (9.59 53.7),...,POINT (9.45 49.2)}
\CommentTok{# time                                       NULL}
\end{Highlighting}
\end{Shaded}

We can see from figure \ref{fig:airst} that the time series are
quite long, but also have large missing value gaps.
Figure \ref{fig:airmap} shows the spatial distribution measurement stations and
mean PM\(_{10}\) values.

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/airst-1} 

}

\caption{space-time diagram of PM$_{10}$ measurements by time and station}\label{fig:airst}
\end{figure}

\begin{verbatim}
# Warning in sp::proj4string(obj): CRS object has comment, which is
# lost in output
\end{verbatim}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/airmap-1} 

}

\caption{locations of PM$_{10}$ measurement stations, showing mean values}\label{fig:airmap}
\end{figure}

We can aggregate these station time series to area means, mostly
as a simple exercise. For this, we use the \texttt{aggregate} method for
\texttt{stars} objects

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\DataTypeTok{a =} \KeywordTok{aggregate}\NormalTok{(aq, }\KeywordTok{st_as_sf}\NormalTok{(DE_NUTS1), mean, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{))}
\CommentTok{# stars object with 2 dimensions and 1 attribute}
\CommentTok{# attribute(s):}
\CommentTok{#       Min. 1st Qu. Median Mean 3rd Qu. Max.  NA's}
\CommentTok{# PM10  1.08    10.9   15.3 17.9    21.8  172 25679}
\CommentTok{# dimension(s):}
\CommentTok{#          from   to     offset  delta            refsys point}
\CommentTok{# geometry    1   16         NA     NA +proj=longlat ... FALSE}
\CommentTok{# time        1 4383 1998-01-01 1 days              Date FALSE}
\CommentTok{#                                                                     values}
\CommentTok{# geometry MULTIPOLYGON (((9.65 49.8, ...,...,MULTIPOLYGON (((10.8 51.6, ...}
\CommentTok{# time                                                                  NULL}
\end{Highlighting}
\end{Shaded}

and we can now for instance show the maps for six arbitrarily chosen days
(figure \ref{fig:airagg}), using

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\NormalTok{a }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(time }\OperatorTok{>=}\StringTok{ "2008-01-01"}\NormalTok{, time }\OperatorTok{<}\StringTok{ "2008-01-07"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{plot}\NormalTok{(}\DataTypeTok{key.pos =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/airagg-1} 

}

\caption{areal mean PM$_{10}$ values, for six days}\label{fig:airagg}
\end{figure}

or create a time series plot of mean values for a single state
(figure \ref{fig:airts}) by

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{suppressPackageStartupMessages}\NormalTok{(}\KeywordTok{library}\NormalTok{(xts))}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{as.xts}\NormalTok{(a)[,}\DecValTok{4}\NormalTok{], }\DataTypeTok{main =}\NormalTok{ DE_NUTS1}\OperatorTok{$}\NormalTok{NAME_}\DecValTok{1}\NormalTok{[}\DecValTok{4}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/airts-1} 

}

\caption{areal mean PM$_{10}$ values, for six days}\label{fig:airts}
\end{figure}

\hypertarget{example-bristol-origin-destination-datacube}{%
\subsection{Example: Bristol origin-destination datacube}\label{example-bristol-origin-destination-datacube}}

The data used for this example come from \citep{geocomp}, and concern
origin-destination (OD) counts: the number of persons going from
region A to region B, by transportation mode. We have feature
geometries for the 102 origin and destination regions, shown in figure \ref{fig:bristol1}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(spDataLarge)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{st_geometry}\NormalTok{(bristol_zones), }\DataTypeTok{axes =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{graticule =} \OtherTok{TRUE}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{st_geometry}\NormalTok{(bristol_zones)[}\DecValTok{33}\NormalTok{], }\DataTypeTok{col =} \StringTok{'red'}\NormalTok{, }\DataTypeTok{add =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/bristol1-1} 

}

\caption{Origin destination data zones for Bristol, UK, with zone 33 (E02003043) colored red}\label{fig:bristol1}
\end{figure}

and the OD counts come in a table
with OD pairs as records, and transportation mode as variables:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(bristol_od)}
\CommentTok{# # A tibble: 6 x 7}
\CommentTok{#   o         d           all bicycle  foot car_driver train}
\CommentTok{#   <chr>     <chr>     <dbl>   <dbl> <dbl>      <dbl> <dbl>}
\CommentTok{# 1 E02002985 E02002985   209       5   127         59     0}
\CommentTok{# 2 E02002985 E02002987   121       7    35         62     0}
\CommentTok{# 3 E02002985 E02003036    32       2     1         10     1}
\CommentTok{# 4 E02002985 E02003043   141       1     2         56    17}
\CommentTok{# 5 E02002985 E02003049    56       2     4         36     0}
\CommentTok{# 6 E02002985 E02003054    42       4     0         21     0}
\end{Highlighting}
\end{Shaded}

We see that many combinations of origin and destination are implicit
zeroes, otherwise these two numbers would have been similar:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{nrow}\NormalTok{(bristol_zones)}\OperatorTok{^}\DecValTok{2} \CommentTok{# all combinations}
\CommentTok{# [1] 10404}
\KeywordTok{nrow}\NormalTok{(bristol_od) }\CommentTok{# non-zero combinations}
\CommentTok{# [1] 2910}
\end{Highlighting}
\end{Shaded}

We will form a three-dimensional vector datacube with origin,
destination and transportation mode as dimensions. For this, we
first ``tidy'' the \texttt{bristol\_od} table to have origin (o), destination (d),
transportation mode (mode), and count (n) as variables, using
\texttt{pivot\_longer}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create O-D-mode array:}
\NormalTok{bristol_tidy <-}\StringTok{ }\NormalTok{bristol_od }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{all) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{pivot_longer}\NormalTok{(}\DecValTok{3}\OperatorTok{:}\DecValTok{6}\NormalTok{, }\DataTypeTok{names_to =} \StringTok{"mode"}\NormalTok{, }\DataTypeTok{values_to =} \StringTok{"n"}\NormalTok{)}
\KeywordTok{head}\NormalTok{(bristol_tidy)}
\CommentTok{# # A tibble: 6 x 4}
\CommentTok{#   o         d         mode           n}
\CommentTok{#   <chr>     <chr>     <chr>      <dbl>}
\CommentTok{# 1 E02002985 E02002985 bicycle        5}
\CommentTok{# 2 E02002985 E02002985 foot         127}
\CommentTok{# 3 E02002985 E02002985 car_driver    59}
\CommentTok{# 4 E02002985 E02002985 train          0}
\CommentTok{# 5 E02002985 E02002987 bicycle        7}
\CommentTok{# 6 E02002985 E02002987 foot          35}
\end{Highlighting}
\end{Shaded}

Next, we form the three-dimensional array \texttt{a}, filled with zeroes:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{od =}\StringTok{ }\NormalTok{bristol_tidy }\OperatorTok{%>%}\StringTok{ }\KeywordTok{pull}\NormalTok{(}\StringTok{"o"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{unique}\NormalTok{()}
\NormalTok{nod =}\StringTok{ }\KeywordTok{length}\NormalTok{(od)}
\NormalTok{mode =}\StringTok{ }\NormalTok{bristol_tidy }\OperatorTok{%>%}\StringTok{ }\KeywordTok{pull}\NormalTok{(}\StringTok{"mode"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{unique}\NormalTok{()}
\NormalTok{nmode =}\StringTok{ }\KeywordTok{length}\NormalTok{(mode)}
\NormalTok{a =}\StringTok{ }\KeywordTok{array}\NormalTok{(0L,  }\KeywordTok{c}\NormalTok{(nod, nod, nmode), }
    \DataTypeTok{dimnames =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{o =}\NormalTok{ od, }\DataTypeTok{d =}\NormalTok{ od, }\DataTypeTok{mode =}\NormalTok{ mode))}
\KeywordTok{dim}\NormalTok{(a)}
\CommentTok{# [1] 102 102   4}
\end{Highlighting}
\end{Shaded}

We see that the dimensions are named with the zone names (o, d)
and the transportation mode name (mode).
Every row of \texttt{bristol\_tidy} denotes an array entry, and we can
use this to to fill the non-zero entries of the \texttt{bristol\_tidy} table
with their appropriate value (\texttt{n}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a[}\KeywordTok{as.matrix}\NormalTok{(bristol_tidy[}\KeywordTok{c}\NormalTok{(}\StringTok{"o"}\NormalTok{, }\StringTok{"d"}\NormalTok{, }\StringTok{"mode"}\NormalTok{)])] =}\StringTok{ }\NormalTok{bristol_tidy}\OperatorTok{$}\NormalTok{n}
\end{Highlighting}
\end{Shaded}

To be sure that there is not an order mismatch between the zones
in \texttt{bristol\_zones} and the zone names in \texttt{bristol\_tidy}, we can
get the right set of zones by:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{order =}\StringTok{ }\KeywordTok{match}\NormalTok{(od, bristol_zones}\OperatorTok{$}\NormalTok{geo_code) }\CommentTok{# it happens this equals 1:102}
\NormalTok{zones =}\StringTok{ }\KeywordTok{st_geometry}\NormalTok{(bristol_zones)[order]}
\end{Highlighting}
\end{Shaded}

(It happens that the order is already correct, but it is good
practice to not assume this).

Next, with zones and modes we can create a stars dimensions object:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(stars)}
\NormalTok{(}\DataTypeTok{d =} \KeywordTok{st_dimensions}\NormalTok{(}\DataTypeTok{o =}\NormalTok{ zones, }\DataTypeTok{d =}\NormalTok{ zones, }\DataTypeTok{mode =}\NormalTok{ mode))}
\CommentTok{#      from  to offset delta refsys point}
\CommentTok{# o       1 102     NA    NA WGS 84 FALSE}
\CommentTok{# d       1 102     NA    NA WGS 84 FALSE}
\CommentTok{# mode    1   4     NA    NA     NA FALSE}
\CommentTok{#                                                                 values}
\CommentTok{# o    MULTIPOLYGON (((-2.51 51.4,...,...,MULTIPOLYGON (((-2.55 51.5,...}
\CommentTok{# d    MULTIPOLYGON (((-2.51 51.4,...,...,MULTIPOLYGON (((-2.55 51.5,...}
\CommentTok{# mode                                                 bicycle,...,train}
\end{Highlighting}
\end{Shaded}

and finally build or stars object from \texttt{a} and \texttt{d}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\DataTypeTok{odm =} \KeywordTok{st_as_stars}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DataTypeTok{N =}\NormalTok{ a), }\DataTypeTok{dimensions =}\NormalTok{ d))}
\CommentTok{# stars object with 3 dimensions and 1 attribute}
\CommentTok{# attribute(s):}
\CommentTok{#    Min. 1st Qu. Median Mean 3rd Qu. Max.}
\CommentTok{# N     0       0      0  4.8       0 1296}
\CommentTok{# dimension(s):}
\CommentTok{#      from  to offset delta refsys point}
\CommentTok{# o       1 102     NA    NA WGS 84 FALSE}
\CommentTok{# d       1 102     NA    NA WGS 84 FALSE}
\CommentTok{# mode    1   4     NA    NA     NA FALSE}
\CommentTok{#                                                                 values}
\CommentTok{# o    MULTIPOLYGON (((-2.51 51.4,...,...,MULTIPOLYGON (((-2.55 51.5,...}
\CommentTok{# d    MULTIPOLYGON (((-2.51 51.4,...,...,MULTIPOLYGON (((-2.55 51.5,...}
\CommentTok{# mode                                                 bicycle,...,train}
\end{Highlighting}
\end{Shaded}

We can take a single slice through from this three-dimensional
array, e.g.~for zone 33 (figure \ref{fig:bristol1}), by \texttt{odm{[},,33{]}},
and plot it with

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(odm[,,}\DecValTok{33}\NormalTok{] }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{, }\DataTypeTok{logz =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{# Warning in st_as_sf.stars(x): working on the first sfc dimension}
\CommentTok{# only}
\CommentTok{# Warning in st_bbox.dimensions(st_dimensions(obj), ...): returning}
\CommentTok{# the bounding box of the first geometry dimension}

\CommentTok{# Warning in st_bbox.dimensions(st_dimensions(obj), ...): returning}
\CommentTok{# the bounding box of the first geometry dimension}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/odm33-1} 

}

\caption{OD matrix sliced for destination zone 33, by transportation mode}\label{fig:odm33}
\end{figure}

the result of which is shown in figure \ref{fig:odm33}.
Subsetting this way, we take all attributes (there is only one: N)
since the first argument is empty, we take all origin regions (second
argument empty), we take destination zone 33 (third argument),
and all transportation modes (fourth argument empty, or missing).

We plotted this particular zone because it has the largest number of travelers
as its destination. We can find this out by summing all origins and
travel modes by destination:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d =}\StringTok{ }\KeywordTok{st_apply}\NormalTok{(odm, }\DecValTok{2}\NormalTok{, sum)}
\KeywordTok{which.max}\NormalTok{(d[[}\DecValTok{1}\NormalTok{]])}
\CommentTok{# [1] 33}
\end{Highlighting}
\end{Shaded}

Other aggregations we can carry out include: total transportation
by OD (102 x 102):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{st_apply}\NormalTok{(odm, }\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{, sum)}
\CommentTok{# stars object with 2 dimensions and 1 attribute}
\CommentTok{# attribute(s):}
\CommentTok{#      Min. 1st Qu. Median Mean 3rd Qu. Max.}
\CommentTok{# sum     0       0      0 19.2      19 1434}
\CommentTok{# dimension(s):}
\CommentTok{#   from  to offset delta refsys point}
\CommentTok{# o    1 102     NA    NA WGS 84 FALSE}
\CommentTok{# d    1 102     NA    NA WGS 84 FALSE}
\CommentTok{#                                                              values}
\CommentTok{# o MULTIPOLYGON (((-2.51 51.4,...,...,MULTIPOLYGON (((-2.55 51.5,...}
\CommentTok{# d MULTIPOLYGON (((-2.51 51.4,...,...,MULTIPOLYGON (((-2.55 51.5,...}
\end{Highlighting}
\end{Shaded}

Origin totals, by mode:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{st_apply}\NormalTok{(odm, }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{), sum)}
\CommentTok{# stars object with 2 dimensions and 1 attribute}
\CommentTok{# attribute(s):}
\CommentTok{#      Min. 1st Qu. Median Mean 3rd Qu. Max.}
\CommentTok{# sum     1    57.5    214  490     771 2903}
\CommentTok{# dimension(s):}
\CommentTok{#      from  to offset delta refsys point}
\CommentTok{# o       1 102     NA    NA WGS 84 FALSE}
\CommentTok{# mode    1   4     NA    NA     NA FALSE}
\CommentTok{#                                                                 values}
\CommentTok{# o    MULTIPOLYGON (((-2.51 51.4,...,...,MULTIPOLYGON (((-2.55 51.5,...}
\CommentTok{# mode                                                 bicycle,...,train}
\end{Highlighting}
\end{Shaded}

Destination totals, by mode:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{st_apply}\NormalTok{(odm, }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{), sum)}
\CommentTok{# stars object with 2 dimensions and 1 attribute}
\CommentTok{# attribute(s):}
\CommentTok{#      Min. 1st Qu. Median Mean 3rd Qu.  Max.}
\CommentTok{# sum     0      13    104  490     408 12948}
\CommentTok{# dimension(s):}
\CommentTok{#      from  to offset delta refsys point}
\CommentTok{# d       1 102     NA    NA WGS 84 FALSE}
\CommentTok{# mode    1   4     NA    NA     NA FALSE}
\CommentTok{#                                                                 values}
\CommentTok{# d    MULTIPOLYGON (((-2.51 51.4,...,...,MULTIPOLYGON (((-2.55 51.5,...}
\CommentTok{# mode                                                 bicycle,...,train}
\end{Highlighting}
\end{Shaded}

Origin totals, summed over modes:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{o =}\StringTok{ }\KeywordTok{st_apply}\NormalTok{(odm, }\DecValTok{1}\NormalTok{, sum)}
\end{Highlighting}
\end{Shaded}

Destination totals, summed over modes (we had this):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d =}\StringTok{ }\KeywordTok{st_apply}\NormalTok{(odm, }\DecValTok{2}\NormalTok{, sum)}
\end{Highlighting}
\end{Shaded}

We plot \texttt{o} and \texttt{d} together after joining them by

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x =}\StringTok{ }\NormalTok{(}\KeywordTok{c}\NormalTok{(o, d, }\DataTypeTok{along =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{od =} \KeywordTok{c}\NormalTok{(}\StringTok{"origin"}\NormalTok{, }\StringTok{"destination"}\NormalTok{))))}
\KeywordTok{plot}\NormalTok{(x, }\DataTypeTok{logz =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/odjoined-1} 

}

\caption{total commutes, summed by origin (left) or destination (right).}\label{fig:odjoined}
\end{figure}

the result of which is shown in figure \ref{fig:odjoined}.

There is something to say for the argument that such maps
give the wrong message, as both amount (color) and polygon
size give an impression of amount. To take out the amount
in the count, we can compute densities (count / km\(^2\)), by

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(units)}
\CommentTok{# udunits database from /usr/share/xml/udunits/udunits2.xml}
\NormalTok{a =}\StringTok{ }\KeywordTok{set_units}\NormalTok{(}\KeywordTok{st_area}\NormalTok{(}\KeywordTok{st_as_sf}\NormalTok{(o)), km}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{o}\OperatorTok{$}\NormalTok{sum_km =}\StringTok{ }\NormalTok{o}\OperatorTok{$}\NormalTok{sum }\OperatorTok{/}\StringTok{ }\NormalTok{a}
\NormalTok{d}\OperatorTok{$}\NormalTok{sum_km =}\StringTok{ }\NormalTok{d}\OperatorTok{$}\NormalTok{sum }\OperatorTok{/}\StringTok{ }\NormalTok{a}
\NormalTok{od =}\StringTok{ }\KeywordTok{c}\NormalTok{(o[}\StringTok{"sum_km"}\NormalTok{], d[}\StringTok{"sum_km"}\NormalTok{], }\DataTypeTok{along =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{od =} \KeywordTok{c}\NormalTok{(}\StringTok{"origin"}\NormalTok{, }\StringTok{"destination"}\NormalTok{)))}
\KeywordTok{plot}\NormalTok{(od, }\DataTypeTok{logz =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/odbykm-1} 

}

\caption{total commutes per square km, by area of origin (left) or destination (right)}\label{fig:odbykm}
\end{figure}

shown in figure \ref{fig:odbykm}. Another way to normalize these
totals would be to divide them by population size.

\hypertarget{tidy-array-data}{%
\subsection{Tidy array data}\label{tidy-array-data}}

The \emph{tidy data} paper \citep{tidy} may suggest that such array data
should be processed not as an array, but in a long table where
each row holds (region, class, year, value), and it is always good
to be able to do this. For primary handling and storage however,
this is often not an option, because

\begin{itemize}
\tightlist
\item
  a lot of array data are collected or generated as array data, e.g.
  by imagery or other sensory devices, or e.g.~by climate models
\item
  it is easier to derive the long table form from the array than
  vice versa
\item
  the long table form requires much more memory, since the space
  occupied by dimension values is \(O(nmp)\), rather than \(O(n+m+p)\)
\item
  when missing-valued cells are dropped, the long table form loses
  the implicit indexing of the array form
\end{itemize}

To put this argument to the extreme, consider for instance that
all image, video and sound data are stored in array form; few
people would make a real case for storing them in a long table
form instead. Nevertheless, R packages like \texttt{tsibble} take this
approach, and have to deal with ambiguous ordering of multiple records
with identical time steps for different spatial features and index
them, which is solved for both \emph{automatically} by using the array
form -- at the cost of using dense arrays, in package \texttt{stars}.

Package \texttt{stars} tries to follow the \href{https://cran.r-project.org/web/packages/tidyverse/vignettes/manifesto.html}{tidy
manifesto}
to handle array sets, and has particularly developed support for the
case where one or more of the dimensions refer to space, and/or time.

\hypertarget{raster-to-vector}{%
\section{raster-to-vector, vector-to-raster}\label{raster-to-vector}}

Section \ref{rasterize} already showed some examples of
raster-to-vector and vector-to-raster conversions, this section
will add some code details and examples.

\hypertarget{vector-to-raster}{%
\subsection{vector-to-raster}\label{vector-to-raster}}

\texttt{st\_as\_stars} is meant as a method to transform objects into \texttt{stars}
objects. However, not all stars objects are \texttt{raster} objects,
and the method for \texttt{sf} objects creates a vector data cube with
the geometry as its spatial (vector) dimension, and attributes
as attributes. When given a feature \emph{geometry} (\texttt{sfc}) object,
\texttt{st\_as\_stars} will rasterize it, as shown in section \ref{warp},
and in figure \ref{fig:asstars}.



\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\DataTypeTok{file =} \KeywordTok{system.file}\NormalTok{(}\StringTok{"gpkg/nc.gpkg"}\NormalTok{, }\DataTypeTok{package=}\StringTok{"sf"}\NormalTok{))}
\CommentTok{# [1] "/home/edzer/R/x86_64-pc-linux-gnu-library/4.0/sf/gpkg/nc.gpkg"}
\KeywordTok{read_sf}\NormalTok{(file) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{st_geometry}\NormalTok{() }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{st_as_stars}\NormalTok{() }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{plot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/asstars-1} 

}

\caption{rasterizing vector geometry using \texttt{st\_as\_stars}}\label{fig:asstars}
\end{figure}

Here, \texttt{st\_as\_stars} can be parameterized to control cell size,
number of cells, and/or extent. The cell values returned are 0 for
cells with center point outside the geometry and 1 for cell with
center point inside or on the border of the geometry. Rasterizing
existing features is done using \texttt{st\_rasterize}, as also shown in
figure \ref{fig:vectoras}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}
\KeywordTok{read_sf}\NormalTok{(file) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{name =} \KeywordTok{as.factor}\NormalTok{(NAME)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{select}\NormalTok{(SID74, SID79, name) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{st_rasterize}\NormalTok{()}
\CommentTok{# stars object with 2 dimensions and 3 attributes}
\CommentTok{# attribute(s):}
\CommentTok{#      SID74           SID79            name       }
\CommentTok{#  Min.   : 0      Min.   : 0      Sampson :  655  }
\CommentTok{#  1st Qu.: 3      1st Qu.: 3      Columbus:  648  }
\CommentTok{#  Median : 5      Median : 6      Robeson :  648  }
\CommentTok{#  Mean   : 8      Mean   :10      Bladen  :  604  }
\CommentTok{#  3rd Qu.:10      3rd Qu.:13      Wake    :  590  }
\CommentTok{#  Max.   :44      Max.   :57      (Other) :30952  }
\CommentTok{#  NA's   :30904   NA's   :30904   NA's    :30904  }
\CommentTok{# dimension(s):}
\CommentTok{#   from  to   offset      delta refsys point values x/y}
\CommentTok{# x    1 461 -84.3239  0.0192484  NAD27 FALSE   NULL [x]}
\CommentTok{# y    1 141  36.5896 -0.0192484  NAD27 FALSE   NULL [y]}
\end{Highlighting}
\end{Shaded}

Similarly, line and point geometries can be rasterized, as shown
in figure \ref{fig:lineras}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{read_sf}\NormalTok{(file) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{st_cast}\NormalTok{(}\StringTok{"MULTILINESTRING"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{select}\NormalTok{(CNTY_ID) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{st_rasterize}\NormalTok{() }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{plot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/lineras-1} 

}

\caption{rasterization of North Carolina boundaries}\label{fig:lineras}
\end{figure}

\hypertarget{projsf}{%
\section{Coordinate transformations and conversions}\label{projsf}}

\hypertarget{st_crs}{%
\subsection{\texorpdfstring{\texttt{st\_crs}}{st\_crs}}\label{st_crs}}

Spatial objects of class \texttt{sf} or \texttt{stars} contain a coordinate
reference system that can be get or replaced with \texttt{st\_crs}, or be
set or replaced in a pipe with \texttt{st\_set\_crs}. Coordinate reference
systems can be set with an EPSG code, like \texttt{st\_crs(4326)} which
will be converted to \texttt{st\_crs(\textquotesingle{}EPSG:4326\textquotesingle{})}, or with a PROJ.4 string
like \texttt{"+proj=utm\ +zone=25\ +south"}, a name like ``WGS84'', or a name
preceded by an authority like ``OGC:CRS84''; alternatives include
a coordinate reference system definition in WKT, WKT-2 (section
\ref{wkt2}) or \href{https://proj.org/specifications/projjson.html}{PROJJSON}.

The object returned contains two fields:

\begin{itemize}
\tightlist
\item
  \texttt{wkt} with the WKT-2 representation
\item
  \texttt{input} with the user input, if any, or a human readable
  description of the coordinate reference system, if available
\end{itemize}

Note that PROJ.4 strings can be used to
\emph{define} some coordinate reference systems, but they cannot be used
to \emph{represent} coordinate reference systems. Conversion of a
WKT-2 in a \texttt{crs} object to a proj4string using the \texttt{\$proj4string}
method, as in

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x =}\StringTok{ }\KeywordTok{st_crs}\NormalTok{(}\StringTok{"OGC:CRS84"}\NormalTok{)}
\NormalTok{x}\OperatorTok{$}\NormalTok{proj4string}
\CommentTok{# [1] "+proj=longlat +datum=WGS84 +no_defs"}
\end{Highlighting}
\end{Shaded}

may succeed but is not in general lossless or invertible. Using
PROJ.4 strings, for instance to \emph{define} a parameterized, projected
coordinate reference system is fine as long as it is associated
with the WGS84 datum.

\hypertarget{st_transform-sf_project}{%
\subsection{\texorpdfstring{\texttt{st\_transform}, \texttt{sf\_project}}{st\_transform, sf\_project}}\label{st_transform-sf_project}}

Coordinate transformations or conversions (section \ref{projlib})
for \texttt{sf} or \texttt{stars} objects are carried out with \texttt{st\_transform},
which takes as its first argument a spatial object of class \texttt{sf} or
\texttt{stars} that has a coordinate reference system set, and as a second
argument with an \texttt{crs} object (or something that can be converted
to it with \texttt{st\_crs}). When PROJ finds more than one possibility
to transform or convert from the source \texttt{crs} to the target \texttt{crs},
it chooses the one with the highest declared accuracy. More fine-grained
control over the options is explained in section \ref{pipelines}.

A lower-level function to transform or convert coordinates \emph{not}
in \texttt{sf} or \texttt{stars} objects is \texttt{sf\_project}: it takes a matrix with
coordinates and a source and target \texttt{crs}, and returns transformed
or converted coordinates.

\hypertarget{sf_proj_info}{%
\subsection{\texorpdfstring{\texttt{sf\_proj\_info}}{sf\_proj\_info}}\label{sf_proj_info}}

Function \texttt{sf\_proj\_info} can be used to query available projections,
ellipsoids, units and prime meridians available in the PROJ
software. It takes a single parameter, \texttt{type}, which can have the
following values:

\begin{itemize}
\tightlist
\item
  \texttt{type\ =\ "proj"} lists the short and long names of available
  projections; short names can be used in a ``+proj=name'' string.
\item
  \texttt{type\ =\ "ellps"} lists available ellipses, with name, long name,
  and ellipsoidal parameters.
\item
  \texttt{type\ =\ "units"} lists the available length units, with conversion
  constant to meters
\item
  \texttt{type\ =\ "prime\_meridians"} lists the prime meridians with their
  position with respect to the Greenwich meridian.
\end{itemize}

\hypertarget{proj.db-datum-grids-cdn.proj.org-local-cache}{%
\subsection{proj.db, datum grids, cdn.proj.org, local cache}\label{proj.db-datum-grids-cdn.proj.org-local-cache}}

Datum grids (section \ref{projlib}) can be installed locally, or
be read from the PROJ datum grid CDN at \url{https://cdn.proj.org/}. If
installed locally, they are read from the PROJ search path, which
is shown by

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sf_proj_search_paths}\NormalTok{()}
\CommentTok{# [1] "/home/edzer/.local/share/proj"}
\CommentTok{# [2] "/usr/share/proj"}
\end{Highlighting}
\end{Shaded}

The main PROJ database is \texttt{proj.db}, an sqlite3 database typically
found at

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{paste0}\NormalTok{(}\KeywordTok{tail}\NormalTok{(}\KeywordTok{sf_proj_search_paths}\NormalTok{(), }\DecValTok{1}\NormalTok{), .Platform}\OperatorTok{$}\NormalTok{file.sep, }\StringTok{"proj.db"}\NormalTok{)}
\CommentTok{# [1] "/usr/share/proj/proj.db"}
\end{Highlighting}
\end{Shaded}

which can be read. The version of the snapshot of the EPSG database
included in each PROJ release is stated in the \texttt{"metadata"} table of
\texttt{proj.db}; the version of the PROJ runtime used by \textbf{sf} is shown by

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sf_extSoftVersion}\NormalTok{()[}\StringTok{"PROJ"}\NormalTok{]}
\CommentTok{#    PROJ }
\CommentTok{# "7.2.1"}
\end{Highlighting}
\end{Shaded}

If for a particular coordinate transformation datum grids are not
locally found, PROJ will search for online datum grids in the PROJ
CDN when

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sf_proj_network}\NormalTok{()}
\CommentTok{# [1] FALSE}
\end{Highlighting}
\end{Shaded}

returns \texttt{TRUE}. By default it is set to \texttt{FALSE}, but

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sf_proj_network}\NormalTok{(}\OtherTok{TRUE}\NormalTok{)}
\CommentTok{# [1] "https://cdn.proj.org"}
\end{Highlighting}
\end{Shaded}

sets it to \texttt{TRUE} and returns the URL of the network resource used;
this resource can also be set to another resource, that may be
faster or less limited.

After querying a datum grid on the CDN, PROJ writes the \emph{portion} of
the grid queried (not, by default, the entire grid) to a local cache,
which is another sqlite3 database found locally in a user directory,
e.g.~at

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{list.files}\NormalTok{(}\KeywordTok{sf_proj_search_paths}\NormalTok{()[}\DecValTok{1}\NormalTok{], }\DataTypeTok{full.names =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{# [1] "/home/edzer/.local/share/proj/cache.db"}
\end{Highlighting}
\end{Shaded}

that will be searched first in subsequent datum grid queries.

\hypertarget{pipelines}{%
\subsection{transformation pipelines}\label{pipelines}}

Internally, PROJ uses a so-called \emph{coordinate operation pipeline},
to represent the sequence of operations to get from a source CRS to
a target CRS. Given multiple options to go from source to target,
\texttt{st\_transform} chooses the one with highest accuracy. We can query
the options available by

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\DataTypeTok{p =} \KeywordTok{sf_proj_pipelines}\NormalTok{(}\StringTok{"EPSG:4326"}\NormalTok{, }\StringTok{"EPSG:22525"}\NormalTok{))}
\CommentTok{# Candidate coordinate operations found:  5 }
\CommentTok{# Strict containment:     FALSE }
\CommentTok{# Axis order auth compl:  FALSE }
\CommentTok{# Source:  EPSG:4326 }
\CommentTok{# Target:  EPSG:22525 }
\CommentTok{# Best instantiable operation has accuracy: 2 m}
\CommentTok{# Description: axis order change (2D) + Inverse of Corrego Alegre}
\CommentTok{#              1970-72 to WGS 84 (2) + UTM zone 25S}
\CommentTok{# Definition:  +proj=pipeline +step +proj=unitconvert +xy_in=deg}
\CommentTok{#              +xy_out=rad +step +inv}
\CommentTok{#              +proj=hgridshift}
\CommentTok{#              +grids=br_ibge_CA7072_003.tif +step}
\CommentTok{#              +proj=utm +zone=25 +south +ellps=intl}
\end{Highlighting}
\end{Shaded}

and see that pipeline with the highest accuracy is summarised; we
can see that it specifies use of a datum grid. Had we not switched
on the network search, we would have obtained a different result:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sf_proj_network}\NormalTok{(}\OtherTok{FALSE}\NormalTok{)}
\CommentTok{# character(0)}
\KeywordTok{sf_proj_pipelines}\NormalTok{(}\StringTok{"EPSG:4326"}\NormalTok{, }\StringTok{"EPSG:22525"}\NormalTok{)}
\CommentTok{# Candidate coordinate operations found:  5 }
\CommentTok{# Strict containment:     FALSE }
\CommentTok{# Axis order auth compl:  FALSE }
\CommentTok{# Source:  EPSG:4326 }
\CommentTok{# Target:  EPSG:22525 }
\CommentTok{# Best instantiable operation has accuracy: 5 m}
\CommentTok{# Description: axis order change (2D) + Inverse of Corrego Alegre}
\CommentTok{#              1970-72 to WGS 84 (4) + UTM zone 25S}
\CommentTok{# Definition:  +proj=pipeline +step +proj=unitconvert +xy_in=deg}
\CommentTok{#              +xy_out=rad +step +proj=push +v_3}
\CommentTok{#              +step +proj=cart +ellps=WGS84 +step}
\CommentTok{#              +proj=helmert +x=206.05 +y=-168.28}
\CommentTok{#              +z=3.82 +step +inv +proj=cart}
\CommentTok{#              +ellps=intl +step +proj=pop +v_3 +step}
\CommentTok{#              +proj=utm +zone=25 +south +ellps=intl}
\CommentTok{# Operation 4 is lacking 1 grid with accuracy 2 m}
\CommentTok{# Missing grid: br_ibge_CA7072_003.tif }
\CommentTok{# URL: https://cdn.proj.org/br_ibge_CA7072_003.tif}
\end{Highlighting}
\end{Shaded}

and a report that a datum grid is missing.
The object returned by \texttt{sf\_proj\_pipelines} is a sub-classed \texttt{data.frame}, with columns

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(p)}
\CommentTok{# [1] "id"           "description"  "definition"   "has_inverse" }
\CommentTok{# [5] "accuracy"     "axis_order"   "grid_count"   "instantiable"}
\CommentTok{# [9] "containment"}
\end{Highlighting}
\end{Shaded}

and we can list for instance the accuracies by

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p}\OperatorTok{$}\NormalTok{accuracy}
\CommentTok{# [1]  5  5  8  2 NA}
\end{Highlighting}
\end{Shaded}

Here, \texttt{NA} refers to ``ballpark accuracy'', which may be anything in the
30-120 m range:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p[}\KeywordTok{is.na}\NormalTok{(p}\OperatorTok{$}\NormalTok{accuracy),]}
\CommentTok{# Candidate coordinate operations found:  1 }
\CommentTok{# Strict containment:     FALSE }
\CommentTok{# Axis order auth compl:  FALSE }
\CommentTok{# Source:  EPSG:4326 }
\CommentTok{# Target:  EPSG:22525 }
\CommentTok{# Best instantiable operation has only ballpark accuracy }
\CommentTok{# Description: axis order change (2D) + Ballpark geographic offset}
\CommentTok{#              from WGS 84 to Corrego Alegre 1970-72}
\CommentTok{#              + UTM zone 25S}
\CommentTok{# Definition:  +proj=pipeline +step +proj=unitconvert +xy_in=deg}
\CommentTok{#              +xy_out=rad +step +proj=utm +zone=25}
\CommentTok{#              +south +ellps=intl}
\end{Highlighting}
\end{Shaded}

The default, most accurate pipeline chosen by \texttt{st\_transform} can
be overriden by specifying \texttt{pipeline} argument, as selected from
the set of options in \texttt{p\$definition}.

\hypertarget{axisorder}{%
\subsection{Axis order}\label{axisorder}}

As mentioned in section \ref{wkt2}, the definition of \texttt{EPSG:4326},

\begin{verbatim}
# GEOGCRS["WGS 84",
#     DATUM["World Geodetic System 1984",
#         ELLIPSOID["WGS 84",6378137,298.257223563,
#             LENGTHUNIT["metre",1]]],
#     PRIMEM["Greenwich",0,
#         ANGLEUNIT["degree",0.0174532925199433]],
#     CS[ellipsoidal,2],
#         AXIS["geodetic latitude (Lat)",north,
#             ORDER[1],
#             ANGLEUNIT["degree",0.0174532925199433]],
#         AXIS["geodetic longitude (Lon)",east,
#             ORDER[2],
#             ANGLEUNIT["degree",0.0174532925199433]],
#     USAGE[
#         SCOPE["Horizontal component of 3D system."],
#         AREA["World."],
#         BBOX[-90,-180,90,180]],
#     ID["EPSG",4326]]
\end{verbatim}

indicates that the first axis is associated with latitude and the
second with longitude; this is also the case for a number of other
ellipsoidal coordinate reference systems. Although this is how
the authority (EPSG) prescribes this, it is not how most datasets
are currently stored! As most other software, package \texttt{sf} by default
ignores this, and interprets ellipsoidal coordinates as (longitude,
latitude) by default. If however data needs to be read e.g.~from a
WFS service that wants to be compliant to the authority, one can set

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{st_axis_order}\NormalTok{(}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

to globally instruct \texttt{sf}, when calling GDAL and PROJ routines, that
authority compliance (latitude, longitude order) is assumed. It is
anticipated that problems may happen in case of authority compliance,
e.g.~with plotting data. At the time of writing this, the \texttt{plot}
method for \texttt{sf} objects respects the axis order flag and will swap
coordinates using the transformation pipeline \texttt{"+proj=pipeline\ +step\ +proj=axisswap\ +order=2,1"} before plotting them, but
e.g.~\texttt{geom\_sf()} in \texttt{ggplot2} has not been modified to do this.
As mentioned earlier, the ambiguity of \texttt{EPSG:4326} is resolved by
replacing it with \texttt{OGC:CRS84}.

\hypertarget{warp}{%
\section{Transforming and warping rasters}\label{warp}}

When using \texttt{st\_transform} on a raster data set, as e.g.~in

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tif =}\StringTok{ }\KeywordTok{system.file}\NormalTok{(}\StringTok{"tif/L7_ETMs.tif"}\NormalTok{, }\DataTypeTok{package =} \StringTok{"stars"}\NormalTok{)}
\KeywordTok{read_stars}\NormalTok{(tif) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{st_transform}\NormalTok{(}\DecValTok{4326}\NormalTok{)}
\CommentTok{# stars object with 3 dimensions and 1 attribute}
\CommentTok{# attribute(s):}
\CommentTok{#              Min. 1st Qu. Median Mean 3rd Qu. Max.}
\CommentTok{# L7_ETMs.tif     1      54     69 68.9      86  255}
\CommentTok{# dimension(s):}
\CommentTok{#      from  to offset delta refsys point}
\CommentTok{# x       1 349     NA    NA WGS 84 FALSE}
\CommentTok{# y       1 352     NA    NA WGS 84 FALSE}
\CommentTok{# band    1   6     NA    NA     NA    NA}
\CommentTok{#                               values x/y}
\CommentTok{# x    [349x352] -34.9165,...,-34.8261 [x]}
\CommentTok{# y     [349x352] -8.0408,...,-7.94995 [y]}
\CommentTok{# band                            NULL    }
\CommentTok{# curvilinear grid}
\end{Highlighting}
\end{Shaded}

we see that a \emph{curvilinear} is created, which means that for every
grid cell the coordinates are computed in the new CRS, which no
longer form a \emph{regular} grid. Plotting such data is extremely slow,
as small polygons are computed for every grid cell and then plotted.
The advantage of this is that no information is lost: grid cell
values remain identical after the projection.

When we start with a raster on a regular grid and want to obtain
a \emph{regular} grid in a new coordinate reference system, we need
to \emph{warp} the grid: we need to recreate a grid at new locations,
and use some rule to assign values to new grid cells. Rules can
involve using the nearest value, or using some form of interpolation.
This operation is not lossless and not invertible.

The best options for warping is to specify the target grid as a
\texttt{stars} object. When only a target CRS is specified, default options
for the target grid are picked that may be completely inappropriate
for the problem at hand. An example workflow that uses only a
target CRS is

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{read_stars}\NormalTok{(tif) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{st_warp}\NormalTok{(}\DataTypeTok{crs =} \KeywordTok{st_crs}\NormalTok{(}\DecValTok{4326}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{st_dimensions}\NormalTok{()}
\CommentTok{#      from  to   offset        delta refsys point values x/y}
\CommentTok{# x       1 350 -34.9166  0.000259243 WGS 84    NA   NULL [x]}
\CommentTok{# y       1 352 -7.94982 -0.000259243 WGS 84    NA   NULL [y]}
\CommentTok{# band    1   6       NA           NA     NA    NA   NULL}
\end{Highlighting}
\end{Shaded}

which creates a pretty close raster, but then the transformation
is also relatively modest. For a workflow that creates a target
raster first, here with exactly the same number of rows and columns
as the original raster one could use:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{r =}\StringTok{ }\KeywordTok{read_stars}\NormalTok{(tif)}
\NormalTok{grd =}\StringTok{ }\KeywordTok{st_bbox}\NormalTok{(r) }\OperatorTok{%>%}
\StringTok{        }\KeywordTok{st_as_sfc}\NormalTok{() }\OperatorTok{%>%}
\StringTok{        }\KeywordTok{st_transform}\NormalTok{(}\DecValTok{4326}\NormalTok{) }\OperatorTok{%>%}
\StringTok{        }\KeywordTok{st_bbox}\NormalTok{() }\OperatorTok{%>%}
\StringTok{        }\KeywordTok{st_as_stars}\NormalTok{(}\DataTypeTok{nx =} \KeywordTok{dim}\NormalTok{(r)[}\StringTok{"x"}\NormalTok{], }\DataTypeTok{ny =} \KeywordTok{dim}\NormalTok{(r)[}\StringTok{"y"}\NormalTok{])}
\KeywordTok{st_warp}\NormalTok{(r, grd)}
\CommentTok{# stars object with 3 dimensions and 1 attribute}
\CommentTok{# attribute(s):}
\CommentTok{#              Min. 1st Qu. Median Mean 3rd Qu. Max. NA's}
\CommentTok{# L7_ETMs.tif     1      54     69 68.9      86  255 6180}
\CommentTok{# dimension(s):}
\CommentTok{#      from  to   offset        delta refsys point values x/y}
\CommentTok{# x       1 349 -34.9166  0.000259666 WGS 84    NA   NULL [x]}
\CommentTok{# y       1 352 -7.94982 -0.000258821 WGS 84    NA   NULL [y]}
\CommentTok{# band    1   6       NA           NA     NA    NA   NULL}
\end{Highlighting}
\end{Shaded}

\hypertarget{exercises-6}{%
\section{Exercises}\label{exercises-6}}

Use R to solve the following exercises.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Find the names of the \texttt{nc} counties that intersect \texttt{LINESTRING(-84\ 35,-78\ 35)}; use \texttt{{[}} for this, and as an alternative use \texttt{st\_join()} for this.
\item
  Repeat this after setting \texttt{sf\_use\_s2(FALSE)}, and \emph{compute} the difference (hint: use \texttt{setdiff()}), and color the counties of the difference using color `\#88000088'.
\item
  Plot the two different lines in a single plot; note that R will plot a straight line always straight in the projection currently used; \texttt{st\_segmentize} can be used to add points on straight line, or on a great circle for ellipsoidal coordinates.
\item
  NDVI, normalized differenced vegetation index, is computed as \texttt{(NIR-R)/(NIR+R)}, with NIR the near infrared and R the red band. Read the \texttt{L7\_ETMs.tif} file into object \texttt{x}, and distribute the band dimensions over attributes by \texttt{split(x,\ "band")}. Then, add attribute NDVI to this object by using an expression that uses the NIR (band 4) and R (band 3) attributes directly.
\item
  Compute NDVI for the \texttt{L7\_ETMs.tif} image by reducing the band dimension, using \texttt{st\_apply} and an a function \texttt{ndvi\ =\ function(x)\ \{\ (x{[}4{]}-x{[}3{]})/(x{[}4{]}+x{[}3{]})\ \}}. Plot the result, and write the result to a GeoTIFF.
\item
  Use \texttt{st\_transform} to transform the \texttt{stars} object read from \texttt{L7\_ETMs.tif} to \texttt{EPSG:4326}. Print the object. Is this a regular grid? Plot the first band using arguments \texttt{axes=TRUE} and \texttt{border=NA}, and explain why this takes such a long time.
\item
  Use \texttt{st\_warp} to warp the \texttt{L7\_ETMs.tif} object to \texttt{EPSG:4326}, and plot the resulting object with \texttt{axes=TRUE}. Why is the plot created much faster than after \texttt{st\_transform}?
\item
  Using a vector representation of the raster \texttt{L7\_ETMs}, plot the
  intersection with a circular area around \texttt{POINT(293716\ 9113692)} with
  radius 75 m, and compute the area-weighted mean pixel values for
  this circle. Compare the area-weighted values with those obtained
  by \texttt{aggregate} using the vector data, and by \texttt{aggregate} using
  the raster data, using \texttt{exact=FALSE} (default) and \texttt{exact=FALSE}.
  Explain the differences.
\end{enumerate}

\hypertarget{large}{%
\chapter{Large data sets}\label{large}}

This chapter describes how large spatial and spatiotemporal datasets
can be handled with R, with a focus on packages \texttt{sf} and \texttt{stars}.
For practical use, we classify large data sets as either

\begin{itemize}
\tightlist
\item
  too large to fit in working memory, or
\item
  also too large to fit on the local hard drive, or
\item
  also too large to download it to locally managed compute
  infrastructure (such as network attached storage)
\end{itemize}

these three categories correspond very roughly to Gigabyte-,
Terabyte- and Petabyte-sized data sets.

\hypertarget{largesf}{%
\section{\texorpdfstring{Vector data: \texttt{sf}}{Vector data: sf}}\label{largesf}}

\hypertarget{reading-from-disk}{%
\subsection{Reading from disk}\label{reading-from-disk}}

Function \texttt{st\_read} reads vector data from disk, using GDAL, and
then keeps the data read in working memory. In case the file is
too large to be read in working memory, several options exist to
read parts of the file. The first is to set argument \texttt{wkt\_filter}
with a WKT text string containing a geometry; only geometries from
the target file that intersect with this geometry will be returned.
An example is

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(sf)}
\NormalTok{(}\DataTypeTok{file =} \KeywordTok{system.file}\NormalTok{(}\StringTok{"gpkg/nc.gpkg"}\NormalTok{, }\DataTypeTok{package=}\StringTok{"sf"}\NormalTok{))}
\CommentTok{# [1] "/home/edzer/R/x86_64-pc-linux-gnu-library/4.0/sf/gpkg/nc.gpkg"}
\NormalTok{bb =}\StringTok{ "POLYGON ((-81.7 36.2, -80.4 36.2, -80.4 36.5, -81.7 36.5, -81.7 36.2))"}
\NormalTok{nc}\FloatTok{.1}\NormalTok{ =}\StringTok{ }\KeywordTok{st_read}\NormalTok{(file, }\DataTypeTok{wkt_filter =}\NormalTok{ bb)}
\CommentTok{# Reading layer `nc.gpkg' from data source }
\CommentTok{#   `/home/edzer/R/x86_64-pc-linux-gnu-library/4.0/sf/gpkg/nc.gpkg' }
\CommentTok{#   using driver `GPKG'}
\CommentTok{# Simple feature collection with 8 features and 14 fields}
\CommentTok{# Geometry type: MULTIPOLYGON}
\CommentTok{# Dimension:     XY}
\CommentTok{# Bounding box:  xmin: -81.9 ymin: 36 xmax: -80 ymax: 36.6}
\CommentTok{# Geodetic CRS:  NAD27}
\end{Highlighting}
\end{Shaded}

The second option is to use the \texttt{query} argument to \texttt{st\_read},
which can be any query in ``OGR SQL'' dialect, which can be used to
select features from a layer, and limit fields. An example is:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{q =}\StringTok{ }\KeywordTok{paste}\NormalTok{(}\StringTok{"select BIR74,SID74,geom from 'nc.gpkg' where BIR74 > 1500"}\NormalTok{)}
\NormalTok{nc}\FloatTok{.2}\NormalTok{ =}\StringTok{ }\KeywordTok{st_read}\NormalTok{(file, }\DataTypeTok{query =}\NormalTok{ q)}
\CommentTok{# Reading query `select BIR74,SID74,geom from 'nc.gpkg' where BIR74 > 1500' from data source `/home/edzer/R/x86_64-pc-linux-gnu-library/4.0/sf/gpkg/nc.gpkg' }
\CommentTok{#   using driver `GPKG'}
\CommentTok{# Simple feature collection with 61 features and 2 fields}
\CommentTok{# Geometry type: MULTIPOLYGON}
\CommentTok{# Dimension:     XY}
\CommentTok{# Bounding box:  xmin: -83.3 ymin: 33.9 xmax: -76.1 ymax: 36.6}
\CommentTok{# Geodetic CRS:  NAD27}
\end{Highlighting}
\end{Shaded}

note that \texttt{nc.gpkg} is the \emph{layer name}, which can be obtained from \texttt{file} using \texttt{st\_layers}.
Sequences of records can be read using \texttt{LIMIT} and \texttt{OFFSET}, to read records 51-60 use

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{q =}\StringTok{ }\KeywordTok{paste}\NormalTok{(}\StringTok{"select BIR74,SID74,geom from 'nc.gpkg' LIMIT 10 OFFSET 50"}\NormalTok{)}
\NormalTok{nc}\FloatTok{.2}\NormalTok{ =}\StringTok{ }\KeywordTok{st_read}\NormalTok{(file, }\DataTypeTok{query =}\NormalTok{ q)}
\CommentTok{# Reading query `select BIR74,SID74,geom from 'nc.gpkg' LIMIT 10 OFFSET 50' from data source `/home/edzer/R/x86_64-pc-linux-gnu-library/4.0/sf/gpkg/nc.gpkg' }
\CommentTok{#   using driver `GPKG'}
\CommentTok{# Simple feature collection with 10 features and 2 fields}
\CommentTok{# Geometry type: MULTIPOLYGON}
\CommentTok{# Dimension:     XY}
\CommentTok{# Bounding box:  xmin: -84 ymin: 35.2 xmax: -75.5 ymax: 36.2}
\CommentTok{# Geodetic CRS:  NAD27}
\end{Highlighting}
\end{Shaded}

Further query options include selection on geometry type, polygon
area. When the dataset queried is a spatial database, then the query
is passed on to the database and not interpreted by GDAL; this means
that more powerful features will be available. Further information
is found in the GDAL documentation under ``OGR SQL dialect''.

Very large files or directories that are zipped can be read
without the need to unzip them, using the \texttt{/vsizip} (for zip),
\texttt{/vsigzip} (for gzip) or \texttt{/vsitar} (for tar files) prefix to files;
this is followed by the path to the zip file, and then followed by
the file inside this zip file. Reading files this way may come at
some computatational cost.

\hypertarget{reading-from-databases-dbplyr}{%
\subsection{Reading from databases, dbplyr}\label{reading-from-databases-dbplyr}}

Although GDAL has support for several spatial databases, and as
mentioned above it passes on SQL in the \texttt{query} argument to the
database, it is sometimes beneficial to directly read from and
write to a spatial database using the R database drivers for this. An
example of this is:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pg <-}\StringTok{ }\NormalTok{DBI}\OperatorTok{::}\KeywordTok{dbConnect}\NormalTok{(}
\NormalTok{    RPostgres}\OperatorTok{::}\KeywordTok{Postgres}\NormalTok{(),}
    \DataTypeTok{host =} \StringTok{"localhost"}\NormalTok{,}
    \DataTypeTok{dbname =} \StringTok{"postgis"}\NormalTok{)}
\KeywordTok{st_read}\NormalTok{(pg, }\DataTypeTok{query =} \StringTok{"select BIR74,wkb_geometry from nc limit 3"}\NormalTok{)}
\CommentTok{# Simple feature collection with 3 features and 1 field}
\CommentTok{# Geometry type: MULTIPOLYGON}
\CommentTok{# Dimension:     XY}
\CommentTok{# Bounding box:  xmin: -81.7 ymin: 36.2 xmax: -80.4 ymax: 36.6}
\CommentTok{# Geodetic CRS:  NAD27}
\CommentTok{#   bir74                   wkb_geometry}
\CommentTok{# 1  1091 MULTIPOLYGON (((-81.5 36.2,...}
\CommentTok{# 2   487 MULTIPOLYGON (((-81.2 36.4,...}
\CommentTok{# 3  3188 MULTIPOLYGON (((-80.5 36.2,...}
\end{Highlighting}
\end{Shaded}

A spatial query might look like

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{q =}\StringTok{ "SELECT BIR74,wkb_geometry FROM nc WHERE \textbackslash{}}
\StringTok{  ST_Intersects(wkb_geometry, 'SRID=4267;POINT (-81.49826 36.4314)');"}
\KeywordTok{st_read}\NormalTok{(pg, }\DataTypeTok{query =}\NormalTok{ q)}
\CommentTok{# Simple feature collection with 1 feature and 1 field}
\CommentTok{# Geometry type: MULTIPOLYGON}
\CommentTok{# Dimension:     XY}
\CommentTok{# Bounding box:  xmin: -81.7 ymin: 36.2 xmax: -81.2 ymax: 36.6}
\CommentTok{# Geodetic CRS:  NAD27}
\CommentTok{#   bir74                   wkb_geometry}
\CommentTok{# 1  1091 MULTIPOLYGON (((-81.5 36.2,...}
\end{Highlighting}
\end{Shaded}

Here, the intersection is done in the database, and uses the spatial
index typically present.

The same mechanism works when using \texttt{dplyr} with a database backend:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr, }\DataTypeTok{warn.conflicts =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{nc_db =}\StringTok{ }\KeywordTok{tbl}\NormalTok{(pg, }\StringTok{"nc"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Spatial queries can be formulated and are passed on to the database:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nc_db }\OperatorTok{%>%}\StringTok{ }
\StringTok{     }\KeywordTok{filter}\NormalTok{(}\KeywordTok{ST_Intersects}\NormalTok{(wkb_geometry, }\StringTok{'SRID=4267;POINT (-81.49826 36.4314)'}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{     }\KeywordTok{collect}\NormalTok{()}
\CommentTok{# # A tibble: 1 x 16}
\CommentTok{#   ogc_fid  area perimeter cnty_ cnty_id name  fips  fipsno}
\CommentTok{#     <int> <dbl>     <dbl> <dbl>   <dbl> <chr> <chr>  <dbl>}
\CommentTok{# 1       1 0.114      1.44  1825    1825 Ashe  37009  37009}
\CommentTok{# # ... with 8 more variables: cress_id <int>, bir74 <dbl>,}
\CommentTok{# #   sid74 <dbl>, nwbir74 <dbl>, bir79 <dbl>, sid79 <dbl>,}
\CommentTok{# #   nwbir79 <dbl>, wkb_geometry <pq_gmtry>}
\NormalTok{nc_db }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(}\KeywordTok{ST_Area}\NormalTok{(wkb_geometry) }\OperatorTok{>}\StringTok{ }\FloatTok{0.1}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{head}\NormalTok{(}\DecValTok{3}\NormalTok{)}
\CommentTok{# # Source:   lazy query [?? x 16]}
\CommentTok{# # Database: postgres [edzer@localhost:5432/postgis]}
\CommentTok{#   ogc_fid  area perimeter cnty_ cnty_id name        fips  fipsno}
\CommentTok{#     <int> <dbl>     <dbl> <dbl>   <dbl> <chr>       <chr>  <dbl>}
\CommentTok{# 1       1 0.114      1.44  1825    1825 Ashe        37009  37009}
\CommentTok{# 2       3 0.143      1.63  1828    1828 Surry       37171  37171}
\CommentTok{# 3       5 0.153      2.21  1832    1832 Northampton 37131  37131}
\CommentTok{# # ... with 8 more variables: cress_id <int>, bir74 <dbl>,}
\CommentTok{# #   sid74 <dbl>, nwbir74 <dbl>, bir79 <dbl>, sid79 <dbl>,}
\CommentTok{# #   nwbir79 <dbl>, wkb_geometry <pq_gmtry>}
\end{Highlighting}
\end{Shaded}

It should be noted that PostGIS' \texttt{ST\_Area} computes the same
area as the \texttt{AREA} field in \texttt{nc}, which is the meaningless value
obtained by assuming the coordinates are projected, although they
are ellipsoidal.

\hypertarget{reading-from-online-resources-or-web-services}{%
\subsection{Reading from online resources or web services}\label{reading-from-online-resources-or-web-services}}

GDAL drivers support reading from online recourses, by prepending
\texttt{/vsicurl/} before the URL starting with e.g.~\texttt{https://}. A number of
similar drivers specialized for particular clouds include \texttt{/vsis3}
for Amazon S3, \texttt{/vsigs} for Google Cloud Storage, \texttt{/vsiaz} for
Azure, \texttt{/vsioss} for Alibaba Cloud, or \texttt{/vsiswift} for OpenStack
Swift Object Storage. These prepositions can be combined e.g.~with
\texttt{/vsizip/} to read a zipped online resource. Depending on the
file format used, reading information this way may always involve
reading the entire file, or reading it multiple times, and may not
always be the most efficient way of handling resources. A format
like ``cloud-optimized geotiff'' (COG) has been specially designed
to be efficient and resource-friendly in many cases, e.g.~for only
reading the metadata, or for only reading overviews (low-resolutions
versions of the full imagery) or spatial segments. COGs can also
be created using the GeoTIFF driver of GDAL, and setting the right
dataset creation options in a \texttt{write\_stars} call.

\hypertarget{apis-openstreetmap}{%
\subsection{API's, OpenStreetMap}\label{apis-openstreetmap}}

Although online resource do not have to be stored files but could be
created server-side on the fly, typical web services for geospatial
data create data on the fly, and give access to this through an API.
As an example, data from \href{https://openstreetmap.org/}{OpenStreetMap}
can be bulk downloaded and read locally, e.g.~using the GDAL vector
driver, but more typical a user wants to obtain a small subset of
the data or use the data for a small query. Several R packages exist
that query openstreetmap data:

\begin{itemize}
\tightlist
\item
  Package \texttt{OpenStreetMap} downloads data as raster tiles, typically
  used as backdrop or reference for plotting other features
\item
  Package \texttt{osmdata} downloads vector data as points, lines or polygons
  in \texttt{sf} or \texttt{sp} format
\item
  Package \texttt{osmar} returns vector data, but in addition the network
  topology (as an \texttt{igraph} object) that contains how road elements
  form a network, and has functions that compute the shortest route.
\end{itemize}

When provided with a correctly formulated API call in the URL the
highly configurable GDAL OSM driver (in \texttt{st\_read}) can read an
``.osm'' file (xml) and returns a dataset with five layers: \texttt{points}
that have significant tags, \texttt{lines} with non-area ``way'' features,
\texttt{multilinestrings} with ``relation'' features, \texttt{multipolygons} with
``relation'' features and \texttt{other\_relations}. A simple and very small
bounding box query to OpenStreetMap could look like

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{download.file}\NormalTok{(}
  \StringTok{"https://openstreetmap.org/api/0.6/map?bbox=7.595,51.969,7.598,51.970"}\NormalTok{,}
  \StringTok{"data/ms.osm"}\NormalTok{, }\DataTypeTok{method =} \StringTok{"auto"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

and from this file we can read the layer \texttt{lines}, and plot its
first attribute by

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{o =}\StringTok{ }\KeywordTok{read_sf}\NormalTok{(}\StringTok{"data/ms.osm"}\NormalTok{, }\StringTok{"lines"}\NormalTok{)}
\NormalTok{p =}\StringTok{ }\KeywordTok{read_sf}\NormalTok{(}\StringTok{"data/ms.osm"}\NormalTok{, }\StringTok{"multipolygons"}\NormalTok{)}
\NormalTok{bb =}\StringTok{ }\KeywordTok{st_bbox}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DataTypeTok{xmin=}\FloatTok{7.595}\NormalTok{, }\DataTypeTok{ymin =} \FloatTok{51.969}\NormalTok{, }\DataTypeTok{xmax =} \FloatTok{7.598}\NormalTok{, }\DataTypeTok{ymax =} \FloatTok{51.970}\NormalTok{),}
    \DataTypeTok{crs =} \DecValTok{4326}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{st_as_sfc}\NormalTok{(bb), }\DataTypeTok{axes =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{lwd =} \DecValTok{2}\NormalTok{, }\DataTypeTok{lty =} \DecValTok{2}\NormalTok{, }\DataTypeTok{cex.axis =} \FloatTok{.5}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(o[,}\DecValTok{1}\NormalTok{], }\DataTypeTok{lwd =} \DecValTok{2}\NormalTok{, }\DataTypeTok{add =} \OtherTok{TRUE}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{st_geometry}\NormalTok{(p), }\DataTypeTok{border =} \OtherTok{NA}\NormalTok{, }\DataTypeTok{col =} \StringTok{'#88888888'}\NormalTok{, }\DataTypeTok{add =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/overpass-1} 

}

\caption{OpenStreetMap vector data}\label{fig:overpass}
\end{figure}

the result of which is shown in figure \ref{fig:overpass}.
The overpass API provides a more generic and powerful query
functionality to OpenStreetMap data.

\hypertarget{raster-data-stars}{%
\section{\texorpdfstring{Raster data: \texttt{stars}}{Raster data: stars}}\label{raster-data-stars}}

A common challenge with raster datasets is not only that they come
in large files (single Sentinel-2 tiles are around 1 Gb), but that
many of these files, potentially thousands, are needed to address
the area and time period of interest. At time of writing this, the
Copernicus program that runs all Sentinel satellites publishes 160
Tb of images per day. This means that a classic pattern in using R,
consisting of

\begin{itemize}
\tightlist
\item
  downloading data to local disc,
\item
  loading the data in memory,
\item
  analysing it
\end{itemize}

is not going to work.

Cloud-based Earth Observation processing platforms like Google Earth
Engine \citep{gorelick} or \href{https://www.sentinel-hub.com/}{Sentinel Hub}
recognize this and let users work with datasets up to the petabyte
range rather easily and with a great deal of interactivity. They
share the following properties:

\begin{itemize}
\tightlist
\item
  computations are posponed as long as possible (lazy evaluation),
\item
  only the data you ask for are being computed and returned, and nothing more,
\item
  storing intermediate results is avoided in favour of on-the-fly computations,
\item
  maps with useful results are generated and shown quickly to allow for interactive model development.
\end{itemize}

This is similar to the \texttt{dbplyr} interface to databases and
cloud-based analytics environments, but differs in the aspect of
\emph{what} we want to see quickly: rather than the first \(n\) records
of a \texttt{dbplyr} table, we want a quick \emph{overview} of the results,
in the form of a map covering the whole area, or part of it, but
at screen resolution rather than native (observation) resolution.

If for instance we want to ``see'' results for the United States on
screen with 1000 x 1000 pixels, we only need to compute results
for this many pixels, which corresponds roughly to data
on a grid with 3000 m x 3000 m grid cells. For Sentinel-2
data with 10 m resolution, this means we can subsample with
a factor 300, giving 3 km x 3 km resolution. Processing,
storage and network requirements then drop a factor \(300^2 \approx 10^5\), compared
to working on the native 10 m x 10 m resolution. On the platforms
mentioned, zooming in the map triggers further computations on a
finer resolution and smaller extent.

A simple optimisation that follows these lines is how stars' plot
method works: in the case of plotting large rasters, it subsamples
the array before it plots, drastically saving time. The degree
of subsampling is derived from the plotting region size and the
plotting resolution (pixel density). For vector devices, such as pdf,
R sets plot resolution to 75 dpi, corresponding to 0.3 mm per pixel.
Enlarging plots may reveal this, but replotting to an enlarged
devices will create a plot at target density.

\hypertarget{stars-proxy-objects}{%
\subsection{\texorpdfstring{\texttt{stars} proxy objects}{stars proxy objects}}\label{stars-proxy-objects}}

To handle datasets that are too large to fit in memory, \texttt{stars}
provides \texttt{stars\_proxy} objects. To demonstrate its use, we will
use the \texttt{starsdata} package, an R data package with larger datasets
(around 1 Gb total). It can be installed by

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{options}\NormalTok{(}\DataTypeTok{timeout =} \DecValTok{600}\NormalTok{) }\CommentTok{# or large in case of slow network}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"starsdata"}\NormalTok{, }\DataTypeTok{repos =} \StringTok{"http://pebesma.staff.ifgi.de"}\NormalTok{, }
    \DataTypeTok{type =} \StringTok{"source"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can ``load'' a Sentinel-2 image from it by

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f =}\StringTok{ "sentinel/S2A_MSIL1C_20180220T105051_N0206_R051_T32ULE_20180221T134037.zip"}
\NormalTok{granule =}\StringTok{ }\KeywordTok{system.file}\NormalTok{(}\DataTypeTok{file =}\NormalTok{ f, }\DataTypeTok{package =} \StringTok{"starsdata"}\NormalTok{)}
\KeywordTok{file.size}\NormalTok{(granule)}
\CommentTok{# [1] 7.69e+08}
\NormalTok{base_name =}\StringTok{ }\KeywordTok{strsplit}\NormalTok{(}\KeywordTok{basename}\NormalTok{(granule), }\StringTok{".zip"}\NormalTok{)[[}\DecValTok{1}\NormalTok{]]}
\NormalTok{s2 =}\StringTok{ }\KeywordTok{paste0}\NormalTok{(}\StringTok{"SENTINEL2_L1C:/vsizip/"}\NormalTok{, granule, }\StringTok{"/"}\NormalTok{, base_name, }
    \StringTok{".SAFE/MTD_MSIL1C.xml:10m:EPSG_32632"}\NormalTok{)}
\NormalTok{(}\DataTypeTok{p =} \KeywordTok{read_stars}\NormalTok{(s2, }\DataTypeTok{proxy =} \OtherTok{TRUE}\NormalTok{))}
\CommentTok{# stars_proxy object with 1 attribute in 1 file(s):}
\CommentTok{# $`MTD_MSIL1C.xml:10m:EPSG_32632`}
\CommentTok{# [1] "[...]/MTD_MSIL1C.xml:10m:EPSG_32632"}
\CommentTok{# }
\CommentTok{# dimension(s):}
\CommentTok{#      from    to offset delta            refsys point    values}
\CommentTok{# x       1 10980  3e+05    10 WGS 84 / UTM z...    NA      NULL}
\CommentTok{# y       1 10980  6e+06   -10 WGS 84 / UTM z...    NA      NULL}
\CommentTok{# band    1     4     NA    NA                NA    NA B4,...,B8}
\CommentTok{#      x/y}
\CommentTok{# x    [x]}
\CommentTok{# y    [y]}
\CommentTok{# band}
\KeywordTok{object.size}\NormalTok{(p)}
\CommentTok{# 11160 bytes}
\end{Highlighting}
\end{Shaded}

and we see that this does not actually load \emph{any} of the pixel
values, but keeps the reference to the dataset and fills the
dimensions table. (The convoluted \texttt{s2} name is needed to point
GDAL to the right file inside the \texttt{.zip} file containing 115 files
in total).

The idea of a proxy object is that we can build expressions like

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p2 =}\StringTok{ }\NormalTok{p }\OperatorTok{*}\StringTok{ }\DecValTok{2}
\end{Highlighting}
\end{Shaded}

but that the computations for this are postponed. Only when we
really need the data, e.g.~because we want to plot it, is \texttt{p\ *\ 2} evaluated. We need data when either

\begin{itemize}
\tightlist
\item
  we want to \texttt{plot} data, or
\item
  we want to write an object to disk, with \texttt{write\_stars}, or
\item
  we want to explicitly load an object in memory, with \texttt{st\_as\_stars}
\end{itemize}

In case the entire object does not fit in memory, \texttt{plot} and
\texttt{write\_stars} choose different strategies to deal with this:

\begin{itemize}
\tightlist
\item
  \texttt{plot} fetches only the pixels that can be seen, rather than all
  pixels available, and
\item
  \texttt{write\_stars} reads, processes, and writes data chunk by chunk.
\end{itemize}

Downsampling and chunking is implemented for spatially dense images,
not e.g.~for dense time series, or other dense dimensions.

As an example, the output of \texttt{plot(p)}, shown in figure \ref{fig:plotp}

\begin{verbatim}
# downsample set to c(56)
\end{verbatim}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/plotp-1} 

}

\caption{Plot of downsampled 10 m bands of a Sentinel-2 scene}\label{fig:plotp}
\end{figure}

only fetches the pixels that can be seen on the plot device, rather
than the 10980 x 10980 pixels available in each band. The downsampling
ratio taken is

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\DataTypeTok{ds =} \KeywordTok{floor}\NormalTok{(}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{prod}\NormalTok{(}\KeywordTok{dim}\NormalTok{(p)) }\OperatorTok{/}\StringTok{ }\KeywordTok{prod}\NormalTok{(}\KeywordTok{dev.size}\NormalTok{(}\StringTok{"px"}\NormalTok{)))))}
\CommentTok{# [1] 56}
\end{Highlighting}
\end{Shaded}

meaning that for every 56 \(\times\) 56 sub-image in the
original image, only one pixel is read, and plotted. This value is
still a bit too low as it ignores the white space and space for
the key on the plotting device.

\hypertarget{operations-on-proxy-objects}{%
\subsection{Operations on proxy objects}\label{operations-on-proxy-objects}}

A few dedicated methods are available for \texttt{stars\_proxy} objects:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{methods}\NormalTok{(}\DataTypeTok{class =} \StringTok{"stars_proxy"}\NormalTok{)}
\CommentTok{#  [1] [              [[<-           [<-            adrop         }
\CommentTok{#  [5] aggregate      aperm          as.data.frame  c             }
\CommentTok{#  [9] coerce         dim            droplevels     filter        }
\CommentTok{# [13] hist           initialize     is.na          mapView       }
\CommentTok{# [17] Math           merge          mutate         Ops           }
\CommentTok{# [21] plot           predict        print          pull          }
\CommentTok{# [25] replace_na     select         show           slice         }
\CommentTok{# [29] slotsFromS3    split          st_apply       st_as_sf      }
\CommentTok{# [33] st_as_stars    st_crop        st_mosaic      st_redimension}
\CommentTok{# [37] st_sample      st_set_bbox    transmute      write_stars   }
\CommentTok{# see '?methods' for accessing help and source code}
\end{Highlighting}
\end{Shaded}

We have seen \texttt{plot} and \texttt{print} in action; \texttt{dim} reads out
the dimension from the dimensions metadata table.

The three methods that actually fetch data are \texttt{st\_as\_stars},
\texttt{plot} and \texttt{write\_stars}. \texttt{st\_as\_stars} reads the actual data into a
\texttt{stars} object, its argument \texttt{downsample} controls the downsampling
rate. \texttt{plot} does this too, choosing an appropriate \texttt{downsample}
value from the device resolution, and plots the object. \texttt{write\_stars}
writes a \texttt{star\_proxy} object to disc.

All other methods for \texttt{stars\_proxy} objects do not actually operate
on the raster data but add the operations to a \emph{to do} list,
attached to the object. Only when actual raster data are fetched,
e.g.~by calling \texttt{plot} or \texttt{st\_as\_stars}, the commands in this list
are executed.

\texttt{st\_crop} limits the extent (area) of the raster that will be
read. \texttt{c} combines \texttt{stars\_proxy} objects, but still doesn't read
any data. \texttt{adrop} drops empty dimensions, \texttt{aperm} changes dimension
order.

\texttt{write\_stars} reads and processes its input chunk-wise; it has an
argument \texttt{chunk\_size} that lets users control the size of spatial
chunks.

\hypertarget{very-large-data-cubes}{%
\section{Very large data cubes}\label{very-large-data-cubes}}

At some stage, data sets need to be analysed that are so large that
downloading them is no longer feasible; even when local storage would
be sufficient, network bandwidth may become limiting. Examples are
satellite image archives such as those from Landsat and Copernicus
(Sentinel-x), or model computations such as the ERA5 \citep{era5}, a
model reanalysis of the global atmosphere, land surface and ocean
waves from 1950 onwards. In such cases it may be most helpful to
either gain access to (typically: rent) virtual machines in a cloud
where these data are available and nearby (i.e., the data should be
stored in the same data center as where the virtual machine is),
or to use a system that lets the user carry out such computations
without having to worry about virtual machines. Both options will
be discussed.

\hypertarget{finding-and-processing-assets}{%
\subsection{Finding and processing assets}\label{finding-and-processing-assets}}

When working on a virtual machine on a cloud, a first task is usally
to find the assets (files) to work on. It looks attractive to obtain
a file listing, and then parse file names such as

\begin{verbatim}
S2A_MSIL1C_20180220T105051_N0206_R051_T32ULE_20180221T134037.zip
\end{verbatim}

for their metadata including the date of acquisition and the code
of the spatial tile covered. Obtaining such a file listing however
is usually computationally very demanding, as is the processing of
the result, when the number of tiles runs in the many millions.

A solution to this is to use a catalogue. The recently developed
and increasingly deployed STAC, short for \emph{spatiotemporal asset
catalogue}, provides an API that can be used to query image
collections by properties like bounding box, date, band, and cloud
coverage. The R package \texttt{rstac} \citep{R-rstac} provides an R interface
to create queries, and manage the information returned.

Processing the resulting files may involve creating a data cube at
a lower spatial and/or temporal resolution, from images that may
span a range of coordinate reference systems (e.g., several UTM
zones). An R package that can do that is gdalcubes \citep{R-gdalcubes, appel2019demand}, which can also directly use STAC output
\citep{appelblog}.

\hypertarget{processing-data-gee-openeo}{%
\subsection{Processing data: GEE, openEO}\label{processing-data-gee-openeo}}

Platforms that do not require the management and programming of
virtual machines \emph{in} the cloud but provide direct access to the
imagery managed include GEE, openEO, and the climate data store.

Google Earth Engine (GEE) is a cloud platform that allows users
to compute on large amounts of Earth Observation data as well
as modelling products \citep{gorelick}. It has powerful analysis
capabilities, including most of the data cube operations explained
in section \ref{dcoperations}. It has an IDE where scripts can
be written in JavaScript, and a Python interface to the same
functionality. The code of GEE is not open source, and cannot be
extended by arbitrary user-defined functions in languages like
Python or R. R package \texttt{rgee} \citep{R-rgee} provides an R client
interface to GEE.

Cloud-based data cube processing platforms built entirely around
open source software are emerging, several of which using the openEO
API \citep{openEO}. This API allows for user-defined functions (UDFs)
written in Python or R that are being passed on through the API and
executed at the pixel level, e.g.~to aggregate or reduce dimensions.
UDFs in R represent the data chunk to be processed as a \texttt{stars}
object, in Python \texttt{xarray} objects are used.

Other platforms include the Copernicus climate data store \citep{cds}
or atmosphere data store, which allow processing of atmospheric
or climate data from ECMWF, including ERA5. An R package with an
interface to both data stores is \texttt{ecmwfr} \citep{R-ecmwfr}.

\hypertarget{exercises-7}{%
\section{Exercises}\label{exercises-7}}

Use R to solve the following exercises.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  For the S2 image (above), find out in which order the bands are by
  using \texttt{st\_get\_dimension\_values()}, and try to find out (e.g.~by internet
  search) which spectral bands / colors they correspond to.
\item
  Compute NDVI for the S2 image, using \texttt{st\_apply} and an an appropriate
  \texttt{ndvi} function. Plot the result to screen, and then write the
  result to a GeoTIFF. Explain the difference in runtime between
  plotting and writing.
\item
  Plot an RGB composite of the S2 image, using the \texttt{rgb} argument
  to \texttt{plot()}, and then by using \texttt{st\_rgb()} first.
\item
  Select five random points from the bounding box of \texttt{S2}, and extract
  the band values at these points; convert the object returned to an \texttt{sf}
  object.
\item
  For the 10 km radius circle around \texttt{POINT(390000\ \ 5940000)}, use
  \texttt{aggregate} to compute the mean pixel values of the S2 image
  when downsampling the images with factor 30, and on the original
  resolution. Compute the relative difference between the results.
\item
  Use \texttt{hist} to compute the histogram on the downsampled S2 image.
  Also do this for each of the bands. Use \texttt{ggplot2} to compute a
  single plot with all four histograms.
\item
  Use \texttt{st\_crop} to crop the S2 image to the area covered by the 10 km circle.
  Plot the results. Explore the effect of setting argument \texttt{crop\ =\ FALSE}
\item
  With the downsampled image, compute the logical layer where all four
  bands have pixel values higher than 1000. Use a raster algebra expression
  on the four bands (use \texttt{split} first), or use \texttt{st\_apply} for this.
\end{enumerate}

\hypertarget{plotting}{%
\chapter{Plotting spatial data}\label{plotting}}

Together with timelines, maps belong to the most powerful graphs,
perhaps because we can immediately relate where we are, or have
been, on the space of the plot. Two recent books on visualisation
\citep{Healy, Wilke} contain chapters on visualising geospatial data or
maps. Here, we will not try to preach the do's and don'ts of maps,
but rather point out a number of possibilities how to do things,
point out challenges along the way and ways to mitigate them.

\hypertarget{transform}{%
\section{Every plot is a projection}\label{transform}}

The world is round, but plotting devices are flat. As mentioned
in section \ref{projections}, any time we visualise, in any
way, the world on a flat device, we project: we convert ellipsoidal
coordinates into Cartesian coordinates. This includes the cases
where we think we ``do nothing'' (figure \ref{fig:world}, left),
or where show the world ``as it is'', e.g.~as seen from space (figure
\ref{fig:world}, right).



\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(sf)}
\KeywordTok{library}\NormalTok{(rnaturalearth)}
\NormalTok{w <-}\StringTok{ }\KeywordTok{ne_countries}\NormalTok{(}\DataTypeTok{scale =} \StringTok{"medium"}\NormalTok{, }\DataTypeTok{returnclass =} \StringTok{"sf"}\NormalTok{)}
\KeywordTok{suppressWarnings}\NormalTok{(}\KeywordTok{st_crs}\NormalTok{(w) <-}\StringTok{ }\KeywordTok{st_crs}\NormalTok{(}\DecValTok{4326}\NormalTok{))}
\KeywordTok{layout}\NormalTok{(}\KeywordTok{matrix}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mar =} \KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{st_geometry}\NormalTok{(w))}

\CommentTok{# sphere:}
\KeywordTok{library}\NormalTok{(s2)}
\NormalTok{g =}\StringTok{ }\KeywordTok{as_s2_geography}\NormalTok{(}\OtherTok{TRUE}\NormalTok{) }\CommentTok{# Earth}
\NormalTok{co =}\StringTok{ }\KeywordTok{s2_data_countries}\NormalTok{()}
\NormalTok{oc =}\StringTok{ }\KeywordTok{s2_difference}\NormalTok{(g, }\KeywordTok{s2_union_agg}\NormalTok{(co)) }\CommentTok{# oceans}
\NormalTok{b =}\StringTok{ }\KeywordTok{s2_buffer_cells}\NormalTok{(}\KeywordTok{as_s2_geography}\NormalTok{(}\StringTok{"POINT(-30 -10)"}\NormalTok{), }\DecValTok{9800000}\NormalTok{) }\CommentTok{# visible half}
\NormalTok{i =}\StringTok{ }\KeywordTok{s2_intersection}\NormalTok{(b, oc) }\CommentTok{# visible ocean}
\NormalTok{co =}\StringTok{ }\KeywordTok{s2_intersection}\NormalTok{(b, co)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{st_transform}\NormalTok{(}\KeywordTok{st_as_sfc}\NormalTok{(i), }\StringTok{"+proj=ortho +lat_0=-10 +lon_0=-30"}\NormalTok{), }\DataTypeTok{col =} \StringTok{'lightblue'}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{st_transform}\NormalTok{(}\KeywordTok{st_as_sfc}\NormalTok{(co), }\StringTok{"+proj=ortho +lat_0=-10 +lon_0=-30"}\NormalTok{), }\DataTypeTok{col =} \OtherTok{NA}\NormalTok{, }\DataTypeTok{add =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/world-1} 

}

\caption{Earth country boundaries; left: mapping long/lat linearly to \(x\) and \(y\) (plate carrÃ©e); right: as seen from infinite distance (orthographic)}\label{fig:world}
\end{figure}

The left plot of figure \ref{fig:world} was obtained by

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(sf)}
\KeywordTok{library}\NormalTok{(rnaturalearth)}
\NormalTok{w <-}\StringTok{ }\KeywordTok{ne_countries}\NormalTok{(}\DataTypeTok{scale =} \StringTok{"medium"}\NormalTok{, }\DataTypeTok{returnclass =} \StringTok{"sf"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{st_geometry}\NormalTok{(w))}
\end{Highlighting}
\end{Shaded}

and we see that this is the default projection for data with ellipsoidal coordinates, as indicated by

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{st_is_longlat}\NormalTok{(w)}
\CommentTok{# [1] TRUE}
\end{Highlighting}
\end{Shaded}

The projection taken in figure \ref{fig:world} (left) is the
equirectangular (or equidistant cylindrical) projection, which maps
longitude and latitude linearly to the \(x\) and \(y\) axis, keeping
an aspect ratio of 1. Were we to do this for smaller areas not on
the equator, it makes sense to choose a plot ratio such that one
distance unit E-W equals one distance unit N-S on the center of
the plotted area, and this is the default behaviour of the \texttt{plot()}
method for unprojected \texttt{sf} or \texttt{stars} datasets, as well as the
default for \texttt{ggplot2::geom\_sf()} (section @rer(geomsf)).

We can also carry out this projection before plotting. Say we want to
plot Germany, then after loading the (rough) country outline,
we use \texttt{st\_transform} to project:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{DE =}\StringTok{ }\KeywordTok{st_geometry}\NormalTok{(}\KeywordTok{ne_countries}\NormalTok{(}\DataTypeTok{country =} \StringTok{"germany"}\NormalTok{, }\DataTypeTok{returnclass =} \StringTok{"sf"}\NormalTok{))}
\NormalTok{DE.eqc =}\StringTok{ }\KeywordTok{st_transform}\NormalTok{(DE, }\StringTok{"+proj=eqc +lat_ts=51.14 +lon_0=90w"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The \texttt{eqc} refers to the ``equidistant cylindrical'' projection of PROJ;
the projection parameter here is \texttt{lat\_ts}, the latitude of true
scale (i.e., one length unit N-S equals one length unit E-W),
which was here chosen as the middle of the bounding box latitudes

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(}\KeywordTok{mean}\NormalTok{(}\KeywordTok{st_bbox}\NormalTok{(DE)[}\KeywordTok{c}\NormalTok{(}\StringTok{"ymin"}\NormalTok{, }\StringTok{"ymax"}\NormalTok{)]), }\DataTypeTok{digits =} \DecValTok{4}\NormalTok{)}
\CommentTok{# [1] 51.14}
\end{Highlighting}
\end{Shaded}

When we now plot both maps (figure \ref{fig:eqc}), they look
identical up to the values along the axes: degrees for ellipsoidal
(left), and metres for projected (Cartesian) coordinates.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(DE, }\DataTypeTok{axes =} \OtherTok{TRUE}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(DE.eqc, }\DataTypeTok{axes =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{sds_files/figure-latex/eqc-1} 

}

\caption{Germany in equirectangular projection: with axis units degrees (left) and metres in the equidistant cylindrical projection (right)}\label{fig:eqc}
\end{figure}

\hypertarget{what-is-a-good-projection-for-my-data}{%
\subsection{What is a good projection for my data?}\label{what-is-a-good-projection-for-my-data}}

There is unfortunately no silver bullet here. Projections that
maintain all distances do not exist; only globes do. The most
used projections try to preserve

\begin{itemize}
\tightlist
\item
  areas (equal area),
\item
  directions (conformal, e.g.~Mercator),
\item
  some properties of distances (e.g.~equirectangular preserves distances along meridians, azimuthal equidistant preserves distances to a central point)
\end{itemize}

or some compromise of these. Parameters of projections decide what
is shown in the center of a map and what on the fringes, which
areas are up and which are down, and which areas are most enlarged.
All these choices are in the end political decisions.

It is often entertaining and at times educational to play around with
the different projections and understand their consequences. When
the primary purpose of the map however is not to entertain or educate
projection varieties, it may be preferrable to choose a well-known or
less surprising projection, and move the discussion which projection
should be preferred to a decision process on its own.

\hypertarget{plotting-points-lines-polygons-grid-cells}{%
\section{Plotting points, lines, polygons, grid cells}\label{plotting-points-lines-polygons-grid-cells}}

Since maps are just a special form of plots of statistical data,
the usual rules hold. Frequently occuring challenges include:

\begin{itemize}
\tightlist
\item
  polygons may be very small, and vanish when plotted,
\item
  depending on the data, polygons for different features may well
  overlap, and be visible only partially; using transparent fill
  colors may help indentify them
\item
  when points are plotted with symbols, they may easily overlap and be hidden; density maps (chapter \ref{pointpatterns}) may be more helpful
\item
  lines may be hard to read when coloured and may overlap regardless line width
\end{itemize}

\hypertarget{colors}{%
\subsection{Colors}\label{colors}}

When plotting polygons filled with colors, one has the choice to plot
polygon boundaries, or to suppress these. If polygon boundaries draw
too much attention, an alternative is to colour them in a grey tone,
or another color that doesn't interfere with the fill colors. When
suppressing boundaries entirely, polygons with (nearly) identical
colors will no longer be visually distinguishable. If the property
indicating the fill color is constant over the region, such as land
cover type, then this is not a problem but if the property is an
aggregation, the region over which it was aggregated gets lost,
and by that the proper interpretation: especially for extensive
variables, e.g.~the amount of people living in a polygon, this
strongly misleads. But even with polygon boundaries, using filled
polygons for such variables may not be a good idea.

The use of continuous color scales that have no noticable color
breaks for continuously varying variables may look attractive,
but is often more fancy than useful:

\begin{itemize}
\tightlist
\item
  it impracticle to match a color on the map with a legend value
\item
  colors ramps often stretch non-linearly over the value range,
  making it hard to convey magnitude
\end{itemize}

Only for cases where the identification of values is less
important than the continuity of the map, such as the coloring of
a high resolution digital terrain model, it does serve its goal.
Good colors scales are e.g.~found in packages \texttt{RColorBrewer}
\citep{R-RColorBrewer}, viridis \citep{R-viridis} or colorspace
\citep{R-colorspace, colorspace}.

\hypertarget{classintervals}{%
\subsection{\texorpdfstring{Color breaks: \texttt{classInt}}{Color breaks: classInt}}\label{classintervals}}

When plotting continuous geometry attributes using a limited set
of colors (or symbols), classes need to be made from the data.
R package \texttt{classInt} \citep{R-classInt} provides a number of methods to
do so. The default method is ``quantile'':

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(classInt)}
\CommentTok{# set.seed(1) needed ?}
\NormalTok{r =}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{(cI <-}\StringTok{ }\KeywordTok{classIntervals}\NormalTok{(r))}
\CommentTok{# style: quantile}
\CommentTok{#   one of 1.49e+10 possible partitions of this variable into 8 classes}
\CommentTok{#   [-2.61,-1.26)  [-1.26,-0.356) [-0.356,-0.131)  [-0.131,0.091) }
\CommentTok{#              13              12              13              12 }
\CommentTok{#   [0.091,0.433)   [0.433,0.623)    [0.623,1.11)     [1.11,2.76] }
\CommentTok{#              12              13              12              13}
\NormalTok{cI}\OperatorTok{$}\NormalTok{brks}
\CommentTok{# [1] -2.612 -1.257 -0.356 -0.131  0.091  0.433  0.623  1.113  2.755}
\end{Highlighting}
\end{Shaded}

it takes argument \texttt{n} for the number of intervals, and a \texttt{style}
that can be one of ``fixed'', ``sd'', ``equal'', ``pretty'', ``quantile'',
``kmeans'', ``hclust'', ``bclust'', ``fisher'' or ``jenks''. Style ``pretty''
may not obey \texttt{n}; if \texttt{n} is missing, `nclass.Sturges' is used;
two other methods are available for choosing \texttt{n} automatically. If
the number of observations is greater than 3000, a 10\% sample is used
to create the breaks for ``fisher'' and ``jenks''.

\hypertarget{graticule}{%
\subsection{Graticule and other navigation aids}\label{graticule}}

A graticules is a network of lines on a map that follow constant latitude or
longitude. On figure \ref{fig:first-map} a graticule is drawn
in grey, on figure \ref{fig:firstgather} in white.
Graticules are often drawn in maps to give reference where
something is. In our first map in figure \ref{fig:first-map} we can
read that the area plotted is near 35\(^o\) North and 80\(^o\) West.
Had we plotted the lines in the projected coordinate system, they
would have been straight and their actual numbers would not have
been very informative, apart from giving an interpretation of size
or distances when the unit is known, and familiar to the map reader.
Graticules, by that, also shed light on which projection
was used: equirectangular or Mercator projections have straight
vertical and horizontal lines, conic projections have straight but
diverging meridians, equal area may have curved meridians.

The real navigation aid on figure \ref{fig:world} and most other
maps are geographical features like the state outline, country
outlines, coast lines, rivers, roads, railways and so on. If these
are added sparsely and sufficiently, a graticule can as well be
omitted. In such cases, maps look good without axes, tics, and labels,
leaving up a lot of plotting space to be filled with actual map data.

\hypertarget{base-plot}{%
\section{\texorpdfstring{Base \texttt{plot}}{Base plot}}\label{base-plot}}

The \texttt{plot} method for \texttt{sf} and \texttt{stars} objects try to make quick,
useful, exploratory plots; for higher quality plots and more
configurability, alternatives with more control and/or better
defaults are offered for instance by packages \texttt{ggplot2} \citep{R-ggplot2},
\texttt{tmap} \citep{R-tmap, tmap} or \texttt{mapsf} \citep{R-mapsf}.

By default, the plot method tries to plot ``all'' it is given.
This means that:

\begin{itemize}
\tightlist
\item
  given a geometry only (\texttt{sfc}), the geometry is plotted, without colors,
\item
  given a geometry and an attribute, the geometry is colored according to
  the values of the attribute, using a qualitative color scale for \texttt{factor}
  or \texttt{logical} attributes and a continuous scale otherwise,
\item
  given multiple attributes, multiple maps are plotted, each with a color
  scale but a legend is omitted by default, as color assignment is
  done on a per sub-map basis,
\item
  for \texttt{stars} objects with multiple attributes, only the first
  attribute is plotted; for three-dimensional raster cubes, all
  slices over the third dimension are plotted.
\end{itemize}

\hypertarget{adding-to-plots-with-legends}{%
\subsection{Adding to plots with legends}\label{adding-to-plots-with-legends}}

The \texttt{plot} methods for \texttt{stars} and \texttt{sf} objects may show a color key
on one of the sides (e.g., figure \ref{fig:first-map}). To do this
with \texttt{base::plot}, the plot region is split in two and two plots are
created: one with the map, and one with the legend. By default, the
\texttt{plot} function resets the graphics device (using \texttt{layout(matrix(1))}
so that subsequent plots are not hindered by the device being split
in two. If one wants to \emph{add} to an existing plot having a color
legend, this is however what is needed, and resetting the plotting
device needs to be prevented by setting argument \texttt{reset\ =\ FALSE},
and use \texttt{add\ =\ TRUE} in a subsequent call to \texttt{plot}, an example
is

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(sf)}
\NormalTok{nc =}\StringTok{ }\KeywordTok{read_sf}\NormalTok{(}\KeywordTok{system.file}\NormalTok{(}\StringTok{"gpkg/nc.gpkg"}\NormalTok{, }\DataTypeTok{package=}\StringTok{"sf"}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(nc[}\StringTok{"BIR74"}\NormalTok{], }\DataTypeTok{reset =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{key.pos =} \DecValTok{4}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{st_buffer}\NormalTok{(nc[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{], units}\OperatorTok{::}\KeywordTok{set_units}\NormalTok{(}\DecValTok{10}\NormalTok{, km)), }\DataTypeTok{col =} \StringTok{'NA'}\NormalTok{, }
     \DataTypeTok{border =} \StringTok{'red'}\NormalTok{, }\DataTypeTok{lwd =} \DecValTok{2}\NormalTok{, }\DataTypeTok{add =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/figreset-1} 

}

\caption{Annotating base plots that have a legend}\label{fig:figreset}
\end{figure}

which is shown in figure \ref{fig:figreset}. Annotating \texttt{stars}
plots can be done in the same way when a \emph{single} stars layer is
shown. Annotating \texttt{stars} plots with multiple cube slices can be
done by adding a ``hook'' function that will be called on every slice
shown, as in

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(stars)}
\NormalTok{r =}\StringTok{ }\KeywordTok{read_stars}\NormalTok{(}\KeywordTok{system.file}\NormalTok{(}\StringTok{"tif/L7_ETMs.tif"}\NormalTok{, }\DataTypeTok{package =} \StringTok{"stars"}\NormalTok{))}
\NormalTok{circ =}\StringTok{ }\KeywordTok{st_bbox}\NormalTok{(r) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{st_as_sfc}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\KeywordTok{st_sample}\NormalTok{(}\DecValTok{5}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{st_buffer}\NormalTok{(}\DecValTok{300}\NormalTok{)}
\NormalTok{hook =}\StringTok{ }\ControlFlowTok{function}\NormalTok{() }\KeywordTok{plot}\NormalTok{(circ, }\DataTypeTok{col =} \OtherTok{NA}\NormalTok{, }\DataTypeTok{border =} \StringTok{'yellow'}\NormalTok{, }\DataTypeTok{add =} \OtherTok{TRUE}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(r, }\DataTypeTok{hook =}\NormalTok{ hook, }\DataTypeTok{key.pos =} \DecValTok{4}\NormalTok{)}
\CommentTok{# downsample set to c(2,2,1)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/starshook-1} 

}

\caption{annotated multi-slice stars plot}\label{fig:starshook}
\end{figure}

and as shown in figure \ref{fig:starshook}.

Base plot methods have access to the resolution of the screen device
and hence the base plot method for \texttt{stars} and \texttt{stars\_proxy} object
will downsample dense rasters and only plot pixels at a density
that makes sense for the device available.

\hypertarget{projections-in-base-plots}{%
\subsection{Projections in base plots}\label{projections-in-base-plots}}

The base \texttt{plot} method plots data with ellipsoidal coordinates
using the equirectangular projection, using a latitude parameter
equal to the middle latitude of the data bounding box (figure
\ref{fig:eqc}). To control this parameter, either a projection
to another equirectangular can be applied before plotting, or the
parameter \texttt{asp} can be set to override, e.g.~\texttt{asp=1} would lead to
plate carrÃ©e (figure \ref{fig:world} left). Subsequent plots need
to be in the same coordinate reference system in order to make
sense with overplotting, this is not being checked.

\hypertarget{colors-and-color-breaks}{%
\subsection{Colors and color breaks}\label{colors-and-color-breaks}}

In base plots, \texttt{nbreaks} can be used to set the number of color
breaks, and \texttt{breaks} either to the numeric vector with actual breaks,
or to a value for the \texttt{style} argument in \texttt{classInt::classIntervals}.

\hypertarget{maps-with-ggplot2}{%
\section{\texorpdfstring{Maps with \texttt{ggplot2}}{Maps with ggplot2}}\label{maps-with-ggplot2}}

Package \texttt{ggplot2} \citep{R-ggplot2, ggplot2} can create
more complex an nicer looking graphs; it has a geometry \texttt{geom\_sf}
that was developed in conjunction with the development of \texttt{sf}, and
helps creating beautiful maps; an introduction to this is found in
\citep{moreno}, a first example is shown in figure \ref{fig:firstgather}.
The code used for this plot is:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{suppressPackageStartupMessages}\NormalTok{(}\KeywordTok{library}\NormalTok{(tidyverse))}
\NormalTok{nc}\FloatTok{.32119}\NormalTok{ =}\StringTok{ }\KeywordTok{st_transform}\NormalTok{(nc, }\DecValTok{32119}\NormalTok{) }
\NormalTok{year_labels =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"SID74"}\NormalTok{ =}\StringTok{ "1974 - 1978"}\NormalTok{, }\StringTok{"SID79"}\NormalTok{ =}\StringTok{ "1979 - 1984"}\NormalTok{)}
\NormalTok{nc}\FloatTok{.32119} \OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(SID74, SID79) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{pivot_longer}\NormalTok{(}\KeywordTok{starts_with}\NormalTok{(}\StringTok{"SID"}\NormalTok{)) ->}\StringTok{ }\NormalTok{nc_longer}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_sf}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ nc_longer, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{fill =}\NormalTok{ value)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(}\OperatorTok{~}\StringTok{ }\NormalTok{name, }\DataTypeTok{ncol =} \DecValTok{1}\NormalTok{, }\DataTypeTok{labeller =} \KeywordTok{labeller}\NormalTok{(}\DataTypeTok{name =}\NormalTok{ year_labels)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{breaks =} \DecValTok{34}\OperatorTok{:}\DecValTok{36}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_gradientn}\NormalTok{(}\DataTypeTok{colors =} \KeywordTok{sf.colors}\NormalTok{(}\DecValTok{20}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{panel.grid.major =} \KeywordTok{element_line}\NormalTok{(}\DataTypeTok{color =} \StringTok{"white"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

where we see that two attributes had to be stacked (\texttt{pivot\_longer})
before plotting them as facets: this is the idea of ``tidy'' data,
and the \texttt{pivot\_longer} method for \texttt{sf} objects automatically stacks
the geometry column too.

Because \texttt{ggplot2} creates graphics \emph{objects} before plotting them,
it can control the cooordinate reference system of all elements
involved, and will transform or convert all subsequent objects to
the coordinate reference system of the first. It will also draw a
graticule for the (default) thin white lines on a grey background,
and uses a datum (by default: WGS84) for this. \texttt{geom\_sf()} can be
combined with other geoms, for instance to allow for annotating
plots.

For package \texttt{stars}, a \texttt{geom\_stars} has, at the moment of writing
this, rather limited scope: it uses \texttt{geom\_sf} for map layout and vector data
cubes, and adds \texttt{geom\_raster} for regular rasters and \texttt{geom\_rect}
for rectilinear rasters. It downsamples if the user specifies a
downsampling rate, but has no access to the screen dimensions to
automatically choose a downsampling rate. This may be just enough,
for instance figure
\ref{fig:ggplotstars} is created by the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{library}\NormalTok{(stars)}
\NormalTok{r =}\StringTok{ }\KeywordTok{read_stars}\NormalTok{(}\KeywordTok{system.file}\NormalTok{(}\StringTok{"tif/L7_ETMs.tif"}\NormalTok{, }\DataTypeTok{package =} \StringTok{"stars"}\NormalTok{))}
\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_stars}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ r) }\OperatorTok{+}
\StringTok{        }\KeywordTok{facet_wrap}\NormalTok{(}\OperatorTok{~}\NormalTok{band) }\OperatorTok{+}\StringTok{ }\KeywordTok{coord_equal}\NormalTok{() }\OperatorTok{+}
\StringTok{        }\KeywordTok{theme_void}\NormalTok{() }\OperatorTok{+}
\StringTok{        }\KeywordTok{scale_x_discrete}\NormalTok{(}\DataTypeTok{expand=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{)) }\OperatorTok{+}\StringTok{ }
\StringTok{        }\KeywordTok{scale_y_discrete}\NormalTok{(}\DataTypeTok{expand=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{)) }\OperatorTok{+}
\StringTok{        }\KeywordTok{scale_fill_viridis_c}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/ggplotstars-1} 

}

\caption{Simple raster plot with ggplot2}\label{fig:ggplotstars}
\end{figure}

More elaborate \texttt{ggplot2}-based plots with \texttt{stars} objects may be
obtained using package \texttt{ggspatial} \citep{R-ggspatial}. Non-compatible
but nevertheless \texttt{ggplot2}-style plots can be created with \texttt{tmap},
a package dedicated to creating high quality maps

When combining several feature sets with varying coordinate reference
systems, using \texttt{geom\_sf}, all sets are transformed to the reference
system of the first set. To get further control over the ``base''
coordinate reference system, \texttt{coord\_sf} can be used. This allows
for instance working in a projected system, while combining graphics
elements that are \emph{not} \texttt{sf} objects but regular \texttt{data.frame}s
with ellipsoidal coordinates associated to WGS84. A
twitter thread by Claus Wilke illustrating this is found
\href{https://twitter.com/ClausWilke/status/1275938314055561216}{here}.

\hypertarget{tmap}{%
\section{\texorpdfstring{Maps with \texttt{tmap}}{Maps with tmap}}\label{tmap}}

Package \texttt{tmap} \citep{R-tmap, tmap} takes a fresh look on plotting
spatial data in R; it resembles \texttt{ggplot2} in the sense that it
composes graphics objects before printing, by building on the \texttt{grid}
package, and by concatenating map elements with a \texttt{+} between them,
but otherwise it is entirely independent from, and incompatible
with, \texttt{ggplot2}. It has a number of options that allow for highly
professional looking maps, and many defaults have been carefully
chosen. To recreate figure \ref{fig:firstgather}, for instance,
we use

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tmap)}
\KeywordTok{system.file}\NormalTok{(}\StringTok{"gpkg/nc.gpkg"}\NormalTok{, }\DataTypeTok{package =} \StringTok{"sf"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{read_sf}\NormalTok{() }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{st_transform}\NormalTok{(}\StringTok{'EPSG:32119'}\NormalTok{) ->}\StringTok{ }\NormalTok{nc}\FloatTok{.32119}
\KeywordTok{tm_shape}\NormalTok{(nc}\FloatTok{.32119}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{tm_polygons}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"SID74"}\NormalTok{, }\StringTok{"SID79"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/tmapnc-1} 

}

\caption{tmap: using ... with two attributes}\label{fig:tmapnc}
\end{figure}

to create figure \ref{fig:tmapnc} and

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nc}\FloatTok{.32119} \OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(SID74, SID79) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{pivot_longer}\NormalTok{(}\KeywordTok{starts_with}\NormalTok{(}\StringTok{"SID"}\NormalTok{), }\DataTypeTok{values_to =} \StringTok{"SID"}\NormalTok{) ->}\StringTok{ }\NormalTok{nc_longer}
\KeywordTok{tm_shape}\NormalTok{(nc_longer) }\OperatorTok{+}\StringTok{ }\KeywordTok{tm_polygons}\NormalTok{(}\StringTok{"SID"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{tm_facets}\NormalTok{(}\DataTypeTok{by =} \StringTok{"name"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/tmapnc2-1} 

}

\caption{tmap: using ...}\label{fig:tmapnc2}
\end{figure}

to create figure \ref{fig:tmapnc2}.

Package \texttt{tmap} also has support for \texttt{stars} objects, an example created with

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tm_shape}\NormalTok{(r) }\OperatorTok{+}\StringTok{ }\KeywordTok{tm_raster}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/tmapstars-1} 

}

\caption{Simple raster plot with tmap}\label{fig:tmapstars}
\end{figure}

is shown in figure \ref{fig:tmapstars}. More examples of the use of \textbf{tmap} are given in Chapter \ref{area}.

\hypertarget{interactive-maps-leaflet-mapview-tmap}{%
\section{\texorpdfstring{Interactive maps: \texttt{leaflet}, \texttt{mapview}, \texttt{tmap}}{Interactive maps: leaflet, mapview, tmap}}\label{interactive-maps-leaflet-mapview-tmap}}

Interactive maps as shown in figure \ref{fig:mapviewfigure} can be
created with R packages \texttt{leaflet}, \texttt{mapview} or \texttt{tmap}. \texttt{mapview}
adds a number of capabilities to \texttt{leaflet} including a map legend,
configurable pop-up windows when clicking features, support for
raster data, and scalable maps with very large feature sets using
the filegeobuf file format, as well as facet maps that react
synchronously to zoom and pan actions. Package \texttt{tmap} has the nice
option that after giving

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tmap_mode}\NormalTok{(}\StringTok{"view"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

all usual \texttt{tmap} commands are applied to an interactive html/leaflet widget,
whereas after

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tmap_mode}\NormalTok{(}\StringTok{"plot"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

again all output is sent to R own graphics device.

\hypertarget{exercises-8}{%
\section{Exercises}\label{exercises-8}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  For the countries Indonesia and Canada, create individual plots using
  equirectangular, orthographic, and Lambert equal area projections, while
  choosing projection parameters sensible for the area.
\item
  Recreate the plot in figure \ref{fig:figreset} with \texttt{ggplot2} and with \texttt{tmap}.
\item
  Recreate the plot in figure \ref{fig:tmapstars} using the \texttt{viridis} color ramp.
\item
  View the interactive plot in figure \ref{fig:tmapstars} using the ``view''
  (interactive) mode of \texttt{tmap}, and explore which interactions are possible; also
  explore adding \texttt{+\ tm\_facets(as.layers=TRUE)} and try switching layers on and off.
\end{enumerate}

\hypertarget{part-models-for-spatial-data}{%
\part{Models for Spatial Data}\label{part-models-for-spatial-data}}

\hypertarget{statistical-modelling-of-spatial-data}{%
\chapter{Statistical modelling of spatial data}\label{statistical-modelling-of-spatial-data}}

Spatial data almost always (and everywhere) has the property that it
is spatially structured: observations done closeby in space tend to
be more similar than observations done at larger distance from each
other. This phenomenon, in the geography domain attributed to Waldo
Tobler (as in ``Waldo Tobler's first law of geography'') was already
noted by \citep{fisher1937design} and was a motivation for developing
randomized block design in agricultural experiments: allocating
treatments randomly to blocks avoids that spatial structure gets
mixed up (or: confounds) with a signal caused by the treatment.

The often heard argument that spatially structured data \emph{means}
that the data is spatially correlated, which would \emph{exclude}
estimation methods that assume independent observations is false.
Correlation is a property of two random variables, and there are
different ways in which spatial data can be approached with random
variables: either the observation locations are random (leading to
design-based inference) or the observed values are random (leading
to model-based inference). The next section points out the difference
between these two.

\hypertarget{design}{%
\section{Design-based and model-based inference}\label{design}}

Statistical inference means the action of estimating parameters
about a population from sample data. Suppose we denote the variable
of interest with \(z(s)\), where \(z\) is the attribute value measured
at location \(s\), and we are interested in estimating the
mean value of \(z(s)\) over a domain \(D\),
\[z(s)=\frac{1}{|D|} \int_{ u \in D} z(u)du,\]
with \(|D|\) the area of \(D\), from sample data \(z(s_1),...,z(s_n)\).

Then, there are two possibilities to proceed: model-based, or
design-based. A model-based approach considers \(z(s)\) to be a
realisation of a superpopulation \(Z(s)\) (using capital letters to
indicate random variables), and could for instance postulate a
model for its spatial variability in the form of
\[Z(s) = m + e(s), \  \ \mbox{E}(e(s)) = 0, \  \ \mbox{Cov(e(s))} = \Sigma(\theta)\]
which would require choosing the covariance function \(\Sigma()\) and
estimating its parameters \(\theta\) form \(z(s)\), and then computing a
block kriging prediction \(\hat{Z}(D)\) (section \ref{blockkriging}).
This approach makes no assumptions about the sample \(z(s)\), but of
course it should allow for choosing the covariance function and
estimating its parameters; inference is conditional to the validity
of the postulated model.

Rather than assuming a superpopulation model, the design-based
approach \citep{de1990model, brus2021, breidt2017model} assumes
randomness in the locations, which is justified (only) when using
random sampling. It \emph{requires} that the sample data were obtained
by probability sampling, meaning that some form of spatial random
sampling was used where all elements of \(z(s)\) had a known and
positive probability of being included in the sample obtained. The
random process is that of sampling: \(z(s_1)\) is a realisation of
the random process \(z(S_1)\), the first observation taken \emph{over
repeated random sampling}. Design-based estimaters only need
these inclusion probabilities to estimate mean values with standard
errors. This means that for instance given a simple random sample,
the unweighted sample mean is used to estimate the population mean,
and no model parameters need to be fit.

The misconception here, as explained in {[}brus2021{]}, is that this is
only the case when working under model-based approaches: \(Z(s_1)\)
and \(Z(s_2)\) may well be correlated (``model-dependent''), but although
in a particular random sampling (realisation) \(z(s_1)\) and \(z(s_2)\)
\emph{may} be close in space, the corresponding random variables \(z(S_1)\)
and \(z(S_2)\) considered over repeated random sampling are not close
together, and are design-independent. Both situations can co-exist
without contradiction, and are a consequence of choosing to work
under one inference framework or the other.

The choice whether to work under a design-based or model-based
framework depends on the purpose of the study and the
data collection process. The model-based framework lends itself best for cases
* where predictions are required for individual locations, or
for areas too small to be sampled
* when the available data were not collected using a known random
sampling scheme (i.e., the inclusion probabilities are unknown,
or are zero over particular areas or/and times)
Design-based approaches are most suitable when
* observations were collected using a spatial random sampling process
* aggregated properties of the entire sample region (or sub-region)
are needed.
* estimates are required that are not sensitive to potential model
misspecification, e.g.~when needed for regulatory or legal purposes.

In case a sampling procedure is to be planned \citep{de2006sampling}, some
form of spatial random sampling is definitely worth considering since
it opens up the possibility of following both inference frameworks.

\hypertarget{predictive-models-with-coordinates}{%
\section{Predictive models with coordinates}\label{predictive-models-with-coordinates}}

In data science projects, coordinates may be seen as features in a
larger set of predictors (or features, or covariates) and treated
accordingly. There are some catches with doing so.

As usual when working with predictors, it is good to choose
predictive methods that are not sensitive to shifts in origin
or shifts in unit (scale). Assuming a two-dimensional problem,
predictive models should also not be sensitive to arbitrary rotations
of the x- and y- or latitude and longitude axes. For projected (2D,
Cartesian) coordinates this can be assured e.g.~by using polynomials
of order \(n\) as \((x+y)^n\), rather than \((x)^n + (y)^n\); for a second
order polynomial this involves including the term \(xy\), so that an
ellipsoidal-shape trend surface does not have to be aligned with
the \(x-\) or \(y-\)axis. For a GAM model with spline components, one
would use a spline in two dimensions rather than two independent
splines in \(x\) and \(y\). An exception to this ``rule'' is when e.g.
a pure latitude effect is desired, for instance to account for
solar energy influx.

When the area covered by the data is large, the difference between
using ellipsoidal coordinates and projected coordinates
will automatically become larger, and hence choosing one of both
will have an effect on predictive modelling. For very large extents,
e.g.~global models, polynomials or splines in latitude and longitude
will not make sense as they ignore the circular nature of longitude
and the coordinate singularities at the poles. Here, spherical
harmonics, base functions that are continuous on the sphere with
increasing spatial frequencies can replace polynomials or be used
as spline base functions.

In many cases, the spatial coordinates over which samples were
collected also define the space over which predictions are made,
setting them apart from other features. Many simple predictive
approaches, including most machine learning methods, assume
sample data to be independent. When samples are collected by
spatially random sampling over the spatial target area, this
assumption may be justified when working under a design-based context
\citep{brusejss}. This context however treats the coordinate space as
the variable over which we randomize, which affords predicting
values for a new \emph{randomly chosen} location but rules out making
predictions for fixed locations; this implies that averages over
areas over which samples were collected can be obtained, but not
spatial interpolations. In case predictions for fixed locations
are required, or in case data were not collected by spatial
random sampling, a model-based approach (as taken in chapter
\ref{interpolation}) is needed and typically some form of spatial
and/or temporal autocorrelation of residuals must be assumed.

A common case is where sample data are collected opportunistically
(``whatever could be found''), and are then used in a predictive
framework that does not weigh them. This has a consequence that the
resulting model may be biased towards over-represented areas (in
predictor space and/or in spatial coordinates space), and that simple
(random) cross validation statistics may be over-optimistic when
taken as performance measures for spatial prediction \citep{meyerpebesma}.
Adaptive cross validation measures, e.g.~spatial cross validation
may help getting more relevant measures for predictive performance.

\hypertarget{further-reading}{%
\section{Further reading}\label{further-reading}}

There is a large number of papers and books available on analysing
and statistical modelling of spatial and spatiotemporal data, and
a very large number of R packages help doing so. Several \href{https://cran.r-project.org/web/views/}{CRAN task
views} try to maintain an
overview of the R packages, e.g.~on

\begin{itemize}
\tightlist
\item
  spatial data \citep{ctvspatial}
\item
  spatiotemporal data \citep{ctvspatiotemporal}
\item
  tracking data \citep{ctvtracking}, see also \citep{https://doi.org/10.1111/1365-2656.13116}
\end{itemize}

The introductions to subsequent chapters contain more pointers
to relevant literature references. Introductions to using the
integrated nested Laplace approximation (INLA) for analysing spatial
data are given in \citet{BLANGIARDO201333}, \citet{blangiardo2015spatial}, and
\citet{gomez2020bayesian}. \citet{krainski2018advanced} combine the INLA approach
with stochastic partial differential equations. Spatiotemporal
Bayesian modelling of change of support problems is presented in
\citet{stcos} and \citet{R-stcos}.

\hypertarget{pointpatterns}{%
\chapter{Point Pattern Analysis}\label{pointpatterns}}

Point pattern analysis is concerned with describing patterns of
points over space, and making infererence about the process that
could have generated an observed pattern. The main focus here lies on
the information carried in the locations of the points, and typically
these locations are not controlled by sampling but a result of a
process we're interested in, such as animal sightings, accidents,
disease cases, or tree locations. This is opposed to geostatistical
processes (chapter \ref{interpolation}) where we have values of
some phenomenon everywhere but observations limited to a set of
locations \emph{that we can control}, at least in principle. Hence, in
geostatistical problems the prime interest is not in the observation
locations but in estimating the value of the observed phenomenon
at unobserved locations. Point pattern analysis typically assumes
that for an observed area, all points are available, meaning that
locations without a point are not unobserved as in a geostatistical
process, but are observed and contain no point. In terms of random
processes, in point processes locations are random variables, where
in geostatistical processes the measured variable is a random field
with locations fixed.

This chapter is confined to describing the very basics of point
pattern analysis, using package \texttt{spatstat} \citep{R-spatstat}, and
related packages by the same authors. The \texttt{spatstat} book of
\citet{baddeley2015spatial} gives a comprehensive introduction to point
pattern theory and the use of the \texttt{spatstat} package family,
which we will not try to copy. Inclusion of particular topics in
this chapter should not be seen as an expression that these are
more relevant than others. In particular, this chapter tries to
illustrate interfaces existing between \texttt{spatstat} and the more
spatial data science oriented packages \texttt{sf} and \texttt{stars}.
A further books that introduces point patterns analysis is
\citet{STOYAN2017125}. An R package for analysing spatiotemporal
point processes is discussed in \citet{stpp}.

Important concepts of point patterns analysis are the distinction
between a point \emph{pattern} and a point \emph{process}: the latter is the
stochastic process that, when sampled, generates a point pattern.
A data set is always a point pattern, and inference involves figuring
out what kind of process could have generated a pattern like the one
we observed. Properties of a spatial point process are:

\begin{itemize}
\tightlist
\item
  first order properties or intensity function, which measures the
  number of points per area unit; this function is spatially varying
  for a \emph{inhomogeneous} point process. *
\item
  second order properties, e.g.~pairwise interactions: given a
  constant or varying intensity function, are points distributed
  independently \emph{from one another}, or do the tend to attract
  each other (clustering) or repulse each other (appear regularly
  distributed, compared to independence)
\end{itemize}

\hypertarget{observation-window}{%
\section{Observation window}\label{observation-window}}

Point patterns have an observation window. Consider the points
randomly generated randomly by

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(sf)}
\NormalTok{n =}\StringTok{ }\DecValTok{30}
\NormalTok{xy =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{runif}\NormalTok{(n), }\DataTypeTok{y =} \KeywordTok{runif}\NormalTok{(n)) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{st_as_sf}\NormalTok{(}\DataTypeTok{coords =} \KeywordTok{c}\NormalTok{(}\StringTok{"x"}\NormalTok{, }\StringTok{"y"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

then these points are (by construction) uniformly distributed,
or completely spatially random, over the domain \([0,1] \times [0,1]\). For
a larger domain, they are not uniform, for the two square windows
\texttt{w1} and \texttt{w2} created by

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{w1 =}\StringTok{ }\KeywordTok{st_bbox}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DataTypeTok{xmin =} \DecValTok{0}\NormalTok{, }\DataTypeTok{ymin =} \DecValTok{0}\NormalTok{, }\DataTypeTok{xmax =} \DecValTok{1}\NormalTok{, }\DataTypeTok{ymax =} \DecValTok{1}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{        }\KeywordTok{st_as_sfc}\NormalTok{() }
\NormalTok{w2 =}\StringTok{ }\KeywordTok{st_sfc}\NormalTok{(}\KeywordTok{st_point}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{))) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{st_buffer}\NormalTok{(}\FloatTok{1.2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

this is shown in figure \ref{fig:pp1}.

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/pp1-1} 

}

\caption{Depending on the observation window (grey), the same point pattern can appear completely spatially random (left), or clustered (right)}\label{fig:pp1}
\end{figure}

Point patterns in \texttt{spatstat} are objects of class \texttt{ppp} that contain
points and an observation window (an object of class \texttt{owin}).
We can create a \texttt{ppp} from points by

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{suppressPackageStartupMessages}\NormalTok{(}\KeywordTok{library}\NormalTok{(spatstat))}
\KeywordTok{as.ppp}\NormalTok{(xy)}
\CommentTok{# Planar point pattern: 30 points}
\CommentTok{# window: rectangle = [0.009, 0.999] x [0.103, 0.996] units}
\end{Highlighting}
\end{Shaded}

where we see that the bounding box of the points is used as observation
window when no window is specified. If we add a polygonal geometry as the
first feature of the dataset, then this is used as observation window:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\DataTypeTok{pp1 =} \KeywordTok{c}\NormalTok{(w1, }\KeywordTok{st_geometry}\NormalTok{(xy)) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.ppp}\NormalTok{())}
\CommentTok{# Planar point pattern: 30 points}
\CommentTok{# window: polygonal boundary}
\CommentTok{# enclosing rectangle: [0, 1] x [0, 1] units}
\NormalTok{c1 =}\StringTok{ }\KeywordTok{st_buffer}\NormalTok{(}\KeywordTok{st_centroid}\NormalTok{(w2), }\FloatTok{1.2}\NormalTok{)}
\NormalTok{(}\DataTypeTok{pp2 =} \KeywordTok{c}\NormalTok{(c1, }\KeywordTok{st_geometry}\NormalTok{(xy)) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.ppp}\NormalTok{())}
\CommentTok{# Planar point pattern: 30 points}
\CommentTok{# window: polygonal boundary}
\CommentTok{# enclosing rectangle: [-0.2, 2.2] x [-0.7, 1.7] units}
\end{Highlighting}
\end{Shaded}

To test for homogeneity, one could carry out a quadrat count, using
an appropriate quadrat layout (a 3 x 3 layout is shown in figure
\ref{fig:quadrat})

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/quadrat-1} 

}

\caption{3 x 3 quadrat counts for the two point patterns}\label{fig:quadrat}
\end{figure}

and carry out a \(\chi^2\) test on these counts:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{quadrat.test}\NormalTok{(pp1, }\DataTypeTok{nx=}\DecValTok{3}\NormalTok{, }\DataTypeTok{ny=}\DecValTok{3}\NormalTok{)}
\CommentTok{# Warning: Some expected counts are small; chi^2 approximation may}
\CommentTok{# be inaccurate}
\CommentTok{# }
\CommentTok{#   Chi-squared test of CSR using quadrat counts}
\CommentTok{# }
\CommentTok{# data:  pp1}
\CommentTok{# X2 = 8, df = 8, p-value = 0.9}
\CommentTok{# alternative hypothesis: two.sided}
\CommentTok{# }
\CommentTok{# Quadrats: 9 tiles (irregular windows)}
\KeywordTok{quadrat.test}\NormalTok{(pp2, }\DataTypeTok{nx=}\DecValTok{3}\NormalTok{, }\DataTypeTok{ny=}\DecValTok{3}\NormalTok{)}
\CommentTok{# Warning: Some expected counts are small; chi^2 approximation may}
\CommentTok{# be inaccurate}
\CommentTok{# }
\CommentTok{#   Chi-squared test of CSR using quadrat counts}
\CommentTok{# }
\CommentTok{# data:  pp2}
\CommentTok{# X2 = 43, df = 8, p-value = 2e-06}
\CommentTok{# alternative hypothesis: two.sided}
\CommentTok{# }
\CommentTok{# Quadrats: 9 tiles (irregular windows)}
\end{Highlighting}
\end{Shaded}

where we should take the p-values with a large grain of salt because
we have too small expected counts.

Kernel densities can be computed using \texttt{density}, where kernel shape and
bandwidth can be controlled. Here, cross validation is used by function
\texttt{bw.diggle} to specify the bandwidth parameter \texttt{sigma}; plots are shown in
figure \ref{fig:bwdiggle}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{den1 <-}\StringTok{ }\KeywordTok{density}\NormalTok{(pp1, }\DataTypeTok{sigma =}\NormalTok{ bw.diggle)}
\NormalTok{den2 <-}\StringTok{ }\KeywordTok{density}\NormalTok{(pp2, }\DataTypeTok{sigma =}\NormalTok{ bw.diggle)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/bwdiggle-1} 

}

\caption{Kernel densities for both point patterns}\label{fig:bwdiggle}
\end{figure}

The density maps created this way are obviously raster images, and we can
convert them into stars object, e.g.~by

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(stars)}
\NormalTok{s1 =}\StringTok{ }\KeywordTok{st_as_stars}\NormalTok{(den1)}
\NormalTok{(}\DataTypeTok{s2 =} \KeywordTok{st_as_stars}\NormalTok{(den2))}
\CommentTok{# stars object with 2 dimensions and 1 attribute}
\CommentTok{# attribute(s):}
\CommentTok{#        Min.  1st Qu. Median Mean 3rd Qu. Max. NA's}
\CommentTok{# v  1.03e-14 0.000153  0.304 6.77    13.1 42.7 3492}
\CommentTok{# dimension(s):}
\CommentTok{#   from  to offset   delta refsys point values x/y}
\CommentTok{# x    1 128   -0.2 0.01875     NA    NA   NULL [x]}
\CommentTok{# y    1 128   -0.7 0.01875     NA    NA   NULL [y]}
\end{Highlighting}
\end{Shaded}

and we can verify that the area under the density surface is similar
to the sample size (30), by

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(s1[[}\DecValTok{1}\NormalTok{]], }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}\OperatorTok{*}\KeywordTok{st_dimensions}\NormalTok{(s1)}\OperatorTok{$}\NormalTok{x}\OperatorTok{$}\NormalTok{delta}\OperatorTok{^}\DecValTok{2}
\CommentTok{# [1] 29}
\KeywordTok{sum}\NormalTok{(s2[[}\DecValTok{1}\NormalTok{]], }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}\OperatorTok{*}\KeywordTok{st_dimensions}\NormalTok{(s2)}\OperatorTok{$}\NormalTok{x}\OperatorTok{$}\NormalTok{delta}\OperatorTok{^}\DecValTok{2}
\CommentTok{# [1] 30.7}
\end{Highlighting}
\end{Shaded}

More exciting applications involve e.g.~modelling the density
surface as a function of external variables. Suppose we want
to model the density of \texttt{pp2} as a Poisson point process (meaning
that points do not interact with each other), where
the intensity is a function of distance to the center of the
``cluster'', and these distance are available in a \texttt{stars} object:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pt =}\StringTok{ }\KeywordTok{st_sfc}\NormalTok{(}\KeywordTok{st_point}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{)))}
\NormalTok{s2}\OperatorTok{$}\NormalTok{dist =}\StringTok{ }\KeywordTok{st_distance}\NormalTok{(}\KeywordTok{st_as_sf}\NormalTok{(s2, }\DataTypeTok{as_points=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{na.rm =} \OtherTok{FALSE}\NormalTok{), pt)}
\end{Highlighting}
\end{Shaded}

we can then model the densities using \texttt{ppm}, where the \emph{name} of the
point pattern object is used as the left-hand-side of the \texttt{formula}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\DataTypeTok{m =} \KeywordTok{ppm}\NormalTok{(pp2 }\OperatorTok{~}\StringTok{ }\NormalTok{dist, }\DataTypeTok{data =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{dist =} \KeywordTok{as.im}\NormalTok{(s2[}\StringTok{"dist"}\NormalTok{]))))}
\CommentTok{# Nonstationary Poisson process}
\CommentTok{# }
\CommentTok{# Log intensity:  ~dist}
\CommentTok{# }
\CommentTok{# Fitted trend coefficients:}
\CommentTok{# (Intercept)        dist }
\CommentTok{#        4.54       -4.25 }
\CommentTok{# }
\CommentTok{#             Estimate  S.E. CI95.lo CI95.hi Ztest  Zval}
\CommentTok{# (Intercept)     4.54 0.341    3.87    5.21   *** 13.32}
\CommentTok{# dist           -4.25 0.701   -5.62   -2.88   *** -6.06}
\end{Highlighting}
\end{Shaded}

The returned object is of class \texttt{ppm}, and can be plotted: figure \ref{fig:ppm}
shows the predicted surface, the prediction standard error can also be plotted.

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/ppm-1} 

}

\caption{Predicted densities of a ppm model}\label{fig:ppm}
\end{figure}

The model also has a \texttt{predict} method, which returns an \texttt{im} object that
can be converted into a \texttt{stars} object by

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{predict}\NormalTok{(m, }\DataTypeTok{covariates =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{dist =} \KeywordTok{as.im}\NormalTok{(s2[}\StringTok{"dist"}\NormalTok{]))) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{st_as_stars}\NormalTok{()}
\CommentTok{# stars object with 2 dimensions and 1 attribute}
\CommentTok{# attribute(s):}
\CommentTok{#      Min. 1st Qu. Median Mean 3rd Qu. Max. NA's}
\CommentTok{# v  0.0694   0.527   2.12 6.62     7.3 89.9 3492}
\CommentTok{# dimension(s):}
\CommentTok{#   from  to offset   delta refsys point values x/y}
\CommentTok{# x    1 128   -0.2 0.01875     NA    NA   NULL [x]}
\CommentTok{# y    1 128   -0.7 0.01875     NA    NA   NULL [y]}
\end{Highlighting}
\end{Shaded}

\hypertarget{coordinate-reference-systems-1}{%
\section{Coordinate reference systems}\label{coordinate-reference-systems-1}}

All routines in \texttt{spatstat} are layed out for two-dimensional data with
Cartesian coordinates. If we try to convert an object with ellipsoidal
coordinates, we get an error:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{system.file}\NormalTok{(}\StringTok{"gpkg/nc.gpkg"}\NormalTok{, }\DataTypeTok{package =} \StringTok{"sf"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{read_sf}\NormalTok{() }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{st_geometry}\NormalTok{() }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{st_centroid}\NormalTok{() }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{as.ppp}\NormalTok{()}
\CommentTok{# Error: Only projected coordinates may be converted to spatstat class objects}
\end{Highlighting}
\end{Shaded}

Also, when converting to a \texttt{spatstat} data structure (e.g.~to a
\texttt{ppp}, create a density image, convert back to \texttt{stars}) we loose
the coordinate reference system we started with. It can be set
back e.g.~by using \texttt{st\_set\_crs}.

\hypertarget{marked-point-patterns-points-on-linear-networks}{%
\section{Marked point patterns, points on linear networks}\label{marked-point-patterns-points-on-linear-networks}}

A few more data types can be converted to and from \texttt{spatstat}.
Marked point patterns are point patterns that have a ``mark'', which
is either a categorical label or a numeric label for each point.
A dataset available in \texttt{spatstat} with marks is the \texttt{longleaf} pines
dataset, containing diameter at breast height as numeric mark:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{longleaf}
\CommentTok{# Marked planar point pattern: 584 points}
\CommentTok{# marks are numeric, of storage type  'double'}
\CommentTok{# window: rectangle = [0, 200] x [0, 200] metres}
\NormalTok{ll =}\StringTok{ }\KeywordTok{st_as_sf}\NormalTok{(longleaf)}
\KeywordTok{head}\NormalTok{(ll)}
\CommentTok{# Simple feature collection with 6 features and 2 fields}
\CommentTok{# Geometry type: GEOMETRY}
\CommentTok{# Dimension:     XY}
\CommentTok{# Bounding box:  xmin: 0 ymin: 0 xmax: 200 ymax: 200}
\CommentTok{# CRS:           NA}
\CommentTok{#    mark  label                           geom}
\CommentTok{# NA   NA window POLYGON ((0 0, 200 0, 200 2...}
\CommentTok{# 1  32.9  point                POINT (200 8.8)}
\CommentTok{# 2  53.5  point                 POINT (199 10)}
\CommentTok{# 3  68.0  point               POINT (194 22.4)}
\CommentTok{# 4  17.7  point               POINT (168 35.6)}
\CommentTok{# 5  36.9  point               POINT (184 45.4)}
\end{Highlighting}
\end{Shaded}

Values can be converted back to \texttt{ppp} with

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{as.ppp}\NormalTok{(ll)}
\CommentTok{# Warning in as.ppp.sf(ll): only first attribute column is used for}
\CommentTok{# marks}
\CommentTok{# Marked planar point pattern: 584 points}
\CommentTok{# marks are numeric, of storage type  'double'}
\CommentTok{# window: polygonal boundary}
\CommentTok{# enclosing rectangle: [0, 200] x [0, 200] units}
\end{Highlighting}
\end{Shaded}

Line segments, in \texttt{spatstat} objects of class \texttt{psp} can be converted
back and forth to simple feature with \texttt{LINESTRING} geometries
following a \texttt{POLYGON} feature with the observation window, as in

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(}\KeywordTok{st_as_sf}\NormalTok{(copper}\OperatorTok{$}\NormalTok{SouthLines), }\DataTypeTok{n =} \DecValTok{5}\NormalTok{)}
\CommentTok{# Simple feature collection with 91 features and 1 field}
\CommentTok{# Geometry type: GEOMETRY}
\CommentTok{# Dimension:     XY}
\CommentTok{# Bounding box:  xmin: -0.335 ymin: 0.19 xmax: 35 ymax: 158}
\CommentTok{# CRS:           NA}
\CommentTok{# First 5 features:}
\CommentTok{#     label                           geom}
\CommentTok{# 1  window POLYGON ((-0.335 0.19, 35 0...}
\CommentTok{# 2 segment LINESTRING (3.36 0.19, 10.4...}
\CommentTok{# 3 segment LINESTRING (12.5 0.263, 11....}
\CommentTok{# 4 segment LINESTRING (11.2 0.197, -0....}
\CommentTok{# 5 segment LINESTRING (6.35 12.8, 16.5...}
\end{Highlighting}
\end{Shaded}

Finally, point patterns on linear networks, in \texttt{spatstat}
represented by \texttt{lpp} objects, can be converted to \texttt{sf} by

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(}\KeywordTok{st_as_sf}\NormalTok{(chicago), }\DataTypeTok{n =} \DecValTok{5}\NormalTok{)}
\CommentTok{# Simple feature collection with 620 features and 4 fields}
\CommentTok{# Geometry type: GEOMETRY}
\CommentTok{# Dimension:     XY}
\CommentTok{# Bounding box:  xmin: 0.389 ymin: 153 xmax: 1280 ymax: 1280}
\CommentTok{# CRS:           NA}
\CommentTok{# First 5 features:}
\CommentTok{#     label seg tp marks                           geom}
\CommentTok{# 1  window  NA NA  <NA> POLYGON ((0.389 153, 1282 1...}
\CommentTok{# 2 segment  NA NA  <NA> LINESTRING (0.389 1254, 110...}
\CommentTok{# 3 segment  NA NA  <NA> LINESTRING (110 1252, 111 1...}
\CommentTok{# 4 segment  NA NA  <NA> LINESTRING (110 1252, 198 1...}
\CommentTok{# 5 segment  NA NA  <NA> LINESTRING (198 1277, 198 1...}
\end{Highlighting}
\end{Shaded}

where we only see the first five features; the points are also
in this object, as variable \texttt{label} indicates

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(}\KeywordTok{st_as_sf}\NormalTok{(chicago)}\OperatorTok{$}\NormalTok{label)}
\CommentTok{# }
\CommentTok{#   point segment  window }
\CommentTok{#     116     503       1}
\end{Highlighting}
\end{Shaded}

Potential information about network \emph{structure}, as of which
\texttt{LINESTRING} is connected to others, is not present in the \texttt{sf}
object. Package \texttt{sfnetworks} \citep{R-sfnetworks} would be a candidate
package to hold such information, or e.g.~to pass on network data
imported from OpenStreetMaps to \texttt{spatstat}.

\hypertarget{spatial-sampling-and-simulating-a-point-process}{%
\section{Spatial sampling and simulating a point process}\label{spatial-sampling-and-simulating-a-point-process}}

Package \texttt{sf} contains an \texttt{st\_sample} method that samples points
from \texttt{MULTIPOINT}, linear or polygonal geometries, using different
spatial sampling strategies. It natively supports strategies
``random'', ``hexagonal'' and ``regular'', where ``regular'' refers to
sampling on a square regular grid, and ``hexagonal'' essentially
gives a triangular grid. For type ``random'', it can return exactly
the number of requested points, for other types this is approximate.

\texttt{st\_sample} also interfaces point process simulation functions of
\texttt{spatstat}, when other values for sampling type are chosen. For instance
the \texttt{spatstat} function \texttt{rThomas} is invoked when setting \texttt{type\ =\ Thomas}
(figure \ref{fig:rThomas}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kappa =}\StringTok{ }\DecValTok{30} \OperatorTok{/}\StringTok{ }\KeywordTok{st_area}\NormalTok{(w2) }\CommentTok{# intensity}
\NormalTok{th =}\StringTok{ }\KeywordTok{st_sample}\NormalTok{(w2, }\DataTypeTok{kappa =}\NormalTok{ kappa, }\DataTypeTok{mu =} \DecValTok{3}\NormalTok{, }\DataTypeTok{scale =} \FloatTok{0.05}\NormalTok{, }\DataTypeTok{type =} \StringTok{"Thomas"}\NormalTok{)}
\KeywordTok{nrow}\NormalTok{(th)}
\CommentTok{# [1] 82}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/rThomas-1} 

}

\caption{Thomas process with mu = 3 and scale = 0.05}\label{fig:rThomas}
\end{figure}

The help function obtained by \texttt{?rThomas} details the meaning of the
parameters \texttt{kappa}, \texttt{mu} and \texttt{scale}. Simulating point processes
means that the intensity is given, not the sample size. The sample
size within the observation window obtained this way is a random
variable.

\hypertarget{simulating-points-on-the-globe}{%
\section{Simulating points on the globe}\label{simulating-points-on-the-globe}}

Another spatial random sampling type supported by \texttt{sf} natively
(in \texttt{st\_sample}) is simulation of random points on the sphere. An
example of this is shown in figure \ref{fig:srsglobe}, where points
were constrained to those in oceans.

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/srsglobe-1} 

}

\caption{Points randomly sampled over the oceans}\label{fig:srsglobe}
\end{figure}

\hypertarget{exercises-9}{%
\section{Exercises}\label{exercises-9}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  After loading \texttt{spatstat}, recreate the plot obtained by \texttt{plot(longleaf)}
  by using \texttt{ggplot2} and \texttt{geom\_sf()}, and by \texttt{sf::plot()}.
\item
  Convert the sample locations of the NO\(_2\) data used in chapter \ref{interpolation}
  to a \texttt{ppp} object, with a proper window.
\item
  Compute and plot the density of the NO\(_2\) dataset, import the density as a \texttt{stars}
  object and compute the volume under the surface.
\end{enumerate}

\hypertarget{interpolation}{%
\chapter{Spatial Interpolation}\label{interpolation}}

Spatial interpolation is the activity of estimating values spatially
continuous variables for spatial locations where they have not
been observed, based on observations. The statistical methodology
for spatial interpolation, called geostatistics, is concerned with
the modelling, prediction and simulation of spatially continuous
phenomena. The typical problem is a missing value problem: we
observe a property of a phenomenon \(Z(s)\) at a limited number
of sample locations \(s_i, i = 1,...,n\), and are interested in
the property value at all locations \(s_0\) covering an area of
interest, so we have to predict it for unobserved locations. This
is also called \emph{kriging}, or Gaussian Process prediction.
In case \(Z(s)\) contains a white noise component \(\epsilon\), as in
\(Z(s)=S(s)+\epsilon(s)\) (possibly reflecting measurement error)
an alternative but similar goal is to predict \(S(s)\), which may be
called spatial filtering or smoothing.

In this chapter we will show simple approaches for handling
geostatistical data, will demonstrate simple interpolation methods,
explore modelling spatial correlation, spatial prediction and
simulation. We will use package \texttt{gstat} \citep{R-gstat, gstatcg},
which offers a fairly wide palette of models and options for
non-Bayesian geostatistical analysis. Bayesian methods with
R implementations are found in e.g.~\citet{DiggleTawnMoyeed:98},
\citet{Diggle:2007}, \citet{blangiardo2015spatial}, and \citet{wikle2019spatio}. An overview
and comparisons of methods for large datasets is given in
\citet{Heaton2018}.

\hypertarget{a-first-dataset}{%
\section{A first dataset}\label{a-first-dataset}}

We can read NO\(_2\) data, which is prepared in chapter \ref{stgeostatistics}, from
package \texttt{gstat} using

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\NormalTok{no2 =}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\KeywordTok{system.file}\NormalTok{(}\StringTok{"external/no2.csv"}\NormalTok{, }\DataTypeTok{package =} \StringTok{"gstat"}\NormalTok{))}
\CommentTok{# }
\CommentTok{# -- Column specification -----------------------------------------------------------------------------------------------------------------------------------------------}
\CommentTok{# cols(}
\CommentTok{#   .default = col_character(),}
\CommentTok{#   station_start_date = col_date(format = ""),}
\CommentTok{#   station_end_date = col_logical(),}
\CommentTok{#   station_longitude_deg = col_double(),}
\CommentTok{#   station_latitude_deg = col_double(),}
\CommentTok{#   station_altitude = col_double(),}
\CommentTok{#   lau_level1_code = col_double(),}
\CommentTok{#   lau_level2_code = col_double(),}
\CommentTok{#   NO2 = col_double()}
\CommentTok{# )}
\CommentTok{# i Use `spec()` for the full column specifications.}
\end{Highlighting}
\end{Shaded}

and convert it into an \texttt{sf} object using

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(sf)}
\NormalTok{crs =}\StringTok{ }\KeywordTok{st_crs}\NormalTok{(}\StringTok{"EPSG:32632"}\NormalTok{)}
\NormalTok{no2.sf =}\StringTok{ }\KeywordTok{st_as_sf}\NormalTok{(no2, }\DataTypeTok{coords =} \KeywordTok{c}\NormalTok{(}\StringTok{"station_longitude_deg"}\NormalTok{, }\StringTok{"station_latitude_deg"}\NormalTok{), }\DataTypeTok{crs =} \StringTok{"OGC:CRS84"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{st_transform}\NormalTok{(crs)}
\end{Highlighting}
\end{Shaded}

Next, we can load country boundaries and plot these data using \texttt{ggplot}, shown
in figure \ref{fig:plotDE}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(air, }\DataTypeTok{package =} \StringTok{"spacetime"}\NormalTok{) }\CommentTok{# this loads German boundaries into DE_NUTS1}
\NormalTok{de <-}\StringTok{ }\KeywordTok{st_transform}\NormalTok{(}\KeywordTok{st_as_sf}\NormalTok{(DE_NUTS1), crs)}
\end{Highlighting}
\end{Shaded}



\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/plotDE-1} 

}

\caption{Mean NO\(_2\) concentrations in air for rural background stations in Germany over 2017}\label{fig:plotDE}
\end{figure}

If we want to interpolate, we first need to decide where. This
is typically done on a regular grid covering the area of
interest. Starting with the country outline \texttt{de} we can create a
regular grid with 10 km grid cells (pixels) by

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# build a grid over Germany:}
\KeywordTok{library}\NormalTok{(stars)}
\KeywordTok{st_bbox}\NormalTok{(de) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{st_as_stars}\NormalTok{(}\DataTypeTok{dx =} \DecValTok{10000}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{st_crop}\NormalTok{(de) ->}\StringTok{ }\NormalTok{grd}
\NormalTok{grd}
\CommentTok{# stars object with 2 dimensions and 1 attribute}
\CommentTok{# attribute(s):}
\CommentTok{#         Min. 1st Qu. Median Mean 3rd Qu. Max. NA's}
\CommentTok{# values     0       0      0    0       0    0 2076}
\CommentTok{# dimension(s):}
\CommentTok{#   from to  offset  delta            refsys point values x/y}
\CommentTok{# x    1 65  280741  10000 WGS 84 / UTM z...    NA   NULL [x]}
\CommentTok{# y    1 87 6101239 -10000 WGS 84 / UTM z...    NA   NULL [y]}
\end{Highlighting}
\end{Shaded}

Here, we chose grid cells to be not too fine, so that we still see
them in plots.

Perhaps the simples intepolation method is inverse distance weighted
interpolation, which is a weighted average, using weights inverse
proportional to distances from the interpolation location:

\[
\hat{z}(s_0) = \frac{\sum_{i=1}^{n} w_i z(s_i)}{\sum_{i=1}^n w_i}
\]

with \(w_i = |s_0-s_i|^p\), and the inverse distance power typically
taken as 2, or optimized using cross validation. We can compute
inverse distance interpolated values using \texttt{gstat::idw},

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(gstat)}
\CommentTok{# }
\CommentTok{# Attaching package: 'gstat'}
\CommentTok{# The following object is masked from 'package:spatstat.core':}
\CommentTok{# }
\CommentTok{#     idw}
\NormalTok{i =}\StringTok{ }\KeywordTok{idw}\NormalTok{(NO2}\OperatorTok{~}\DecValTok{1}\NormalTok{, no2.sf, grd)}
\CommentTok{# [inverse distance weighted interpolation]}
\end{Highlighting}
\end{Shaded}

and plot them in figure \ref{fig:idw}.



\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_stars}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ i, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{fill =}\NormalTok{ var1.pred, }\DataTypeTok{x =}\NormalTok{ x, }\DataTypeTok{y =}\NormalTok{ y)) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{geom_sf}\NormalTok{(}\DataTypeTok{data =} \KeywordTok{st_cast}\NormalTok{(de, }\StringTok{"MULTILINESTRING"}\NormalTok{), }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{col =} \StringTok{'grey'}\NormalTok{)) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{geom_sf}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ no2.sf)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/idw-1} 

}

\caption{Inverse distance weighted interpolated values for NO\(_2\) over Germany}\label{fig:idw}
\end{figure}

\hypertarget{sample-variogram}{%
\section{Sample variogram}\label{sample-variogram}}

In order to make spatial predictions using geostatistical methods, we first need to identify a model for the mean and for the spatial correlation. In the simplest model, \(Z(s) = m + e(s)\), the mean is an unknown constant \(m\), and in this case the spatial correlation can be modelled using the variogram, \(\gamma(h) = 0.5 E (Z(s)-Z(s+h))^2\). For processes with a finite variance \(C(0)\), the variogram is related to the covariogram or covariance function through \(\gamma(h) = C(0)-C(h)\).

The sample variogram is obtained by computing estimates of \(\gamma(h)\) for distance intervals, \(h_i = [h_{i,0},h_{i,1}]\):
\[
\hat{\gamma}(h_i) = \frac{1}{2N(h_i)}\sum_{j=1}^{N(h_i)}(z(s_i)-z(s_i+h'))^2, \ \ h_{i,0} \le h' < h_{i,1}
\]

with \(N(h_i)\) the number of sample pairs available for distance interval \(h_i\).
Function \texttt{gstat::variogram} computes sample variograms,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{v =}\StringTok{ }\KeywordTok{variogram}\NormalTok{(NO2}\OperatorTok{~}\DecValTok{1}\NormalTok{, no2.sf)}
\end{Highlighting}
\end{Shaded}

and the result of plotting this is shown in figure \ref{fig:vgm}.

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/vgm-1} 

}

\caption{Sample variogram plot}\label{fig:vgm}
\end{figure}

Function \texttt{variogram} chooses default for maximum distance (\texttt{cutoff}: one third of the length of the bounding box diagonal) and (constant) interval widths (\texttt{width}: cutoff divided by 15). These defaults can be changed, e.g.~by

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(gstat)}
\NormalTok{v0 =}\StringTok{ }\KeywordTok{variogram}\NormalTok{(NO2}\OperatorTok{~}\DecValTok{1}\NormalTok{, no2.sf, }\DataTypeTok{cutoff =} \DecValTok{100000}\NormalTok{, }\DataTypeTok{width =} \DecValTok{10000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

shown in figure \ref{fig:vgm2}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(v0, }\DataTypeTok{plot.numbers =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/vgm2-1} 

}

\caption{Sample variogram plot with adjusted cutoff and lag width}\label{fig:vgm2}
\end{figure}

Note that the formula \texttt{NO2\textasciitilde{}1} is used to select the variable of interest from the data file (\texttt{NO2}), and to specify the mean model: \texttt{\textasciitilde{}1} refers to an intercept-only (unknown, constant mean) model.

\hypertarget{fitting-variogram-models}{%
\section{Fitting variogram models}\label{fitting-variogram-models}}

In order to progress toward spatial predictions, we need a variogram \emph{model} \(\gamma(h)\) for (potentially) all distances \(h\), rather than the set of estimates derived above: in case we would for instance connect these estimates with straight lines, or assume they reflect constant values over their respective distance intervals, this would lead to statisical models with non-positive covariance matrices, which is a complicated way of expressing that you are in a lot of trouble.

To avoid these troubles we fit parametric models \(\gamma(h)\) to the estimates \(\hat{\gamma}(h_i)\), where we take \(h_i\) as the mean value of all the \(h'\) values involved in estimating \(\hat{\gamma}(h_i)\). For this, when we fit a model like the exponential variogram, fitted by

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{v.m =}\StringTok{ }\KeywordTok{fit.variogram}\NormalTok{(v, }\KeywordTok{vgm}\NormalTok{(}\DecValTok{1}\NormalTok{, }\StringTok{"Exp"}\NormalTok{, }\DecValTok{50000}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

and shown in figure \ref{fig:fitvariogrammodel}.

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/fitvariogrammodel-1} 

}

\caption{Sample variogram with fitted model}\label{fig:fitvariogrammodel}
\end{figure}

The fitting is done by weighted least squares, minimizing
\(\sum_{i=1}^{n}w_i(\gamma(h_i)-\hat{\gamma}(h_i))^2\), with \(w_i\) by
default equal to \(N(h_i)/h^2\), other fitting schemes are available
through argument \texttt{fit.method}.

\hypertarget{kriging}{%
\section{Kriging interpolation}\label{kriging}}

Typically, when we interpolate a variable, we do that on
points on a regular grid covering the target area. We first create
a \texttt{stars} object with a raster covering the target area, and NA's
outside it:

Kriging involves the prediction of \(Z(s_0)\) at arbitrary locations
\(s_0\). We can krige NO\(_2\) by using \texttt{gstat::krige}, with the model for the
trend, the data, the prediction grid, and the variogram model (figure \ref{fig:krigeovergermany}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{k =}\StringTok{ }\KeywordTok{krige}\NormalTok{(NO2}\OperatorTok{~}\DecValTok{1}\NormalTok{, no2.sf, grd, v.m)}
\CommentTok{# [using ordinary kriging]}
\end{Highlighting}
\end{Shaded}



\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/krigeovergermany-1} 

}

\caption{Kriged NO\(_2\) concentrations over Germany}\label{fig:krigeovergermany}
\end{figure}

\hypertarget{blockkriging}{%
\section{Areal means: block kriging}\label{blockkriging}}

Computing areal means can be done in several ways. The simples is to take
the average of point samples falling inside the target polygons:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a =}\StringTok{ }\KeywordTok{aggregate}\NormalTok{(no2.sf[}\StringTok{"NO2"}\NormalTok{], }\DataTypeTok{by =}\NormalTok{ de, }\DataTypeTok{FUN =}\NormalTok{ mean)}
\end{Highlighting}
\end{Shaded}

A more complicated way is to use \emph{block kriging} \citep{jh78}, which
uses \emph{all} the data to estimate the mean of the variable over the
target area. With \texttt{krige}, this can be done by giving the target
areas (polygons) as the \texttt{newdata} argument:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{b =}\StringTok{ }\KeywordTok{krige}\NormalTok{(NO2}\OperatorTok{~}\DecValTok{1}\NormalTok{, no2.sf, de, v.m)}
\CommentTok{# [using ordinary kriging]}
\end{Highlighting}
\end{Shaded}

we can now merge the two maps into a single object to create a single plot (figure \ref{fig:aggregations}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{b}\OperatorTok{$}\NormalTok{sample =}\StringTok{ }\NormalTok{a}\OperatorTok{$}\NormalTok{NO2}
\NormalTok{b}\OperatorTok{$}\NormalTok{kriging =}\StringTok{ }\NormalTok{b}\OperatorTok{$}\NormalTok{var1.pred}
\end{Highlighting}
\end{Shaded}



\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/aggregations-1} 

}

\caption{Aggregated NO\(_2\) values from simple averaging (left) and block kriging (right)}\label{fig:aggregations}
\end{figure}

We see that the signal is similar, but that the simple means are
more variable than the block kriging values; this may be due to
the smoothing effect of kriging: data points outside the target
area are weighted, too.

To compare the standard errors of means, for the sample mean we
can get a rough guess of the standard error by \(\sqrt{(\sigma^2/n)}\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SE =}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{var}\NormalTok{(x)}\OperatorTok{/}\KeywordTok{length}\NormalTok{(x))}
\NormalTok{a =}\StringTok{ }\KeywordTok{aggregate}\NormalTok{(no2.sf[}\StringTok{"NO2"}\NormalTok{], de, SE)}
\end{Highlighting}
\end{Shaded}

which would have been the actual estimate in design-based inference
if the sample was obtained by spatially random sampling.
The block kriging variance is the model-based estimate, and is
a by-product of kriging. We combine and rename the two:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{b}\OperatorTok{$}\NormalTok{sample =}\StringTok{ }\NormalTok{a}\OperatorTok{$}\NormalTok{NO2}
\NormalTok{b}\OperatorTok{$}\NormalTok{kriging =}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(b}\OperatorTok{$}\NormalTok{var1.var)}
\end{Highlighting}
\end{Shaded}



\texttt{\{r\ aggrSE,fig.cap\ =\ \textquotesingle{}Standard errors for mean NO\(_2\) values obtained by simple averaging (left) and block kriging (right)\textquotesingle{},\ b\ \%\textgreater{}\%\ select(sample,\ kriging)\ \%\textgreater{}\%\ \ \ \ \ \ \ \ \ \ pivot\_longer(1:2,\ names\_to\ =\ "var",\ values\_to\ =\ "NO2")\ -\textgreater{}\ b2\ ggplot()\ +\ geom\_sf(data\ =\ b2,\ mapping\ =\ aes(fill\ =\ NO2))\ +\ facet\_wrap(\textasciitilde{}var)\ +\ \ \ \ \ \ scale\_fill\_gradientn(colors\ =\ sf.colors(20))}
where we see that the simple approach gives clearly more variability
and mostly larger values for prediction errors of areal means,
compared to block kriging.

\hypertarget{conditional-simulation}{%
\section{Conditional simulation}\label{conditional-simulation}}

In case one or more conditional realisation of the field \(Z(s)\)
are needed rather than their conditional mean, we can obtain this
by \emph{conditional simulation}. A reason for wanting this may be the
need to estimate areal mean values of \(g(Z(s))\) with \(g(\cdot)\)
a non-linear function; a simple example is the areal fraction where
\(Z(s)\) exceeds a threshold.

The standard approach used by \texttt{gstat} is to use the sequential
simulation algorithm for this. This is a simple algorithm that randomly
steps through the prediction locations and at each location:

\begin{itemize}
\tightlist
\item
  carries out a kriging prediction
\item
  draws a random variable from the normal distribution with mean and variance equal to the kriging variance
\item
  adds this value to the conditioning dataset
\item
  finds a new random simulation location
\end{itemize}

until all locations have been visited.

This is caried out by \texttt{gstat::krige} when \texttt{nsim} is set to a positive
value. In addition, it is good to constrain \texttt{nmax}, the (maximum)
number of nearest neigbours to include in kriging estimation,
because the dataset grows each step, leading otherwise quickly to
very long computing times and large memory requirements (figure
\ref{fig:plotkrigingvalues}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{s =}\StringTok{ }\KeywordTok{krige}\NormalTok{(NO2}\OperatorTok{~}\DecValTok{1}\NormalTok{, no2.sf, grd, v.m, }\DataTypeTok{nmax =} \DecValTok{30}\NormalTok{, }\DataTypeTok{nsim =} \DecValTok{10}\NormalTok{)}
\CommentTok{# drawing 10 GLS realisations of beta...}
\CommentTok{# [using conditional Gaussian simulation]}
\end{Highlighting}
\end{Shaded}



\begin{verbatim}
# Loading required package: viridisLite
# 
# Attaching package: 'viridis'
# The following object is masked from 'package:scales':
# 
#     viridis_pal
# The following object is masked from 'package:maps':
# 
#     unemp
\end{verbatim}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/plotkrigingvalues-1} 

}

\caption{Ten conditional simulations for NO\(_2\) values}\label{fig:plotkrigingvalues}
\end{figure}

Alternative methods for conditional simulation have recently been
added to \texttt{gstat}, and include \texttt{krigeSimCE} implementing the circular
embedding method \citep{JSSv055i09}, and \texttt{krigeSTSimTB} implementing
the turning bands method \citep{schlather}. These are of particular
of interest for larger datasets or conditional simulations of
spatiotemporal data.

\hypertarget{trend-models}{%
\section{Trend models}\label{trend-models}}

Kriging and conditional simulation, as used so far in this
chapter, assume that all spatial variability is a random process,
characterized by a spatial covariance model. In case we have other
variables that are meaningfully correlated with the target variable,
we can use them in a linear regression model for the trend,
\[
Z(s) = \sum_{j=0}^p \beta_j X_p(s) + e(s)
\]
with \(X_0(s) = 1\) and \(\beta_0\) an intercept, but with the other
\(\beta_j\) regression coefficients. This typically reduces both the
spatial correlation in the residual \(e(s)\), as well as its variance,
and leads to more accurate predictions and more similar conditional
simulations.

\hypertarget{a-population-grid}{%
\subsection{A population grid}\label{a-population-grid}}

As a potential predictor for NO2 in the air, we use population
density. NO2 is mostly caused by traffic, and traffic is stronger
in more densely populated areas.
Population density is obtained from the \href{https://www.zensus2011.de/DE/Home/Aktuelles/DemografischeGrunddaten.html}{2011 census}, and is downloaded as a csv file with the number of inhabitants per 100 m grid cell. We can aggregate these data to the target grid cells by summing the inhabitants:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{v =}\StringTok{ }\NormalTok{vroom}\OperatorTok{::}\KeywordTok{vroom}\NormalTok{(}\StringTok{"aq/pop/Zensus_Bevoelkerung_100m-Gitter.csv"}\NormalTok{)}
\CommentTok{# Rows: 35785840 Columns: 4}
\CommentTok{# -- Column specification -----------------------------------------------------------------------------------------------------------------------------------------------}
\CommentTok{# Delimiter: ";"}
\CommentTok{# chr (1): Gitter_ID_100m}
\CommentTok{# dbl (3): x_mp_100m, y_mp_100m, Einwohner}
\CommentTok{# }
\CommentTok{# i Use `spec()` to retrieve the full column specification for this data.}
\CommentTok{# i Specify the column types or set `show_col_types = FALSE` to quiet this message.}
\NormalTok{v }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(Einwohner }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{Gitter_ID_100m) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{st_as_sf}\NormalTok{(}\DataTypeTok{coords =} \KeywordTok{c}\NormalTok{(}\StringTok{"x_mp_100m"}\NormalTok{, }\StringTok{"y_mp_100m"}\NormalTok{), }\DataTypeTok{crs =} \DecValTok{3035}\NormalTok{) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{st_transform}\NormalTok{(}\KeywordTok{st_crs}\NormalTok{(grd)) ->}\StringTok{ }\NormalTok{b}
\NormalTok{a =}\StringTok{ }\KeywordTok{aggregate}\NormalTok{(b, }\KeywordTok{st_as_sf}\NormalTok{(grd, }\DataTypeTok{na.rm =} \OtherTok{FALSE}\NormalTok{), sum)}
\end{Highlighting}
\end{Shaded}

Now we have the population counts per grid cell in \texttt{a}. To get to
population density, we need to find the area of each cell; for cells
crossing the country border, this will be less than 10 x 10 km:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grd}\OperatorTok{$}\NormalTok{ID =}\StringTok{ }\DecValTok{1}\OperatorTok{:}\KeywordTok{prod}\NormalTok{(}\KeywordTok{dim}\NormalTok{(grd)) }\CommentTok{# so we can find out later which grid cell we have}
\NormalTok{ii =}\StringTok{ }\KeywordTok{st_intersects}\NormalTok{(grd[}\StringTok{"ID"}\NormalTok{], }\KeywordTok{st_cast}\NormalTok{(}\KeywordTok{st_union}\NormalTok{(de), }\StringTok{"MULTILINESTRING"}\NormalTok{))}
\CommentTok{# Warning in st_intersects.stars(grd["ID"], st_cast(st_union(de),}
\CommentTok{# "MULTILINESTRING")): as_points is NA: assuming here that raster}
\CommentTok{# cells are small polygons, not points}
\NormalTok{grd_sf =}\StringTok{ }\KeywordTok{st_as_sf}\NormalTok{(grd[}\StringTok{"ID"}\NormalTok{], }\DataTypeTok{na.rm =} \OtherTok{FALSE}\NormalTok{)[}\KeywordTok{lengths}\NormalTok{(ii) }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{,]}
\NormalTok{iii =}\StringTok{ }\KeywordTok{st_intersection}\NormalTok{(grd_sf, }\KeywordTok{st_union}\NormalTok{(de))}
\CommentTok{# Warning: attribute variables are assumed to be spatially constant}
\CommentTok{# throughout all geometries}
\NormalTok{grd}\OperatorTok{$}\NormalTok{area =}\StringTok{ }\KeywordTok{st_area}\NormalTok{(grd)[[}\DecValTok{1}\NormalTok{]] }\OperatorTok{+}\StringTok{ }\NormalTok{units}\OperatorTok{::}\KeywordTok{set_units}\NormalTok{(grd}\OperatorTok{$}\NormalTok{values, m}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\CommentTok{# NA's}
\NormalTok{grd}\OperatorTok{$}\NormalTok{area[iii}\OperatorTok{$}\NormalTok{ID] =}\StringTok{ }\KeywordTok{st_area}\NormalTok{(iii)}
\end{Highlighting}
\end{Shaded}

Instead of doing the two-stage procedure above: first finding cells that
have a border crossing it, then computing its area, we could also directly
use \texttt{st\_intersection} on all cells, but that takes considerably longer.
From the counts and areas we can compute densities, and verify totals (figure \ref{fig:popdens}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grd}\OperatorTok{$}\NormalTok{pop_dens =}\StringTok{ }\NormalTok{a}\OperatorTok{$}\NormalTok{Einwohner }\OperatorTok{/}\StringTok{ }\NormalTok{grd}\OperatorTok{$}\NormalTok{area}
\KeywordTok{sum}\NormalTok{(grd}\OperatorTok{$}\NormalTok{pop_dens }\OperatorTok{*}\StringTok{ }\NormalTok{grd}\OperatorTok{$}\NormalTok{area, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{) }\CommentTok{# verify}
\CommentTok{# 80323301 [1]}
\KeywordTok{sum}\NormalTok{(b}\OperatorTok{$}\NormalTok{Einwohner)}
\CommentTok{# [1] 80324282}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/popdens-1} 

}

\caption{Population density for 100 m x 100 m grid cells}\label{fig:popdens}
\end{figure}

We need to divide the number of inhabitants by the number of 100
m grid points contributing to it, in order to convert population
counts into population density.

To obtain population density values at monitoring network stations,
we can use

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\DataTypeTok{a =} \KeywordTok{aggregate}\NormalTok{(grd[}\StringTok{"pop_dens"}\NormalTok{], no2.sf, mean))}
\CommentTok{# stars object with 1 dimensions and 1 attribute}
\CommentTok{# attribute(s):}
\CommentTok{#               Min.  1st Qu.   Median     Mean  3rd Qu.    Max.}
\CommentTok{# pop_dens  3.37e-06 4.98e-05 8.93e-05 0.000195 0.000237 0.00224}
\CommentTok{#           NA's}
\CommentTok{# pop_dens     1}
\CommentTok{# dimension(s):}
\CommentTok{#          from to offset delta            refsys point}
\CommentTok{# geometry    1 74     NA    NA WGS 84 / UTM z...  TRUE}
\CommentTok{#                                                     values}
\CommentTok{# geometry POINT (545414 5930802),...,POINT (835252 5630738)}
\NormalTok{no2.sf}\OperatorTok{$}\NormalTok{pop_dens =}\StringTok{ }\KeywordTok{st_as_sf}\NormalTok{(a)[[}\DecValTok{1}\NormalTok{]]}
\KeywordTok{summary}\NormalTok{(}\KeywordTok{lm}\NormalTok{(NO2}\OperatorTok{~}\KeywordTok{sqrt}\NormalTok{(pop_dens), no2.sf))}
\CommentTok{# }
\CommentTok{# Call:}
\CommentTok{# lm(formula = NO2 ~ sqrt(pop_dens), data = no2.sf)}
\CommentTok{# }
\CommentTok{# Residuals:}
\CommentTok{#    Min     1Q Median     3Q    Max }
\CommentTok{#  -7.96  -2.15  -0.50   1.60   8.10 }
\CommentTok{# }
\CommentTok{# Coefficients:}
\CommentTok{#                Estimate Std. Error t value Pr(>|t|)    }
\CommentTok{# (Intercept)       4.561      0.697    6.54  8.0e-09 ***}
\CommentTok{# sqrt(pop_dens)  325.006     49.927    6.51  9.2e-09 ***}
\CommentTok{# ---}
\CommentTok{# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1}
\CommentTok{# }
\CommentTok{# Residual standard error: 3.15 on 71 degrees of freedom}
\CommentTok{#   (1 observation deleted due to missingness)}
\CommentTok{# Multiple R-squared:  0.374,   Adjusted R-squared:  0.365 }
\CommentTok{# F-statistic: 42.4 on 1 and 71 DF,  p-value: 9.19e-09}
\end{Highlighting}
\end{Shaded}

and the corresponding scatterplot is shown in \ref{fig:no2scat}.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{sds_files/figure-latex/no2scat-1} 

}

\caption{Scatter plot of 2017 annual mean NO2 concentration against population density, for rural background air quality stations}\label{fig:no2scat}
\end{figure}

Prediction under this new model involves first modelling a residual
variogram (figure \ref{fig:predictusingpopulationdensity}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{no2.sf =}\StringTok{ }\NormalTok{no2.sf[}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(no2.sf}\OperatorTok{$}\NormalTok{pop_dens),]}
\NormalTok{vr =}\StringTok{ }\KeywordTok{variogram}\NormalTok{(NO2}\OperatorTok{~}\KeywordTok{sqrt}\NormalTok{(pop_dens), no2.sf)}
\NormalTok{vr.m =}\StringTok{ }\KeywordTok{fit.variogram}\NormalTok{(vr, }\KeywordTok{vgm}\NormalTok{(}\DecValTok{1}\NormalTok{, }\StringTok{"Exp"}\NormalTok{, }\DecValTok{50000}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/predictusingpopulationdensity-1} 

}

\caption{Residual variogram after subtracting population density trend}\label{fig:predictusingpopulationdensity}
\end{figure}

and subsequently, kriging prediction is done by (figure \ref{fig:residualkriging})

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kr =}\StringTok{ }\KeywordTok{krige}\NormalTok{(NO2}\OperatorTok{~}\KeywordTok{sqrt}\NormalTok{(pop_dens), no2.sf, grd[}\StringTok{"pop_dens"}\NormalTok{], vr.m)}
\CommentTok{# [using universal kriging]}
\NormalTok{k}\OperatorTok{$}\NormalTok{kr1 =}\StringTok{ }\NormalTok{k}\OperatorTok{$}\NormalTok{var1.pred}
\NormalTok{k}\OperatorTok{$}\NormalTok{kr2 =}\StringTok{ }\NormalTok{kr}\OperatorTok{$}\NormalTok{var1.pred}
\KeywordTok{st_redimension}\NormalTok{(k[}\KeywordTok{c}\NormalTok{(}\StringTok{"kr1"}\NormalTok{, }\StringTok{"kr2"}\NormalTok{)], }
    \DataTypeTok{along =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{what =} \KeywordTok{c}\NormalTok{(}\StringTok{"kriging"}\NormalTok{, }\StringTok{"residual kriging"}\NormalTok{))) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{setNames}\NormalTok{(}\StringTok{"NO2"}\NormalTok{) ->}\StringTok{ }\NormalTok{km}
\end{Highlighting}
\end{Shaded}



\begin{verbatim}
# Coordinate system already present. Adding new coordinate system, which will replace the existing one.
\end{verbatim}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/residualkriging-1} 

}

\caption{Kriging NO\(_2\) values using population density as a trend variable}\label{fig:residualkriging}
\end{figure}

where, critically, the \texttt{pop\_dens} values are now available for
prediction locations in object \texttt{grd}. We see some clear differences:
the map with population density in the trend follows the extremes of
the population density rather than those of the measurement stations,
and has a range that extends that of the former. It should be taken
with a large grain of salt however, since the stations used were
filtered for the category ``rural background'', indicating that they
represent conditions of lower populations density. The scatter plot
of Figure \ref{fig:no2scat} reveals that the the population density
at the locations of stations is much more limited than that in the
population density map, and hence the right-hand side map is based on
strongly extrapolating the relationship shown in \ref{fig:no2scat}.

\hypertarget{exercises-10}{%
\section{Exercises}\label{exercises-10}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create a plot like the one in figure @ref(fig:residualkriging\}
  that has the inverse distance interpolated map of figure
  \ref{fig:idw} added on left side.
\item
  Create a scatter plot of the map values of the idw and kriging
  map, and a scatter plot of map values of idw and residual kriging.
\item
  Carry out a \emph{block kriging} by setting the \texttt{block} argument in
  \texttt{krige()}, and do this for block sizes of 10 km (the grid cell size),
  50 km and 200 km. Compare the resulting maps of estimates, and
  resulting map of kriging standard errors.
\item
  Based on the residual kriging results obtained above, compute
  maps of the lower and upper boundary of a 95\% confidence interval,
  when assuming that the kriging error is normally distributed,
  and shown them in a plot with a single (joint) legend.
\end{enumerate}

\hypertarget{stgeostatistics}{%
\chapter{Multivariate and Spatiotemporal Geostatistics}\label{stgeostatistics}}

Building on the simple interpolation methods presented in chapter
\ref{interpolation}, this chapter works out a case study for
spatiotemporal interpolation, using NO\(_2\) air quality data, and
populations density as covariate.

\hypertarget{preparing}{%
\section{Preparing the air quality dataset}\label{preparing}}

The dataset we work with is an air quality dataset obtained from
the European Environmental Agency (EEA). European member states report
air quality measurements to this Agency. So-called \emph{validated}
data are quality controlled by member states, and are reported on a
yearly basis. They form the basis for policy compliancy evaluations.

The EEA's \href{https://www.eea.europa.eu/data-and-maps/data/aqereporting-8}{air quality
e-reporting}
website gives access to the data reported by European member states.
We decided to download hourly (time series) data, which is the data primarily measured.
A web form helps convert simple selection criteria into an http GET request. The following URL

\begin{verbatim}
https://fme.discomap.eea.europa.eu/fmedatastreaming/AirQualityDownload/AQData_Extract.fmw?CountryCode=DE&CityName=&Pollutant=8&Year_from=2017&Year_to=2017&Station=&Samplingpoint=&Source=E1a&Output=TEXT&UpdateDate=
\end{verbatim}

was created to select all validated (\texttt{Source=E1a}) \(NO_2\)
(\texttt{Pollutant=8}) data for 2017 (\texttt{Year\_from}, \texttt{Year\_to}) from Germany
(\texttt{CountryCode=DE}). It returns a text file with a set of URLs to CSV
files, each containing the hourly values for the whole period for a
single measurement station. These files were downloaded and converted
to the right encoding using the \texttt{dos2unix} command line utility.

In the following, we will read all the files into a list,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{files =}\StringTok{ }\KeywordTok{list.files}\NormalTok{(}\StringTok{"aq"}\NormalTok{, }\DataTypeTok{pattern =} \StringTok{"*.csv"}\NormalTok{, }\DataTypeTok{full.names =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{r =}\StringTok{ }\KeywordTok{lapply}\NormalTok{(files[}\OperatorTok{-}\DecValTok{1}\NormalTok{], }\ControlFlowTok{function}\NormalTok{(f) }\KeywordTok{read.csv}\NormalTok{(f))}
\end{Highlighting}
\end{Shaded}

then convert the time variable into a \texttt{POSIXct} variable, and put them in time order by

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{Sys.setenv}\NormalTok{(}\DataTypeTok{TZ =} \StringTok{"UTC"}\NormalTok{) }\CommentTok{# make sure times are not interpreted in local time zone}
\NormalTok{r =}\StringTok{ }\KeywordTok{lapply}\NormalTok{(r, }\ControlFlowTok{function}\NormalTok{(f) \{}
\NormalTok{        f}\OperatorTok{$}\NormalTok{t =}\StringTok{ }\KeywordTok{as.POSIXct}\NormalTok{(f}\OperatorTok{$}\NormalTok{DatetimeBegin) }
\NormalTok{        f[}\KeywordTok{order}\NormalTok{(f}\OperatorTok{$}\NormalTok{t), ] }
\NormalTok{    \}}
\NormalTok{) }
\end{Highlighting}
\end{Shaded}

We remove smaller datasets, which for this dataset have no hourly data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{r =}\StringTok{ }\NormalTok{r[}\KeywordTok{sapply}\NormalTok{(r, nrow) }\OperatorTok{>}\StringTok{ }\DecValTok{1000}\NormalTok{]}
\KeywordTok{names}\NormalTok{(r) =}\StringTok{  }\KeywordTok{sapply}\NormalTok{(r, }\ControlFlowTok{function}\NormalTok{(f) }\KeywordTok{unique}\NormalTok{(f}\OperatorTok{$}\NormalTok{AirQualityStationEoICode))}
\KeywordTok{length}\NormalTok{(r) }\OperatorTok{==}\StringTok{ }\KeywordTok{length}\NormalTok{(}\KeywordTok{unique}\NormalTok{(}\KeywordTok{names}\NormalTok{(r)))}
\CommentTok{# [1] TRUE}
\end{Highlighting}
\end{Shaded}

and then combine all files using \texttt{xts::cbind}, so that they are matched based on matching times:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(xts)}
\NormalTok{r =}\StringTok{ }\KeywordTok{lapply}\NormalTok{(r, }\ControlFlowTok{function}\NormalTok{(f) }\KeywordTok{xts}\NormalTok{(f}\OperatorTok{$}\NormalTok{Concentration, f}\OperatorTok{$}\NormalTok{t))}
\NormalTok{aq =}\StringTok{ }\KeywordTok{do.call}\NormalTok{(cbind, r)}
\end{Highlighting}
\end{Shaded}

A usual further selection for this dataset is to select stations
for which 75\% of the hourly values measured are valid, i.e.~drop
those with more than 25\% hourly values missing.
Knowing that \texttt{mean(is.na(x))} gives the \emph{fraction} of missing values
in a vector \texttt{x}, we can apply this function to the columns (stations):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sel =}\StringTok{ }\KeywordTok{apply}\NormalTok{(aq, }\DecValTok{2}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{mean}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(x)) }\OperatorTok{<}\StringTok{ }\FloatTok{0.25}\NormalTok{)}
\NormalTok{aqsel =}\StringTok{ }\NormalTok{aq[, sel]}
\end{Highlighting}
\end{Shaded}

Next, the \href{http://ftp.eea.europa.eu/www/AirBase_v8/AirBase_v8_stations.zip}{station metadata} was read
and filtered for rural background stations in Germany (\texttt{"DE"}) by

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{read.csv}\NormalTok{(}\StringTok{"aq/AirBase_v8_stations.csv"}\NormalTok{, }\DataTypeTok{sep =} \StringTok{"}\CharTok{\textbackslash{}t}\StringTok{"}\NormalTok{, }\DataTypeTok{stringsAsFactors =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{as_tibble}\NormalTok{()  }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{filter}\NormalTok{(country_iso_code }\OperatorTok{==}\StringTok{ "DE"}\NormalTok{, station_type_of_area }\OperatorTok{==}\StringTok{ "rural"}\NormalTok{, }
\NormalTok{                 type_of_station }\OperatorTok{==}\StringTok{ "Background"}\NormalTok{) ->}\StringTok{ }\NormalTok{a2}
\end{Highlighting}
\end{Shaded}

These stations contain coordinates, and an \texttt{sf} object with (static) station metadata is created by

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(sf)}
\NormalTok{a2.sf =}\StringTok{ }\KeywordTok{st_as_sf}\NormalTok{(a2, }\DataTypeTok{coords =} \KeywordTok{c}\NormalTok{(}\StringTok{"station_longitude_deg"}\NormalTok{, }\StringTok{"station_latitude_deg"}\NormalTok{), }\DataTypeTok{crs =} \StringTok{'EPSG:4326'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We now subset the air quality measurements to include only stations that are of type rural background:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sel =}\StringTok{  }\KeywordTok{colnames}\NormalTok{(aqsel) }\OperatorTok{%in%}\StringTok{ }\NormalTok{a2}\OperatorTok{$}\NormalTok{station_european_code}
\NormalTok{aqsel =}\StringTok{ }\NormalTok{aqsel[, sel]}
\end{Highlighting}
\end{Shaded}

We can compute station means, and join these to stations locations by

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tb =}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{NO2 =} \KeywordTok{apply}\NormalTok{(aqsel, }\DecValTok{2}\NormalTok{, mean, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{), }\DataTypeTok{station_european_code =} \KeywordTok{colnames}\NormalTok{(aqsel))}
\NormalTok{crs =}\StringTok{ }\DecValTok{32632}
\KeywordTok{right_join}\NormalTok{(a2.sf, tb) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{st_transform}\NormalTok{(crs) ->}\StringTok{ }\NormalTok{no2.sf }
\CommentTok{# Joining, by = "station_european_code"}
\end{Highlighting}
\end{Shaded}

Station mean NO\(_2\) concentrations, along with country borders, are shown in in figure \ref{fig:plotDE}.

\hypertarget{cokriging}{%
\section{Multivariable geostatistics}\label{cokriging}}

Multivariable geostatics involves the \emph{joint} modelling, prediction
and simulation of multiple variables,
\[Z_1(s) = X_1 \beta_1 + e_1(s)\]
\[...\]
\[Z_n(s) = X_n \beta_n + e_n(s).\]
In addition to having observations, trend models, and variograms
for each variable, the \emph{cross} variogram for each pair of residual
variables, describing the covariance of \(e_i(s), e_j(s+h)\),
is required. If this cross covariance is non-zero, knowledge of
\(e_j(s+h)\) may help predict (or simulate) \(e_i(s)\). This is
especially true if \(Z_j(s)\) is more densely sample than \(Z_i(s)\).
Prediction and simulation under this model are called cokriging
and cosimulation. Examples using gstat are found when running the
demo scripts

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(gstat)}
\KeywordTok{demo}\NormalTok{(cokriging)}
\KeywordTok{demo}\NormalTok{(cosimulation)}
\end{Highlighting}
\end{Shaded}

and are further illustrated and discussed in \citep{asdar}.

In case the different variables considered are observed at
the same set of locations, for instance different air quality
parameters, then the statistical \emph{gain} of using cokriging
as opposed to direct (univariable) kriging is often modest,
when not negligible. A gain may however be that the prediction
is truly multivariable: in addition to the prediction vector
\(\hat{Z(s_0)}=(\hat{Z}_1(s_0),...,\hat{Z}_n(s_0))\) we get the full
covariance matrix of the prediction error \citep{ver1993multivariable}.
This means for instance that if we are interested in some
linear combination of \(\hat{Z}(s_0)\), such as \(\hat{Z}_2(s_0) - \hat{Z}_1(s_0)\), that we can get the standard error of that
combination because we have the correlations between the prediction
errors.

Although sets of direct and cross variograms can be computed
and fitted automatically, multivariable geostatistical modelling
becomes quickly hard to manage when the number of variables gets
large, because the number of direct and cross variograms required
is \(n(n+1)/2\).

In case different variables refer to the same variable take at
different time steps, one could use a multivariable (cokriging)
prediction approach, but this would not allow for e.g.~interpolation
between two time steps. For this, and for handling the case of
having data observed at many time instances, one can also model
its variation as a function of continuous space \emph{and} time, as of
\(Z(s,t)\), which we will do in the next section.

\hypertarget{spatiotemporal-geostatistics}{%
\section{Spatiotemporal geostatistics}\label{spatiotemporal-geostatistics}}

Spatiotemporal geostatistical processes are modelled as variables
having a value everywhere in space and time, \(Z(s,t)\), with \(s\) and
\(t\) the continuously indext space and time index. Given observations
\(Z(s_i,t_j)\) and a variogram (covariance) model \(\gamma(s,t)\) we can
predict \(Z(s_0,t_0)\) at arbitrary space/time locations \((s_0,t_0)\)
using standard Gaussian process theory.

Several books have been written recently about modern approaches
to handling and modelling spatiotemporal geostatistical data,
including \citep{wikle2019spatio} and \citep{blangiardo2015spatial}. Here,
we will use \citep{RJ-2016-014} and give some simple examples using the
dataset also used for the previous chapter.

\hypertarget{a-spatiotemporal-variogram-model}{%
\subsection{A spatiotemporal variogram model}\label{a-spatiotemporal-variogram-model}}

Starting with the spatiotemporal matrix of NO\(_2\) data in \texttt{aq}
constructed at the beginning of this chapter, we will first select
the measurements taken at rural background stations:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{aqx =}\StringTok{ }\NormalTok{aq[ , }\KeywordTok{colnames}\NormalTok{(aq) }\OperatorTok{%in%}\StringTok{ }\NormalTok{a2}\OperatorTok{$}\NormalTok{station_european_code]}
\end{Highlighting}
\end{Shaded}

Then we will select the spatial locations for these stations by

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sfc =}\StringTok{ }\KeywordTok{st_geometry}\NormalTok{(a2.sf)[}\KeywordTok{match}\NormalTok{(}\KeywordTok{colnames}\NormalTok{(aqx), a2.sf}\OperatorTok{$}\NormalTok{station_european_code)]}
\end{Highlighting}
\end{Shaded}

and finally build a \texttt{stars} object with time and station as dimensions:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(stars)}
\KeywordTok{st_as_stars}\NormalTok{(}\DataTypeTok{NO2 =} \KeywordTok{as.matrix}\NormalTok{(aqx)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{st_set_dimensions}\NormalTok{(}\DataTypeTok{names =} \KeywordTok{c}\NormalTok{(}\StringTok{"time"}\NormalTok{, }\StringTok{"station"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{st_set_dimensions}\NormalTok{(}\StringTok{"time"}\NormalTok{, }\KeywordTok{index}\NormalTok{(aqx)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{st_set_dimensions}\NormalTok{(}\StringTok{"station"}\NormalTok{, sfc) ->}\StringTok{ }\NormalTok{no2.st}
\end{Highlighting}
\end{Shaded}

From this, we can compute the spatiotemporal variogram using

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{v.st =}\StringTok{ }\KeywordTok{variogramST}\NormalTok{(NO2}\OperatorTok{~}\DecValTok{1}\NormalTok{, no2.st[,}\DecValTok{1}\OperatorTok{:}\NormalTok{(}\DecValTok{24}\OperatorTok{*}\DecValTok{31}\NormalTok{)], }\DataTypeTok{tlags =} \DecValTok{0}\OperatorTok{:}\DecValTok{48}\NormalTok{, }
    \DataTypeTok{cores =} \KeywordTok{getOption}\NormalTok{(}\StringTok{"mc.cores"}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}



which is shown in figure \ref{fig:plotvariograms}.

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/plotvariograms-1} 

}

\caption{Spatiotemporal sample variogram for hourly NO\(_2\) concentrations at rural background stations in Germany over 2027}\label{fig:plotvariograms}
\end{figure}

To this sample variogram, we can fit a variogram model. One relatively
flexible model we try here is the product-sum model \citep{RJ-2016-014}, fitted by

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# product-sum}
\NormalTok{prodSumModel <-}\StringTok{ }\KeywordTok{vgmST}\NormalTok{(}\StringTok{"productSum"}\NormalTok{,}
    \DataTypeTok{space=}\KeywordTok{vgm}\NormalTok{(}\DecValTok{150}\NormalTok{, }\StringTok{"Exp"}\NormalTok{, }\DecValTok{200}\NormalTok{, }\DecValTok{0}\NormalTok{),}
    \DataTypeTok{time=} \KeywordTok{vgm}\NormalTok{(}\DecValTok{20}\NormalTok{, }\StringTok{"Sph"}\NormalTok{,   }\DecValTok{40}\NormalTok{, }\DecValTok{0}\NormalTok{),}
    \DataTypeTok{k=}\DecValTok{2}\NormalTok{)}
\NormalTok{StAni =}\StringTok{ }\KeywordTok{estiStAni}\NormalTok{(v.st, }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{200000}\NormalTok{))}
\NormalTok{(fitProdSumModel <-}\StringTok{ }\KeywordTok{fit.StVariogram}\NormalTok{(v.st, prodSumModel, }\DataTypeTok{fit.method =} \DecValTok{7}\NormalTok{,}
    \DataTypeTok{stAni =}\NormalTok{ StAni, }\DataTypeTok{method =} \StringTok{"L-BFGS-B"}\NormalTok{,}
    \DataTypeTok{control =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{parscale =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{10}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\FloatTok{0.1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{10}\NormalTok{)),}
    \DataTypeTok{lower =} \KeywordTok{rep}\NormalTok{(}\FloatTok{0.0001}\NormalTok{, }\DecValTok{7}\NormalTok{)))}
\CommentTok{# space component: }
\CommentTok{#   model psill range}
\CommentTok{# 1   Nug  26.3     0}
\CommentTok{# 2   Exp 140.5   432}
\CommentTok{# time component: }
\CommentTok{#   model psill range}
\CommentTok{# 1   Nug  1.21   0.0}
\CommentTok{# 2   Sph 15.99  40.1}
\CommentTok{# k: 0.0322469094848839}
\end{Highlighting}
\end{Shaded}

and shown in figure \ref{fig:prodsummodelplot}.

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/prodsummodelplot-1} 

}

\caption{Product-sum model, fitted to the spatiotemporal sample variogram}\label{fig:prodsummodelplot}
\end{figure}

which can also be plotted as wireframes, shown in figure \ref{fig:modelwire}.

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/modelwire-1} 

}

\caption{Wireframe plot of the fitted spatiotemporal variogram model}\label{fig:modelwire}
\end{figure}

Hints about the fitting strategy and alternative models for
spatiotemporal variograms are given in \citep{RJ-2016-014}.

With this fitted model, and given the observations, we can
carry out kriging or simulation at arbitrary points in space and
time. For instance, we could estimate (or simulate) values in the
time series that are now missing: this occurs regularly, and in
section \ref{kriging} we used means over time series based on
simply ignoring up to 25\% of the observations: substituting these
with estimated or simulated values based on neigbouring (in space
and time) observations before computing yearly mean values seems
a more reasonable approach.

More in general, we can estimate at arbitrary locations and time
points, and we will illustrate this with predicting time series at
particular locations, and and predicting spatial slices \citep{RJ-2016-014}.
We can create
a \texttt{stars} object for two randomly picked spatial points and all time instances
is created by

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pt =}\StringTok{ }\KeywordTok{st_sample}\NormalTok{(de, }\DecValTok{2}\NormalTok{)}
\NormalTok{t =}\StringTok{ }\KeywordTok{st_get_dimension_values}\NormalTok{(no2.st, }\DecValTok{1}\NormalTok{)}
\KeywordTok{st_as_stars}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DataTypeTok{pts =} \KeywordTok{matrix}\NormalTok{(}\DecValTok{1}\NormalTok{, }\KeywordTok{length}\NormalTok{(t), }\KeywordTok{length}\NormalTok{(pt)))) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{st_set_dimensions}\NormalTok{(}\DataTypeTok{names =} \KeywordTok{c}\NormalTok{(}\StringTok{"time"}\NormalTok{, }\StringTok{"station"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{st_set_dimensions}\NormalTok{(}\StringTok{"time"}\NormalTok{, t) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{st_set_dimensions}\NormalTok{(}\StringTok{"station"}\NormalTok{, pt) ->}\StringTok{ }\NormalTok{new_pt}
\end{Highlighting}
\end{Shaded}

and we obtain the spatiotemporal predictions at these two points using \texttt{krigeST} by

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{no2.st <-}\StringTok{ }\KeywordTok{st_transform}\NormalTok{(no2.st, crs)}
\NormalTok{new_ts <-}\StringTok{ }\KeywordTok{krigeST}\NormalTok{(NO2}\OperatorTok{~}\DecValTok{1}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ no2.st[}\StringTok{"NO2"}\NormalTok{], }\DataTypeTok{newdata =}\NormalTok{ new_pt,}
         \DataTypeTok{nmax =} \DecValTok{50}\NormalTok{, }\DataTypeTok{stAni =}\NormalTok{ StAni, }\DataTypeTok{modelList =}\NormalTok{ fitProdSumModel,}
         \DataTypeTok{progress =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

where the results are shown in figure \ref{fig:plotxts}.

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/plotxts-1} 

}

\caption{Time series plot of spatiotemporal predictions for two points}\label{fig:plotxts}
\end{figure}

Alternatively, we can create spatiotemporal predictions for a set of time-stamped
raster maps, evenly spaced over the year 2017, created by

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t4 =}\StringTok{ }\NormalTok{t[(}\DecValTok{1}\OperatorTok{:}\DecValTok{4} \OperatorTok{-}\StringTok{ }\FloatTok{0.5}\NormalTok{) }\OperatorTok{*}\StringTok{ }\NormalTok{(}\DecValTok{3}\OperatorTok{*}\DecValTok{24}\OperatorTok{*}\DecValTok{30}\NormalTok{)]}
\NormalTok{d =}\StringTok{ }\KeywordTok{dim}\NormalTok{(grd)}
\KeywordTok{st_as_stars}\NormalTok{(}\DataTypeTok{pts =} \KeywordTok{array}\NormalTok{(}\DecValTok{1}\NormalTok{, }\KeywordTok{c}\NormalTok{(d[}\DecValTok{1}\NormalTok{], d[}\DecValTok{2}\NormalTok{], }\DataTypeTok{time=}\KeywordTok{length}\NormalTok{(t4)))) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{st_set_dimensions}\NormalTok{(}\StringTok{"time"}\NormalTok{, t4) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{st_set_dimensions}\NormalTok{(}\StringTok{"x"}\NormalTok{, }\KeywordTok{st_get_dimension_values}\NormalTok{(grd, }\StringTok{"x"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{st_set_dimensions}\NormalTok{(}\StringTok{"y"}\NormalTok{, }\KeywordTok{st_get_dimension_values}\NormalTok{(grd, }\StringTok{"y"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{st_set_crs}\NormalTok{(crs) ->}\StringTok{ }\NormalTok{grd.st}
\end{Highlighting}
\end{Shaded}

and the subsequent predictions are obtained by

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{new_int <-}\StringTok{ }\KeywordTok{krigeST}\NormalTok{(NO2}\OperatorTok{~}\DecValTok{1}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ no2.st[}\StringTok{"NO2"}\NormalTok{], }\DataTypeTok{newdata =}\NormalTok{ grd.st,}
         \DataTypeTok{nmax =} \DecValTok{200}\NormalTok{, }\DataTypeTok{stAni =}\NormalTok{ StAni, }\DataTypeTok{modelList =}\NormalTok{ fitProdSumModel,}
         \DataTypeTok{progress =} \OtherTok{FALSE}\NormalTok{)}
\KeywordTok{names}\NormalTok{(new_int)[}\DecValTok{2}\NormalTok{] =}\StringTok{ "NO2"}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# Coordinate system already present. Adding new coordinate system, which will replace the existing one.
\end{verbatim}

\begin{figure}

{\centering \includegraphics{sds_files/figure-latex/plotspatiotemporalpredictions-1} 

}

\caption{Spatiotemporal predictions for four selected time slices}\label{fig:plotspatiotemporalpredictions}
\end{figure}

and shown in figure \ref{fig:plotspatiotemporalpredictions}.

A larger value for \texttt{nmax} was needed here to decrease the visible
disturbance (sharp edges) caused by discrete neighbourhood
selections, which are now done in space \emph{and} time.

\hypertarget{exercises-11}{%
\section{Exercises}\label{exercises-11}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Which fraction of the stations is removed in section \ref{preparing} when
  the criterion applied that a station must be 75\% complete?
\item
  From the hourly time series in \texttt{no2.st}, compute daily mean concentrations
  using \texttt{aggregate}, and compute the spatiotemporal variogram of this. How does
  it compare to the variogram of hourly values?
\item
  Carry out a spatiotemporal interpolation for daily mean values for the days
  corresponding to those shown in \ref{fig:plotspatiotemporalpredictions}, and
  compare the results.
\item
  Following the example in the demo scripts pointed
  at in section \ref{cokriging}, carry out a cokriging
  on the daily mean station data for the four days shown in
  \ref{fig:plotspatiotemporalpredictions}. What are the differences
  of this approach to spatiotemporal kriging?
\end{enumerate}

\hypertarget{area}{%
\chapter{Proximity and Areal Data}\label{area}}

Areal units of observation are very often used when simultaneous observations are aggregated within non-overlapping boundaries. The boundaries may be those of administrative entities, and may be related to underlying spatial processes, such as commuting flows, but are usually arbitrary. If they do not match the underlying and unobserved spatial processes in one or more variables of interest, proximate areal units will contain parts of the underlying processes, engendering spatial autocorrelation. By proximity, we mean \emph{closeness} in ways that make sense for the data generation processes thought to be involved. In cross-sectional geostatistical analysis with point support, measured distance makes sense for typical data generation processes. In similar analysis of areal data, sharing a border may make more sense, because that is what we do know, but we cannot measure the distance between the areas in as adequate a way.

With support of data we mean the physical size (length, area, volume) associated with an individual observational unit (measurement). It is possible to represent the support of areal data by a point, despite the fact that the data have polygonal support. The centroid of the polygon may be taken as a representative point, or the centroid of the largest polygon in a multi-polygon object. When data with intrinsic point support are treated as areal data, the change of support goes the other way, from the known point to a non-overlapping tessellation such as a Voronoi diagram or Dirichlet tessellation or Thiessen polygons often through a Delaunay triangulation using projected coordinates. Here, different metrics may also be chosen, or distances measured on a network rather than on the plane. There is also a literature using weighted Voronoi diagrams in local spatial analysis \citep[see for example][]{doi:10.1080/13658810601034267, doi:10.1080/13658810701587891, SHE201570}.

When the intrinsic support of the data is as points, but the underlying process is between proximate observations rather than driven chiefly by distance however measured between observations, the data may be aggregate counts or totals (polling stations, retail turnover) or represent a directly observed characteristic of the observation (opening hours of the polling station). Obviously, the risk of mis-representing the footprint of the underlying spatial processes remains in all of these cases, not least because the observations are taken as encompassing the entirety of the underlying process in the case of tessellation of the whole area of interest. This is distinct from the geostatistical setting in which observations are rather samples taken using some scheme within the area of interest. It is also partly distinct from the practice of taking areal sample plots within the area of interest but covering only a small proportion of the area, typically used in ecological and environmental research.

In order to explore and analyse areal data of these kinds in Chapters \ref{spatautocorr}, \ref{spatglmm} and \ref{spatecon}, methods are needed to represent the proximity of observations. This chapter then considers a subset of the such methods, where the spatial processes are considered as working through proximity understood in the first instance as contiguity, as a graph linking observations taken as neighbours. This graph is typically undirected and unweighted, but may be directed and/or weighted in certain settings, which then leads to further issues with regard to symmetry. In principle, proximity would be expected to operate symmetrically in space, that is that the influence of \(i\) on \(j\) and of \(j\) on \(i\) based on their relative positions should be equivalent. Edge effects are not considered in standard treatments.

\hypertarget{representing-proximity-in-spdep}{%
\section{\texorpdfstring{Representing proximity in \textbf{spdep}}{Representing proximity in spdep}}\label{representing-proximity-in-spdep}}

Handling spatial autocorrelation using relationships to neighbours on a graph takes the graph as given, chosen by the analyst. This differs from the geostatistical approach in which the analyst chooses the binning of the empirical variogram and function used, and then the way the variogram is fitted. Both involve a priori choices, but represent the underlying correlation in different ways \citep{wall:04}. In Bavaud \citeyearpar{bavaud:98} and work citing his contribution, attempts have been made to place graph-based neighbours in a broader context.

One issue arising in the creation of objects representing neighbourhood relationships is that of no-neighbour areal units \citep{bivand+portnov:04}. Islands or units separated by rivers may not be recognised as neighbours when the units have areal support and when using topological relationships such as shared boundaries. In some settings, for example \texttt{mrf} (Markov Random Field) terms in \texttt{mgcv::gam()} and similar model fitting functions that require undirected connected graphs, a requirement which is violated when there are disconnected subgraphs.

No-neighbour observations can also occur when a distance threshold is used between points, where the threshold is smaller than the maximum nearest neighbour distance. Shared boundary contiguities are not affected by using geographical, unprojected coordinates, but all point-based approaches use distance in one way or another, and need to calculate distances in an appropriate way.

The \textbf{spdep} package provides an \texttt{nb} class for neighbours, a list of length equal to the number of observations, with integer vector components. No-neighbours are encoded as an integer vector with a single element \texttt{0L}, and observations with neighbours as sorted integer vectors containing values in \texttt{1L:n} pointing to the neighbouring observations. This is a typical row-oriented sparse representation of neighbours. \textbf{spdep} provides many ways of constructing \texttt{nb} objects, and the representation and construction functions are widely used in other packages.

\textbf{spdep} builds on the \texttt{nb} representation (undirected or directed graphs) with the \texttt{listw} object, a list with three components, an \texttt{nb} object, a matching list of numerical weights, and a single element character vector containing the single letter name of the way in which the weights were calculated. The most frequently used approach in the social sciences is calculating weights by row standardization, so that all the non-zero weights for one observation will be the inverse of the cardinality of its set of neighbours (\texttt{1/card(nb{[}{[}i{]}{]})}).

We will be using election data from the 2015 Polish Presidential election in this chapter, with 2495 municipalities and Warsaw boroughs (see Figure \ref{fig:plotpolpres15} for a \textbf{tmap} map (section \ref{tmap}) of the municipality types), and complete count data from polling stations aggregated to these areal units. The data are an \textbf{sf} \texttt{sf} object:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(sf)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(pol_pres15, }\DataTypeTok{package=}\StringTok{"spDataLarge"}\NormalTok{)}
\NormalTok{pol_pres15 }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{subset}\NormalTok{(}\DataTypeTok{select=}\KeywordTok{c}\NormalTok{(TERYT, name, types)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{head}\NormalTok{()}
\CommentTok{# Simple feature collection with 6 features and 3 fields}
\CommentTok{# Geometry type: MULTIPOLYGON}
\CommentTok{# Dimension:     XY}
\CommentTok{# Bounding box:  xmin: 235000 ymin: 367000 xmax: 281000 ymax: 413000}
\CommentTok{# Projected CRS: ETRS89 / Poland CS92}
\CommentTok{#    TERYT                name       types}
\CommentTok{# 1 020101         BOLESÅAWIEC       Urban}
\CommentTok{# 2 020102         BOLESÅAWIEC       Rural}
\CommentTok{# 3 020103            GROMADKA       Rural}
\CommentTok{# 4 020104        NOWOGRODZIEC Urban/rural}
\CommentTok{# 5 020105          OSIECZNICA       Rural}
\CommentTok{# 6 020106 WARTA BOLESÅAWIECKA       Rural}
\CommentTok{#                         geometry}
\CommentTok{# 1 MULTIPOLYGON (((261089 3855...}
\CommentTok{# 2 MULTIPOLYGON (((254150 3837...}
\CommentTok{# 3 MULTIPOLYGON (((275346 3846...}
\CommentTok{# 4 MULTIPOLYGON (((251770 3770...}
\CommentTok{# 5 MULTIPOLYGON (((263424 4060...}
\CommentTok{# 6 MULTIPOLYGON (((267031 3870...}
\end{Highlighting}
\end{Shaded}



\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tmap)}
\KeywordTok{tm_shape}\NormalTok{(pol_pres15) }\OperatorTok{+}\StringTok{ }\KeywordTok{tm_fill}\NormalTok{(}\StringTok{"types"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{sds_files/figure-latex/plotpolpres15-1} 

}

\caption{Polish municipality types 2015}\label{fig:plotpolpres15}
\end{figure}

For safety's sake, we impose topological validity:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{ (}\OperatorTok{!}\KeywordTok{all}\NormalTok{(}\KeywordTok{st_is_valid}\NormalTok{(pol_pres15))) pol_pres15 <-}\StringTok{ }\KeywordTok{st_make_valid}\NormalTok{(pol_pres15)}
\end{Highlighting}
\end{Shaded}

Between early 2002 and April 2019, \textbf{spdep} contained functions for constructing and handling neighbour and spatial weights objects, tests for spatial autocorrelation, and model fitting functions. The latter have been split out into \textbf{spatialreg}, and will be discussed in the next chapter. \textbf{spdep} \citep{R-spdep} now accommodates objects represented using \textbf{sf} classes and \textbf{sp} classes directly.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(spdep)}
\CommentTok{# Loading required package: sp}
\CommentTok{# Loading required package: spData}
\end{Highlighting}
\end{Shaded}

\hypertarget{contiguous-neighbours}{%
\section{Contiguous neighbours}\label{contiguous-neighbours}}

The \texttt{poly2nb()} function in \textbf{spdep} takes the boundary points making up the polygon boundaries in the object passed as the \texttt{pl=} argument, typically an \texttt{"sf"} or \texttt{"sfc"} object with \texttt{"POLYGON"} or \texttt{"MULTIPOLYGON"} geometries. For each observation, the function checks whether at least one (\texttt{queen=TRUE}, default), or at least two (rook, \texttt{queen=FALSE}) points are within \texttt{snap=} distance units of each other. The distances are planar in the raw coordinate units, ignoring geographical projections. Once the required number of sufficiently close points is found, the search is stopped.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{args}\NormalTok{(poly2nb)}
\CommentTok{# function (pl, row.names = NULL, snap = sqrt(.Machine$double.eps), }
\CommentTok{#     queen = TRUE, useC = TRUE, foundInBox = NULL, small_n = 500) }
\CommentTok{# NULL}
\end{Highlighting}
\end{Shaded}

From \textbf{spdep} 1.1-7, the GEOS interface of the \textbf{sf} package is used within \texttt{poly2nb()} if \texttt{foundInBox=NULL} to find the candidate neighbours and populate \texttt{foundInBox} internally. In this case, this use of spatial indexing (STRtree queries) in GEOS through \textbf{sf} is the default:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{system.time}\NormalTok{(pol_pres15 }\OperatorTok{%>%}\StringTok{ }\KeywordTok{poly2nb}\NormalTok{(}\DataTypeTok{queen=}\OtherTok{TRUE}\NormalTok{) ->}\StringTok{ }\NormalTok{nb_q)}
\CommentTok{#    user  system elapsed }
\CommentTok{#    1.93    0.00    1.93}
\end{Highlighting}
\end{Shaded}

Earlier, \texttt{foundInBox=} accepted the output of the \textbf{rgeos} \texttt{gUnarySTRtreeQuery()} function to list candidate neighbours, that is polygons whose bounding boxes intersect the bounding boxes of other polygons. The print method shows the summary structure of the neighbour object:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nb_q}
\CommentTok{# Neighbour list object:}
\CommentTok{# Number of regions: 2495 }
\CommentTok{# Number of nonzero links: 14242 }
\CommentTok{# Percentage nonzero weights: 0.229 }
\CommentTok{# Average number of links: 5.71}
\end{Highlighting}
\end{Shaded}

From \textbf{sf} version 1.0-0, the \textbf{s2} package \citep{R-s2} is used by default for spherical geometries, as \texttt{st\_intersects()} used in \texttt{poly2nb()} passes calculation to \texttt{s2::s2\_intersects\_matrix()} (see \ref{spherical}). From \textbf{spdep} version 1.1-9, if \texttt{sf\_use\_s2()}is \texttt{TRUE}, spherical intersection is used to find candidate neighbours; as with GEOS, the underlying \texttt{s2} library uses fast spatial indexing.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sf_use_s2}\NormalTok{(}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(pol_pres15 }\OperatorTok{%>%}\StringTok{ }\KeywordTok{st_transform}\NormalTok{(}\StringTok{"OGC:CRS84"}\NormalTok{) ->}\StringTok{ }\NormalTok{pol_pres15_ll) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{poly2nb}\NormalTok{(}\DataTypeTok{queen=}\OtherTok{TRUE}\NormalTok{) ->}\StringTok{ }\NormalTok{nb_q_s2}
\end{Highlighting}
\end{Shaded}

Spherical and planar intersection of the input polygons yield the same contiguity neighbours in this case; in both cases valid input geometries are desirable:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{all.equal}\NormalTok{(nb_q, nb_q_s2, }\DataTypeTok{check.attributes=}\OtherTok{FALSE}\NormalTok{)}
\CommentTok{# [1] TRUE}
\end{Highlighting}
\end{Shaded}

Note that \texttt{nb} objects record both symmetric neighbour relationships, because these objects admit asymmetric relationships as well, but these duplications are not needed for object construction.

Most of the \textbf{spdep} functions for constructing neighbour objects take a \texttt{row.names=} argument, the value of which is stored as a \texttt{region.id} attribute. If not given, the values are taken from \texttt{row.names()} of the first argument. These can be used to check that the neighbours object is in the same order as data. If \texttt{nb} objects are subsetted, the indices change to continue to be within \texttt{1:length(subsetted\_nb)}, but the \texttt{region.id} attribute values point back to the object from which it was constructed. This is used in out-of-sample prediction from spatial regression models discussed briefly in section @ref(spatecon\_pred).

We can also check that this undirected graph is connected using the \texttt{n.comp.nb()} function; while some model estimation techniques do not support graphs that are not connected, it is helpful to be aware of possible problems \citep{FRENISTERRANTINO201825}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(nb_q }\OperatorTok{%>%}\StringTok{ }\KeywordTok{n.comp.nb}\NormalTok{())}\OperatorTok{$}\NormalTok{nc}
\CommentTok{# [1] 1}
\end{Highlighting}
\end{Shaded}

This approach is equivalent to treating the neighbour object as a graph and using graph analysis on that graph \citep{csardi+nepusz:06, R-igraph}, by first coercing to a binary sparse matrix \citep{R-Matrix}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(Matrix, }\DataTypeTok{warn.conflicts=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{nb_q }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{nb2listw}\NormalTok{(}\DataTypeTok{style=}\StringTok{"B"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{as}\NormalTok{(}\StringTok{"CsparseMatrix"}\NormalTok{) ->}\StringTok{ }\NormalTok{smat}
\KeywordTok{library}\NormalTok{(igraph, }\DataTypeTok{warn.conflicts=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{(smat }\OperatorTok{%>%}\StringTok{ }
\StringTok{        }\KeywordTok{graph.adjacency}\NormalTok{() ->}\StringTok{ }\NormalTok{g1) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{count_components}\NormalTok{()}
\CommentTok{# [1] 1}
\end{Highlighting}
\end{Shaded}

Neighbour objects may be exported and imported in GAL format for exchange with other software, using \texttt{write.nb.gal()} and \texttt{read.gal()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tf <-}\StringTok{ }\KeywordTok{tempfile}\NormalTok{(}\DataTypeTok{fileext=}\StringTok{".gal"}\NormalTok{)}
\KeywordTok{write.nb.gal}\NormalTok{(nb_q, tf)}
\end{Highlighting}
\end{Shaded}

\hypertarget{graph-based-neighbours}{%
\section{Graph-based neighbours}\label{graph-based-neighbours}}

If areal units are an appropriate representation, but only points on the plane have been observed, contiguity relationships may be approximated using graph-based neighbours. In this case, the imputed boundaries tessellate the plane such that points closer to one observation than any other fall within its polygon. The simplest form is by using triangulation, here using the \texttt{deldir()} function in the \textbf{deldir} package. Because the function returns from \(i\) and to \(j\) identifiers, it is easy to construct a long representation of a \texttt{listw} object, as used in the S-Plus SpatialStats module and the \texttt{sn2listw()} function internally to construct an \texttt{nb} object (ragged wide representation). Alternatives such as GEOS often fail to return sufficient information to permit the neighbours to be identified.

The output of these functions is then converted to the \texttt{nb} representation using \texttt{graph2nb()}, with the possible use of the \texttt{sym=} argument to coerce to symmetry. We take the centroids of the largest component polygon for each observation as the point representation; population-weighted centroids might have been a better choice if they were available:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pol_pres15 }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{st_geometry}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{st_centroid}\NormalTok{(}\DataTypeTok{of_largest_polygon=}\OtherTok{TRUE}\NormalTok{) ->}\StringTok{ }\NormalTok{coords }
\NormalTok{(coords }\OperatorTok{%>%}\StringTok{ }\KeywordTok{tri2nb}\NormalTok{() ->}\StringTok{ }\NormalTok{nb_tri)}
\CommentTok{# Neighbour list object:}
\CommentTok{# Number of regions: 2495 }
\CommentTok{# Number of nonzero links: 14930 }
\CommentTok{# Percentage nonzero weights: 0.24 }
\CommentTok{# Average number of links: 5.98}
\end{Highlighting}
\end{Shaded}

The average number of neighbours is similar to the Queen boundary contiguity case, but if we look at the distribution of edge lengths using \texttt{nbdists()}, we can see that although the upper quartile is about 15 km, the maximum is almost 300 km, an edge along much of one side of the convex hull. The short minimum distance is also of interest, as many centroids of urban municipalities are very close to the centroids of their surrounding rural counterparts.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nb_tri }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{nbdists}\NormalTok{(coords) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{unlist}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{summary}\NormalTok{()}
\CommentTok{#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. }
\CommentTok{#     247    9847   12151   13485   14994  296974}
\end{Highlighting}
\end{Shaded}

Triangulated neighbours also yield a connected graph:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(nb_tri }\OperatorTok{%>%}\StringTok{ }\KeywordTok{n.comp.nb}\NormalTok{())}\OperatorTok{$}\NormalTok{nc}
\CommentTok{# [1] 1}
\end{Highlighting}
\end{Shaded}

Graph-based approaches include \texttt{soi.graph()} - discussed here, \texttt{relativeneigh()} and \texttt{gabrielneigh()}.

The Sphere of Influence \texttt{soi.graph()} function takes triangulated neighbours and prunes off neighbour relationships represented by edges that are unusually long for each point, especially around the convex hull \citep{avis+horton:1985}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(nb_tri }\OperatorTok{%>%}\StringTok{ }
\StringTok{        }\KeywordTok{soi.graph}\NormalTok{(coords) }\OperatorTok{%>%}\StringTok{ }
\StringTok{        }\KeywordTok{graph2nb}\NormalTok{() ->}\StringTok{ }\NormalTok{nb_soi)}
\CommentTok{# Neighbour list object:}
\CommentTok{# Number of regions: 2495 }
\CommentTok{# Number of nonzero links: 12792 }
\CommentTok{# Percentage nonzero weights: 0.205 }
\CommentTok{# Average number of links: 5.13}
\end{Highlighting}
\end{Shaded}

Unpicking the triangulated neighbours does however remove the connected character of the underlying graph:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(nb_soi }\OperatorTok{%>%}\StringTok{ }\KeywordTok{n.comp.nb}\NormalTok{() ->}\StringTok{ }\NormalTok{n_comp)}\OperatorTok{$}\NormalTok{nc}
\CommentTok{# [1] 16}
\end{Highlighting}
\end{Shaded}

The algorithm has stripped out longer edges leading to urban and rural municipality pairs where their centroids are very close to each other because the rural ones completely surround the urban, giving 15 pairs of neighbours unconnected to the main graph:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(n_comp}\OperatorTok{$}\NormalTok{comp.id)}
\CommentTok{# }
\CommentTok{#    1    2    3    4    5    6    7    8    9   10   11   12   13 }
\CommentTok{# 2465    2    2    2    2    2    2    2    2    2    2    2    2 }
\CommentTok{#   14   15   16 }
\CommentTok{#    2    2    2}
\end{Highlighting}
\end{Shaded}

The largest length edges along the convex hull have been removed, but ``holes'' have appeared where the unconnected pairs of neighbours have appeared. The differences between \texttt{nb\_tri} and \texttt{nb\_soi} are shown in orange in Figure \ref{fig:plotnbdiff}.



\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{opar <-}\StringTok{ }\KeywordTok{par}\NormalTok{(}\DataTypeTok{mar=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{)}\OperatorTok{+}\FloatTok{0.5}\NormalTok{)}
\NormalTok{pol_pres15 }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{st_geometry}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{plot}\NormalTok{(}\DataTypeTok{border=}\StringTok{"grey"}\NormalTok{, }\DataTypeTok{lwd=}\FloatTok{0.5}\NormalTok{)}
\NormalTok{nb_soi }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{plot}\NormalTok{(}\DataTypeTok{coords=}\NormalTok{coords, }\DataTypeTok{add=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{points=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{lwd=}\FloatTok{0.5}\NormalTok{)}
\NormalTok{nb_tri }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{diffnb}\NormalTok{(nb_soi) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{plot}\NormalTok{(}\DataTypeTok{coords=}\NormalTok{coords, }\DataTypeTok{col=}\StringTok{"orange"}\NormalTok{, }\DataTypeTok{add=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{points=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{lwd=}\FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{sds_files/figure-latex/plotnbdiff-1} 

}

\caption{Triangulated (orange + black) and sphere of influence neighbours (black)}\label{fig:plotnbdiff}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(opar)}
\end{Highlighting}
\end{Shaded}

\hypertarget{distance-based-neighbours}{%
\section{Distance-based neighbours}\label{distance-based-neighbours}}

Distance-based neighbours can be constructed using \texttt{dnearneigh()}, with a distance band with lower \texttt{d1=} and upper \texttt{d2=} bounds controlled by the \texttt{bounds=} argument. If spherical coordinates are used and either specified in the coordinates object \texttt{x} or with \texttt{x} as a two column matrix and \texttt{longlat=TRUE}, great circle distances in km will be calculated assuming the WGS84 reference ellipsoid, or if \texttt{use\_s2=TRUE}, a 200-cell \texttt{s2} buffer is constructed around each point, points falling within the buffer chosen, and then chosen points beyond \texttt{d2=} dropped. From tests, it appears that spherical spatial indexing is not used, and so is slower than the legacy brute-force approach (see chapter \ref{spherical}).

From \textbf{spdep} 1.1-7, two arguments have been added, to use functionality in the \textbf{dbscan} package \citep{R-dbscan} for finding neighbours using planar spatial indexing in two or three dimensions by default, and not to test the symmetry of the output neighbour object.

The \texttt{knearneigh()} function for \(k\)-nearest neighbours returns a \texttt{knn} object, converted to an \texttt{nb} object using \texttt{knn2nb()}. It can also use great circle distances, not least because nearest neighbours may differ when uprojected coordinates are treated as planar. \texttt{k=} should be a small number. For projected coordinates, the \textbf{dbscan} package is used to compute nearest neighbours more efficiently. Note that \texttt{nb} objects constructed in this way are most unlikely to be symmetric, hence \texttt{knn2nb()} has a \texttt{sym=} argument to permit the imposition of symmetry, which will mean that all units have at least \texttt{k=} neighbours, not that all units will have exactly \texttt{k=} neighbours. From \textbf{spdep} version 1.1-9 and when \texttt{sf\_use\_s2()} is \texttt{TRUE}, \texttt{knearneigh()} will use fast spherical spatial indexing when the input object is of class \texttt{"sf"} or \texttt{"sfc"}.

The \texttt{nbdists()} function returns the length of neighbour relationship edges in the units of the coordinates if the coordinates are projected, in km otherwise. In order to set the upper limit for distance bands, one may first find the maximum first nearest neighbour distance, using \texttt{unlist()} to remove the list structure of the returned object. From \textbf{spdep} version 1.1-9 and when \texttt{sf\_use\_s2()} is \texttt{TRUE}, \texttt{nbdists()} will use fast spherical distance calculations when the input object is of class \texttt{"sf"} or \texttt{"sfc"}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{coords }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{knearneigh}\NormalTok{(}\DataTypeTok{k=}\DecValTok{1}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{knn2nb}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{nbdists}\NormalTok{(coords) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{unlist}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{summary}\NormalTok{()}
\CommentTok{#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. }
\CommentTok{#     247    6663    8538    8275   10124   17979}
\end{Highlighting}
\end{Shaded}

Here the largest first nearest neighbour distance is just under 18 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{system.time}\NormalTok{(coords }\OperatorTok{%>%}\StringTok{ }\KeywordTok{dnearneigh}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{18000}\NormalTok{) ->}\StringTok{ }\NormalTok{nb_d18)}
\CommentTok{#    user  system elapsed }
\CommentTok{#   0.272   0.008   0.281}
\end{Highlighting}
\end{Shaded}

For this moderate number of observations, use of spatial indexing does not yield advantages in run times:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{system.time}\NormalTok{(coords }\OperatorTok{%>%}\StringTok{ }\KeywordTok{dnearneigh}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{18000}\NormalTok{, }\DataTypeTok{use_kd_tree=}\OtherTok{FALSE}\NormalTok{) ->}\StringTok{ }\NormalTok{nb_d18a)}
\CommentTok{#    user  system elapsed }
\CommentTok{#   0.291   0.000   0.291}
\end{Highlighting}
\end{Shaded}

and the output objects are the same:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{all.equal}\NormalTok{(nb_d18, nb_d18a, }\DataTypeTok{check.attributes=}\OtherTok{FALSE}\NormalTok{)}
\CommentTok{# [1] TRUE}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nb_d18}
\CommentTok{# Neighbour list object:}
\CommentTok{# Number of regions: 2495 }
\CommentTok{# Number of nonzero links: 20358 }
\CommentTok{# Percentage nonzero weights: 0.327 }
\CommentTok{# Average number of links: 8.16}
\end{Highlighting}
\end{Shaded}

However, even though there are no no-neighbour observations (their presence is reported by the print method for \texttt{nb} objects), the graph is not connected, as a pair of observations are each others' only neighbours.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(nb_d18 }\OperatorTok{%>%}\StringTok{ }\KeywordTok{n.comp.nb}\NormalTok{() ->}\StringTok{ }\NormalTok{n_comp)}\OperatorTok{$}\NormalTok{nc}
\CommentTok{# [1] 2}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(n_comp}\OperatorTok{$}\NormalTok{comp.id)}
\CommentTok{# }
\CommentTok{#    1    2 }
\CommentTok{# 2493    2}
\end{Highlighting}
\end{Shaded}

Adding 300 m to the threshold gives us a neighbour object with no no-neighbour units, and all units can be reached from all others across the graph.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(coords }\OperatorTok{%>%}\StringTok{ }\KeywordTok{dnearneigh}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{18300}\NormalTok{) ->}\StringTok{ }\NormalTok{nb_d183)}
\CommentTok{# Neighbour list object:}
\CommentTok{# Number of regions: 2495 }
\CommentTok{# Number of nonzero links: 21086 }
\CommentTok{# Percentage nonzero weights: 0.339 }
\CommentTok{# Average number of links: 8.45}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(nb_d183 }\OperatorTok{%>%}\StringTok{ }\KeywordTok{n.comp.nb}\NormalTok{())}\OperatorTok{$}\NormalTok{nc}
\CommentTok{# [1] 1}
\end{Highlighting}
\end{Shaded}

One characteristic of distance-based neighbours is that more densely settled areas, with units which are smaller in terms of area (Warsaw boroughs are much smaller on average, but have almost 30 neighbours). Having many neighbours smooths the neighbour relationship across more neighbours.

For use later, we also construct a neighbour object with no-neighbour units, using a threshold of 16 km:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(coords }\OperatorTok{%>%}\StringTok{ }\KeywordTok{dnearneigh}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{16000}\NormalTok{) ->}\StringTok{ }\NormalTok{nb_d16)}
\CommentTok{# Neighbour list object:}
\CommentTok{# Number of regions: 2495 }
\CommentTok{# Number of nonzero links: 15850 }
\CommentTok{# Percentage nonzero weights: 0.255 }
\CommentTok{# Average number of links: 6.35 }
\CommentTok{# 7 regions with no links:}
\CommentTok{# 569 1371 1522 2374 2385 2473 2474}
\end{Highlighting}
\end{Shaded}

It is possible to control the numbers of neighbours directly using \(k\)-nearest neighbours, either accepting asymmetric neighbours:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{((coords }\OperatorTok{%>%}\StringTok{ }\KeywordTok{knearneigh}\NormalTok{(}\DataTypeTok{k=}\DecValTok{6}\NormalTok{) ->}\StringTok{ }\NormalTok{knn_k6) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{knn2nb}\NormalTok{() ->}\StringTok{ }\NormalTok{nb_k6)}
\CommentTok{# Neighbour list object:}
\CommentTok{# Number of regions: 2495 }
\CommentTok{# Number of nonzero links: 14970 }
\CommentTok{# Percentage nonzero weights: 0.24 }
\CommentTok{# Average number of links: 6 }
\CommentTok{# Non-symmetric neighbours list}
\end{Highlighting}
\end{Shaded}

or imposing symmetry:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(knn_k6 }\OperatorTok{%>%}\StringTok{ }\KeywordTok{knn2nb}\NormalTok{(}\DataTypeTok{sym=}\OtherTok{TRUE}\NormalTok{) ->}\StringTok{ }\NormalTok{nb_k6s)}
\CommentTok{# Neighbour list object:}
\CommentTok{# Number of regions: 2495 }
\CommentTok{# Number of nonzero links: 16810 }
\CommentTok{# Percentage nonzero weights: 0.27 }
\CommentTok{# Average number of links: 6.74}
\end{Highlighting}
\end{Shaded}

Here the size of \texttt{k=} is sufficient to ensure connectedness, although the graph is not planar as edges cross at locations other than nodes, which is not the case for contiguous or graph-based neighbours.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(nb_k6s }\OperatorTok{%>%}\StringTok{ }\KeywordTok{n.comp.nb}\NormalTok{())}\OperatorTok{$}\NormalTok{nc}
\CommentTok{# [1] 1}
\end{Highlighting}
\end{Shaded}

In the case of points on the sphere (see chapter \ref{spherical}), the output of \texttt{st\_centroid()} will differ, so rather than inverse projecting the points, we extract points as geographical coordinates from the inverse projected polygon geometries:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sf_use_s2}\NormalTok{(}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pol_pres15_ll }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{st_geometry}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{st_centroid}\NormalTok{(}\DataTypeTok{of_largest_polygon=}\OtherTok{TRUE}\NormalTok{) ->}\StringTok{ }\NormalTok{coords_ll}
\end{Highlighting}
\end{Shaded}

From \textbf{spdep} version 1.1-9, fast spatial indexing in \textbf{s2} is used to find the nearest neighbours:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(coords_ll }\OperatorTok{%>%}\StringTok{ }\KeywordTok{knearneigh}\NormalTok{(}\DataTypeTok{k=}\DecValTok{6}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{knn2nb}\NormalTok{() ->}\StringTok{ }\NormalTok{nb_k6_ll)}
\CommentTok{# Neighbour list object:}
\CommentTok{# Number of regions: 2495 }
\CommentTok{# Number of nonzero links: 14970 }
\CommentTok{# Percentage nonzero weights: 0.24 }
\CommentTok{# Average number of links: 6 }
\CommentTok{# Non-symmetric neighbours list}
\end{Highlighting}
\end{Shaded}

These neighbours differ from the planar \texttt{k=6} nearest neighbours as would be expected:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{isTRUE}\NormalTok{(}\KeywordTok{all.equal}\NormalTok{(nb_k6, nb_k6_ll, }\DataTypeTok{check.attributes=}\OtherTok{FALSE}\NormalTok{))}
\CommentTok{# [1] TRUE}
\end{Highlighting}
\end{Shaded}

The \texttt{nbdists()} function also uses \textbf{s2} to find distances on the sphere when the \texttt{"sf"} or \texttt{"sfc"}input object is in geographical coordinates (distances returned in kilometres):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nb_q }\OperatorTok{%>%}\StringTok{ }\KeywordTok{nbdists}\NormalTok{(coords_ll) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{unlist}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\KeywordTok{summary}\NormalTok{()}
\CommentTok{#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. }
\CommentTok{#     0.2     9.8    12.2    12.7    15.1    33.1}
\end{Highlighting}
\end{Shaded}

These differ a little for the same weights object when planar coordinates are used (distances returned in the metric of the points):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nb_q }\OperatorTok{%>%}\StringTok{ }\KeywordTok{nbdists}\NormalTok{(coords) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{unlist}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\KeywordTok{summary}\NormalTok{()}
\CommentTok{#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. }
\CommentTok{#     247    9822   12173   12651   15117   33102}
\end{Highlighting}
\end{Shaded}

\hypertarget{weights-specification}{%
\section{Weights specification}\label{weights-specification}}

Once neighbour objects are available, further choices need to made in specifying the weights objects. The \texttt{nb2listw()} function is used to create a \texttt{listw} weights object with an \texttt{nb} object, a matching list of weights vectors, and a style specification. Because handling no-neighbour observations now begins to matter, the \texttt{zero.policy=} argument is introduced. By default, this is \texttt{FALSE}, indicating that no-neighbour observations will cause an error, as the spatially lagged value for an observation with no neighbours is not available. By convention, zero is substituted for the lagged value, as the cross product of a vector of zero-valued weights and a data vector, hence the name of \texttt{zero.policy}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{args}\NormalTok{(nb2listw)}
\CommentTok{# function (neighbours, glist = NULL, style = "W", zero.policy = NULL) }
\CommentTok{# NULL}
\end{Highlighting}
\end{Shaded}

We will be using the helper function \texttt{spweights.constants()} below to show some consequences of varing style choices. It returns constants for a \texttt{listw} object, \(n\) is the number of observations, \texttt{n1} to \texttt{n3} are \(n-1, \ldots\), \texttt{nn} is \(n^2\) and \(S_0\), \(S_1\) and \(S_2\) are constants, \(S_0\) being the sum of the weights. There is a full discussion of the constants in Bivand and Wong \citeyearpar{Bivand2018}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{args}\NormalTok{(spweights.constants)}
\CommentTok{# function (listw, zero.policy = NULL, adjust.n = TRUE) }
\CommentTok{# NULL}
\end{Highlighting}
\end{Shaded}

The \texttt{"B"} binary style gives a weight of unity to each neighbour relationship, and typically upweights units with no boundaries on the edge of the study area.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(nb_q }\OperatorTok{%>%}\StringTok{ }
\StringTok{        }\KeywordTok{nb2listw}\NormalTok{(}\DataTypeTok{style=}\StringTok{"B"}\NormalTok{) ->}\StringTok{ }\NormalTok{lw_q_B) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{spweights.constants}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{data.frame}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{subset}\NormalTok{(}\DataTypeTok{select=}\KeywordTok{c}\NormalTok{(n, S0, S1, S2))}
\CommentTok{#      n    S0    S1     S2}
\CommentTok{# 1 2495 14242 28484 357280}
\end{Highlighting}
\end{Shaded}

The \texttt{"W"} row-standardized style upweights units around the edge of the study area that necessarily have fewer neighbours. This style first gives a weight of unity to each neighbour relationship, then divides these weights by the per unit sums of weights. Naturally this leads to division by zero where there are no neighbours, a not-a-number result, unless the chosen policy is to permit no-neighbour observations. We can see that \(S_0\) is now equal to \(n\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(nb_q }\OperatorTok{%>%}\StringTok{ }
\StringTok{        }\KeywordTok{nb2listw}\NormalTok{(}\DataTypeTok{style=}\StringTok{"W"}\NormalTok{) ->}\StringTok{ }\NormalTok{lw_q_W) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{spweights.constants}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{data.frame}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{subset}\NormalTok{(}\DataTypeTok{select=}\KeywordTok{c}\NormalTok{(n, S0, S1, S2))}
\CommentTok{#      n   S0  S1    S2}
\CommentTok{# 1 2495 2495 958 10406}
\end{Highlighting}
\end{Shaded}

Inverse distance weights are used in a number of scientific fields. Some use dense inverse distance matrices, but many of the inverse distances are close to zero, so have little practical contribution, especially as the spatial process matrix is itself dense. Inverse distance weights may be constructed by taking the lengths of edges, changing units to avoid most weights being too large or small (here from m to km), taking the inverse, and passing through the \texttt{glist=} argument to \texttt{nb2listw()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nb_d183 }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{nbdists}\NormalTok{(coords) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{lapply}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(x) }\DecValTok{1}\OperatorTok{/}\NormalTok{(x}\OperatorTok{/}\DecValTok{1000}\NormalTok{)) ->}\StringTok{ }\NormalTok{gwts}
\NormalTok{(nb_d183 }\OperatorTok{%>%}\StringTok{ }\KeywordTok{nb2listw}\NormalTok{(}\DataTypeTok{glist=}\NormalTok{gwts, }\DataTypeTok{style=}\StringTok{"B"}\NormalTok{) ->}\StringTok{ }\NormalTok{lw_d183_idw_B) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{spweights.constants}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{data.frame}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{subset}\NormalTok{(}\DataTypeTok{select=}\KeywordTok{c}\NormalTok{(n, S0, S1, S2))}
\CommentTok{#      n   S0  S1   S2}
\CommentTok{# 1 2495 1841 534 7265}
\end{Highlighting}
\end{Shaded}

No-neighbour handling is by default to prevent the construction of a weights object, making the analyst take a position on how to proceed.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{try}\NormalTok{(nb_d16 }\OperatorTok{%>%}\StringTok{ }\KeywordTok{nb2listw}\NormalTok{(}\DataTypeTok{style=}\StringTok{"B"}\NormalTok{) ->}\StringTok{ }\NormalTok{lw_d16_B)}
\CommentTok{# Error in nb2listw(., style = "B") : Empty neighbour sets found}
\end{Highlighting}
\end{Shaded}

Use can be made of the \texttt{zero.policy=} argument to many functions used with \texttt{nb} and \texttt{listw} objects.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nb_d16 }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{nb2listw}\NormalTok{(}\DataTypeTok{style=}\StringTok{"B"}\NormalTok{, }\DataTypeTok{zero.policy=}\OtherTok{TRUE}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{spweights.constants}\NormalTok{(}\DataTypeTok{zero.policy=}\OtherTok{TRUE}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{data.frame}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{subset}\NormalTok{(}\DataTypeTok{select=}\KeywordTok{c}\NormalTok{(n, S0, S1, S2))}
\CommentTok{#      n    S0    S1     S2}
\CommentTok{# 1 2488 15850 31700 506480}
\end{Highlighting}
\end{Shaded}

Note that by default the \texttt{adjust.n=} argument to \texttt{spweights.constants()} is set by default to \texttt{TRUE}, subtracting the count of no-neighbour observations from the observation count, so \(n\) is smaller with possible consequences for inference. The complete count can be retrieved by changing the argument.

\hypertarget{higher-order-neighbours}{%
\section{Higher order neighbours}\label{higher-order-neighbours}}

We recall the characteristics of the neighbour object based on Queen contiguities:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nb_q}
\CommentTok{# Neighbour list object:}
\CommentTok{# Number of regions: 2495 }
\CommentTok{# Number of nonzero links: 14242 }
\CommentTok{# Percentage nonzero weights: 0.229 }
\CommentTok{# Average number of links: 5.71}
\end{Highlighting}
\end{Shaded}

If we wish to create an object showing \(i\) to \(k\) neighbours, where \(i\) is a neighbour of \(j\), and \(j\) in turn is a neighbour of \(k\), so taking two steps on the neighbour graph, we can use \texttt{nblag()}, which automatically removes \(i\) to \(i\) self-neighbours:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(nb_q }\OperatorTok{%>%}\StringTok{ }\KeywordTok{nblag}\NormalTok{(}\DecValTok{2}\NormalTok{) ->}\StringTok{ }\NormalTok{nb_q2)[[}\DecValTok{2}\NormalTok{]]}
\CommentTok{# Neighbour list object:}
\CommentTok{# Number of regions: 2495 }
\CommentTok{# Number of nonzero links: 32930 }
\CommentTok{# Percentage nonzero weights: 0.529 }
\CommentTok{# Average number of links: 13.2}
\end{Highlighting}
\end{Shaded}

The \texttt{nblag\_cumul()} function cumulates the list of neighbours for the whole list of lags:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{nblag_cumul}\NormalTok{(nb_q2)}
\CommentTok{# Neighbour list object:}
\CommentTok{# Number of regions: 2495 }
\CommentTok{# Number of nonzero links: 47172 }
\CommentTok{# Percentage nonzero weights: 0.758 }
\CommentTok{# Average number of links: 18.9}
\end{Highlighting}
\end{Shaded}

while the set operation \texttt{union.nb()} takes two objects, giving here the same outcome:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{union.nb}\NormalTok{(nb_q2[[}\DecValTok{2}\NormalTok{]], nb_q2[[}\DecValTok{1}\NormalTok{]])}
\CommentTok{# Neighbour list object:}
\CommentTok{# Number of regions: 2495 }
\CommentTok{# Number of nonzero links: 47172 }
\CommentTok{# Percentage nonzero weights: 0.758 }
\CommentTok{# Average number of links: 18.9}
\end{Highlighting}
\end{Shaded}

Returning to the graph representation of the same neighbour object, we can ask how many steps might be needed to traverse the graph:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{diameter}\NormalTok{(g1)}
\CommentTok{# [1] 52}
\end{Highlighting}
\end{Shaded}

We step out from each observation across the graph to establish the number of steps needed to reach each other observation by the shortest path, once again finding the same count, and that the municipality is called Lutowiska, close to the Ukrainian border in the far south east of the country.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{g1 }\OperatorTok{%>%}\StringTok{ }\KeywordTok{shortest.paths}\NormalTok{() ->}\StringTok{ }\NormalTok{sps}
\NormalTok{(sps }\OperatorTok{%>%}\StringTok{ }\KeywordTok{apply}\NormalTok{(}\DecValTok{2}\NormalTok{, max) ->}\StringTok{ }\NormalTok{spmax) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{max}\NormalTok{()}
\CommentTok{# [1] 52}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mr <-}\StringTok{ }\KeywordTok{which.max}\NormalTok{(spmax)}
\NormalTok{pol_pres15}\OperatorTok{$}\NormalTok{name0[mr]}
\CommentTok{# [1] "Lutowiska"}
\end{Highlighting}
\end{Shaded}

Figure \ref{fig:shortestpath} shows that contiguity neighbours represent the same kinds of relationships with other observations as distance. Some approaches prefer distance neighbours on the basis that, for example, inverse distance neighbours show clearly how all observations are related to each other. However, the development of tests for spatial autocorrelation and spatial regression models has involved the inverse of a spatial process model, which in turn can be represented as the sum of a power series of the product of a coefficient and a spatial weights matrix, so intrinsically acknowledging the relationships of all observations with all other. Sparse contiguity neighbour objects accommodate rich dependency structures without the need to make the structures explicit.



\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gridExtra}\OperatorTok{::}\KeywordTok{grid.arrange}\NormalTok{(}\KeywordTok{tmap_grob}\NormalTok{(tm1), g1, }\DataTypeTok{nrow=}\DecValTok{1}\NormalTok{)}
\CommentTok{# Note that the aspect ratio for which the grob has been generated is 1.44}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{sds_files/figure-latex/shortestpath-1} 

}

\caption{Relationship of shortest paths to distance for Lutowiska; left panel: shortest path counts from Lutowiska; right panel: plot of shortest paths from Lutowiska to other observations and distances (km) from Lutowiska to other observations}\label{fig:shortestpath}
\end{figure}

\hypertarget{exercises-12}{%
\section{Exercises}\label{exercises-12}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Which kinds of geometry support are appropriate for which functions creating neighbour objects?
\item
  Which functions creating neighbour objects are only appropriate for planar representations?
\item
  What difference might the choice of \texttt{rook} rather than \texttt{queen} contiguities make on a chessboard?
\item
  What are the relationships between neighbour set cardinalities (neighbour counts) and row-standardized weights, and how do they open analyses up to edge effects? Use the chessboard you constructed in exercise 3 for both \texttt{rook} and \texttt{queen} neighbours.
\end{enumerate}

\hypertarget{spatautocorr}{%
\chapter{Measures of spatial autocorrelation}\label{spatautocorr}}

When analysing areal data, it has long been recognised that, if present, spatial autocorrelation changes how we may infer, relative to the default position of independent observations. In the presence of spatial autocorrelation, we can predict the values of observation \(i\) from the values observed at \(j \in N_i\), the set of its proximate neighbours. Early results \citep{moran48, geary:54}, entered into research practice gradually, for example the social sciences \citep{duncanetal61}. These results were then collated and extended to yield a set of basic tools of analysis \citep{cliff+ord:73, cliff+ord:81}.

Cliff and Ord \citeyearpar{cliff+ord:73} generalised and extended the expression of the spatial weights matrix representation as part of the framework for establishing the distribution theory for join count, Moran's \(I\) and Geary's \(C\) statistics. This development of what have become known as global measures, returning a single value of autocorrelation for the total study area, has been supplemented by local measures returning values for each areal unit \citep{getis+ord:92, anselin:95}.

\hypertarget{measures-and-process-mis-specification}{%
\section{Measures and process mis-specification}\label{measures-and-process-mis-specification}}

It is not and has never been the case that Tobler's first law of geography: ``Everything is related to everything else, but near things are more related than distant things'' always holds absolutely. This is and has always been an oversimplification, disguising the underlying entitation, support and other mis-specification problems. Are the units of observation appropriate for the scale of the underlying spatial process? Could the spatial patterning of the variable of interest for the chosen entitation be accounted for by another variable?

\citet{10.2307/143141} was published in the same special issue of \emph{Economic Geography} as \citet{10.2307/143140}, but Olsson does grasp the important point that spatial autocorrelation is not inherent in spatial phenomena, but often is engendered by inappropriate entitation, by omitted variables and/or inappropriate functional form. The key quote from Olsson is on p.~228:

\begin{quote}
The existence of such autocorrelations makes it tempting to agree with Tobler (1970, 236 {[}the original refers to the pagination of a conference paper{]}) that `everything is related to everything else, but near things are more related than distant things.' On the other hand, the fact that the autocorrelations seem to hide systematic specification errors suggests that the elevation of this statement to the status of `the first law of geography' is at best premature. At worst, the statement may represent the spatial variant of the post hoc fallacy, which would mean that coincidence has been mistaken for a causal relation.
\end{quote}

The status of the ``first law'' is very similar to the belief that John Snow induced the cause of cholera as water-borne from a map. It may be a good way of selling GIS, but it is inaccurate; Snow had a strong working hypothesis prior to visiting Soho, and the map was prepared after the Broad street pump was disabled as documentation that the hypothesis held \citep{BRODY200064}.

Measures of spatial autocorrelation unfortunately pick up other mis-specifications in the way that we model data \citep{schabenberger+gotway:2005, McMillen:2003}. For reference, Moran's \(I\) is given as \citep[page 17]{cliff+ord:81}:

\[
I = \frac{n \sum_{(2)} w_{ij} z_i z_j}{S_0 \sum_{i=1}^{n} z_i^2}
\]
where \(x_i, i=1, \ldots, n\) are \(n\) observations on the numeric variable of interest, \(z_i = x_i - \bar{x}\), \(\bar{x} = \sum_{i=1}^{n} x_i / n\), \(\sum_{(2)} = \stackrel{\sum_{i=1}^{n} \sum_{j=1}^{n}}{i \neq j}\), \(w_{ij}\) are the spatial weights, and \(S_0 = \sum_{(2)} w_{ij}\).
First we test a random variable using the Moran test, here under the normality assumption (argument \texttt{randomisation=FALSE}, default \texttt{TRUE}). Inference is made on the statistic \(Z(I) = \frac{I - E(I)}{\sqrt{\mathrm{Var}(I)}}\), the z-value compared with the Normal distribution for \(E(I)\) and \(\mathrm{Var}(I)\) for the chosen assumptions; this \texttt{x} does not show spatial autocorrelation with these spatial weights:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{glance_htest <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(ht) }\KeywordTok{c}\NormalTok{(ht}\OperatorTok{$}\NormalTok{estimate, }
    \StringTok{"Std deviate"}\NormalTok{=}\KeywordTok{unname}\NormalTok{(ht}\OperatorTok{$}\NormalTok{statistic), }
    \StringTok{"p.value"}\NormalTok{=}\KeywordTok{unname}\NormalTok{(ht}\OperatorTok{$}\NormalTok{p.value))}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{(pol_pres15 }\OperatorTok{%>%}\StringTok{ }
\StringTok{        }\KeywordTok{nrow}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{        }\KeywordTok{rnorm}\NormalTok{() ->}\StringTok{ }\NormalTok{x) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{moran.test}\NormalTok{(lw_q_B, }\DataTypeTok{randomisation=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{alternative=}\StringTok{"two.sided"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{glance_htest}\NormalTok{()}
\CommentTok{# Moran I statistic       Expectation          Variance }
\CommentTok{#         -0.004772         -0.000401          0.000140 }
\CommentTok{#       Std deviate           p.value }
\CommentTok{#         -0.369320          0.711889}
\end{Highlighting}
\end{Shaded}

The test however detects quite strong positive spatial autocorrelation when we insert a gentle trend into the data, but omit to include it in the mean model, thus creating a missing variable problem but finding spatial autocorrelation instead:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beta <-}\StringTok{ }\FloatTok{0.0015}
\NormalTok{coords }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{st_coordinates}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{subset}\NormalTok{(}\DataTypeTok{select=}\DecValTok{1}\NormalTok{, }\DataTypeTok{drop=}\OtherTok{TRUE}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\NormalTok{(}\ControlFlowTok{function}\NormalTok{(x) x}\OperatorTok{/}\DecValTok{1000}\NormalTok{)() ->}\StringTok{ }\NormalTok{t}
\NormalTok{(x }\OperatorTok{+}\StringTok{ }\NormalTok{beta }\OperatorTok{*}\StringTok{ }\NormalTok{t ->}\StringTok{ }\NormalTok{x_t) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{moran.test}\NormalTok{(lw_q_B, }\DataTypeTok{randomisation=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{alternative=}\StringTok{"two.sided"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{glance_htest}\NormalTok{()}
\CommentTok{# Moran I statistic       Expectation          Variance }
\CommentTok{#          0.043403         -0.000401          0.000140 }
\CommentTok{#       Std deviate           p.value }
\CommentTok{#          3.701491          0.000214}
\end{Highlighting}
\end{Shaded}

If we test the residuals of a linear model including the trend, the apparent spatial autocorrelation disappears:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{lm}\NormalTok{(x_t }\OperatorTok{~}\StringTok{ }\NormalTok{t) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{lm.morantest}\NormalTok{(lw_q_B, }\DataTypeTok{alternative=}\StringTok{"two.sided"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{glance_htest}\NormalTok{()}
\CommentTok{# Observed Moran I      Expectation         Variance }
\CommentTok{#        -0.004777        -0.000789         0.000140 }
\CommentTok{#      Std deviate          p.value }
\CommentTok{#        -0.337306         0.735886}
\end{Highlighting}
\end{Shaded}

A comparison of implementations of measures of spatial autocorrelation shows that a wide range of measures is available in R in a number of packages, chiefly in the \textbf{spdep} package \citep{R-spdep}, and that differences from other implementations can be attributed to design decisions \citep{Bivand2018}. The \textbf{spdep} package also includes the only implementations of exact and saddlepoint approximations to global and local Moran's I for regression residuals \citep{tiefelsdorf:02, bivandetal:09}.

\hypertarget{global-measures}{%
\section{Global measures}\label{global-measures}}

Global measures consider the average level of spatial autocorrelation across all observations; they can of course be biassed (as most spatial statistics) by edge effects where important spatial process components fall outside the study area.

\hypertarget{join-count-tests-for-categorical-data}{%
\subsection{Join-count tests for categorical data}\label{join-count-tests-for-categorical-data}}

We will begin by examining join count statistics, where \texttt{joincount.test()} takes a \texttt{"factor"} vector of values \texttt{fx=} and a \texttt{listw} object, and returns a list of \texttt{htest} (hypothesis test) objects defined in the \textbf{stats} package, one \texttt{htest} object for each level of the \texttt{fx=} argument. The observed counts are of neighbours with the same factor levels, known as same-colour joins.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{args}\NormalTok{(joincount.test)}
\CommentTok{# function (fx, listw, zero.policy = NULL, alternative = "greater", }
\CommentTok{#     sampling = "nonfree", spChk = NULL, adjust.n = TRUE) }
\CommentTok{# NULL}
\end{Highlighting}
\end{Shaded}

The function takes an \texttt{alternative=} argument for hypothesis testing, a \texttt{sampling=} argument showing the basis for the construction of the variance of the measure, where the default \texttt{"nonfree"} choice corresponds to analytical permutation; the \texttt{spChk=} argument is retained for backward compatibility. For reference, the counts of factor levels for the type of municipality or Warsaw borough are:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(pol_pres15 }\OperatorTok{%>%}\StringTok{ }
\StringTok{        }\KeywordTok{st_drop_geometry}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{        }\KeywordTok{subset}\NormalTok{(}\DataTypeTok{select=}\NormalTok{types, }\DataTypeTok{drop=}\OtherTok{TRUE}\NormalTok{) ->}\StringTok{ }\NormalTok{Types) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{table}\NormalTok{()}
\CommentTok{# .}
\CommentTok{#          Rural          Urban    Urban/rural Warsaw Borough }
\CommentTok{#           1563            303            611             18}
\end{Highlighting}
\end{Shaded}

Since there are four levels, we re-arrange the list of \texttt{htest} objects to give a matrix of estimated results. The observed same-colour join counts are tabulated with their expectations based on the counts of levels of the input factor, so that few joins would be expected between for example Warsaw boroughs, because there are very few of them. The variance calculation uses the underlying constants of the chosen \texttt{listw} object and the counts of levels of the input factor. The z-value is obtained in the usual way by dividing the difference between the observed and expected join counts by the square root of the variance.

The join count test was subsequently adapted for multi-colour join counts \citep{upton+fingleton:85}. The implementation as \texttt{joincount.mult()} in \textbf{spdep} returns a table based on nonfree sampling, and does not report p-values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Types }\OperatorTok{%>%}\StringTok{ }\KeywordTok{joincount.multi}\NormalTok{(}\DataTypeTok{listw=}\NormalTok{lw_q_B)}
\CommentTok{#                               Joincount Expected Variance z-value}
\CommentTok{# Rural:Rural                    3087.000 2793.920 1126.534    8.73}
\CommentTok{# Urban:Urban                     110.000  104.719   93.299    0.55}
\CommentTok{# Urban/rural:Urban/rural         656.000  426.526  331.759   12.60}
\CommentTok{# Warsaw Borough:Warsaw Borough    41.000    0.350    0.347   68.96}
\CommentTok{# Urban:Rural                     668.000 1083.941  708.209  -15.63}
\CommentTok{# Urban/rural:Rural              2359.000 2185.769 1267.131    4.87}
\CommentTok{# Urban/rural:Urban               171.000  423.729  352.190  -13.47}
\CommentTok{# Warsaw Borough:Rural             12.000   64.393   46.460   -7.69}
\CommentTok{# Warsaw Borough:Urban              9.000   12.483   11.758   -1.02}
\CommentTok{# Warsaw Borough:Urban/rural        8.000   25.172   22.354   -3.63}
\CommentTok{# Jtot                           3227.000 3795.486 1496.398  -14.70}
\end{Highlighting}
\end{Shaded}

So far, we have used binary weights, so the sum of join counts multiplied by the weight on that join remains integer. If we change to row standardised weights, where the weights almost always fractions of 1, the counts, expectations and variances change, but there are few major changes in the z-values.

Using an inverse distance based \texttt{listw} object does, however, change the z-values markedly, because closer centroids are upweighted relatively strongly:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Types }\OperatorTok{%>%}\StringTok{ }\KeywordTok{joincount.multi}\NormalTok{(}\DataTypeTok{listw=}\NormalTok{lw_d183_idw_B)}
\CommentTok{#                               Joincount Expected Variance z-value}
\CommentTok{# Rural:Rural                    3.46e+02 3.61e+02 4.93e+01   -2.10}
\CommentTok{# Urban:Urban                    2.90e+01 1.35e+01 2.23e+00   10.39}
\CommentTok{# Urban/rural:Urban/rural        4.65e+01 5.51e+01 9.61e+00   -2.79}
\CommentTok{# Warsaw Borough:Warsaw Borough  1.68e+01 4.53e-02 6.61e-03  206.38}
\CommentTok{# Urban:Rural                    2.02e+02 1.40e+02 2.36e+01   12.73}
\CommentTok{# Urban/rural:Rural              2.25e+02 2.83e+02 3.59e+01   -9.59}
\CommentTok{# Urban/rural:Urban              3.65e+01 5.48e+01 8.86e+00   -6.14}
\CommentTok{# Warsaw Borough:Rural           5.65e+00 8.33e+00 1.73e+00   -2.04}
\CommentTok{# Warsaw Borough:Urban           9.18e+00 1.61e+00 2.54e-01   15.01}
\CommentTok{# Warsaw Borough:Urban/rural     3.27e+00 3.25e+00 5.52e-01    0.02}
\CommentTok{# Jtot                           4.82e+02 4.91e+02 4.16e+01   -1.38}
\end{Highlighting}
\end{Shaded}

\hypertarget{morans-i}{%
\subsection{\texorpdfstring{Moran's \(I\)}{Moran's I}}\label{morans-i}}

The implementation of Moran's \(I\) in \textbf{spdep} in the \texttt{moran.test()} function has similar arguments to those of \texttt{joincount.test()}, but \texttt{sampling=} is replaced by \texttt{randomisation=} to indicate the underlying analytical approach used for calculating the variance of the measure. It is also possible to use ranks rather than numerical values \citep[p.~46]{cliff+ord:81}. The \texttt{drop.EI2=} agrument may be used to reproduce results where the final component of the variance term is omitted as found in some legacy software implementations.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{args}\NormalTok{(moran.test)}
\CommentTok{# function (x, listw, randomisation = TRUE, zero.policy = NULL, }
\CommentTok{#     alternative = "greater", rank = FALSE, na.action = na.fail, }
\CommentTok{#     spChk = NULL, adjust.n = TRUE, drop.EI2 = FALSE) }
\CommentTok{# NULL}
\end{Highlighting}
\end{Shaded}

The default for the \texttt{randomisation=} argument is \texttt{TRUE}, but here we will simply show that the test under normality is the same as a test of least squares residuals with only the intercept used in the mean model; the analysed variable is first round turnout proportion of registered voters in municipalities and Warsaw burroughs in the 2015 Polish presidential election. The spelling of randomisation is that of Cliff and Ord \citeyearpar{cliff+ord:73}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(pol_pres15 }\OperatorTok{%>%}\StringTok{ }
\StringTok{        }\KeywordTok{st_drop_geometry}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{        }\KeywordTok{subset}\NormalTok{(}\DataTypeTok{select=}\NormalTok{I_turnout, }\DataTypeTok{drop=}\OtherTok{TRUE}\NormalTok{) ->}\StringTok{ }\NormalTok{z) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{moran.test}\NormalTok{(}\DataTypeTok{listw=}\NormalTok{lw_q_B, }\DataTypeTok{randomisation=}\OtherTok{FALSE}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{glance_htest}\NormalTok{()}
\CommentTok{# Moran I statistic       Expectation          Variance }
\CommentTok{#          0.691434         -0.000401          0.000140 }
\CommentTok{#       Std deviate           p.value }
\CommentTok{#         58.461349          0.000000}
\end{Highlighting}
\end{Shaded}

The \texttt{lm.morantest()} function also takes a \texttt{resfun=} argument to set the function used to extract the residuals used for testing, and clearly lets us model other salient features of the response variable \citep[p.~203]{cliff+ord:81}. To compare with the standard test, we are only using the intercept here and, as can be seen, the results are the same.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{lm}\NormalTok{(I_turnout }\OperatorTok{~}\StringTok{ }\DecValTok{1}\NormalTok{, pol_pres15) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{lm.morantest}\NormalTok{(}\DataTypeTok{listw=}\NormalTok{lw_q_B) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{glance_htest}\NormalTok{()}
\CommentTok{# Observed Moran I      Expectation         Variance }
\CommentTok{#         0.691434        -0.000401         0.000140 }
\CommentTok{#      Std deviate          p.value }
\CommentTok{#        58.461349         0.000000}
\end{Highlighting}
\end{Shaded}

The only difference between tests under normality and randomisation is that an extra term is added if the kurtosis of the variable of interest indicates a flatter or more peaked distribution, where the measure used is the classical measure of kurtosis. Under the default randomisation assumption of analytical randomisation, the results are largely unchanged.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(z }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{moran.test}\NormalTok{(}\DataTypeTok{listw=}\NormalTok{lw_q_B) ->}\StringTok{ }\NormalTok{mtr) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{glance_htest}\NormalTok{()}
\CommentTok{# Moran I statistic       Expectation          Variance }
\CommentTok{#          0.691434         -0.000401          0.000140 }
\CommentTok{#       Std deviate           p.value }
\CommentTok{#         58.459835          0.000000}
\end{Highlighting}
\end{Shaded}

From the very beginning in the early 1970s, interest was shown in Monte Carlo tests, also known as Hope-type tests and as permutation bootstrap. By default, \texttt{moran.mc()} returns a \texttt{"htest"} object, but may simply use \texttt{boot::boot()} internally and return a \texttt{"boot"} object when \texttt{return\_boot=TRUE}. In addition the number of simulations needs to be given as \texttt{nsim=}; that is the number of times the values of the observations are shuffled at random.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{z }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{moran.mc}\NormalTok{(}\DataTypeTok{listw=}\NormalTok{lw_q_B, }\DataTypeTok{nsim=}\DecValTok{999}\NormalTok{, }\DataTypeTok{return_boot =} \OtherTok{TRUE}\NormalTok{) ->}\StringTok{ }\NormalTok{mmc}
\end{Highlighting}
\end{Shaded}

The bootstrap permutation retains the outcomes of each of the random permutations, reporting the observed value of the statistic, here Moran's \(I\), the difference between this value and the mean of the simulations under randomisation (equivalent to \(E(I)\)), and the standard deviation of the simulations under randomisation.

If we compare the Monte Carlo and analytical variances of \(I\) under randomisation, we typically see few differences, arguably rendering Monte Carlo testing unnecessary.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{c}\NormalTok{(}\StringTok{"Permutation bootstrap"}\NormalTok{=}\KeywordTok{var}\NormalTok{(mmc}\OperatorTok{$}\NormalTok{t), }
  \StringTok{"Analytical randomisation"}\NormalTok{=}\KeywordTok{unname}\NormalTok{(mtr}\OperatorTok{$}\NormalTok{estimate[}\DecValTok{3}\NormalTok{]))}
\CommentTok{#    Permutation bootstrap Analytical randomisation }
\CommentTok{#                 0.000144                 0.000140}
\end{Highlighting}
\end{Shaded}

Geary's global \(C\) is implemented in \texttt{geary.test()} largely following the same argument structure as \texttt{moran.test()}. The Getis-Ord \(G\) test includes extra arguments to accommodate differences between implementations, as Bivand and Wong \citeyearpar{Bivand2018} found multiple divergences from the original definitions, often to omit no-neighbour observations generated when using distance band neighbours. It is given by \citep[page 194]{getis+ord:92}. For \(G^*\), the \(\sum_{(2)}\) constraint is relaxed by including \(i\) as a neighbour of itself (thereby also removing the no-neighbour problem, because all observations have at least one neighbour).

Finally, the empirical Bayes Moran's \(I\) takes account of the denominator in assessing spatial autocorrelation in rates data \citep{assuncao+reis:99}. Until now, we have considered the proportion of valid votes cast in relation to the numbers entitled to vote by spatial entity, but using \texttt{EBImoran.mc()} we can try to accommodate uncertainty in extreme rates in entities with small numbers entitled to vote. There is, however, little impact on the outcome in this case.

Global measures of spatial autocorrelation using spatial weights objects based on graphs of neighbours are, as we have seen, rather blunt tools, which for interpretation depend critically on a reasoned mean model of the variable in question. If the mean model is just the intercept, the global measures will respond to all kinds of mis-specification, not only spatial autocorrelation. The choice of entities for aggregation of data will typically be a key source of mis-specification.

\hypertarget{local-measures}{%
\section{Local measures}\label{local-measures}}

Building on insights from the weaknesses of global measures, local indicators of spatial association began to appear in the first half of the 1990s \citep{anselin:95, getis+ord:92, getis+ord:96}.

In addition, the Moran plot was introduced, plotting the values of the variable of interest against their spatially lagged values, typically using row-standardised weights to make the axes more directly comparable \citep{anselin:96}. The \texttt{moran.plot()} function also returns an influence measures object used to label observations exerting more than propotional influence on the slope of the line representing global Moran's \(I\). In Figure \ref{fig:moranplot}, we can see that there are many spatial entities exerting such influence. These pairs of observed and lagged observed values make up in aggregate the global measure, but can also be explored in detail. The quadrants of the Moran plot also show low-low pairs in the lower left quadrant, high-high in the upper right quadrant, and fewer low-high and high-low pairs in the upper left and lower right quadrants.



\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{moran.plot}\NormalTok{(}\DataTypeTok{listw=}\NormalTok{lw_q_W, }\DataTypeTok{labels=}\NormalTok{pol_pres15}\OperatorTok{$}\NormalTok{TERYT, }\DataTypeTok{cex=}\DecValTok{1}\NormalTok{, }\DataTypeTok{pch=}\StringTok{"."}\NormalTok{,}
        \DataTypeTok{xlab=}\StringTok{"I round turnout"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"lagged turnout"}\NormalTok{) ->}\StringTok{ }\NormalTok{infl_W}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{sds_files/figure-latex/moranplot-1} 

}

\caption{Moran plot of I round turnout, row standardised weights}\label{fig:moranplot}
\end{figure}

If we extract the hat value influence measure from the returned object, Figure \ref{fig:moranhat} suggests that some edge entities exert more than proportional influence (perhaps because of row standardisation), as do entities in or near larger urban areas.



\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pol_pres15}\OperatorTok{$}\NormalTok{hat_value <-}\StringTok{ }\NormalTok{infl_W}\OperatorTok{$}\NormalTok{hat}
\KeywordTok{tm_shape}\NormalTok{(pol_pres15) }\OperatorTok{+}\StringTok{ }\KeywordTok{tm_fill}\NormalTok{(}\StringTok{"hat_value"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{sds_files/figure-latex/moranhat-1} 

}

\caption{Moran plot hat values, row standardised neighbours}\label{fig:moranhat}
\end{figure}

\hypertarget{local-morans-i_i}{%
\subsection{\texorpdfstring{Local Moran's \(I_i\)}{Local Moran's I\_i}}\label{local-morans-i_i}}

Bivand and Wong \citeyearpar{Bivand2018} discuss issues impacting the use of local indicators, such as local Moran's \(I_i\) and local Getis-Ord \(G_i\). Some issues affect the calculation of the local indicators, others inference from their values. Because \(n\) statistics may be being calculated from the same number of observations, there are multiple comparison problems that need to be addressed. Although the apparent detection of hotspots from values of local indicators has been quite widely adopted, it remains fraught with difficulty because adjustment of the inferential basis to accommodate multiple comparisons is not often chosen, and as in the global case, mis-specification also remains a source of confusion. Further, interpreting local spatial autocorrelation in the presence of global spatial autocorrelation is challenging \citep{ord+getis:01, tiefelsdorf:02, bivandetal:09}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{args}\NormalTok{(localmoran)}
\CommentTok{# function (x, listw, zero.policy = NULL, na.action = na.fail, }
\CommentTok{#     conditional = FALSE, alternative = "greater", p.adjust.method = "none", }
\CommentTok{#     mlvar = TRUE, spChk = NULL, adjust.x = FALSE) }
\CommentTok{# NULL}
\end{Highlighting}
\end{Shaded}

The \texttt{mlvar=} and \texttt{adjust.x=} arguments to \texttt{localmoran()} are discussed in Bivand and Wong \citeyearpar{Bivand2018}, and permit matching with other implementations. The \texttt{p.adjust.method=} argument uses an untested speculation implemented in \texttt{p.adjustSP()} that adjustment should only take into account the cardinality of the neighbour set of each observation when adjusting for multiple comparisons; using \texttt{stats::p.adjust()} is preferable.

Taking \texttt{"two.sided"} p-values, we obtain:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{localmoran}\NormalTok{(}\DataTypeTok{listw=}\NormalTok{lw_q_W, }\DataTypeTok{alternative=}\StringTok{"two.sided"}\NormalTok{) ->}\StringTok{ }\NormalTok{locm}
\end{Highlighting}
\end{Shaded}

The \(I_i\) local indicators when summed and divided by the sum of the spatial weights equal global Moran's \(I\), showing the possible presence of positive and negative local spatial autocorrelation:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{all.equal}\NormalTok{(}\KeywordTok{sum}\NormalTok{(locm[,}\DecValTok{1}\NormalTok{])}\OperatorTok{/}\KeywordTok{Szero}\NormalTok{(lw_q_W), }\KeywordTok{unname}\NormalTok{(}\KeywordTok{moran.test}\NormalTok{(z, lw_q_W)}\OperatorTok{$}\NormalTok{estimate[}\DecValTok{1}\NormalTok{]))}
\CommentTok{# [1] TRUE}
\end{Highlighting}
\end{Shaded}

Using \texttt{stats::p.adjust()} to adjust for multiple comparisons, we see that almost 29\% of the 2495 local measures have p-values \textless{} 0.05 if no adjustment is applied, but only 12\% using Bonferroni adjustment to control the familywise error rate, with two other choices shown: \texttt{fdr} is the \citet{fdr-BH} false discovery rate and \texttt{BY} \citep{10.1214/aos/1013699998}, another false discovery rate adjustment:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pva <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(pv) }\KeywordTok{cbind}\NormalTok{(}\StringTok{"none"}\NormalTok{=pv, }\StringTok{"bonferroni"}\NormalTok{=}\KeywordTok{p.adjust}\NormalTok{(pv, }\StringTok{"bonferroni"}\NormalTok{),}
    \StringTok{"fdr"}\NormalTok{=}\KeywordTok{p.adjust}\NormalTok{(pv, }\StringTok{"fdr"}\NormalTok{), }\StringTok{"BY"}\NormalTok{=}\KeywordTok{p.adjust}\NormalTok{(pv, }\StringTok{"BY"}\NormalTok{))}
\NormalTok{locm }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{subset}\NormalTok{(}\DataTypeTok{select=}\StringTok{"Pr(z != 0)"}\NormalTok{, }\DataTypeTok{drop=}\OtherTok{TRUE}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{pva}\NormalTok{() ->}\StringTok{ }\NormalTok{pvsp}
\NormalTok{f <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{sum}\NormalTok{(x }\OperatorTok{<}\StringTok{ }\FloatTok{0.05}\NormalTok{)}
\KeywordTok{apply}\NormalTok{(pvsp, }\DecValTok{2}\NormalTok{, f)}
\CommentTok{#       none bonferroni        fdr         BY }
\CommentTok{#        715        297        576        424}
\end{Highlighting}
\end{Shaded}

In the global measure case, bootstrap permutations may be used as an alternative to analytical methods for possible inference, where both the theoretical development of the analytical variance of the measure, and the permutation scheme, shuffle all of the observed values. In the local case, conditional permutation may be used, fixing the value at observation \(i\) and randomly sampling from the remaining \(n-1\) values to find randomised values at neighbours, and is provided as \texttt{localmoran\_perm()}, which may use multiple nodes to sample in parallel if provided, and permits the setting of a seed for the random number generator across the compute nodes:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(parallel)}
\KeywordTok{set.coresOption}\NormalTok{(}\KeywordTok{ifelse}\NormalTok{(}\KeywordTok{detectCores}\NormalTok{() }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\KeywordTok{detectCores}\NormalTok{()}\OperatorTok{-}\NormalTok{1L))}
\CommentTok{# NULL}
\KeywordTok{system.time}\NormalTok{(z }\OperatorTok{%>%}\StringTok{ }
\StringTok{        }\KeywordTok{localmoran_perm}\NormalTok{(}\DataTypeTok{listw=}\NormalTok{lw_q_W, }\DataTypeTok{nsim=}\DecValTok{499}\NormalTok{, }\DataTypeTok{alternative=}\StringTok{"two.sided"}\NormalTok{,}
            \DataTypeTok{iseed=}\DecValTok{1}\NormalTok{) ->}\StringTok{ }\NormalTok{locm_p)}
\CommentTok{#    user  system elapsed }
\CommentTok{#    1.16    3.50    1.75}
\end{Highlighting}
\end{Shaded}

The outcome is that almost 32\% of observations have two sided p-values \textless{} 0.05 without multiple comparison adjustment, and under 3\% with Bonferroni adjustment.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{locm_p }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{subset}\NormalTok{(}\DataTypeTok{select=}\StringTok{"Pr(z != 0)"}\NormalTok{, }\DataTypeTok{drop=}\OtherTok{TRUE}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{pva}\NormalTok{() ->}\StringTok{ }\NormalTok{pvsp}
\KeywordTok{apply}\NormalTok{(pvsp, }\DecValTok{2}\NormalTok{, f)}
\CommentTok{#       none bonferroni        fdr         BY }
\CommentTok{#        797         76        463        161}
\end{Highlighting}
\end{Shaded}

We can see what is happening by tabulating counts of the standard deviate of local Moran's \(I\), where the two-sided \(\alpha=0.05\) bounds would be \(0.025\) and \(0.975\), but Bonferroni adjustment is close to \(0.00001\) and \(0.99999\). Without adjustment, almost 800 observations are significant, with Bonferroni adjustment, only almost 70 in the conditional permutation case:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{brks <-}\StringTok{ }\KeywordTok{qnorm}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.00001}\NormalTok{, }\FloatTok{0.0001}\NormalTok{, }\FloatTok{0.001}\NormalTok{, }\FloatTok{0.01}\NormalTok{, }\FloatTok{0.025}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.975}\NormalTok{, }\FloatTok{0.99}\NormalTok{, }\FloatTok{0.999}\NormalTok{, }\FloatTok{0.9999}\NormalTok{,}
    \FloatTok{0.99999}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\NormalTok{(locm_p }\OperatorTok{%>%}\StringTok{ }
\StringTok{        }\KeywordTok{subset}\NormalTok{(}\DataTypeTok{select=}\NormalTok{Z.Ii, }\DataTypeTok{drop=}\OtherTok{TRUE}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{        }\KeywordTok{cut}\NormalTok{(brks) }\OperatorTok{%>%}\StringTok{ }
\StringTok{        }\KeywordTok{table}\NormalTok{()->}\StringTok{ }\NormalTok{tab)}
\CommentTok{# .}
\CommentTok{#  (-Inf,-4.26] (-4.26,-3.72] (-3.72,-3.09] (-3.09,-2.33] }
\CommentTok{#             0             0             1             4 }
\CommentTok{# (-2.33,-1.96]     (-1.96,0]      (0,1.96]   (1.96,2.33] }
\CommentTok{#             5           459          1239           195 }
\CommentTok{#   (2.33,3.09]   (3.09,3.72]   (3.72,4.26]   (4.26, Inf] }
\CommentTok{#           316           145            55            76}
\KeywordTok{sum}\NormalTok{(tab[}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{, }\DecValTok{8}\OperatorTok{:}\DecValTok{12}\NormalTok{)])}
\CommentTok{# [1] 797}
\KeywordTok{sum}\NormalTok{(tab[}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{12}\NormalTok{)])}
\CommentTok{# [1] 76}
\end{Highlighting}
\end{Shaded}

In an important clarification, \citet{sauer_oshan_rey_wolf_2021} show that the comparison of standard deviates for local Moran's \(I_i\) based on analytical formulae and conditional permutation in \citet{Bivand2018} was based on a misunderstanding. \citet{sokaletal:98} provide alternative analytical formulae for standard deviates of local Moran's \(I_i\) based either on total or conditional permutation, but the analytical formulae used in \citet{Bivand2018}, based on earlier practice, only use total permutation, and consequently do not match the simulation conditional permutations. Thanks to a timely pull request, \texttt{localmoran()} now has a \texttt{conditional=} argument using alternative formulae from the appendix of \citet{sokaletal:98}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{localmoran}\NormalTok{(}\DataTypeTok{listw=}\NormalTok{lw_q_W, }\DataTypeTok{conditional=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{alternative=}\StringTok{"two.sided"}\NormalTok{) ->}\StringTok{ }\NormalTok{locm_c}
\end{Highlighting}
\end{Shaded}

yielding standard deviates that correspond closely to those from simulation:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{locm_c }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{subset}\NormalTok{(}\DataTypeTok{select=}\StringTok{"Pr(z != 0)"}\NormalTok{, }\DataTypeTok{drop=}\OtherTok{TRUE}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{pva}\NormalTok{() ->}\StringTok{ }\NormalTok{pvsp}
\KeywordTok{apply}\NormalTok{(pvsp, }\DecValTok{2}\NormalTok{, f)}
\CommentTok{#       none bonferroni        fdr         BY }
\CommentTok{#        789         69        468        156}
\end{Highlighting}
\end{Shaded}



\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pol_pres15}\OperatorTok{$}\NormalTok{locm_Z <-}\StringTok{ }\NormalTok{locm[, }\StringTok{"Z.Ii"}\NormalTok{]}
\NormalTok{pol_pres15}\OperatorTok{$}\NormalTok{locm_c_Z <-}\StringTok{ }\NormalTok{locm_c[, }\StringTok{"Z.Ii"}\NormalTok{]}
\NormalTok{pol_pres15}\OperatorTok{$}\NormalTok{locm_p_Z <-}\StringTok{ }\NormalTok{locm_p[, }\StringTok{"Z.Ii"}\NormalTok{]}
\KeywordTok{tm_shape}\NormalTok{(pol_pres15) }\OperatorTok{+}\StringTok{ }\KeywordTok{tm_fill}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"locm_Z"}\NormalTok{, }\StringTok{"locm_c_Z"}\NormalTok{, }\StringTok{"locm_p_Z"}\NormalTok{), }\DataTypeTok{breaks=}\NormalTok{brks,}
    \DataTypeTok{midpoint=}\DecValTok{0}\NormalTok{, }\DataTypeTok{title=}\StringTok{"Standard deviates of}\CharTok{\textbackslash{}n}\StringTok{Local Moran's I"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{tm_facets}\NormalTok{(}\DataTypeTok{free.scales=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{ncol=}\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{tm_layout}\NormalTok{(}\DataTypeTok{panel.labels=}\KeywordTok{c}\NormalTok{(}\StringTok{"Analytical total"}\NormalTok{,}
        \StringTok{"Analytical conditional"}\NormalTok{, }\StringTok{"Conditional permutation"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{sds_files/figure-latex/localmoranZ-1} 

}

\caption{Analytical total and conditional permutation, and bootstrap conditional permutation standard deviates of local Moran's I for first round turnout, row-standardised neighbours}\label{fig:localmoranZ}
\end{figure}

Figure \ref{fig:localmoranZ} shows that conditional permutation scales back the proportion of standard deviate values taking extreme values, especially positive values. The analytical total standard deviates of local Moran's \(I\) should probably not be used since alternatives are available, not least thanks to the clarification by \citet{sauer_oshan_rey_wolf_2021}.

In presenting local Moran's \(I\), use is often made of ``hotspot'' maps. Because \(I_i\) takes high values both for strong positive autocorrelation of low and high values of the input variable, it is hard to show where ``clusters'' of similar neighbours with low or high values of the input variable occur. The quadrants of the Moran plot are used, by creating a categorical quadrant variable interacting the input variable and its spatial lag split at their means. The quadrant categories are then set to NA if, for the chosen standard deviate and adjustment, \(I_i\) would be considered insignificant. Here, for the Bonferroni adjusted conditional analytical standard deviates, 14 observations belong to ``Low-Low clusters'', and 55 to ``High-High clusters'':

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{quadr <-}\StringTok{ }\KeywordTok{interaction}\NormalTok{(}
    \KeywordTok{cut}\NormalTok{(infl_W}\OperatorTok{$}\NormalTok{x, }\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\OtherTok{Inf}\NormalTok{, }\KeywordTok{mean}\NormalTok{(infl_W}\OperatorTok{$}\NormalTok{x), }\OtherTok{Inf}\NormalTok{), }\DataTypeTok{labels=}\KeywordTok{c}\NormalTok{(}\StringTok{"Low X"}\NormalTok{, }\StringTok{"High X"}\NormalTok{)),}
    \KeywordTok{cut}\NormalTok{(infl_W}\OperatorTok{$}\NormalTok{wx, }\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\OtherTok{Inf}\NormalTok{, }\KeywordTok{mean}\NormalTok{(infl_W}\OperatorTok{$}\NormalTok{wx), }\OtherTok{Inf}\NormalTok{), }\DataTypeTok{labels=}\KeywordTok{c}\NormalTok{(}\StringTok{"Low WX"}\NormalTok{, }\StringTok{"High WX"}\NormalTok{)),}
    \DataTypeTok{sep=}\StringTok{" : "}\NormalTok{)}
\NormalTok{a <-}\StringTok{ }\KeywordTok{table}\NormalTok{(quadr)}
\NormalTok{pol_pres15}\OperatorTok{$}\NormalTok{hs_an_q <-}\StringTok{ }\NormalTok{quadr}
\KeywordTok{is.na}\NormalTok{(pol_pres15}\OperatorTok{$}\NormalTok{hs_an_q) <-}\StringTok{ }\OperatorTok{!}\NormalTok{(pol_pres15}\OperatorTok{$}\NormalTok{locm_Z }\OperatorTok{<}\StringTok{ }\NormalTok{brks[}\DecValTok{6}\NormalTok{] }\OperatorTok{|}\StringTok{ }
\StringTok{        }\NormalTok{pol_pres15}\OperatorTok{$}\NormalTok{locm_Z }\OperatorTok{>}\StringTok{ }\NormalTok{brks[}\DecValTok{8}\NormalTok{])}
\NormalTok{b <-}\StringTok{ }\KeywordTok{table}\NormalTok{(pol_pres15}\OperatorTok{$}\NormalTok{hs_an_q)}
\NormalTok{pol_pres15}\OperatorTok{$}\NormalTok{hs_ac_q <-}\StringTok{ }\NormalTok{quadr}
\KeywordTok{is.na}\NormalTok{(pol_pres15}\OperatorTok{$}\NormalTok{hs_ac_q) <-}\StringTok{ }\OperatorTok{!}\NormalTok{(pol_pres15}\OperatorTok{$}\NormalTok{locm_c_Z }\OperatorTok{<}\StringTok{ }\NormalTok{brks[}\DecValTok{2}\NormalTok{] }\OperatorTok{|}\StringTok{ }
\StringTok{        }\NormalTok{pol_pres15}\OperatorTok{$}\NormalTok{locm_c_Z }\OperatorTok{>}\StringTok{ }\NormalTok{brks[}\DecValTok{12}\NormalTok{])}
\NormalTok{c <-}\StringTok{ }\KeywordTok{table}\NormalTok{(pol_pres15}\OperatorTok{$}\NormalTok{hs_ac_q)}
\NormalTok{pol_pres15}\OperatorTok{$}\NormalTok{hs_cp_q <-}\StringTok{ }\NormalTok{quadr}
\KeywordTok{is.na}\NormalTok{(pol_pres15}\OperatorTok{$}\NormalTok{hs_cp_q) <-}\StringTok{ }\OperatorTok{!}\NormalTok{(pol_pres15}\OperatorTok{$}\NormalTok{locm_p_Z }\OperatorTok{<}\StringTok{ }\NormalTok{brks[}\DecValTok{2}\NormalTok{] }\OperatorTok{|}\StringTok{ }
\StringTok{        }\NormalTok{pol_pres15}\OperatorTok{$}\NormalTok{locm_p_Z }\OperatorTok{>}\StringTok{ }\NormalTok{brks[}\DecValTok{12}\NormalTok{])}
\NormalTok{d <-}\StringTok{ }\KeywordTok{table}\NormalTok{(pol_pres15}\OperatorTok{$}\NormalTok{hs_cp_q)}
\KeywordTok{t}\NormalTok{(}\KeywordTok{rbind}\NormalTok{(}\StringTok{"Moran plot quadrants"}\NormalTok{=a, }\StringTok{"Unadjusted analytical total"}\NormalTok{=b, }
    \StringTok{"Bonferroni analytical cond."}\NormalTok{=c, }\StringTok{"Bonferroni cond. perm."}\NormalTok{=d))}
\CommentTok{#                  Moran plot quadrants Unadjusted analytical total}
\CommentTok{# Low X : Low WX                   1040                         370}
\CommentTok{# High X : Low WX                   264                           3}
\CommentTok{# Low X : High WX                   213                           0}
\CommentTok{# High X : High WX                  978                         342}
\CommentTok{#                  Bonferroni analytical cond.}
\CommentTok{# Low X : Low WX                            14}
\CommentTok{# High X : Low WX                            0}
\CommentTok{# Low X : High WX                            0}
\CommentTok{# High X : High WX                          55}
\CommentTok{#                  Bonferroni cond. perm.}
\CommentTok{# Low X : Low WX                       18}
\CommentTok{# High X : Low WX                       0}
\CommentTok{# Low X : High WX                       0}
\CommentTok{# High X : High WX                     58}
\end{Highlighting}
\end{Shaded}



\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tm_shape}\NormalTok{(pol_pres15) }\OperatorTok{+}\StringTok{ }\KeywordTok{tm_fill}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"hs_an_q"}\NormalTok{, }\StringTok{"hs_ac_q"}\NormalTok{, }\StringTok{"hs_cp_q"}\NormalTok{), }\DataTypeTok{colorNA=}\StringTok{"grey95"}\NormalTok{,}
    \DataTypeTok{textNA=}\StringTok{"Not significant"}\NormalTok{, }\DataTypeTok{title=}\StringTok{"Turnout hotspot status}\CharTok{\textbackslash{}n}\StringTok{Local Moran's I"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{tm_facets}\NormalTok{(}\DataTypeTok{free.scales=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{ncol=}\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{tm_layout}\NormalTok{(}\DataTypeTok{panel.labels=}
            \KeywordTok{c}\NormalTok{(}\StringTok{"Unadjusted analytical total"}\NormalTok{, }\StringTok{"Bonferroni analytical cond."}\NormalTok{, }
              \StringTok{"Cond. perm. with Bonferroni"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{sds_files/figure-latex/Iihotspots-1} 

}

\caption{Local Moran's I hotspot maps \(\alpha = 0.05\): left panel upper: unadjusted analytical standard deviates; right upper panel: Bonferroni adjusted analytical conditional standard deviates; left lower panel: Bonferroni adjusted bootstrap conditional permutation standard deviates, first round turnout, row-standardised neighbours}\label{fig:Iihotspots}
\end{figure}

Figure \ref{fig:Iihotspots} shows the impact of using analytical or conditional permutation standard deviates, and no or Bonferroni adjustment, reducing the counts of observations in ``Low-Low clusters'' from 370 to 14, and ``High-High clusters'' from 342 to 54; the ``High-High clusters'' are metropolitan areas.

\citet{tiefelsdorf:02} argues that standard approaches to the calculation of the standard deviates of local Moran's \(I_i\) should be supplemented by numerical estimates, and shows that Saddlepoint approximations are a computationally efficient way of achieving this goal. The \texttt{localmoran.sad()} function takes a fitted linear model as its first argument, so we first fit a null (intercept only) model, but use case weights because the numbers entitled to vote vary greatly between observations:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{lm}\NormalTok{(z }\OperatorTok{~}\StringTok{ }\DecValTok{1}\NormalTok{, }\DataTypeTok{weights=}\NormalTok{pol_pres15}\OperatorTok{$}\NormalTok{I_entitled_to_vote) ->}\StringTok{ }\NormalTok{lm_null}
\end{Highlighting}
\end{Shaded}

Saddlepoint approximation is much more computationally intensive than conditional permutation:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{system.time}\NormalTok{(lm_null }\OperatorTok{%>%}\StringTok{ }\KeywordTok{localmoran.sad}\NormalTok{(}\DataTypeTok{nb=}\NormalTok{nb_q, }\DataTypeTok{style=}\StringTok{"W"}\NormalTok{, }\DataTypeTok{alternative=}\StringTok{"two.sided"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{        }\KeywordTok{summary}\NormalTok{() ->}\StringTok{ }\NormalTok{locm_sad_null)}
\CommentTok{#    user  system elapsed }
\CommentTok{#  19.769   0.104  19.874}
\end{Highlighting}
\end{Shaded}

However, standard approaches do not permit richer mean models with covariates or case weights. Next we add the categorical variable distinguising between rural, urban and other types of observational unit:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{lm}\NormalTok{(z }\OperatorTok{~}\StringTok{ }\NormalTok{Types, }\DataTypeTok{weights=}\NormalTok{pol_pres15}\OperatorTok{$}\NormalTok{I_entitled_to_vote) ->}\StringTok{ }\NormalTok{lm_types}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{system.time}\NormalTok{(lm_types }\OperatorTok{%>%}\StringTok{ }\KeywordTok{localmoran.sad}\NormalTok{(}\DataTypeTok{nb=}\NormalTok{nb_q, }\DataTypeTok{style=}\StringTok{"W"}\NormalTok{, }\DataTypeTok{alternative=}\StringTok{"two.sided"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{        }\KeywordTok{summary}\NormalTok{() ->}\StringTok{ }\NormalTok{locm_sad_types)}
\CommentTok{#    user  system elapsed }
\CommentTok{#  21.052   0.004  21.088}
\end{Highlighting}
\end{Shaded}

To conclude, we add the spatially lagged categories, although the spatial lag of a categorical variable, represented by dummies in the model matrix, is not well defined:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spatialreg}\OperatorTok{::}\KeywordTok{lmSLX}\NormalTok{(z }\OperatorTok{~}\StringTok{ }\NormalTok{Types, }\DataTypeTok{listw=}\NormalTok{lw_q_W, }
    \DataTypeTok{weights=}\NormalTok{pol_pres15}\OperatorTok{$}\NormalTok{I_entitled_to_vote) ->}\StringTok{ }\NormalTok{lm_Dtypes}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{system.time}\NormalTok{(lm_Dtypes }\OperatorTok{%>%}\StringTok{ }\KeywordTok{localmoran.sad}\NormalTok{(}\DataTypeTok{nb=}\NormalTok{nb_q, }\DataTypeTok{style=}\StringTok{"W"}\NormalTok{, }\DataTypeTok{alternative=}\StringTok{"two.sided"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{        }\KeywordTok{summary}\NormalTok{() ->}\StringTok{ }\NormalTok{locm_sad_Dtypes)}
\CommentTok{#    user  system elapsed }
\CommentTok{#  21.869   0.008  21.906}
\end{Highlighting}
\end{Shaded}

(ref:localmoranZfc\_sad) Saddlepoint weighted null model, weighted types model, weighted Durbin types model, and analytical conditional standard deviates of local Moran's I for first round turnout, row-standardised neighbours

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pol_pres15}\OperatorTok{$}\NormalTok{locm_sad_null_Z <-}\StringTok{ }\NormalTok{locm_sad_null[, }\StringTok{"Saddlepoint"}\NormalTok{]}
\NormalTok{pol_pres15}\OperatorTok{$}\NormalTok{locm_sad_types_Z <-}\StringTok{ }\NormalTok{locm_sad_types[, }\StringTok{"Saddlepoint"}\NormalTok{]}
\NormalTok{pol_pres15}\OperatorTok{$}\NormalTok{locm_sad_Dtypes_Z <-}\StringTok{ }\NormalTok{locm_sad_Dtypes[, }\StringTok{"Saddlepoint"}\NormalTok{]}
\KeywordTok{tm_shape}\NormalTok{(pol_pres15) }\OperatorTok{+}\StringTok{ }\KeywordTok{tm_fill}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"locm_sad_null_Z"}\NormalTok{, }\StringTok{"locm_sad_types_Z"}\NormalTok{,}
    \StringTok{"locm_sad_Dtypes_Z"}\NormalTok{, }\StringTok{"locm_c_Z"}\NormalTok{), }\DataTypeTok{breaks=}\NormalTok{brks, }\DataTypeTok{midpoint=}\DecValTok{0}\NormalTok{,}
    \DataTypeTok{title=}\StringTok{"Standard deviates of}\CharTok{\textbackslash{}n}\StringTok{Local Moran's I"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{tm_facets}\NormalTok{(}\DataTypeTok{free.scales=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{ncol=}\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{tm_layout}\NormalTok{(}\DataTypeTok{panel.labels=}\KeywordTok{c}\NormalTok{(}
        \StringTok{"Saddlepoint weighted null"}\NormalTok{,}
        \StringTok{"Saddlepoint weighted types"}\NormalTok{,}
        \StringTok{"Saddlepoint weighted Durbin types"}\NormalTok{,}
        \StringTok{"Analytical conditional"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\textbackslash{}begin\{figure\}

\{\centering \includegraphics[width=1\linewidth]{sds_files/figure-latex/localmoranZ_sad-1}

\}

\textbackslash{}caption\{(ref:localmoranZfc\_sad)\}(\#fig:localmoranZ\_sad)
\textbackslash{}end\{figure\}
Figure @ref(fig:localmoranZ\_sad) includes the analytical conditional standard deviates for comparison (lower right panel), but in general it can be seen that the Saddlepoint approximation standard deviates lie closer to zero, possibly because some of the mis-specification in the mean model has been removed by using richer versions, and possibly because the approximation approach is inherently local, relating regression residual values at \(i\) to those of its neighbours. It is also possible to use Saddlepoint approximation where the global spatial process has been incorporated, removing the conflation of global and local spatial autocorrelation in standard approaches. The same can also be accomplished using exxact methods \citep{bivandetal:09}.

\hypertarget{local-getis-ord-g_i}{%
\subsection{\texorpdfstring{Local Getis-Ord \(G_i\)}{Local Getis-Ord G\_i}}\label{local-getis-ord-g_i}}

The local Getis-Ord \(G\) measure is reported as a standard deviate, and may also take the \(G^*_i\) form where self-neighbours are inserted into the neighbour object using \texttt{include.self()}. The observed and expected values of local \(G\) with their analytical variances may also be returned if \texttt{return\_internals=TRUE}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{system.time}\NormalTok{(z }\OperatorTok{%>%}\StringTok{ }
\StringTok{        }\KeywordTok{localG}\NormalTok{(lw_q_W) ->}\StringTok{ }\NormalTok{locG)}
\CommentTok{#    user  system elapsed }
\CommentTok{#   0.024   0.000   0.024}
\KeywordTok{system.time}\NormalTok{(z }\OperatorTok{%>%}\StringTok{ }
\StringTok{        }\KeywordTok{localG_perm}\NormalTok{(lw_q_W, }\DataTypeTok{nsim=}\DecValTok{499}\NormalTok{, }\DataTypeTok{iseed=}\DecValTok{1}\NormalTok{) ->}\StringTok{ }\NormalTok{locG_p)}
\CommentTok{#    user  system elapsed }
\CommentTok{#   0.025   1.024   1.420}
\end{Highlighting}
\end{Shaded}

Once again we face the problem of multiple comparisons, with the count of areal unit p-values \textless{} 0.05 being reduced by an order of magnitude when employing Bonferroni correction:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{locG }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{c}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{abs}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{pnorm}\NormalTok{(}\DataTypeTok{lower.tail =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\NormalTok{(}\ControlFlowTok{function}\NormalTok{(x) x}\OperatorTok{*}\DecValTok{2}\NormalTok{)() }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{pva}\NormalTok{() ->}\StringTok{ }\NormalTok{pvsp}
\KeywordTok{apply}\NormalTok{(pvsp, }\DecValTok{2}\NormalTok{, f)}
\CommentTok{#       none bonferroni        fdr         BY }
\CommentTok{#        789         69        468        156}
\end{Highlighting}
\end{Shaded}



\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{Zi=}\NormalTok{locm_c[,}\DecValTok{4}\NormalTok{], }\DataTypeTok{Zi_perm=}\NormalTok{locm_p[,}\DecValTok{4}\NormalTok{])) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{Zi,}
    \DataTypeTok{y=}\NormalTok{Zi_perm), }\DataTypeTok{alpha=}\FloatTok{0.2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Analytical conditional"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Bootstrap conditional"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{coord_fixed}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Local Moran's I"}\NormalTok{)}
\NormalTok{p2 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{Zi=}\KeywordTok{c}\NormalTok{(locG), }\DataTypeTok{Zi_perm=}\KeywordTok{c}\NormalTok{(locG_p))) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{Zi, }
    \DataTypeTok{y=}\NormalTok{Zi_perm), }\DataTypeTok{alpha=}\FloatTok{0.2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Analytical conditional"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Bootstrap conditional"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{coord_fixed}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Local G"}\NormalTok{)}
\NormalTok{gridExtra}\OperatorTok{::}\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{nrow=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{sds_files/figure-latex/localZvalues-1} 

}

\caption{Plots of analytical conditional against bootstrap standard deviates; left: local Moran's I; right: local G; first round turnout, row-standardised neighbours}\label{fig:localZvalues}
\end{figure}

Figure \ref{fig:localZvalues} shows that, when using analytical conditional standard deviates, the values from analytical and bootstrap estimates coincide for both \(I_i\) and \(G_i\). In both cases, one may argue that the bootstrap approach is superfluous in exploratory spatial data analysis.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pol_pres15}\OperatorTok{$}\NormalTok{locG_Z <-}\StringTok{ }\KeywordTok{c}\NormalTok{(locG)}
\NormalTok{pol_pres15}\OperatorTok{$}\NormalTok{hs_G <-}\StringTok{ }\KeywordTok{cut}\NormalTok{(}\KeywordTok{c}\NormalTok{(locG), }\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\OtherTok{Inf}\NormalTok{, brks[}\DecValTok{2}\NormalTok{], brks[}\DecValTok{12}\NormalTok{], }\OtherTok{Inf}\NormalTok{), }
    \DataTypeTok{labels=}\KeywordTok{c}\NormalTok{(}\StringTok{"Low"}\NormalTok{, }\StringTok{"Not significant"}\NormalTok{, }\StringTok{"High"}\NormalTok{))}
\KeywordTok{table}\NormalTok{(pol_pres15}\OperatorTok{$}\NormalTok{hs_G)}
\CommentTok{# }
\CommentTok{#             Low Not significant            High }
\CommentTok{#              14            2426              55}
\end{Highlighting}
\end{Shaded}



\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1 <-}\StringTok{ }\KeywordTok{tm_shape}\NormalTok{(pol_pres15) }\OperatorTok{+}\StringTok{ }\KeywordTok{tm_fill}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"locG_Z"}\NormalTok{), }\DataTypeTok{midpoint=}\DecValTok{0}\NormalTok{, }\DataTypeTok{title=}\StringTok{"Standard}\CharTok{\textbackslash{}n}\StringTok{deviate"}\NormalTok{)}
\NormalTok{m2 <-}\StringTok{ }\KeywordTok{tm_shape}\NormalTok{(pol_pres15) }\OperatorTok{+}\StringTok{ }\KeywordTok{tm_fill}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"hs_G"}\NormalTok{), }
    \DataTypeTok{title=}\StringTok{"Local G Bonferroni}\CharTok{\textbackslash{}n}\StringTok{adjusted hotspot status"}\NormalTok{)}
\KeywordTok{tmap_arrange}\NormalTok{(m1, m2, }\DataTypeTok{nrow=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{sds_files/figure-latex/localG-1} 

}

\caption{Left: analytical standard deviates of local G; right: Bonferroni hotspots; first round turnout, row-standardised neighbours}\label{fig:localG}
\end{figure}

As can be seen from Figure \ref{fig:localZvalues}, we do not need to contrast the two estimation methods, and showing the mapped standard deviate is as informative as the ``hotspot'' status for the chosen adjustment (Figure \ref{fig:localG}). In the case of \(G_i\), the values taken by the measure reflect the values of the input variable, so a ``High cluster'' is found for observations with high values of the input variable, here high turnout in metropolitan areas.

Very recently, Geoda has been wrapped for R as \textbf{rgeoda} \citep{R-rgeoda}, and will provide very similar functionalities for the exploration of spatial autocorrelation in areal data as \textbf{spdep}. The active objects are kept as pointers to a compiled code workspace; using compiled code for all operations (as in Geoda itself) makes \textbf{rgeoda} perform fast, but leaves less flexible when modifications or enhancements are desired.

The contiguity neighbours it constructs are the same as those found by \texttt{poly2nb()}, as almost are the \(I_i\) measures. The difference is as established by \citet{Bivand2018}, that \texttt{localmoran()} calculates the input variable variance divinding by \(n\), but Geoda uses \((n-1)\), as can be reproduced by setting \texttt{mlvar=FALSE}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(rgeoda)}
\CommentTok{# Loading required package: digest}
\CommentTok{# }
\CommentTok{# Attaching package: 'rgeoda'}
\CommentTok{# The following object is masked from 'package:spdep':}
\CommentTok{# }
\CommentTok{#     skater}
\KeywordTok{system.time}\NormalTok{(Geoda_w <-}\StringTok{ }\KeywordTok{queen_weights}\NormalTok{(pol_pres15))}
\CommentTok{# here}
\CommentTok{#    user  system elapsed }
\CommentTok{#   0.136   0.008   0.144}
\KeywordTok{summary}\NormalTok{(Geoda_w)}
\CommentTok{#                      name               value}
\CommentTok{# 1 number of observations:                2495}
\CommentTok{# 2          is symmetric:                 TRUE}
\CommentTok{# 3               sparsity: 0.00228786229774178}
\CommentTok{# 4        # min neighbors:                   1}
\CommentTok{# 5        # max neighbors:                  13}
\CommentTok{# 6       # mean neighbors:    5.70821643286573}
\CommentTok{# 7     # median neighbors:                   6}
\CommentTok{# 8           has isolates:               FALSE}
\KeywordTok{system.time}\NormalTok{(lisa <-}\StringTok{ }\KeywordTok{local_moran}\NormalTok{(Geoda_w, pol_pres15[}\StringTok{"I_turnout"}\NormalTok{], }
    \DataTypeTok{cpu_threads=}\KeywordTok{ifelse}\NormalTok{(parallel}\OperatorTok{::}\KeywordTok{detectCores}\NormalTok{() }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, parallel}\OperatorTok{::}\KeywordTok{detectCores}\NormalTok{()}\OperatorTok{-}\NormalTok{1L),}
    \DataTypeTok{permutations=}\DecValTok{499}\NormalTok{, }\DataTypeTok{seed=}\DecValTok{1}\NormalTok{))}
\CommentTok{#    user  system elapsed }
\CommentTok{#   0.437   0.004   0.115}
\KeywordTok{all.equal}\NormalTok{(}\KeywordTok{card}\NormalTok{(nb_q), }\KeywordTok{lisa_num_nbrs}\NormalTok{(lisa), }\DataTypeTok{check.attributes=}\OtherTok{FALSE}\NormalTok{)}
\CommentTok{# [1] TRUE}
\KeywordTok{all.equal}\NormalTok{(}\KeywordTok{lisa_values}\NormalTok{(lisa), }\KeywordTok{localmoran}\NormalTok{(pol_pres15}\OperatorTok{$}\NormalTok{I_turnout, }
    \DataTypeTok{listw=}\NormalTok{lw_q_W, }\DataTypeTok{mlvar=}\OtherTok{FALSE}\NormalTok{)[,}\DecValTok{1}\NormalTok{], }\DataTypeTok{check.attributes=}\OtherTok{FALSE}\NormalTok{)}
\CommentTok{# [1] TRUE}
\end{Highlighting}
\end{Shaded}

\hypertarget{exercises-13}{%
\section{Exercises}\label{exercises-13}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Why are join-count measures on a chessboard so different between \texttt{rook} and \texttt{queen} neighbours?
\item
  Please repeat the simulation shown in section 15.1 using the chessboard polygons and the row-standardized \texttt{queen} contiguity neighbours. Why is it important to understand that spatial autocorrelation usually signals (unavoidable) mis-specification in our data?
\item
  Do we need to use conditional permutation in inference for local measures of spatial autocorrelation?
\item
  Why is false discovery rate adjustment recommended for local measures of spatial autocorrelation?
\item
  Compare the local Moran's \(I_i\) standard deviate values for the simulated data from exercise 15.2 for the analytical conditional approach, and Saddlepoint approximation. Consider the advantages and disadvantages of the Saddlepoint approximation approach.
\end{enumerate}

\hypertarget{spatglmm}{%
\chapter{Spatial Regression}\label{spatglmm}}

Even though it may be tempting to focus on interpreting the map pattern of an areal support response variable of interest, the pattern may largely derive from covariates (and their functional forms), as well as the respective spatial footprints of the variables in play. Spatial autoregressive models in two dimensions began without covariates and with clear links to time series \citep{whittle:54}. Extensions included tests for spatial autocorrelation in linear model residuals, and models applying the autoregressive component to the response or the residuals, where the latter matched the tests for residuals \citep{CliffOrd:72, cliff+ord:73}. These ``lattice'' models of areal data typically express the dependence between observations using a graph of neighbours in the form of a contiguity matrix.

Of course, handling a spatial correlation structure in a generalised least squares model or a (generalized) linear or nonlinear mixed effects model such as those provided in the \textbf{nlme} and many other packages does not have to use a graph of neighbours \citep{R:Pinheiro+Bates:2000}. These models are also spatial regression models, using functions of the distance between observations, and fitted variograms to model the spatial autocorrelation present; such models have been held to yield a clearer picture of the underlying processes \citep{wall:04}, building on geostatistics. For example, the \textbf{glmmTMB} package successfully uses this approach to spatial regression \citep{brookesetal:17}. Here we will only consider spatial regression using spatial weights matrices.

\hypertarget{markov-random-field-and-multilevel-models-with-spatial-weights}{%
\section{Markov random field and multilevel models with spatial weights}\label{markov-random-field-and-multilevel-models-with-spatial-weights}}

There is a large literature in disease mapping using conditional autoregressive (CAR) and intrinsic CAR (ICAR) models in spatially structured random effects. These extend to multilevel models, in which the spatially structured random effects may apply at different levels of the model \citep{bivandetal17a}. In order to try out some of the variants, we need to remove the no-neighbour observations from the tract level, and from the model output zone aggregated level, in two steps as reducing the tract level induces a no-neighbour outcome at the model output zone level. Many of the model estimating functions take \texttt{family=} arguments, and fit generalized linear mixed effects models with per-observation spatial random effects structured using a Markov random field representation of relationships between neighbours. In the multilevel case, the random effects may be modelled at the group level, which is the case presented in the following examples.

We follow \citet{vgr_clubuc3m:19} in summarizing \citet{R:Pinheiro+Bates:2000} and \citet{mcculloch+searle:2001} to describe the mixed-effects model representation of spatial regression models. In a Gaussian linear mixed model setting, a random effect \(u\) is added to the model, with response \(Y\), fixed covariates \(X\), their coefficients \(\beta\) and error term \(\varepsilon_i \sim N(0, \sigma^2), i=1,\dots, n\):

\[
Y = X \beta + Z u + \varepsilon
\]
\(Z\) is a fixed design matrix for the random effects. If there are \(n\) random effects, it will be an \(n \times n\) identity matrix, if instead the observations are aggregated into \(m\) groups, so with \(m < n\) random effects, it will be an \(n \times m\) matrix showing which group each observation belongs to. The random effects are modelled as a multivariate Normal distribution \(u \sim N(0, \sigma^2_u \Sigma)\), and \(\Sigma\) is the square variance-covariance matrix of the random effects.

A division has grown up, possibly unhelpfully, between scientific fields using CAR models \citep{besag:74}, and simultaneous autoregressive models (SAR) \citep{ord:75, hepple:76}. Although CAR and SAR models are closely related, these fields have found it difficult to share experience of applying similar models, often despite referring to key work summarising the models \citep{ripley:81, ripley:88, Cressie:1993}. Ripley gives the SAR variance as \citeyearpar[page 89]{ripley:81}, here shown as the inverse \(\Sigma^{-1}\) (also known as the precision matrix):

\[
\Sigma^{-1} = [(I - \rho W)'(I - \rho W)]
\]

where \(\rho\) is a spatial autocorrelation parameter and \(W\) is a nonsingular spatial weights matrix that represents spatial dependence. The CAR variance is:

\[
\Sigma^{-1} = (I - \rho W)
\]
where \(W\) is a symmetric and strictly positive definite spatial weights matrix. In the case of the intrinsic CAR model, avoiding the estimation of a spatial autocorrelation parameter, we have:

\[
\Sigma^{-1} = M = \mathrm{diag}(n_i) - W
\]
where \(W\) is a symmetric and strictly positive definite spatial weights matrix as before and \(n_i\) are the row sums of \(W\). The Besag-York-MolliÃ© model includes intrinsic CAR spatially structured random effects and an unstructured random effects. The Leroux model combines matrix components for unstructured and spatially structured random effects, where the spatially structured random effects are taken as following an intrinsic CAR specification:

\[
\Sigma^{-1} = [(1 - \rho) I_n + \rho M]
\]
References to the definitions of these models may be found in \citet{gÃ³mez2020bayesian}, and estimation issues affecting the Besag-York-MolliÃ© and Leroux models are reviewed by \citet{JSSv063c01}.

More recent books expounding the theoretical bases for modelling with areal data simply point out the similarities between SAR and CAR models in relevant chapters \citep{gaetan+guyon:10, vanlieshout:19}; the interested reader is invited to consult these sources for background information.

\hypertarget{boston-house-value-data-set}{%
\subsection{Boston house value data set}\label{boston-house-value-data-set}}

Here we shall use the Boston housing data set, which has been restructured and furnished with census tract boundaries \citep{bivand17}. The original data set used 506 census tracts and a hedonic model to try to estimate willingness to pay for clean air. The response was constructed from counts of ordinal answers to a 1970 census question about house value. The response is left and right censored in the census source and has been treated as Gaussian. The key covariate was created from a calibrated meteorological model showing the annual nitrogen oxides (NOX) level for a smaller number of model output zones. The numbers of houses responding also varies by tract and model output zone. There are several other covariates, some measured at the tract level, some by town only, where towns broadly correspond to the air pollution model output zones.

We can start by reading in the 506 tract data set from \textbf{spData} \citep{R-spData}, and creating a contiguity neighbour object and from that again a row standardized spatial weights object.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(sf)}
\KeywordTok{library}\NormalTok{(spData)}
\NormalTok{boston_}\DecValTok{506}\NormalTok{ <-}\StringTok{ }\KeywordTok{st_read}\NormalTok{(}\KeywordTok{system.file}\NormalTok{(}\StringTok{"shapes/boston_tracts.shp"}\NormalTok{, }\DataTypeTok{package=}\StringTok{"spData"}\NormalTok{)[}\DecValTok{1}\NormalTok{])}
\CommentTok{# Reading layer `boston_tracts' from data source }
\CommentTok{#   `/home/edzer/R/x86_64-pc-linux-gnu-library/4.0/spData/shapes/boston_tracts.shp' }
\CommentTok{#   using driver `ESRI Shapefile'}
\CommentTok{# Simple feature collection with 506 features and 36 fields}
\CommentTok{# Geometry type: POLYGON}
\CommentTok{# Dimension:     XY}
\CommentTok{# Bounding box:  xmin: -71.5 ymin: 42 xmax: -70.6 ymax: 42.7}
\CommentTok{# Geodetic CRS:  NAD27}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nb_q <-}\StringTok{ }\NormalTok{spdep}\OperatorTok{::}\KeywordTok{poly2nb}\NormalTok{(boston_}\DecValTok{506}\NormalTok{)}
\NormalTok{lw_q <-}\StringTok{ }\NormalTok{spdep}\OperatorTok{::}\KeywordTok{nb2listw}\NormalTok{(nb_q, }\DataTypeTok{style=}\StringTok{"W"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

If we examine the median house values, we find that those for censored values have been assigned as missing, and that 17 tracts are affected.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(boston_}\DecValTok{506}\OperatorTok{$}\NormalTok{censored)}
\CommentTok{# }
\CommentTok{#  left    no right }
\CommentTok{#     2   489    15}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(boston_}\DecValTok{506}\OperatorTok{$}\NormalTok{median)}
\CommentTok{#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's }
\CommentTok{#    5600   16800   21000   21749   24700   50000      17}
\end{Highlighting}
\end{Shaded}

Next, we can subset to the remaining 489 tracts with non-censored house values, and the neighbour object to match. The neighbour object now has one observation with no neighbours.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston_}\DecValTok{506}\OperatorTok{$}\NormalTok{CHAS <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(boston_}\DecValTok{506}\OperatorTok{$}\NormalTok{CHAS)}
\NormalTok{boston_}\DecValTok{489}\NormalTok{ <-}\StringTok{ }\NormalTok{boston_}\DecValTok{506}\NormalTok{[}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(boston_}\DecValTok{506}\OperatorTok{$}\NormalTok{median),]}
\NormalTok{nb_q_}\DecValTok{489}\NormalTok{ <-}\StringTok{ }\NormalTok{spdep}\OperatorTok{::}\KeywordTok{poly2nb}\NormalTok{(boston_}\DecValTok{489}\NormalTok{)}
\NormalTok{lw_q_}\DecValTok{489}\NormalTok{ <-}\StringTok{ }\NormalTok{spdep}\OperatorTok{::}\KeywordTok{nb2listw}\NormalTok{(nb_q_}\DecValTok{489}\NormalTok{, }\DataTypeTok{style=}\StringTok{"W"}\NormalTok{, }\DataTypeTok{zero.policy=}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The \texttt{NOX\_ID} variable specifies the upper level aggregation, letting us aggregate the tracts to air pollution model output zones. We can create aggregate neighbour and row standardized spatial weights objects, and aggregate the \texttt{NOX} variable taking means, and the \texttt{CHAS} Charles River dummy variable for observations on the river. Here we follow the principles outlined in section \ref{extensiveintensive} for spatially extensive and intensive variables; neither \texttt{NOX} nor \texttt{CHAS} can be summed as they are not count variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{agg_}\DecValTok{96}\NormalTok{ <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\KeywordTok{as.character}\NormalTok{(boston_}\DecValTok{506}\OperatorTok{$}\NormalTok{NOX_ID))}
\NormalTok{boston_}\DecValTok{96}\NormalTok{ <-}\StringTok{ }\KeywordTok{aggregate}\NormalTok{(boston_}\DecValTok{506}\NormalTok{[, }\StringTok{"NOX_ID"}\NormalTok{], }\DataTypeTok{by=}\NormalTok{agg_}\DecValTok{96}\NormalTok{, unique)}
\NormalTok{nb_q_}\DecValTok{96}\NormalTok{ <-}\StringTok{ }\NormalTok{spdep}\OperatorTok{::}\KeywordTok{poly2nb}\NormalTok{(boston_}\DecValTok{96}\NormalTok{)}
\NormalTok{lw_q_}\DecValTok{96}\NormalTok{ <-}\StringTok{ }\NormalTok{spdep}\OperatorTok{::}\KeywordTok{nb2listw}\NormalTok{(nb_q_}\DecValTok{96}\NormalTok{)}
\NormalTok{boston_}\DecValTok{96}\OperatorTok{$}\NormalTok{NOX <-}\StringTok{ }\KeywordTok{aggregate}\NormalTok{(boston_}\DecValTok{506}\OperatorTok{$}\NormalTok{NOX, agg_}\DecValTok{96}\NormalTok{, mean)}\OperatorTok{$}\NormalTok{x}
\NormalTok{boston_}\DecValTok{96}\OperatorTok{$}\NormalTok{CHAS <-}\StringTok{ }\KeywordTok{aggregate}\NormalTok{(}\KeywordTok{as.integer}\NormalTok{(boston_}\DecValTok{506}\OperatorTok{$}\NormalTok{CHAS)}\OperatorTok{-}\DecValTok{1}\NormalTok{, agg_}\DecValTok{96}\NormalTok{, max)}\OperatorTok{$}\NormalTok{x}
\end{Highlighting}
\end{Shaded}

The response is aggregated using the \texttt{weightedMedian()} function in \textbf{matrixStats}, and midpoint values for the house value classes. Counts of houses by value class were punched to check the published census values, which can be replicated using \texttt{weightedMedian()} at the tract level. Here we find two output zones with calculated weighted medians over the upper census question limit of USD 50,000, and remove them subsequently as they also are affected by not knowing the appropriate value to insert for the top class by value. This is a case of spatially extensive aggregation, for which the summation of counts is appropriate:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nms <-}\StringTok{ }\KeywordTok{names}\NormalTok{(boston_}\DecValTok{506}\NormalTok{)}
\NormalTok{ccounts <-}\StringTok{ }\DecValTok{23}\OperatorTok{:}\DecValTok{31}
\ControlFlowTok{for}\NormalTok{ (nm }\ControlFlowTok{in}\NormalTok{ nms[}\KeywordTok{c}\NormalTok{(}\DecValTok{22}\NormalTok{, ccounts, }\DecValTok{36}\NormalTok{)]) \{}
\NormalTok{  boston_}\DecValTok{96}\NormalTok{[[nm]] <-}\StringTok{ }\KeywordTok{aggregate}\NormalTok{(boston_}\DecValTok{506}\NormalTok{[[nm]], agg_}\DecValTok{96}\NormalTok{, sum)}\OperatorTok{$}\NormalTok{x}
\NormalTok{\}}
\NormalTok{br2 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{3.50}\NormalTok{,  }\FloatTok{6.25}\NormalTok{,  }\FloatTok{8.75}\NormalTok{, }\FloatTok{12.50}\NormalTok{, }\FloatTok{17.50}\NormalTok{, }\FloatTok{22.50}\NormalTok{, }\FloatTok{30.00}\NormalTok{, }\FloatTok{42.50}\NormalTok{, }\FloatTok{60.00}\NormalTok{)}\OperatorTok{*}\DecValTok{1000}
\NormalTok{counts <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(boston_}\DecValTok{96}\NormalTok{)[, nms[ccounts]]}
\NormalTok{f <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) matrixStats}\OperatorTok{::}\KeywordTok{weightedMedian}\NormalTok{(}\DataTypeTok{x=}\NormalTok{br2, }\DataTypeTok{w=}\NormalTok{x, }\DataTypeTok{interpolate=}\OtherTok{TRUE}\NormalTok{)}
\NormalTok{boston_}\DecValTok{96}\OperatorTok{$}\NormalTok{median <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(counts, }\DecValTok{1}\NormalTok{, f)}
\KeywordTok{is.na}\NormalTok{(boston_}\DecValTok{96}\OperatorTok{$}\NormalTok{median) <-}\StringTok{ }\NormalTok{boston_}\DecValTok{96}\OperatorTok{$}\NormalTok{median }\OperatorTok{>}\StringTok{ }\DecValTok{50000}
\KeywordTok{summary}\NormalTok{(boston_}\DecValTok{96}\OperatorTok{$}\NormalTok{median)}
\CommentTok{#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's }
\CommentTok{#    9009   20417   23523   25263   30073   49496       2}
\end{Highlighting}
\end{Shaded}

Before subsetting, we aggregate the remaining covariates by weighted mean using the tract population counts punched from the census \citep{bivand17}; these are spatially intensive variables, not count data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston_}\DecValTok{94}\NormalTok{ <-}\StringTok{ }\NormalTok{boston_}\DecValTok{96}\NormalTok{[}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(boston_}\DecValTok{96}\OperatorTok{$}\NormalTok{median),]}
\NormalTok{nb_q_}\DecValTok{94}\NormalTok{ <-}\StringTok{ }\NormalTok{spdep}\OperatorTok{::}\KeywordTok{subset.nb}\NormalTok{(nb_q_}\DecValTok{96}\NormalTok{, }\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(boston_}\DecValTok{96}\OperatorTok{$}\NormalTok{median))}
\NormalTok{lw_q_}\DecValTok{94}\NormalTok{ <-}\StringTok{ }\NormalTok{spdep}\OperatorTok{::}\KeywordTok{nb2listw}\NormalTok{(nb_q_}\DecValTok{94}\NormalTok{, }\DataTypeTok{style=}\StringTok{"W"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We now have two data sets at each level, at the lower, census tract level, and at the upper, air pollution model output zone level, one including the censored observations, the other excluding them.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston_94a <-}\StringTok{ }\KeywordTok{aggregate}\NormalTok{(boston_}\DecValTok{489}\NormalTok{[,}\StringTok{"NOX_ID"}\NormalTok{], }\KeywordTok{list}\NormalTok{(boston_}\DecValTok{489}\OperatorTok{$}\NormalTok{NOX_ID), unique)}
\NormalTok{nb_q_94a <-}\StringTok{ }\NormalTok{spdep}\OperatorTok{::}\KeywordTok{poly2nb}\NormalTok{(boston_94a)}
\NormalTok{NOX_ID_no_neighs <-}\StringTok{ }\NormalTok{boston_94a}\OperatorTok{$}\NormalTok{NOX_ID[}\KeywordTok{which}\NormalTok{(spdep}\OperatorTok{::}\KeywordTok{card}\NormalTok{(nb_q_94a) }\OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{)]}
\NormalTok{boston_}\DecValTok{487}\NormalTok{ <-}\StringTok{ }\NormalTok{boston_}\DecValTok{489}\NormalTok{[}\KeywordTok{is.na}\NormalTok{(}\KeywordTok{match}\NormalTok{(boston_}\DecValTok{489}\OperatorTok{$}\NormalTok{NOX_ID, NOX_ID_no_neighs)),]}
\NormalTok{boston_}\DecValTok{93}\NormalTok{ <-}\StringTok{ }\KeywordTok{aggregate}\NormalTok{(boston_}\DecValTok{487}\NormalTok{[, }\StringTok{"NOX_ID"}\NormalTok{], }\KeywordTok{list}\NormalTok{(}\DataTypeTok{ids =}\NormalTok{ boston_}\DecValTok{487}\OperatorTok{$}\NormalTok{NOX_ID), unique)}
\KeywordTok{row.names}\NormalTok{(boston_}\DecValTok{93}\NormalTok{) <-}\StringTok{ }\KeywordTok{as.character}\NormalTok{(boston_}\DecValTok{93}\OperatorTok{$}\NormalTok{NOX_ID)}
\NormalTok{nb_q_}\DecValTok{93}\NormalTok{ <-}\StringTok{ }\NormalTok{spdep}\OperatorTok{::}\KeywordTok{poly2nb}\NormalTok{(boston_}\DecValTok{93}\NormalTok{, }\DataTypeTok{row.names=}\KeywordTok{unique}\NormalTok{(}\KeywordTok{as.character}\NormalTok{(boston_}\DecValTok{93}\OperatorTok{$}\NormalTok{NOX_ID)))}
\end{Highlighting}
\end{Shaded}

The original model related the log of median house values by tract to the square of NOX values, including other covariates usually related to house value by tract, such as aggregate room counts, aggregate age, ethnicity, social status, distance to downtown and to the nearest radial road, a crime rate, and town-level variables reflecting land use (zoning, industry), taxation and education \citep{bivand17}. This structure will be used here to exercise issues raised in fitting spatial regression models, including the presence of multiple levels.

\hypertarget{multilevel-models-of-the-boston-data-set}{%
\section{Multilevel models of the Boston data set}\label{multilevel-models-of-the-boston-data-set}}

The ZN, INDUS, NOX, RAD, TAX and PTRATIO variables show effectively no variability within the TASSIM zones, so in a multilevel model the random effect may absorb their influence.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{form <-}\StringTok{ }\KeywordTok{formula}\NormalTok{(}\KeywordTok{log}\NormalTok{(median) }\OperatorTok{~}\StringTok{ }\NormalTok{CRIM }\OperatorTok{+}\StringTok{ }\NormalTok{ZN }\OperatorTok{+}\StringTok{ }\NormalTok{INDUS }\OperatorTok{+}\StringTok{ }\NormalTok{CHAS }\OperatorTok{+}\StringTok{ }\KeywordTok{I}\NormalTok{((NOX}\OperatorTok{*}\DecValTok{10}\NormalTok{)}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{I}\NormalTok{(RM}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{                  }\NormalTok{AGE }\OperatorTok{+}\StringTok{ }\KeywordTok{log}\NormalTok{(DIS) }\OperatorTok{+}\StringTok{ }\KeywordTok{log}\NormalTok{(RAD) }\OperatorTok{+}\StringTok{ }\NormalTok{TAX }\OperatorTok{+}\StringTok{ }\NormalTok{PTRATIO }\OperatorTok{+}\StringTok{ }\KeywordTok{I}\NormalTok{(BB}\OperatorTok{/}\DecValTok{100}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{                  }\KeywordTok{log}\NormalTok{(}\KeywordTok{I}\NormalTok{(LSTAT}\OperatorTok{/}\DecValTok{100}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\hypertarget{iid-random-effects-with-lme4}{%
\subsection{IID random effects with lme4}\label{iid-random-effects-with-lme4}}

The \textbf{lme4} package \citep{R-lme4} lets us add an independent and identically distributed (IID) unstructured random effect at the model output zone level by updating the model formula with a random effects term:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(Matrix)}
\KeywordTok{library}\NormalTok{(lme4)}
\CommentTok{# }
\CommentTok{# Attaching package: 'lme4'}
\CommentTok{# The following object is masked from 'package:nlme':}
\CommentTok{# }
\CommentTok{#     lmList}
\NormalTok{MLM <-}\StringTok{ }\KeywordTok{lmer}\NormalTok{(}\KeywordTok{update}\NormalTok{(form, . }\OperatorTok{~}\StringTok{ }\NormalTok{. }\OperatorTok{+}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{|}\StringTok{ }\NormalTok{NOX_ID)), }\DataTypeTok{data=}\NormalTok{boston_}\DecValTok{487}\NormalTok{, }\DataTypeTok{REML=}\OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

copying the random effect into the \texttt{"sf"} object for mapping below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston_}\DecValTok{93}\OperatorTok{$}\NormalTok{MLM_re <-}\StringTok{ }\KeywordTok{ranef}\NormalTok{(MLM)[[}\DecValTok{1}\NormalTok{]][,}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\hypertarget{iid-and-car-random-effects-with-hglm}{%
\subsection{IID and CAR random effects with hglm}\label{iid-and-car-random-effects-with-hglm}}

The same model may be estimated using the \textbf{hglm} package \citep{R-hglm}, which also permits the modelling of discrete responses, this time using an extra one-sided formula to express the random effects term:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{suppressPackageStartupMessages}\NormalTok{(}\KeywordTok{library}\NormalTok{(hglm))}
\KeywordTok{suppressWarnings}\NormalTok{(HGLM_iid <-}\StringTok{ }\KeywordTok{hglm}\NormalTok{(}\DataTypeTok{fixed=}\NormalTok{form, }\DataTypeTok{random=} \OperatorTok{~}\DecValTok{1} \OperatorTok{|}\StringTok{ }\NormalTok{NOX_ID, }\DataTypeTok{data=}\NormalTok{boston_}\DecValTok{487}\NormalTok{,}
    \DataTypeTok{family=}\KeywordTok{gaussian}\NormalTok{()))}
\NormalTok{boston_}\DecValTok{93}\OperatorTok{$}\NormalTok{HGLM_re <-}\StringTok{ }\KeywordTok{unname}\NormalTok{(HGLM_iid}\OperatorTok{$}\NormalTok{ranef)}
\end{Highlighting}
\end{Shaded}

The same package has been extended to spatially structured SAR and CAR random effects, for which a sparse spatial weights matrix is required \citep{alam-ronnegard-shen:2015}; we choose binary spatial weights:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{W <-}\StringTok{ }\KeywordTok{as}\NormalTok{(spdep}\OperatorTok{::}\KeywordTok{nb2listw}\NormalTok{(nb_q_}\DecValTok{93}\NormalTok{, }\DataTypeTok{style=}\StringTok{"B"}\NormalTok{), }\StringTok{"CsparseMatrix"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We fit a CAR model at the upper level, using the \texttt{rand.family=} argument, where the values of the indexing variable \texttt{NOX\_ID} match the row names of \(W\):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{suppressWarnings}\NormalTok{(HGLM_car <-}\StringTok{ }\KeywordTok{hglm}\NormalTok{(}\DataTypeTok{fixed=}\NormalTok{form, }\DataTypeTok{random=} \OperatorTok{~}\StringTok{ }\DecValTok{1} \OperatorTok{|}\StringTok{ }\NormalTok{NOX_ID, }\DataTypeTok{data=}\NormalTok{boston_}\DecValTok{487}\NormalTok{,}
    \DataTypeTok{family=}\KeywordTok{gaussian}\NormalTok{(), }\DataTypeTok{rand.family=}\KeywordTok{CAR}\NormalTok{(}\DataTypeTok{D=}\NormalTok{W)))}
\NormalTok{boston_}\DecValTok{93}\OperatorTok{$}\NormalTok{HGLM_ss <-}\StringTok{ }\NormalTok{HGLM_car}\OperatorTok{$}\NormalTok{ranef[,}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\hypertarget{sar-random-effects-with-hsar}{%
\subsection{SAR random effects with HSAR}\label{sar-random-effects-with-hsar}}

The \textbf{HSAR} package \citep{R-HSAR} is restricted to the Gaussian response case \citep{dong15, dongetal15}, and requires the specification of a sparse design matrix mapping the upper level entities onto lower level entities:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(HSAR)}
\KeywordTok{library}\NormalTok{(MatrixModels, }\DataTypeTok{warn.conflicts=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{Z <-}\StringTok{ }\KeywordTok{as}\NormalTok{(}\KeywordTok{model.Matrix}\NormalTok{(}\OperatorTok{~}\StringTok{ }\DecValTok{-1} \OperatorTok{+}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(NOX_ID), }\DataTypeTok{data=}\NormalTok{boston_}\DecValTok{487}\NormalTok{, }\DataTypeTok{sparse=}\OtherTok{TRUE}\NormalTok{),}
    \StringTok{"dgCMatrix"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\texttt{hsar()} fits an upper level SAR random effect using MCMC; if \texttt{W=} is a lower level weights matrix rather than \texttt{NULL}, it will also fit a lower level spatial econometrics-style spatial lag model, adding the lower level spatially lagged response to the model:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\KeywordTok{suppressWarnings}\NormalTok{(HSAR_ss <-}\StringTok{ }\KeywordTok{hsar}\NormalTok{(form, }\DataTypeTok{data=}\NormalTok{boston_}\DecValTok{487}\NormalTok{, }\DataTypeTok{W=}\OtherTok{NULL}\NormalTok{, }\DataTypeTok{M=}\NormalTok{W, }\DataTypeTok{Delta=}\NormalTok{Z, }
                              \DataTypeTok{burnin=}\DecValTok{2000}\NormalTok{, }\DataTypeTok{Nsim=}\DecValTok{12000}\NormalTok{, }\DataTypeTok{thinning=}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston_}\DecValTok{93}\OperatorTok{$}\NormalTok{HSAR_ss <-}\StringTok{ }\NormalTok{HSAR_ss}\OperatorTok{$}\NormalTok{Mus[}\DecValTok{1}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\hypertarget{iid-and-icar-random-effects-with-r2bayesx}{%
\subsection{IID and ICAR random effects with R2BayesX}\label{iid-and-icar-random-effects-with-r2bayesx}}

The \textbf{R2BayesX} package \citep{R-R2BayesX} provides flexible support for structured additive regression models, including spatial multilevel models. The models include an IID unstructured random effect at the upper level using the \texttt{"re"} specification in the \texttt{sx()} model term \citep{umlaufetal:15}; we choose the \texttt{"MCMC"} method:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{suppressPackageStartupMessages}\NormalTok{(}\KeywordTok{library}\NormalTok{(R2BayesX))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{BX_iid <-}\StringTok{ }\KeywordTok{bayesx}\NormalTok{(}\KeywordTok{update}\NormalTok{(form, . }\OperatorTok{~}\StringTok{ }\NormalTok{. }\OperatorTok{+}\StringTok{ }\KeywordTok{sx}\NormalTok{(NOX_ID, }\DataTypeTok{bs=}\StringTok{"re"}\NormalTok{)), }\DataTypeTok{family=}\StringTok{"gaussian"}\NormalTok{,}
    \DataTypeTok{data=}\NormalTok{boston_}\DecValTok{487}\NormalTok{, }\DataTypeTok{method=}\StringTok{"MCMC"}\NormalTok{, }\DataTypeTok{iterations=}\DecValTok{12000}\NormalTok{, }\DataTypeTok{burnin=}\DecValTok{2000}\NormalTok{, }\DataTypeTok{step=}\DecValTok{2}\NormalTok{, }\DataTypeTok{seed=}\DecValTok{123}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston_}\DecValTok{93}\OperatorTok{$}\NormalTok{BX_re <-}\StringTok{ }\NormalTok{BX_iid}\OperatorTok{$}\NormalTok{effects[}\StringTok{"sx(NOX_ID):re"}\NormalTok{][[}\DecValTok{1}\NormalTok{]]}\OperatorTok{$}\NormalTok{Mean}
\end{Highlighting}
\end{Shaded}

and the \texttt{"mrf"} (Markov Random Field) spatially structured intrinsic CAR random effect specification based on a graph derived from converting a suitable \texttt{"nb"} object for the upper level. The \texttt{"region.id"} attribute of the \texttt{"nb"} object needs to contain values corresponding to the indexing variable in the \texttt{sx()} effects term, to facilitate the internal construction of design matrix \(Z\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RBX_gra <-}\StringTok{ }\KeywordTok{nb2gra}\NormalTok{(nb_q_}\DecValTok{93}\NormalTok{)}
\KeywordTok{all.equal}\NormalTok{(}\KeywordTok{row.names}\NormalTok{(RBX_gra), }\KeywordTok{attr}\NormalTok{(nb_q_}\DecValTok{93}\NormalTok{, }\StringTok{"region.id"}\NormalTok{))}
\CommentTok{# [1] TRUE}
\end{Highlighting}
\end{Shaded}

As we saw above in the intrinsic CAR model definition, the counts of neighbours are entered on the diagonal, but the current implementation uses a dense, not sparse, matrix:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{all.equal}\NormalTok{(}\KeywordTok{unname}\NormalTok{(}\KeywordTok{diag}\NormalTok{(RBX_gra)), spdep}\OperatorTok{::}\KeywordTok{card}\NormalTok{(nb_q_}\DecValTok{93}\NormalTok{))}
\CommentTok{# [1] TRUE}
\end{Highlighting}
\end{Shaded}

The \texttt{sx()} model term continues to include the indexing variable, and now passes through the intrinsic CAR precision matrix:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{BX_mrf <-}\StringTok{ }\KeywordTok{bayesx}\NormalTok{(}\KeywordTok{update}\NormalTok{(form, . }\OperatorTok{~}\StringTok{ }\NormalTok{. }\OperatorTok{+}\StringTok{ }\KeywordTok{sx}\NormalTok{(NOX_ID, }\DataTypeTok{bs=}\StringTok{"mrf"}\NormalTok{, }\DataTypeTok{map=}\NormalTok{RBX_gra)), }
                 \DataTypeTok{family=}\StringTok{"gaussian"}\NormalTok{, }\DataTypeTok{data=}\NormalTok{boston_}\DecValTok{487}\NormalTok{, }\DataTypeTok{method=}\StringTok{"MCMC"}\NormalTok{, }
                 \DataTypeTok{iterations=}\DecValTok{12000}\NormalTok{, }\DataTypeTok{burnin=}\DecValTok{2000}\NormalTok{, }\DataTypeTok{step=}\DecValTok{2}\NormalTok{, }\DataTypeTok{seed=}\DecValTok{123}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston_}\DecValTok{93}\OperatorTok{$}\NormalTok{BX_ss <-}\StringTok{ }\NormalTok{BX_mrf}\OperatorTok{$}\NormalTok{effects[}\StringTok{"sx(NOX_ID):mrf"}\NormalTok{][[}\DecValTok{1}\NormalTok{]]}\OperatorTok{$}\NormalTok{Mean}
\end{Highlighting}
\end{Shaded}

\hypertarget{iid-icar-and-leroux-random-effects-with-inla}{%
\subsection{IID, ICAR and Leroux random effects with INLA}\label{iid-icar-and-leroux-random-effects-with-inla}}

\citet{JSSv063i20} and \citet{gÃ³mez2020bayesian} present the use of the \textbf{INLA} package \citep{R-INLA} and the \texttt{inla()} model fitting function with spatial regression models:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{suppressPackageStartupMessages}\NormalTok{(}\KeywordTok{library}\NormalTok{(INLA))}
\end{Highlighting}
\end{Shaded}

Although differing in details, the approach by updating the fixed model formula with an unstructured random effects term is very similar to that seen above:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{INLA_iid <-}\StringTok{ }\KeywordTok{inla}\NormalTok{(}\KeywordTok{update}\NormalTok{(form, . }\OperatorTok{~}\StringTok{ }\NormalTok{. }\OperatorTok{+}\StringTok{ }\KeywordTok{f}\NormalTok{(NOX_ID, }\DataTypeTok{model=}\StringTok{"iid"}\NormalTok{)), }\DataTypeTok{family=}\StringTok{"gaussian"}\NormalTok{, }
    \DataTypeTok{data=}\NormalTok{boston_}\DecValTok{487}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston_}\DecValTok{93}\OperatorTok{$}\NormalTok{INLA_re <-}\StringTok{ }\NormalTok{INLA_iid}\OperatorTok{$}\NormalTok{summary.random}\OperatorTok{$}\NormalTok{NOX_ID}\OperatorTok{$}\NormalTok{mean}
\end{Highlighting}
\end{Shaded}

As with most implementations, care is needed to match the indexing variable with the spatial weights; in this case using indices \(1, \dots, 93\) rather than the \texttt{NOX\_ID} variable directly:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ID2 <-}\StringTok{ }\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(boston_}\DecValTok{487}\OperatorTok{$}\NormalTok{NOX_ID))}
\end{Highlighting}
\end{Shaded}

The same sparse binary spatial weights matrix is used, and the intrinsic CAR representation is constructed internally:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{INLA_ss <-}\StringTok{ }\KeywordTok{inla}\NormalTok{(}\KeywordTok{update}\NormalTok{(form, . }\OperatorTok{~}\StringTok{ }\NormalTok{. }\OperatorTok{+}\StringTok{ }\KeywordTok{f}\NormalTok{(ID2, }\DataTypeTok{model=}\StringTok{"besag"}\NormalTok{, }\DataTypeTok{graph=}\NormalTok{W)), }\DataTypeTok{family=}\StringTok{"gaussian"}\NormalTok{,}
    \DataTypeTok{data=}\NormalTok{boston_}\DecValTok{487}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston_}\DecValTok{93}\OperatorTok{$}\NormalTok{INLA_ss <-}\StringTok{ }\NormalTok{INLA_ss}\OperatorTok{$}\NormalTok{summary.random}\OperatorTok{$}\NormalTok{ID2}\OperatorTok{$}\NormalTok{mean}
\end{Highlighting}
\end{Shaded}

The sparse Leroux representation as given by \citet{gÃ³mez2020bayesian} can be constructed in the following way:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{M <-}\StringTok{ }\KeywordTok{Diagonal}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(W), }\KeywordTok{rowSums}\NormalTok{(W)) }\OperatorTok{-}\StringTok{ }\NormalTok{W}
\NormalTok{Cmatrix <-}\StringTok{ }\KeywordTok{Diagonal}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(M), }\DecValTok{1}\NormalTok{) }\OperatorTok{-}\StringTok{  }\NormalTok{M}
\end{Highlighting}
\end{Shaded}

This model can be estimatted using the \texttt{"generic1"} model with the specified precision matrix:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{INLA_lr <-}\StringTok{ }\KeywordTok{inla}\NormalTok{(}\KeywordTok{update}\NormalTok{(form, . }\OperatorTok{~}\StringTok{ }\NormalTok{. }\OperatorTok{+}\StringTok{ }\KeywordTok{f}\NormalTok{(ID2, }\DataTypeTok{model =} \StringTok{"generic1"}\NormalTok{, }\DataTypeTok{Cmatrix =}\NormalTok{ Cmatrix)),}
    \DataTypeTok{family=}\StringTok{"gaussian"}\NormalTok{, }\DataTypeTok{data=}\NormalTok{boston_}\DecValTok{487}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston_}\DecValTok{93}\OperatorTok{$}\NormalTok{INLA_lr <-}\StringTok{ }\NormalTok{INLA_lr}\OperatorTok{$}\NormalTok{summary.random}\OperatorTok{$}\NormalTok{ID2}\OperatorTok{$}\NormalTok{mean}
\end{Highlighting}
\end{Shaded}

\hypertarget{icar-random-effects-with-mgcvgam}{%
\subsection{ICAR random effects with mgcv::gam()}\label{icar-random-effects-with-mgcvgam}}

In a very similar way, the \texttt{gam()} function in the \textbf{mgcv} package \citep{R-mgcv} can take an \texttt{"mrf"} term using a suitable \texttt{"nb"} object for the upper level. In this case the \texttt{"nb"} object needs to have the contents of the \texttt{"region.id"} attribute copied as the names of the neighbour list components, and the indexing variable needs to be a factor \citep{wood:17}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(mgcv)}
\KeywordTok{names}\NormalTok{(nb_q_}\DecValTok{93}\NormalTok{) <-}\StringTok{ }\KeywordTok{attr}\NormalTok{(nb_q_}\DecValTok{93}\NormalTok{, }\StringTok{"region.id"}\NormalTok{)}
\NormalTok{boston_}\DecValTok{487}\OperatorTok{$}\NormalTok{NOX_ID <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(boston_}\DecValTok{487}\OperatorTok{$}\NormalTok{NOX_ID)}
\end{Highlighting}
\end{Shaded}

The specification of the spatially structured term again differs in details from those above, but achieves the same purpose. The \texttt{"REML"} method of \texttt{bayesx()} gives the same results as \texttt{gam()} using \texttt{"REML"} in this case:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{GAM_MRF <-}\StringTok{ }\KeywordTok{gam}\NormalTok{(}\KeywordTok{update}\NormalTok{(form, . }\OperatorTok{~}\StringTok{ }\NormalTok{. }\OperatorTok{+}\StringTok{ }\KeywordTok{s}\NormalTok{(NOX_ID, }\DataTypeTok{bs=}\StringTok{"mrf"}\NormalTok{, }\DataTypeTok{xt=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{nb=}\NormalTok{nb_q_}\DecValTok{93}\NormalTok{))),}
               \DataTypeTok{data=}\NormalTok{boston_}\DecValTok{487}\NormalTok{, }\DataTypeTok{method=}\StringTok{"REML"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The upper level random effects may be extracted by predicting terms; as we can see, the values in all lower-level tracts belonging to the same upper-level air pollution model output zones are identical:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ssre <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(GAM_MRF, }\DataTypeTok{type=}\StringTok{"terms"}\NormalTok{, }\DataTypeTok{se=}\OtherTok{FALSE}\NormalTok{)[, }\StringTok{"s(NOX_ID)"}\NormalTok{]}
\KeywordTok{all}\NormalTok{(}\KeywordTok{sapply}\NormalTok{(}\KeywordTok{tapply}\NormalTok{(ssre, }\KeywordTok{list}\NormalTok{(boston_}\DecValTok{487}\OperatorTok{$}\NormalTok{NOX_ID), c), }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{length}\NormalTok{(}\KeywordTok{unique}\NormalTok{(x)) }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{))}
\CommentTok{# [1] TRUE}
\end{Highlighting}
\end{Shaded}

so we can return the first value for each upper-level unit:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston_}\DecValTok{93}\OperatorTok{$}\NormalTok{GAM_ss <-}\StringTok{ }\KeywordTok{aggregate}\NormalTok{(ssre, }\KeywordTok{list}\NormalTok{(boston_}\DecValTok{487}\OperatorTok{$}\NormalTok{NOX_ID), head, }\DataTypeTok{n=}\DecValTok{1}\NormalTok{)}\OperatorTok{$}\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\hypertarget{upper-level-random-effects-summary}{%
\subsection{Upper level random effects: summary}\label{upper-level-random-effects-summary}}

In the cases of \texttt{hglm()}, \texttt{bayesx()}, \texttt{inla()} and \texttt{gam()}, we could also model discrete responses without further major difficulty, and \texttt{bayesx()}, \texttt{inla()} and \texttt{gam()} also facilitate the generalization of functional form fitting for included covariates.

Unfortunately, the coefficient estimates for the air pollution variable for these multilevel models are not helpful. All are negative as expected, but the inclusion of the model output zone level effects, IID or spatially structured, makes it is hard to disentangle the influence of the scale of observation from that of covariates observed at that scale rather than at the tract level.

Figure \ref{fig:multi-levelmaps1} shows that the air pollution model output zone level IID random effects are very similar across the four model fitting functions reported. In all the maps, the central downtown zones have stronger negative random effect values, but strong positive values are also found in close proximity; suburban areas take values closer to zero.



\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tmap, }\DataTypeTok{warn.conflicts=}\OtherTok{FALSE}\NormalTok{)}
\KeywordTok{tm_shape}\NormalTok{(boston_}\DecValTok{93}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{tm_fill}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"MLM_re"}\NormalTok{, }\StringTok{"HGLM_re"}\NormalTok{, }\StringTok{"INLA_re"}\NormalTok{, }\StringTok{"BX_re"}\NormalTok{), }\DataTypeTok{midpoint=}\DecValTok{0}\NormalTok{,}
    \DataTypeTok{title=}\StringTok{"IID"}\NormalTok{)  }\OperatorTok{+}\StringTok{ }\KeywordTok{tm_facets}\NormalTok{(}\DataTypeTok{free.scales=}\OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{tm_borders}\NormalTok{(}\DataTypeTok{lwd=}\FloatTok{0.3}\NormalTok{, }\DataTypeTok{alpha=}\FloatTok{0.4}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{tm_layout}\NormalTok{(}\DataTypeTok{panel.labels=}\KeywordTok{c}\NormalTok{(}\StringTok{"lmer"}\NormalTok{, }\StringTok{"hglm"}\NormalTok{, }\StringTok{"inla"}\NormalTok{, }\StringTok{"bayesx"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{sds_files/figure-latex/multi-levelmaps1-1} 

}

\caption{Air pollution model output zone level IID random effects estimated using \textbf{lme4}, \textbf{hglm}, \textbf{INLA} and \textbf{R2BayesX}; the range of the response, \texttt{log(median)} is 2.1893.}\label{fig:multi-levelmaps1}
\end{figure}

Figure \ref{fig:multi-levelmaps2} shows that the spatially structured random effects are also very similar to each other, with the \texttt{"SAR"} spatial smooth being perhaps a little smoother than the \texttt{"CAR"} smooths when considering the range of values taken by the random effect term.



\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tm_shape}\NormalTok{(boston_}\DecValTok{93}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{tm_fill}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"HGLM_ss"}\NormalTok{, }\StringTok{"HSAR_ss"}\NormalTok{, }\StringTok{"INLA_lr"}\NormalTok{, }\StringTok{"INLA_ss"}\NormalTok{, }\StringTok{"BX_ss"}\NormalTok{,}
    \StringTok{"GAM_ss"}\NormalTok{), }\DataTypeTok{midpoint=}\DecValTok{0}\NormalTok{, }\DataTypeTok{title=}\StringTok{"SSRE"}\NormalTok{)  }\OperatorTok{+}\StringTok{ }\KeywordTok{tm_facets}\NormalTok{(}\DataTypeTok{free.scales=}\OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{tm_borders}\NormalTok{(}\DataTypeTok{lwd=}\FloatTok{0.3}\NormalTok{, }\DataTypeTok{alpha=}\FloatTok{0.4}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{tm_layout}\NormalTok{(}\DataTypeTok{panel.labels=}\KeywordTok{c}\NormalTok{(}\StringTok{"hglm CAR"}\NormalTok{, }\StringTok{"hsar SAR"}\NormalTok{,}
        \StringTok{"inla Leroux"}\NormalTok{, }\StringTok{"inla ICAR"}\NormalTok{, }\StringTok{"bayesx ICAR"}\NormalTok{, }\StringTok{"gam ICAR"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{sds_files/figure-latex/multi-levelmaps2-1} 

}

\caption{Air pollution model output zone level spatially structured random effects estimated using \textbf{hglm}, \textbf{HSAR}, \textbf{INLA}, \textbf{R2BayesX} and \textbf{mgcv}.}\label{fig:multi-levelmaps2}
\end{figure}

Although there is still a great need for more thorough comparative studies of model fitting functions for spatial regression including multilevel capabilities, there has been much progress over recent years. \citet{VRANCKX2019100302} offer a recent comparative survey of disease mapping spatial regression, typically set in a Poisson regression framework offset by an expected count. In \citet{doi:10.1177/1471082X20967158}, methods for estimating spatial survival models using spatial weights matrices are compared with spatial probit models.

\hypertarget{spatecon}{%
\chapter{Spatial econometrics models}\label{spatecon}}

Spatial autoregression models using spatial weights matrices were described in some detail using maximum likelihood estimation some time ago \citep{cliff+ord:73, cliff+ord:81}. A family of models was elaborated in spatial econometric terms extending earlier work, and in many cases using the simultaneous autoregressive framework and row standardization of spatial weights \citep{a88}. The simultaneous and conditional autoregressive frameworks can be compared, and both can be supplemented using case weights to reflect the relative importance of different observations \citep{WallerGotway:2004}.

Before moving to presentations of issues raised in fitting spatial regression models, it is worth making a few further points. A recent review of spatial regression in a spatial econometrics setting is given by Kelejian and Piras \citeyearpar{kelejian+piras:17}; note that their usage is to call the spatial coefficient of the lagged response \(\lambda\) and that of the lagged residuals \(\rho\), the reverse of other usage \citep{a88, lesage+pace:09}; here we use \(\rho_{\mathrm{Lag}}\) for the spatial coefficient in the spatial lag model, and \(\rho_{\mathrm{Err}}\) for the spatial error model. One interesting finding is that relatively dense spatial weights matrices may downweight model estimates, suggesting that sparser weights are preferable \citep{smith:09}. Another useful finding is that the presence of residual spatial autocorrelation need not bias the estimates of variance of regression coefficients, provided that the covariates themselves do not exhibit spatial autocorrelation \citep{smith+lee12}. In general, however, the footprints of the spatial processes of the response and covariates may not be aligned, and if covariates and the residual are autocorrelated, it is likely that the estimates of variance of regression coefficients will be biassed downwards if attempts are not made to model the spatial processes.

\hypertarget{spatial-econometric-models-definitions}{%
\section{Spatial econometric models: definitions}\label{spatial-econometric-models-definitions}}

In trying to model spatial processes, one of the earliiest spatial econometric representations is to model the spatial autocorrelation in the residual (spatial error model, SEM):

\[
{\mathbf y} = {\mathbf X}{\mathbf \beta} + {\mathbf u},
\qquad {\mathbf u} = \rho_{\mathrm{Err}} {\mathbf W} {\mathbf u} + {\mathbf \varepsilon},
\]
where \({\mathbf y}\) is an \((N \times 1)\) vector of observations on a response variable taken at each of \(N\) locations, \({\mathbf X}\) is an \((N \times k)\) matrix of covariates, \({\mathbf \beta}\) is a \((k \times 1)\) vector of parameters, \({\mathbf u}\) is an \((N \times 1)\) spatially autocorrelated disturbance vector, \({\mathbf \varepsilon}\) is an \((N \times 1)\) vector of independent and identically distributed disturbances and \(\rho_{\mathrm{Err}}\) is a scalar spatial parameter.

This model, and other spatial econometric models, do not fit into the mixed models framework. Here the modelled spatial process interacts directly with the response, covariates, and their coefficients. This modelling framework appears to draw on an older tradition extending time series to two dimensions:

\[
{\mathbf u} = ({\mathbf I} - \rho_{\mathrm{Err}} {\mathbf W})^{-1} {\mathbf \varepsilon},
\qquad {\mathbf y} = {\mathbf X}{\mathbf \beta} + ({\mathbf I} - \rho_{\mathrm{Err}} {\mathbf W})^{-1} {\mathbf \varepsilon},
\qquad ({\mathbf I} - \rho_{\mathrm{Err}} {\mathbf W}) {\mathbf y} = ({\mathbf I} - \rho_{\mathrm{Err}} {\mathbf W}) {\mathbf X}{\mathbf \beta} + {\mathbf \varepsilon}.
\]

If the processes in the covariates and the response match, we should find little difference between the coefficients of a least squares and a SEM, but very often they diverge, suggesting that a Hausman test for this condition should be employed \citep{pace+lesage:08}. This may be related to earlier discussions of a spatial equivalent to the unit root and cointegration where spatial processes match \citep{fingleton:99}.

A model with a spatial process in the response only is termed a spatial lag model (SLM, often SAR - spatial autoregressive) \citep{lesage+pace:09}. Durbin models add the spatially lagged covariates to the covariates included in the spatial model; spatial Durbin models are reviewed by Mur and Angulo \citeyearpar{mur+angulo:06}. If it is chosen to admit a spatial process in the residuals in addition to a spatial process in the response, again two models are formed, a general nested model (GNM) nesting all the others, and a model without spatially lagged covariates (SAC, also known as SARAR - Spatial AutoRegressive-AutoRegressive model). If neither the residuals nor the response are modelled with spatial processes, spatially lagged covariates may be added to a linear model, as a spatially lagged X model (SLX) \citep{elhorst:10, bivand:12, lesage:14, halleck-vega+elhorst:15}. We can write the general nested model (GNM) as:

\[
{\mathbf y} = \rho_{\mathrm{Lag}} {\mathbf W}{\mathbf y} + {\mathbf X}{\mathbf \beta} + {\mathbf W}{\mathbf X}{\mathbf \gamma} + {\mathbf u},
\qquad {\mathbf u} = \rho_{\mathrm{Err}} {\mathbf W} {\mathbf u} + {\mathbf \varepsilon},
\]
where \({\mathbf \gamma}\) is a \((k' \times 1)\) vector of parameters. \(k'\) defines the subset of the intercept and covariates, often \(k' = k-1\) when using row standardised spatial weights and omitting the spatially lagged intercept.

This may be constrained to the double spatial coefficient model SAC/SARAR by setting \({\mathbf \gamma} = 0\), to the spatial Durbin (SDM) by setting \(\rho_{\mathrm{Err}} = 0\), and to the error Durbin model (SDEM) by setting \(\rho_{\mathrm{Lag}} = 0\). Imposing more conditions gives the spatial lag model (SLM) with \({\mathbf \gamma} = 0\) and \(\rho_{\mathrm{Err}} = 0\), the spatial error model (SEM) with \({\mathbf \gamma} = 0\) and \(\rho_{\mathrm{Lag}} = 0\), and the spatially lagged X model (SLX) with \(\rho_{\mathrm{Lag}} = 0\) and \(\rho_{\mathrm{Err}} = 0\).

Although making predictions for new locations for which covariates are observed was raised as an issue some time ago, it has many years to make progress in reviewing the possibilities \citep{bivand:02, goulardetal:17, Laurent2021}. The prediction methods for SLM, SDM, SEM, SDEM, SAC and GNM models fitted with maximum likelihood were contributed as a Google Summer of Coding project by Martin Gubri. This work, and work on similar models with missing data \citep{suesse:18} is also relevant for exploring censored median house values in the Boston data set. Work on prediction also exposed the importance of the reduced form of these models, in which the spatial process in the response interacts with the regression coefficients in the SLM, SDM, SAC and GNM models.

The consequence of these interactions is that a unit change in a covariate will only impact the response as the value of the regression coefficient if the spatial coefficient of the lagged response is zero. Where it is non-zero, global spillovers, impacts, come into play, and these impacts should be reported rather than the regression coefficients \citep{lesage+pace:09, elhorst:10, bivand:12, lesage:14, halleck-vega+elhorst:15}. Local impacts may be reported for SDEM and SLX models, using linear combination to calculate standard errors for the total impacts of each covariate (sums of coefficients on the covariates and their spatial lags).

This can be seen from the GNM data generation process:

\[
({\mathbf I} - \rho_{\mathrm{Err}} {\mathbf W})({\mathbf I} - \rho_{\mathrm{Lag}} {\mathbf W}){\mathbf y} = ({\mathbf I} - \rho_{\mathrm{Err}} {\mathbf W})({\mathbf X}{\mathbf \beta} + {\mathbf W}{\mathbf X}{\mathbf \gamma}) + {\mathbf \varepsilon},
\]
re-writing:

\[
{\mathbf y} = ({\mathbf I} - \rho_{\mathrm{Lag}} {\mathbf W})^{-1}({\mathbf X}{\mathbf \beta} + {\mathbf W}{\mathbf X}{\mathbf \gamma}) + ({\mathbf I} - \rho_{\mathrm{Lag}} {\mathbf W})^{-1}({\mathbf I} - \rho_{\mathrm{Err}} {\mathbf W})^{-1}{\mathbf \varepsilon}.
\]
There is interaction between the \(\rho_{\mathrm{Lag}}\) and \({\mathbf \beta}\) (and \({\mathbf \gamma}\) if present) coefficients. This can be seen from the partial derivatives: \(\partial y_i / \partial x_{jr} = (({\mathbf I} - \rho_{\mathrm{Lag}} {\mathbf W})^{-1} ({\mathbf I} \beta_r + {\mathbf W} \gamma_r))_{ij}\). This dense matrix \(S_r({\mathbf W}) = (({\mathbf I} - \rho_{\mathrm{Lag}} {\mathbf W})^{-1} ({\mathbf I} \beta_r + {\mathbf W} \gamma_r))\) expresses the direct impacts (effects) on its principal diagonal, and indirect impacts in off-diagonal elements.

\citet{PIRAS2014103} revisit and correct \citet{FLORAX2003557} (see also comments by \citet{HENDRY2006309} and \citet{FLORAX2006300}), finding that the common use of pre-test strategies for model selection probably ought to be replaced by the estimation of the most general model appropriate for the relationships being modelled. In the light of this finding, pre-test model selection will not be used here.

Current work in the \textbf{spatialreg} package is focused on refining the handling of spatially lagged covariates using a consistent \texttt{Durbin=} argument taking either a logical value or a formula giving the subset of covariates to add in spatially lagged form. There is a speculation that some covariates, for example some dummy variables, should not be added in spatially lagged form. This then extends to handling these included spatially lagged covariates appropriately in calculating impacts. This work applies to cross-sectional models fitted using MCMC or maximum likelihood, and will offer facilities to spatial panel models.

It is worth mentioning the almost unexplored issues of functional form assumptions, for which flexible structures are useful, including spatial quantile regression presented in the \textbf{McSpatial} package \citep{mcmillen:13}. There are further issues with discrete response variables, covered by some functions in \textbf{McSpatial}, and in the \textbf{spatialprobit} and \textbf{ProbitSpatial} packages \citep{RJ-2013-013, MARTINETTI201730}; the MCMC implementations of the former are based on LeSage and Pace \citeyearpar{lesage+pace:09}. Finally, Wagner and Zeileis \citeyearpar{wagner+zeileis:19} show how an SLM model may be used in the setting of recursive partitioning, with an implementation using \texttt{spatialreg::lagsarlm()} in the \textbf{lagsarlmtree} package.

The review of cross-sectional maximum likelihood and generalized method of moments (GMM) estimators in \textbf{spatialreg} \citep{R-spatialreg} and \textbf{sphet} for spatial econometrics style spatial regression models by Bivand and Piras \citeyearpar{bivand+piras:15} is still largely valid. In the review, estimators in these R packages were compared with alternative implementations available in other programming languages elsewhere. The review did not cover Bayesian spatial econometrics style spatial regression. More has changed with respect to spatial panel estimators described in Millo and Piras \citeyearpar{millo+piras:12}, but will not be covered here.

Because \citet{math9111276} covers many of the features of R packages for spatial econometrics, updating \citet{bivand+piras:15}, and including recent advances in General Method of Moments and spatial panel modelling, this chapter will be restricted to a small number of examples drawing on \citet{bivand17} using the Boston house value data set.

\hypertarget{maximum-likelihood-estimation-in-spatialreg}{%
\section{\texorpdfstring{Maximum likelihood estimation in \textbf{spatialreg}}{Maximum likelihood estimation in spatialreg}}\label{maximum-likelihood-estimation-in-spatialreg}}

For models with single spatial coefficients (SEM and SDEM using \texttt{errorsarlm()}, SLM and SDM using \texttt{lagsarlm()}), the methods initially described by Ord \citeyearpar{ord:75} are used. The following table shows the functions that can be used to estimate the models described above using maximum likelihood.

\begin{longtable}[]{@{}lll@{}}
\toprule
model & model name & maximum likelihood estimation function\tabularnewline
\midrule
\endhead
SEM & spatial error & \texttt{errorsarlm(...,\ Durbin=FALSE,\ ...)}\tabularnewline
SEM & spatial error & \texttt{spautolm(...,\ family="SAR",\ ...)}\tabularnewline
SDEM & spatial Durbin error & \texttt{errorsarlm(...,\ Durbin=TRUE,\ ...)}\tabularnewline
SLM & spatial lag & \texttt{lagsarlm(...,\ Durbin=FALSE,\ ...)}\tabularnewline
SDM & spatial Durbin & \texttt{lagsarlm(...,\ Durbin=TRUE,\ ...)}\tabularnewline
SAC & spatial autoregressive combined & \texttt{sacsarlm(...,\ Durbin=FALSE,\ ...)}\tabularnewline
GNM & general nested & \texttt{sacsarlm(...,\ Durbin=TRUE,\ ...)}\tabularnewline
\bottomrule
\end{longtable}

The estimating functions \texttt{errorsarlm()} and \texttt{lagsarlm()} take similar arguments, where the first two, \texttt{formula=} and \texttt{data=} are shared by most model estimating functions. The third argument is a \texttt{listw} spatial weights object, while \texttt{na.action=} behaves as in other model estimating functions if the spatial weights can reasonably be subsetted to avoid observations with missing values. The \texttt{weights=} argument may be used to provide weights indicating the known degree of per-observation variability in the variance term - this is not available for \texttt{lagsarlm()}.

The \texttt{Durbin=} argument replaces the earlier \texttt{type=} and \texttt{etype=} arguments, and if not given is taken as \texttt{FALSE}. If given, it may be \texttt{FALSE}, \texttt{TRUE} in which case all spatially lagged covariates are included, or a one-sided formula specifying which spatially lagged covariates should be included. The \texttt{method=} argument gives the method for calculating the log determinant term in the log likelihood function, and defaults to \texttt{"eigen"}, suitable for moderately sized data sets. The \texttt{interval=} argument gives the bounds of the domain for the line search using \texttt{stats::optimize()} used for finding the spatial coefficient. The \texttt{tol.solve()} argument, passed through to \texttt{base::solve()}, was needed to handle data sets with differing numerical scales among the coefficients which hindered inversion of the variance-covariance matrix; the default value in \texttt{base::solve()} used to be much larger. The \texttt{control=} argument takes a list of control values to permit more careful adjustment of the running of the estimation function.

The \texttt{sacsarlm()} function may take second spatial weights and interval arguments if the spatial weights used to model the two spatial processes in the SAC and GNM specifications differ. By default, the same spatial weights are used. By default, \texttt{stats::nlminb()} is used for numerical optimization, using a heuristic to choose starting values. Like \texttt{lagsarlm()}, this function does not take a \texttt{weights=} argument.

Where larger data sets are used, a numerical Hessian approach is used to calculate the variance-covariance matrix of coefficients, rather than an analytical asymptotic approach.

\hypertarget{boston-house-value-data-set-examples}{%
\subsection{Boston house value data set examples}\label{boston-house-value-data-set-examples}}

The examples use the objects read and created in chapter \ref{spatglmm}, based on \citet{bivand17}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(spatialreg)}
\CommentTok{# }
\CommentTok{# Attaching package: 'spatialreg'}
\CommentTok{# The following objects are masked from 'package:spdep':}
\CommentTok{# }
\CommentTok{#     as_dgRMatrix_listw, as_dsCMatrix_I, as_dsCMatrix_IrW,}
\CommentTok{#     as_dsTMatrix_listw, as.spam.listw, can.be.simmed,}
\CommentTok{#     cheb_setup, create_WX, do_ldet, eigen_pre_setup,}
\CommentTok{#     eigen_setup, eigenw, errorsarlm, get.ClusterOption,}
\CommentTok{#     get.coresOption, get.mcOption, get.VerboseOption,}
\CommentTok{#     get.ZeroPolicyOption, GMargminImage, GMerrorsar,}
\CommentTok{#     griffith_sone, gstsls, Hausman.test, impacts,}
\CommentTok{#     intImpacts, Jacobian_W, jacobianSetup, l_max, lagmess,}
\CommentTok{#     lagsarlm, lextrB, lextrS, lextrW, lmSLX,}
\CommentTok{#     LU_prepermutate_setup, LU_setup, Matrix_J_setup,}
\CommentTok{#     Matrix_setup, mcdet_setup, MCMCsamp, ME, mom_calc,}
\CommentTok{#     mom_calc_int2, moments_setup, powerWeights, sacsarlm,}
\CommentTok{#     SE_classic_setup, SE_interp_setup, SE_whichMin_setup,}
\CommentTok{#     set.ClusterOption, set.coresOption, set.mcOption,}
\CommentTok{#     set.VerboseOption, set.ZeroPolicyOption,}
\CommentTok{#     similar.listw, spam_setup, spam_update_setup,}
\CommentTok{#     SpatialFiltering, spautolm, spBreg_err, spBreg_lag,}
\CommentTok{#     spBreg_sac, stsls, subgraph_eigenw, trW}
\NormalTok{eigs_}\DecValTok{489}\NormalTok{ <-}\StringTok{ }\KeywordTok{eigenw}\NormalTok{(lw_q_}\DecValTok{489}\NormalTok{)}
\NormalTok{SDEM_}\DecValTok{489}\NormalTok{ <-}\StringTok{ }\KeywordTok{errorsarlm}\NormalTok{(form, }\DataTypeTok{data=}\NormalTok{boston_}\DecValTok{489}\NormalTok{, }\DataTypeTok{listw=}\NormalTok{lw_q_}\DecValTok{489}\NormalTok{, }\DataTypeTok{Durbin=}\OtherTok{TRUE}\NormalTok{, }
                       \DataTypeTok{zero.policy=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{control=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{pre_eig=}\NormalTok{eigs_}\DecValTok{489}\NormalTok{))}
\NormalTok{SEM_}\DecValTok{489}\NormalTok{ <-}\StringTok{ }\KeywordTok{errorsarlm}\NormalTok{(form, }\DataTypeTok{data=}\NormalTok{boston_}\DecValTok{489}\NormalTok{, }\DataTypeTok{listw=}\NormalTok{lw_q_}\DecValTok{489}\NormalTok{, }
                      \DataTypeTok{zero.policy=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{control=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{pre_eig=}\NormalTok{eigs_}\DecValTok{489}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Here we are using the \texttt{control=} list argument to pass through pre-computed eigenvalues for the default \texttt{"eigen"} method.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cbind}\NormalTok{(}\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{model=}\KeywordTok{c}\NormalTok{(}\StringTok{"SEM"}\NormalTok{, }\StringTok{"SDEM"}\NormalTok{)), }
      \KeywordTok{rbind}\NormalTok{(broom}\OperatorTok{::}\KeywordTok{tidy}\NormalTok{(}\KeywordTok{Hausman.test}\NormalTok{(SEM_}\DecValTok{489}\NormalTok{)), }
\NormalTok{            broom}\OperatorTok{::}\KeywordTok{tidy}\NormalTok{(}\KeywordTok{Hausman.test}\NormalTok{(SDEM_}\DecValTok{489}\NormalTok{))))[,}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{]}
\CommentTok{#   model statistic  p.value parameter}
\CommentTok{# 1   SEM      52.0 2.83e-06        14}
\CommentTok{# 2  SDEM      48.7 6.48e-03        27}
\end{Highlighting}
\end{Shaded}

Both Hausman test results for the 489 tract data set suggest that the regression coefficients do differ from their non-spatial counterparts, perhaps indicating that the footprints of the spatial processes do not match.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eigs_}\DecValTok{94}\NormalTok{ <-}\StringTok{ }\KeywordTok{eigenw}\NormalTok{(lw_q_}\DecValTok{94}\NormalTok{)}
\NormalTok{SDEM_}\DecValTok{94}\NormalTok{ <-}\StringTok{ }\KeywordTok{errorsarlm}\NormalTok{(form, }\DataTypeTok{data=}\NormalTok{boston_}\DecValTok{94}\NormalTok{, }\DataTypeTok{listw=}\NormalTok{lw_q_}\DecValTok{94}\NormalTok{, }\DataTypeTok{Durbin=}\OtherTok{TRUE}\NormalTok{,}
                      \DataTypeTok{control=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{pre_eig=}\NormalTok{eigs_}\DecValTok{94}\NormalTok{))}
\NormalTok{SEM_}\DecValTok{94}\NormalTok{ <-}\StringTok{ }\KeywordTok{errorsarlm}\NormalTok{(form, }\DataTypeTok{data=}\NormalTok{boston_}\DecValTok{94}\NormalTok{, }\DataTypeTok{listw=}\NormalTok{lw_q_}\DecValTok{94}\NormalTok{, }\DataTypeTok{control=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{pre_eig=}\NormalTok{eigs_}\DecValTok{94}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

For the 94 air pollution model output zones, the Hausman tests find little difference between coefficients:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cbind}\NormalTok{(}\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{model=}\KeywordTok{c}\NormalTok{(}\StringTok{"SEM"}\NormalTok{, }\StringTok{"SDEM"}\NormalTok{)), }
      \KeywordTok{rbind}\NormalTok{(broom}\OperatorTok{::}\KeywordTok{tidy}\NormalTok{(}\KeywordTok{Hausman.test}\NormalTok{(SEM_}\DecValTok{94}\NormalTok{)), }
\NormalTok{            broom}\OperatorTok{::}\KeywordTok{tidy}\NormalTok{(}\KeywordTok{Hausman.test}\NormalTok{(SDEM_}\DecValTok{94}\NormalTok{))))[, }\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{]}
\CommentTok{#   model statistic p.value parameter}
\CommentTok{# 1   SEM     15.66   0.335        14}
\CommentTok{# 2  SDEM      9.21   0.999        27}
\end{Highlighting}
\end{Shaded}

This is related to the fact that the SEM and SDEM models add little to least squares or SLX at the air pollution model output zone level, using likelihood ratio tests:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cbind}\NormalTok{(}\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{model=}\KeywordTok{c}\NormalTok{(}\StringTok{"SEM"}\NormalTok{, }\StringTok{"SDEM"}\NormalTok{)), }\KeywordTok{rbind}\NormalTok{(broom}\OperatorTok{::}\KeywordTok{tidy}\NormalTok{(}\KeywordTok{LR1.Sarlm}\NormalTok{(SEM_}\DecValTok{94}\NormalTok{)),}
\NormalTok{    broom}\OperatorTok{::}\KeywordTok{tidy}\NormalTok{(}\KeywordTok{LR1.Sarlm}\NormalTok{(SDEM_}\DecValTok{94}\NormalTok{))))[,}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{4}\OperatorTok{:}\DecValTok{6}\NormalTok{)]}
\CommentTok{#   model statistic p.value parameter}
\CommentTok{# 1   SEM     2.593   0.107         1}
\CommentTok{# 2  SDEM     0.216   0.642         1}
\end{Highlighting}
\end{Shaded}

We can use \texttt{spatialreg::LR.Sarlm()} to apply a likelihood ratio test between nested models, but here choose \texttt{lmtest::lrtest()}, which gives the same results, preferring models including spatially lagged covariates both for tracts and model output zones:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{broom}\OperatorTok{::}\KeywordTok{tidy}\NormalTok{(lmtest}\OperatorTok{::}\KeywordTok{lrtest}\NormalTok{(SEM_}\DecValTok{489}\NormalTok{, SDEM_}\DecValTok{489}\NormalTok{))}
\CommentTok{# # A tibble: 2 x 5}
\CommentTok{#    X.Df LogLik    df statistic   p.value}
\CommentTok{#   <dbl>  <dbl> <dbl>     <dbl>     <dbl>}
\CommentTok{# 1    16   273.    NA      NA   NA       }
\CommentTok{# 2    29   311.    13      74.4  1.23e-10}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{broom}\OperatorTok{::}\KeywordTok{tidy}\NormalTok{(lmtest}\OperatorTok{::}\KeywordTok{lrtest}\NormalTok{(SEM_}\DecValTok{94}\NormalTok{, SDEM_}\DecValTok{94}\NormalTok{))}
\CommentTok{# # A tibble: 2 x 5}
\CommentTok{#    X.Df LogLik    df statistic    p.value}
\CommentTok{#   <dbl>  <dbl> <dbl>     <dbl>      <dbl>}
\CommentTok{# 1    16   59.7    NA      NA   NA        }
\CommentTok{# 2    29   81.3    13      43.2  0.0000421}
\end{Highlighting}
\end{Shaded}

The SLX model is fitted using least squares, and also returns a log likelihood value, letting us test whether we need a spatial process in the residuals. In the tract data set we obviously do:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SLX_}\DecValTok{489}\NormalTok{ <-}\StringTok{ }\KeywordTok{lmSLX}\NormalTok{(form, }\DataTypeTok{data=}\NormalTok{boston_}\DecValTok{489}\NormalTok{, }\DataTypeTok{listw=}\NormalTok{lw_q_}\DecValTok{489}\NormalTok{, }\DataTypeTok{zero.policy=}\OtherTok{TRUE}\NormalTok{)}
\NormalTok{broom}\OperatorTok{::}\KeywordTok{tidy}\NormalTok{(lmtest}\OperatorTok{::}\KeywordTok{lrtest}\NormalTok{(SLX_}\DecValTok{489}\NormalTok{, SDEM_}\DecValTok{489}\NormalTok{))}
\CommentTok{# # A tibble: 2 x 5}
\CommentTok{#    X.Df LogLik    df statistic   p.value}
\CommentTok{#   <dbl>  <dbl> <dbl>     <dbl>     <dbl>}
\CommentTok{# 1    28   231.    NA       NA  NA       }
\CommentTok{# 2    29   311.     1      159.  1.55e-36}
\end{Highlighting}
\end{Shaded}

but in the output zone case we do not.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SLX_}\DecValTok{94}\NormalTok{ <-}\StringTok{ }\KeywordTok{lmSLX}\NormalTok{(form, }\DataTypeTok{data=}\NormalTok{boston_}\DecValTok{94}\NormalTok{, }\DataTypeTok{listw=}\NormalTok{lw_q_}\DecValTok{94}\NormalTok{)}
\NormalTok{broom}\OperatorTok{::}\KeywordTok{tidy}\NormalTok{(lmtest}\OperatorTok{::}\KeywordTok{lrtest}\NormalTok{(SLX_}\DecValTok{94}\NormalTok{, SDEM_}\DecValTok{94}\NormalTok{))}
\CommentTok{# # A tibble: 2 x 5}
\CommentTok{#    X.Df LogLik    df statistic p.value}
\CommentTok{#   <dbl>  <dbl> <dbl>     <dbl>   <dbl>}
\CommentTok{# 1    28   81.2    NA    NA      NA    }
\CommentTok{# 2    29   81.3     1     0.216   0.642}
\end{Highlighting}
\end{Shaded}

These outcomes are sustained also when we use the counts of house units by tract and output zones as case weights:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SLX_489w <-}\StringTok{ }\KeywordTok{lmSLX}\NormalTok{(form, }\DataTypeTok{data=}\NormalTok{boston_}\DecValTok{489}\NormalTok{, }\DataTypeTok{listw=}\NormalTok{lw_q_}\DecValTok{489}\NormalTok{, }\DataTypeTok{weights=}\NormalTok{units, }\DataTypeTok{zero.policy=}\OtherTok{TRUE}\NormalTok{)}
\NormalTok{SDEM_489w <-}\StringTok{ }\KeywordTok{errorsarlm}\NormalTok{(form, }\DataTypeTok{data=}\NormalTok{boston_}\DecValTok{489}\NormalTok{, }\DataTypeTok{listw=}\NormalTok{lw_q_}\DecValTok{489}\NormalTok{, }\DataTypeTok{Durbin=}\OtherTok{TRUE}\NormalTok{,}
    \DataTypeTok{weights=}\NormalTok{units, }\DataTypeTok{zero.policy=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{control=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{pre_eig=}\NormalTok{eigs_}\DecValTok{489}\NormalTok{))}
\NormalTok{broom}\OperatorTok{::}\KeywordTok{tidy}\NormalTok{(lmtest}\OperatorTok{::}\KeywordTok{lrtest}\NormalTok{(SLX_489w, SDEM_489w))}
\CommentTok{# # A tibble: 2 x 5}
\CommentTok{#    X.Df LogLik    df statistic   p.value}
\CommentTok{#   <dbl>  <dbl> <dbl>     <dbl>     <dbl>}
\CommentTok{# 1    28   311.    NA       NA  NA       }
\CommentTok{# 2    29   379.     1      136.  1.70e-31}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SLX_94w <-}\StringTok{ }\KeywordTok{lmSLX}\NormalTok{(form, }\DataTypeTok{data=}\NormalTok{boston_}\DecValTok{94}\NormalTok{, }\DataTypeTok{listw=}\NormalTok{lw_q_}\DecValTok{94}\NormalTok{, }\DataTypeTok{weights=}\NormalTok{units)}
\NormalTok{SDEM_94w <-}\StringTok{ }\KeywordTok{errorsarlm}\NormalTok{(form, }\DataTypeTok{data=}\NormalTok{boston_}\DecValTok{94}\NormalTok{, }\DataTypeTok{listw=}\NormalTok{lw_q_}\DecValTok{94}\NormalTok{, }\DataTypeTok{Durbin=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{weights=}\NormalTok{units,}
                       \DataTypeTok{control=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{pre_eig=}\NormalTok{eigs_}\DecValTok{94}\NormalTok{))}
\NormalTok{broom}\OperatorTok{::}\KeywordTok{tidy}\NormalTok{(lmtest}\OperatorTok{::}\KeywordTok{lrtest}\NormalTok{(SLX_94w, SDEM_94w))}
\CommentTok{# # A tibble: 2 x 5}
\CommentTok{#    X.Df LogLik    df statistic p.value}
\CommentTok{#   <dbl>  <dbl> <dbl>     <dbl>   <dbl>}
\CommentTok{# 1    28   97.5    NA    NA      NA    }
\CommentTok{# 2    29   98.0     1     0.917   0.338}
\end{Highlighting}
\end{Shaded}

In this case and based on arguments advanced in \citet{bivand17}, the use of weights is justified because tract counts of reported housing units underlyinng the weighted median values vary from 5 to 3,031, and air pollution model output zone counts vary from 25 to 12,411. Because of this, and because a weighted general nested model has not been developed, we cannot take the GNM as the starting point for general-to-simpler testing, but start rather from the SDEM model, and use the Hausman test to guide the choice of units of observation.

\hypertarget{impacts}{%
\section{Impacts}\label{impacts}}

Global impacts have been seen as crucial for reporting results from fitting models including the spatially lagged response (SLM, SDM, SAC. GNM) for over ten years \citep{lesage+pace:09}. Extension to other models including spatially lagged covariates (SLX, SDEM) has followed \citep{elhorst:10, bivand:12, halleck-vega+elhorst:15}. For SLM, SDM, SAC and GNM models fitted with maximum likelihood or GMM, the variance-covariance matrix of the coefficients is available, and can be used to make random draws from a multivariate Normal distribution with mean set to coefficient values and variance to the estimated variance-covariance matrix. For these models fitted using Bayesian methods, draws are already available. In the SDEM case, the draws on the regression coefficients of the unlagged covariates represent direct impacts, and draws on the coefficients of the spatially lagged covariates represent indirect impacts, and their by-draw sums the total impacts.

Since sampling is not required for inference for SLX and SDEM models, linear combination is used for models fitted using maximum likelihood; results are shown here for the air pollution variable only. The literature has not yet resolved the question of how to report model output, as each covariate is now represented by three impacts. Where spatially lagged covariates are included, two coefficients are replaced by three impacts, here for the air pollution variable of interest.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sum_imp_}\DecValTok{94}\NormalTok{_SDEM <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(}\KeywordTok{impacts}\NormalTok{(SDEM_}\DecValTok{94}\NormalTok{))}
\KeywordTok{rbind}\NormalTok{(}\DataTypeTok{Impacts=}\NormalTok{sum_imp_}\DecValTok{94}\NormalTok{_SDEM}\OperatorTok{$}\NormalTok{mat[}\DecValTok{5}\NormalTok{,], }\DataTypeTok{SE=}\NormalTok{sum_imp_}\DecValTok{94}\NormalTok{_SDEM}\OperatorTok{$}\NormalTok{semat[}\DecValTok{5}\NormalTok{,])}
\CommentTok{#           Direct Indirect   Total}
\CommentTok{# Impacts -0.01276 -0.01845 -0.0312}
\CommentTok{# SE       0.00235  0.00472  0.0053}
\end{Highlighting}
\end{Shaded}

In the SLX and SDEM models, the direct impacts are the consequences for the response of changes in air pollution in the same observational entity, and the indirect (local) impacts are the consequences for the response of changes in air pollution in neighbouring observational entities.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sum_imp_}\DecValTok{94}\NormalTok{_SLX <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(}\KeywordTok{impacts}\NormalTok{(SLX_}\DecValTok{94}\NormalTok{))}
\KeywordTok{rbind}\NormalTok{(}\DataTypeTok{Impacts=}\NormalTok{sum_imp_}\DecValTok{94}\NormalTok{_SLX}\OperatorTok{$}\NormalTok{mat[}\DecValTok{5}\NormalTok{,], }\DataTypeTok{SE=}\NormalTok{sum_imp_}\DecValTok{94}\NormalTok{_SLX}\OperatorTok{$}\NormalTok{semat[}\DecValTok{5}\NormalTok{,])}
\CommentTok{#          Direct Indirect    Total}
\CommentTok{# Impacts -0.0128 -0.01874 -0.03151}
\CommentTok{# SE       0.0028  0.00556  0.00611}
\end{Highlighting}
\end{Shaded}

Applying the same approaches to the weighted spatial regressions, the total impacts of air pollution on house values are reduced, but remain significant:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sum_imp_}\DecValTok{94}\NormalTok{_SDEMw <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(}\KeywordTok{impacts}\NormalTok{(SDEM_94w))}
\KeywordTok{rbind}\NormalTok{(}\DataTypeTok{Impacts=}\NormalTok{sum_imp_}\DecValTok{94}\NormalTok{_SDEMw}\OperatorTok{$}\NormalTok{mat[}\DecValTok{5}\NormalTok{,], }\DataTypeTok{SE=}\NormalTok{sum_imp_}\DecValTok{94}\NormalTok{_SDEMw}\OperatorTok{$}\NormalTok{semat[}\DecValTok{5}\NormalTok{,])}
\CommentTok{#           Direct Indirect    Total}
\CommentTok{# Impacts -0.00592 -0.01076 -0.01668}
\CommentTok{# SE       0.00269  0.00531  0.00559}
\end{Highlighting}
\end{Shaded}

On balance, using a weighted spatial regression representation including only the spatially lagged covariates aggregated to the air pollution model output zone level seems to clear most of the mis-specification issues, and as \citet{bivand17} discusses in more detail, gives a willingness to pay for pollution abatement that is much larger than mis-specified alternative models:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sum_imp_}\DecValTok{94}\NormalTok{_SLXw <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(}\KeywordTok{impacts}\NormalTok{(SLX_94w))}
\KeywordTok{rbind}\NormalTok{(}\DataTypeTok{Impacts=}\NormalTok{sum_imp_}\DecValTok{94}\NormalTok{_SLXw}\OperatorTok{$}\NormalTok{mat[}\DecValTok{5}\NormalTok{,], }\DataTypeTok{SE=}\NormalTok{sum_imp_}\DecValTok{94}\NormalTok{_SLXw}\OperatorTok{$}\NormalTok{semat[}\DecValTok{5}\NormalTok{,])}
\CommentTok{#           Direct Indirect    Total}
\CommentTok{# Impacts -0.00620 -0.01221 -0.01842}
\CommentTok{# SE       0.00326  0.00628  0.00629}
\end{Highlighting}
\end{Shaded}

\hypertarget{spatecon_pred}{%
\section{Predictions}\label{spatecon_pred}}

In the Boston tracts data set, 17 observations of median house values, the response, are censored. We will use the \texttt{predict()} method for \texttt{"Sarlm"} objects to fill in these values; the method was re-written by Martin Gubri based on \citeauthor{goulardetal:17} \citetext{\citeyear{goulardetal:17}; \citealp[see also][]{Laurent2021}}. The \texttt{pred.type=} argument specifies the prediction strategy among those presented in the article.

Using these as an example and comparing some \texttt{pred.type=} variants for the SDEM model and predicting out-of-sample, we can see that there are differences, suggesting that this is a fruitful area for study. There have been a number of alternative proposals for handling missing variables \citep{GOMEZRUBIO2015116, suesse:18}. Another reason for increasing attention on prediction is that it is fundamental for machine learning approaches, in which prediction for validation and test data sets drives model specification choice. The choice of training and other data sets with dependent spatial data remains an open question, and is certainly not as simple as with independent data.

Here, we'll list the predictions for the censored tract observations using three different prediction types, taking the exponent to get back to the USD median house values. Note that the \texttt{row.names()} of the \texttt{newdata=} object are matched with the whole-data spatial weights matrix \texttt{"region.id"} attribute to make out-of-sample prediction possible:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nd <-}\StringTok{ }\NormalTok{boston_}\DecValTok{506}\NormalTok{[}\KeywordTok{is.na}\NormalTok{(boston_}\DecValTok{506}\OperatorTok{$}\NormalTok{median),]}
\NormalTok{t0 <-}\StringTok{ }\KeywordTok{exp}\NormalTok{(}\KeywordTok{predict}\NormalTok{(SDEM_}\DecValTok{489}\NormalTok{, }\DataTypeTok{newdata=}\NormalTok{nd, }\DataTypeTok{listw=}\NormalTok{lw_q, }\DataTypeTok{pred.type=}\StringTok{"TS"}\NormalTok{, }\DataTypeTok{zero.policy=}\OtherTok{TRUE}\NormalTok{))}
\KeywordTok{suppressWarnings}\NormalTok{(t1  <-}\StringTok{ }\KeywordTok{exp}\NormalTok{(}\KeywordTok{predict}\NormalTok{(SDEM_}\DecValTok{489}\NormalTok{, }\DataTypeTok{newdata=}\NormalTok{nd, }\DataTypeTok{listw=}\NormalTok{lw_q, }\DataTypeTok{pred.type=}\StringTok{"KP2"}\NormalTok{,}
                                    \DataTypeTok{zero.policy=}\OtherTok{TRUE}\NormalTok{)))}
\KeywordTok{suppressWarnings}\NormalTok{(t2  <-}\StringTok{ }\KeywordTok{exp}\NormalTok{(}\KeywordTok{predict}\NormalTok{(SDEM_}\DecValTok{489}\NormalTok{, }\DataTypeTok{newdata=}\NormalTok{nd, }\DataTypeTok{listw=}\NormalTok{lw_q, }\DataTypeTok{pred.type=}\StringTok{"KP5"}\NormalTok{,}
                                    \DataTypeTok{zero.policy=}\OtherTok{TRUE}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

We can also use the \texttt{"slm"} model in INLA to predict missing response values as part of the model fitting function call. A certain amount of set-up code is required as the \texttt{"slm"} model is still experimental:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(INLA)}
\NormalTok{W <-}\StringTok{ }\KeywordTok{as}\NormalTok{(lw_q, }\StringTok{"CsparseMatrix"}\NormalTok{)}
\NormalTok{n <-}\StringTok{ }\KeywordTok{nrow}\NormalTok{(W)}
\NormalTok{e <-}\StringTok{ }\KeywordTok{eigenw}\NormalTok{(lw_q)}
\NormalTok{re.idx <-}\StringTok{ }\KeywordTok{which}\NormalTok{(}\KeywordTok{abs}\NormalTok{(}\KeywordTok{Im}\NormalTok{(e)) }\OperatorTok{<}\StringTok{ }\FloatTok{1e-6}\NormalTok{)}
\NormalTok{rho.max <-}\StringTok{ }\DecValTok{1}\OperatorTok{/}\KeywordTok{max}\NormalTok{(}\KeywordTok{Re}\NormalTok{(e[re.idx]))}
\NormalTok{rho.min <-}\StringTok{ }\DecValTok{1}\OperatorTok{/}\KeywordTok{min}\NormalTok{(}\KeywordTok{Re}\NormalTok{(e[re.idx]))}
\NormalTok{rho <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(}\KeywordTok{c}\NormalTok{(rho.min, rho.max))}
\NormalTok{boston_}\DecValTok{506}\OperatorTok{$}\NormalTok{idx <-}\StringTok{ }\DecValTok{1}\OperatorTok{:}\NormalTok{n}
\NormalTok{zero.variance =}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{prec=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{initial =} \DecValTok{25}\NormalTok{, }\DataTypeTok{fixed=}\OtherTok{TRUE}\NormalTok{))}
\NormalTok{args.slm <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{rho.min =}\NormalTok{ rho.min, }\DataTypeTok{rho.max =}\NormalTok{ rho.max, }\DataTypeTok{W =}\NormalTok{ W, }\DataTypeTok{X =} \KeywordTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, n, }\DecValTok{0}\NormalTok{),}
    \DataTypeTok{Q.beta =} \KeywordTok{matrix}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{))}
\NormalTok{hyper.slm <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{prec =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{prior =} \StringTok{"loggamma"}\NormalTok{, }\DataTypeTok{param =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.01}\NormalTok{, }\FloatTok{0.01}\NormalTok{)),}
    \DataTypeTok{rho =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{initial=}\DecValTok{0}\NormalTok{, }\DataTypeTok{prior =} \StringTok{"logitbeta"}\NormalTok{, }\DataTypeTok{param =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)))}
\NormalTok{WX <-}\StringTok{ }\KeywordTok{create_WX}\NormalTok{(}\KeywordTok{model.matrix}\NormalTok{(}\KeywordTok{update}\NormalTok{(form, CMEDV }\OperatorTok{~}\StringTok{ }\NormalTok{.), }\DataTypeTok{data=}\NormalTok{boston_}\DecValTok{506}\NormalTok{), lw_q)}
\NormalTok{SDEM_}\DecValTok{506}\NormalTok{_slm <-}\StringTok{ }\KeywordTok{inla}\NormalTok{(}\KeywordTok{update}\NormalTok{(form, . }\OperatorTok{~}\StringTok{ }\NormalTok{. }\OperatorTok{+}\StringTok{ }\NormalTok{WX }\OperatorTok{+}\StringTok{ }\KeywordTok{f}\NormalTok{(idx, }\DataTypeTok{model=}\StringTok{"slm"}\NormalTok{, }\DataTypeTok{args.slm=}\NormalTok{args.slm,}
    \DataTypeTok{hyper=}\NormalTok{hyper.slm)), }\DataTypeTok{data=}\NormalTok{boston_}\DecValTok{506}\NormalTok{, }\DataTypeTok{family=}\StringTok{"gaussian"}\NormalTok{, }\DataTypeTok{control.family=}
        \KeywordTok{list}\NormalTok{(}\DataTypeTok{hyper=}\NormalTok{zero.variance), }\DataTypeTok{control.compute=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{dic=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{cpo=}\OtherTok{TRUE}\NormalTok{))}
\NormalTok{mvs <-}\StringTok{ }\NormalTok{SDEM_}\DecValTok{506}\NormalTok{_slm}\OperatorTok{$}\NormalTok{marginals.fitted.values[}\KeywordTok{which}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(boston_}\DecValTok{506}\OperatorTok{$}\NormalTok{median))]}
\NormalTok{mv_mean <-}\StringTok{ }\KeywordTok{sapply}\NormalTok{(mvs, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{mean}\NormalTok{(}\KeywordTok{exp}\NormalTok{(x[, }\DecValTok{1}\NormalTok{]), ))}
\NormalTok{mv_sd <-}\StringTok{ }\KeywordTok{sapply}\NormalTok{(mvs, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{sd}\NormalTok{(}\KeywordTok{exp}\NormalTok{(x[, }\DecValTok{1}\NormalTok{])))}
\end{Highlighting}
\end{Shaded}

INLA also provide gridded estimates of the marginal distributions of the predictions, offering a way to assess the uncertainty associated with the predicted values:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{fit_TS=}\NormalTok{t0[,}\DecValTok{1}\NormalTok{], }\DataTypeTok{fit_KP2=}\KeywordTok{c}\NormalTok{(t1), }\DataTypeTok{fit_KP5=}\KeywordTok{c}\NormalTok{(t2), }\DataTypeTok{INLA_slm=}\NormalTok{mv_mean,}
    \DataTypeTok{INLA_slm_sd=}\NormalTok{mv_sd, }\DataTypeTok{censored=}\NormalTok{boston_}\DecValTok{506}\OperatorTok{$}\NormalTok{censored[}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{attr}\NormalTok{(t0, }\StringTok{"region.id"}\NormalTok{))])}
\CommentTok{#     fit_TS fit_KP2 fit_KP5 INLA_slm INLA_slm_sd censored}
\CommentTok{# 13   23912   29477   28147    33941       15901    right}
\CommentTok{# 14   28126   27001   28516    34512       17148    right}
\CommentTok{# 15   30553   36184   32476    45061       21574    right}
\CommentTok{# 17   18518   19621   18878    22842       10045    right}
\CommentTok{# 43    9564    6817    7561     7420        3332     left}
\CommentTok{# 50    8371    7196    7383     7508        3518     left}
\CommentTok{# 312  51477   53301   54173    62103       31468    right}
\CommentTok{# 313  45921   45823   47095    51123       25328    right}
\CommentTok{# 314  44196   44586   45361    46778       22165    right}
\CommentTok{# 317  43427   45707   45442    52252       24140    right}
\CommentTok{# 337  39879   42072   41127    44763       19567    right}
\CommentTok{# 346  44708   46694   46108    49153       20254    right}
\CommentTok{# 355  48188   49068   48911    53177       23605    right}
\CommentTok{# 376  42881   45883   44966    51685       23040    right}
\CommentTok{# 408  44294   44615   45670    50515       23959    right}
\CommentTok{# 418  38211   43375   41914    47610       21486    right}
\CommentTok{# 434  41647   41690   42398    45045       20239    right}
\end{Highlighting}
\end{Shaded}

The spatial regression toolbox remains incomplete, and it will take time to fill in blanks. It remains unfortunate that the several traditions in spatial regression seldom seem to draw on each others' understandings and advances.

\hypertarget{older}{%
\chapter{Older R Spatial Packages}\label{older}}

\hypertarget{retire}{%
\section{\texorpdfstring{Retiring \texttt{rgdal} and \texttt{rgeos}}{Retiring rgdal and rgeos}}\label{retire}}

R users who have been around a bit longer, in particular before
packages like \texttt{sf} and \texttt{stars} were developed, may be more familiar
with older packages like \texttt{maptools}, \texttt{sp}, \texttt{rgeos}, and \texttt{rgdal}. A
fair question is whether they should migrate existing code and/or
existing R packages depending on these packages. The answer is: yes.

Unless someone steps up to volunteer maintaining packages \texttt{maptools},
\texttt{rgdal} and \texttt{rgeos}, the plan is to retire packages by the end of
2023. Retirement means that maintenance will halt, and that as a
consequence the packages will sooner or later disapper from CRAN.
One reason for retirement is that their maintainer has retired,
another that their role has been superseded by the newer packages.
We hold it not for very likely that a new maintainer will take over,
in part because much of the code of these packages has over a few
decades gradually evolved along with developments in the GEOS,
GDAL and PROJ libraries, and now contains numerous constructs that
are no longer necessary and make it hard to read.

Before \texttt{rgeos} and \texttt{rgdal} retire, existing ties that package \texttt{sp}
has to \texttt{rgdal} and \texttt{rgeos} can and will be replaced by ties to
package \texttt{sf}. This only involves validation of coordinate reference
system identifiers, and checking whether rings are holes or exterior
rings. Theoretically one could replace \texttt{rgdal} and \texttt{rgeos} with
packages that would call into \texttt{sf} for their ties to the GEOS,
GDAL and PROJ libraries but that would involve a major effort.

\hypertarget{links-and-differences-between-sf-and-sp}{%
\section{links and differences between sf and sp}\label{links-and-differences-between-sf-and-sp}}

There are a number of differences between \texttt{sf} and \texttt{sp}. The most
notable is that \texttt{sp} classes are formal, S4 classes where \texttt{sf} uses
the (more) informal S3 class hierarchy. \texttt{sf} objects derive from
data.frames, or from tibbles, and as such can be used easier with
much of the other infrastructure of R and e.g.~with the tidyverse
package family. \texttt{sf} objects keep geometry in a list-column, meaning
that a geometry is \emph{always} a list element. Package \texttt{sp} used data
structures much less strictly, and for instance all coordinates of
\texttt{SpatiaPoints} or \texttt{SpatialPixels} are kept in matrices, which is
much more performant for certain problems but is not possible with
a list-column. Conversion from an \texttt{sf} object \texttt{x} to its \texttt{sp}
equivalent is done by

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(sp)}
\NormalTok{y =}\StringTok{ }\KeywordTok{as}\NormalTok{(x, }\StringTok{"Spatial"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

and the conversion the other way around is done by

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x0 =}\StringTok{ }\KeywordTok{st_as_sf}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

There are some limitations to conversions like this:

\begin{itemize}
\tightlist
\item
  \texttt{sp} does not distinguish between \texttt{LINESTRING} and
  \texttt{MULTILINESTRING} geometries, or between \texttt{POLYGON} or \texttt{MULTIPOLYGON},
  so e.g.~a \texttt{LINESTRING} will after conversion to \texttt{sp} come back as a
  \texttt{MULTILINESTRING}
\item
  \texttt{sp} does have no representation for \texttt{GEOMETRYCOLLECTION}
  geometries, or \texttt{sf} objects with geometries \emph{not} in the ``big seven''
  (section \ref{seven})
\item
  \texttt{sf} or \texttt{sfc} objects of geometry type \texttt{GEOMETRY}, with mixed
  geometry types, cannot be converted into \texttt{sp} objects
\item
  attribute-geometry relationship attributes get lost when converting
  to \texttt{sp}
\item
  \texttt{sf} objects with more than one geometry list-column will, when
  converting to \texttt{sp}, loose their secondary list-column(s).
\end{itemize}

\hypertarget{migration-code-and-packages}{%
\section{migration code and packages}\label{migration-code-and-packages}}

The wiki page of the GitHub site for sf, found at

\begin{verbatim}
https://github.com/r-spatial/sf/wiki/Migrating
\end{verbatim}

contains a list of methods and functions in \texttt{rgeos}, \texttt{rgdal} and
\texttt{sp} and the corresponding \texttt{sf} method or function. This may help
converting existing code or packages.

A simple approach to migrate code is when only \texttt{rgdal::readOGR} is
used to read \texttt{file}. As an alternative, one might use

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x =}\StringTok{ }\KeywordTok{as}\NormalTok{(sf}\OperatorTok{::}\KeywordTok{read_sf}\NormalTok{(}\StringTok{"file"}\NormalTok{), }\StringTok{"Spatial"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

however possible arguments to \texttt{readOGR}, when used, would need more
care.

An effort by us is underway to convert all code of our earlier book
``Applied Spatial Data Analysis with R'' (with Virgilio Gomez-Rubio,
\citet{asdar}) to run entirely without \texttt{rgdal}, \texttt{rgeos} and \texttt{maptools} and
where possible without \texttt{sp}. The scripts are found at
\url{https://github.com/rsbivand/sf_asdar2ed} .

\hypertarget{package-raster-and-terra}{%
\section{Package raster and terra}\label{package-raster-and-terra}}

Package \texttt{raster} has been a workhorse package for analysing raster
data with R since 2010, and has since then grown into a package for
``Geographic Data Analysis and Modeling'' \citep{R-raster}, indicating that
it is used for all kinds of raster data. The \texttt{raster} package uses
\texttt{sp} objects for vector data, and \texttt{rgdal} to read and write data to
formats served by the GDAL library. A follow-up package, \texttt{terra},
for ``Spatial Data Analysis'' \citep{R-terra}, ``is very similar to the
`raster' package; but {[}\ldots{}{]} can do more, is easier to use, and
{[}\ldots{}{]} is faster''. The \texttt{terra} package comes with its own classes for
vector data, but accepts many \texttt{sf} objects, with similar restrictions
as listed above for conversion to \texttt{sp}.

Raster maps, or stacks of them from package \texttt{raster} or \texttt{terra}
can be converted to \texttt{stars} objects using \texttt{st\_as\_stars()}. Package
\texttt{sf} contains an \texttt{st\_as\_sf()} method for \texttt{SpatVector} objects from
package \texttt{terra}. Package \texttt{terra} has its own direct links to GDAL,
GEOS and PROJ so no longer needs other packages for that. Migration
from \texttt{raster} to \texttt{terra} may become more important once \texttt{rgdal}
is no longer easily installable (section \ref{retire}).

The online book ``Spatial Data Science with R'', written by Robert
Hijmans and found at \url{https://rspatial.org/terra} details the \texttt{terra}
approach to spatial data analysis. Package \texttt{sf} and \texttt{stars} and
several other r-spatial packages discussed in this book reside on the
\texttt{r-spatial} github organisation (note the hyphen between \texttt{r} and
\texttt{spatial}, which is absent on Hijmans' organisation), which has a
blog site, with links to this book, found at \url{https://r-spatial.org/}
. Packages \texttt{sf} and \texttt{stars} on one hand and \texttt{terra} on the other have
a lot of goals in common, but try to reach them in different ways,
emphasizing different aspects of data analysis, software engineering,
and community management. Although this may confuse some users, we
believe that these differences are beneficial, encourage diversity
and choice, and hopefully work as an encouragement for others to
continue trying out new ideas when using R for spatial data problems.

\hypertarget{r-basics}{%
\chapter*{R basics}\label{r-basics}}
\addcontentsline{toc}{chapter}{R basics}

This chapter provides some minimal set of R basics that may make
it easier to read this book. A more comprehensive book on R basics
is given in \citep{advr}, chapter 2.

\hypertarget{pipes}{%
\section*{Pipes}\label{pipes}}
\addcontentsline{toc}{section}{Pipes}

The \texttt{\%\textgreater{}\%} (pipe) symbols should be read as \emph{then}: we read

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a }\OperatorTok{%>%}\StringTok{ }\KeywordTok{b}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\KeywordTok{c}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\KeywordTok{d}\NormalTok{(}\DataTypeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

as \emph{with \texttt{a} do \texttt{b} then \texttt{c} then \texttt{d} with \texttt{n} being 10}, and that is just alternative syntax for

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{d}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\KeywordTok{b}\NormalTok{(a)), }\DataTypeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

or

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tmp1 <-}\StringTok{ }\KeywordTok{b}\NormalTok{(a)}
\NormalTok{tmp2 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(tmp1)}
\NormalTok{tmp3 <-}\StringTok{ }\KeywordTok{d}\NormalTok{(tmp2, }\DataTypeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To many, the pipe-form is easier to read because execution order
follows reading order, from left to right. Like nested function
calls, it avoids the need to choose names for intermediate results,
but like nested function calls, it is hard to debug intermediate
results that diverge from out expectations. Note that the
intermediate results do exist in memory, so neither form saves
memory allocation. The pipe that appeared natively in R 4.1.0,
\texttt{\textbar{}\textgreater{}}, can be used anywhere in this book where \texttt{\%\textgreater{}\%} is used. The
reason we kept to the \texttt{\%\textgreater{}\%} pipe is to not exclude users of older
R version to use the code sections in this book.

\hypertarget{data-structures}{%
\section*{Data structures}\label{data-structures}}
\addcontentsline{toc}{section}{Data structures}

As pointed out by \citep{extending}, \emph{everything that exists in R is an
object}. This includes objects that make things happen, such as
language objects or functions, but also the more basic ``things'',
such as data objects. Some basic R data structures will now be
discussed.

\hypertarget{homogeneous-vectors}{%
\subsection*{Homogeneous vectors}\label{homogeneous-vectors}}
\addcontentsline{toc}{subsection}{Homogeneous vectors}

Data objects contain data, and possibly metadata. Data is always
in the form of a vector, which can have different type. We can
find the type by \texttt{typeof}, and vector length by \texttt{length}. Vectors
are created by \texttt{c}, which combines individual elements:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{typeof}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{)}
\CommentTok{# [1] "integer"}
\KeywordTok{length}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{)}
\CommentTok{# [1] 10}
\KeywordTok{typeof}\NormalTok{(}\FloatTok{1.0}\NormalTok{)}
\CommentTok{# [1] "double"}
\KeywordTok{length}\NormalTok{(}\FloatTok{1.0}\NormalTok{)}
\CommentTok{# [1] 1}
\KeywordTok{typeof}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"foo"}\NormalTok{, }\StringTok{"bar"}\NormalTok{))}
\CommentTok{# [1] "character"}
\KeywordTok{length}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"foo"}\NormalTok{, }\StringTok{"bar"}\NormalTok{))}
\CommentTok{# [1] 2}
\KeywordTok{typeof}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\OtherTok{FALSE}\NormalTok{))}
\CommentTok{# [1] "logical"}
\end{Highlighting}
\end{Shaded}

Vectors of this kind can only have a single type.

Note that vectors can have zero length, e.g.~in,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{i =}\StringTok{ }\KeywordTok{integer}\NormalTok{(}\DecValTok{0}\NormalTok{)}
\KeywordTok{typeof}\NormalTok{(i)}
\CommentTok{# [1] "integer"}
\NormalTok{i}
\CommentTok{# integer(0)}
\KeywordTok{length}\NormalTok{(i)}
\CommentTok{# [1] 0}
\end{Highlighting}
\end{Shaded}

We can retrieve (or in assignments: replace) elements in a vector
using \texttt{{[}} or \texttt{{[}{[}}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{)}
\NormalTok{a[}\DecValTok{2}\NormalTok{]}
\CommentTok{# [1] 2}
\NormalTok{a[[}\DecValTok{2}\NormalTok{]]}
\CommentTok{# [1] 2}
\NormalTok{a[}\DecValTok{2}\OperatorTok{:}\DecValTok{3}\NormalTok{]}
\CommentTok{# [1] 2 3}
\NormalTok{a[}\DecValTok{2}\OperatorTok{:}\DecValTok{3}\NormalTok{] =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{,}\DecValTok{6}\NormalTok{)}
\NormalTok{a}
\CommentTok{# [1] 1 5 6}
\NormalTok{a[[}\DecValTok{3}\NormalTok{]] =}\StringTok{ }\DecValTok{10}
\NormalTok{a}
\CommentTok{# [1]  1  5 10}
\end{Highlighting}
\end{Shaded}

where the difference is that \texttt{{[}} can operate on an index \emph{range}
(or multiple indexes), and \texttt{{[}{[}} operates on a single vector value.

\hypertarget{heterogeneous-vectors-list}{%
\subsection*{Heterogeneous vectors: list}\label{heterogeneous-vectors-list}}
\addcontentsline{toc}{subsection}{Heterogeneous vectors: list}

An additional vector type is the \texttt{list}, which can combine any types in
its elements:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{l <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DecValTok{3}\NormalTok{, }\OtherTok{TRUE}\NormalTok{, }\StringTok{"foo"}\NormalTok{)}
\KeywordTok{typeof}\NormalTok{(l)}
\CommentTok{# [1] "list"}
\KeywordTok{length}\NormalTok{(l)}
\CommentTok{# [1] 3}
\end{Highlighting}
\end{Shaded}

For lists, there is a further distinction between \texttt{{[}} and \texttt{{[}{[}}: the single
\texttt{{[}} returns always a list, and \texttt{{[}{[}} returns the \emph{contents} of a list element:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{l[}\DecValTok{1}\NormalTok{]}
\CommentTok{# [[1]]}
\CommentTok{# [1] 3}
\NormalTok{l[[}\DecValTok{1}\NormalTok{]]}
\CommentTok{# [1] 3}
\end{Highlighting}
\end{Shaded}

For replacement, one case use \texttt{{[}} when providing a list, and \texttt{{[}{[}} when providing
a new value:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{l[}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{] =}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DecValTok{4}\NormalTok{, }\OtherTok{FALSE}\NormalTok{)}
\NormalTok{l}
\CommentTok{# [[1]]}
\CommentTok{# [1] 4}
\CommentTok{# }
\CommentTok{# [[2]]}
\CommentTok{# [1] FALSE}
\CommentTok{# }
\CommentTok{# [[3]]}
\CommentTok{# [1] "foo"}
\NormalTok{l[[}\DecValTok{3}\NormalTok{]] =}\StringTok{ "bar"}
\NormalTok{l}
\CommentTok{# [[1]]}
\CommentTok{# [1] 4}
\CommentTok{# }
\CommentTok{# [[2]]}
\CommentTok{# [1] FALSE}
\CommentTok{# }
\CommentTok{# [[3]]}
\CommentTok{# [1] "bar"}
\end{Highlighting}
\end{Shaded}

In case list elements are \emph{named}, as in

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{l =}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{first =} \DecValTok{3}\NormalTok{, }\DataTypeTok{second =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{third =} \StringTok{"foo"}\NormalTok{)}
\NormalTok{l}
\CommentTok{# $first}
\CommentTok{# [1] 3}
\CommentTok{# }
\CommentTok{# $second}
\CommentTok{# [1] TRUE}
\CommentTok{# }
\CommentTok{# $third}
\CommentTok{# [1] "foo"}
\end{Highlighting}
\end{Shaded}

we can use names as in \texttt{l{[}{[}"second"{]}{]}} and this can be
abbreviated to

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{l}\OperatorTok{$}\NormalTok{second}
\CommentTok{# [1] TRUE}
\NormalTok{l}\OperatorTok{$}\NormalTok{second =}\StringTok{ }\OtherTok{FALSE}
\NormalTok{l}
\CommentTok{# $first}
\CommentTok{# [1] 3}
\CommentTok{# }
\CommentTok{# $second}
\CommentTok{# [1] FALSE}
\CommentTok{# }
\CommentTok{# $third}
\CommentTok{# [1] "foo"}
\end{Highlighting}
\end{Shaded}

This is convenient, but also requires name look-up in the names
attribute (see below).

\hypertarget{null-and-removing-list-elements}{%
\subsubsection*{NULL and removing list elements}\label{null-and-removing-list-elements}}
\addcontentsline{toc}{subsubsection}{NULL and removing list elements}

\texttt{NULL} is the null value in R; it is special in the sense that it doesn't work
in simple comparisons:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{3} \OperatorTok{==}\StringTok{ }\OtherTok{NULL} \CommentTok{# not FALSE!}
\CommentTok{# logical(0)}
\OtherTok{NULL} \OperatorTok{==}\StringTok{ }\OtherTok{NULL} \CommentTok{# not even TRUE!}
\CommentTok{# logical(0)}
\end{Highlighting}
\end{Shaded}

but has to be treated specially, using \texttt{is.null}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{is.null}\NormalTok{(}\OtherTok{NULL}\NormalTok{)}
\CommentTok{# [1] TRUE}
\end{Highlighting}
\end{Shaded}

When we want to remove one or more list elements, we can do so by creating
a new list that does not contain the elements that needed removal, as in

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{l =}\StringTok{ }\NormalTok{l[}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{)] }\CommentTok{# remove second, implicitly}
\NormalTok{l}
\CommentTok{# $first}
\CommentTok{# [1] 3}
\CommentTok{# }
\CommentTok{# $third}
\CommentTok{# [1] "foo"}
\end{Highlighting}
\end{Shaded}

but we can also assign \texttt{NULL} to the element we want to eliminate:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{l}\OperatorTok{$}\NormalTok{second =}\StringTok{ }\OtherTok{NULL}
\NormalTok{l}
\CommentTok{# $first}
\CommentTok{# [1] 3}
\CommentTok{# }
\CommentTok{# $third}
\CommentTok{# [1] "foo"}
\end{Highlighting}
\end{Shaded}

\hypertarget{attributes}{%
\subsection*{Attributes}\label{attributes}}
\addcontentsline{toc}{subsection}{Attributes}

We can glue arbitrary metadata objects to data objects, as in

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a =}\StringTok{ }\DecValTok{1}\OperatorTok{:}\DecValTok{3}
\KeywordTok{attr}\NormalTok{(a, }\StringTok{"some_meta_data"}\NormalTok{) =}\StringTok{ "foo"}
\NormalTok{a}
\CommentTok{# [1] 1 2 3}
\CommentTok{# attr(,"some_meta_data")}
\CommentTok{# [1] "foo"}
\end{Highlighting}
\end{Shaded}

and this can be retrieved, or replaced by

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{attr}\NormalTok{(a, }\StringTok{"some_meta_data"}\NormalTok{)}
\CommentTok{# [1] "foo"}
\KeywordTok{attr}\NormalTok{(a, }\StringTok{"some_meta_data"}\NormalTok{) =}\StringTok{ "bar"}
\KeywordTok{attr}\NormalTok{(a, }\StringTok{"some_meta_data"}\NormalTok{)}
\CommentTok{# [1] "bar"}
\end{Highlighting}
\end{Shaded}

In essence, the attribute of an object is a named list, and we can
get or set the complete list by

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{attributes}\NormalTok{(a)}
\CommentTok{# $some_meta_data}
\CommentTok{# [1] "bar"}
\KeywordTok{attributes}\NormalTok{(a) =}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{some_meta_data =} \StringTok{"foo"}\NormalTok{)}
\KeywordTok{attributes}\NormalTok{(a)}
\CommentTok{# $some_meta_data}
\CommentTok{# [1] "foo"}
\end{Highlighting}
\end{Shaded}

A number of attributes are treated specially by R, see e.g.~\texttt{?attributes}.

\hypertarget{object-class-and-class-attribute}{%
\subsubsection*{object class and class attribute}\label{object-class-and-class-attribute}}
\addcontentsline{toc}{subsubsection}{object class and class attribute}

Every object in R ``has a class'', meaning that \texttt{class(obj)} returns
a character vector with the class of \texttt{obj}. Some objects have
an \emph{implicit} class, e.g.~vectors

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{)}
\CommentTok{# [1] "integer"}
\KeywordTok{class}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\OtherTok{FALSE}\NormalTok{))}
\CommentTok{# [1] "logical"}
\KeywordTok{class}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"TRUE"}\NormalTok{, }\StringTok{"FALSE"}\NormalTok{))}
\CommentTok{# [1] "character"}
\end{Highlighting}
\end{Shaded}

but we can also set the class explicit, either by using \texttt{attr} or by
using \texttt{class} in the left-hand side of an expression:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a =}\StringTok{ }\DecValTok{1}\OperatorTok{:}\DecValTok{3}
\KeywordTok{class}\NormalTok{(a) =}\StringTok{ "foo"}
\NormalTok{a}
\CommentTok{# [1] 1 2 3}
\CommentTok{# attr(,"class")}
\CommentTok{# [1] "foo"}
\KeywordTok{class}\NormalTok{(a)}
\CommentTok{# [1] "foo"}
\KeywordTok{attributes}\NormalTok{(a)}
\CommentTok{# $class}
\CommentTok{# [1] "foo"}
\end{Highlighting}
\end{Shaded}

in which case the newly set class overrides the earlier implicit class. This way,
we can add methods for class \texttt{foo}, e.g.~by

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{print.foo =}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x, ...) }\KeywordTok{print}\NormalTok{(}\KeywordTok{paste}\NormalTok{(}\StringTok{"an object of class foo with length"}\NormalTok{, }\KeywordTok{length}\NormalTok{(x)))}
\KeywordTok{print}\NormalTok{(a)}
\CommentTok{# [1] "an object of class foo with length 3"}
\end{Highlighting}
\end{Shaded}

Providing such methods are generally intended to create more usable
software, but at the same time they may make the objects more opaque. It is
sometimes useful to see what an object ``is made of'' by printing it after the
class attribute is removed, as in

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{unclass}\NormalTok{(a)}
\CommentTok{# [1] 1 2 3}
\end{Highlighting}
\end{Shaded}

As a more elaborate example, consider the case where a polygon is made using
package sf:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(sf)}
\NormalTok{p =}\StringTok{ }\KeywordTok{st_polygon}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\KeywordTok{rbind}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{))))}
\NormalTok{p}
\CommentTok{# POLYGON ((0 0, 1 0, 1 1, 0 0))}
\end{Highlighting}
\end{Shaded}

which prints the well-known-text form; to understand what the data structure is
like, we can use

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{unclass}\NormalTok{(p)}
\CommentTok{# [[1]]}
\CommentTok{#      [,1] [,2]}
\CommentTok{# [1,]    0    0}
\CommentTok{# [2,]    1    0}
\CommentTok{# [3,]    1    1}
\CommentTok{# [4,]    0    0}
\end{Highlighting}
\end{Shaded}

\hypertarget{the-dim-attribute}{%
\subsubsection*{the dim attribute}\label{the-dim-attribute}}
\addcontentsline{toc}{subsubsection}{the dim attribute}

The \texttt{dim} attribute sets the matrix or array dimensions:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a =}\StringTok{ }\DecValTok{1}\OperatorTok{:}\DecValTok{8}
\KeywordTok{class}\NormalTok{(a)}
\CommentTok{# [1] "integer"}
\KeywordTok{attr}\NormalTok{(a, }\StringTok{"dim"}\NormalTok{) =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{) }\CommentTok{# or: dim(a) = c(2,4)}
\KeywordTok{class}\NormalTok{(a)}
\CommentTok{# [1] "matrix" "array"}
\NormalTok{a}
\CommentTok{#      [,1] [,2] [,3] [,4]}
\CommentTok{# [1,]    1    3    5    7}
\CommentTok{# [2,]    2    4    6    8}
\KeywordTok{attr}\NormalTok{(a, }\StringTok{"dim"}\NormalTok{) =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{) }\CommentTok{# or: dim(a) = c(2,2,2)}
\KeywordTok{class}\NormalTok{(a)}
\CommentTok{# [1] "array"}
\NormalTok{a}
\CommentTok{# , , 1}
\CommentTok{# }
\CommentTok{#      [,1] [,2]}
\CommentTok{# [1,]    1    3}
\CommentTok{# [2,]    2    4}
\CommentTok{# }
\CommentTok{# , , 2}
\CommentTok{# }
\CommentTok{#      [,1] [,2]}
\CommentTok{# [1,]    5    7}
\CommentTok{# [2,]    6    8}
\end{Highlighting}
\end{Shaded}

\hypertarget{various-names-attributes}{%
\subsection*{various names attributes}\label{various-names-attributes}}
\addcontentsline{toc}{subsection}{various names attributes}

Named vectors carry their names in a \texttt{names} attribute. We saw examples
for lists above, an example for a numeric vector is:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DataTypeTok{first =} \DecValTok{3}\NormalTok{, }\DataTypeTok{second =} \DecValTok{4}\NormalTok{, }\DataTypeTok{last =} \DecValTok{5}\NormalTok{)}
\NormalTok{a[}\StringTok{"second"}\NormalTok{]}
\CommentTok{# second }
\CommentTok{#      4}
\KeywordTok{attributes}\NormalTok{(a)}
\CommentTok{# $names}
\CommentTok{# [1] "first"  "second" "last"}
\end{Highlighting}
\end{Shaded}

More name attributes are e.g.~\texttt{dimnames} of matrices or arrays,
which not only names dimensions, but also the labels associated
with each of the dimensions:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a =}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\KeywordTok{dimnames}\NormalTok{(a) =}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{rows =} \KeywordTok{c}\NormalTok{(}\StringTok{"row1"}\NormalTok{, }\StringTok{"row2"}\NormalTok{), }\DataTypeTok{cols =} \KeywordTok{c}\NormalTok{(}\StringTok{"col1"}\NormalTok{, }\StringTok{"col2"}\NormalTok{))}
\NormalTok{a}
\CommentTok{#       cols}
\CommentTok{# rows   col1 col2}
\CommentTok{#   row1    1    3}
\CommentTok{#   row2    2    4}
\KeywordTok{attributes}\NormalTok{(a)}
\CommentTok{# $dim}
\CommentTok{# [1] 2 2}
\CommentTok{# }
\CommentTok{# $dimnames}
\CommentTok{# $dimnames$rows}
\CommentTok{# [1] "row1" "row2"}
\CommentTok{# }
\CommentTok{# $dimnames$cols}
\CommentTok{# [1] "col1" "col2"}
\end{Highlighting}
\end{Shaded}

Data.frame objects have rows and columns, and each have names:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{a =} \DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{, }\DataTypeTok{b =} \KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\OtherTok{FALSE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{))}
\KeywordTok{attributes}\NormalTok{(df)}
\CommentTok{# $names}
\CommentTok{# [1] "a" "b"}
\CommentTok{# }
\CommentTok{# $class}
\CommentTok{# [1] "data.frame"}
\CommentTok{# }
\CommentTok{# $row.names}
\CommentTok{# [1] 1 2 3}
\end{Highlighting}
\end{Shaded}

\hypertarget{using-structure}{%
\subsection*{\texorpdfstring{using \texttt{structure}}{using structure}}\label{using-structure}}
\addcontentsline{toc}{subsection}{using \texttt{structure}}

When programming, the pattern of adding or modifying attributes before returning
an object is extremely common, an example being:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f =}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) \{}
\NormalTok{   a =}\StringTok{ }\KeywordTok{create_obj}\NormalTok{(x) }\CommentTok{# call some other function}
   \KeywordTok{attributes}\NormalTok{(a) =}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{class =} \StringTok{"foo"}\NormalTok{, }\DataTypeTok{meta =} \DecValTok{33}\NormalTok{)}
\NormalTok{   a}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The last two statements can be contracted in

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f =}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) \{}
\NormalTok{   a =}\StringTok{ }\KeywordTok{create_obj}\NormalTok{(x) }\CommentTok{# call some other function}
   \KeywordTok{structure}\NormalTok{(a, }\DataTypeTok{class =} \StringTok{"foo"}\NormalTok{, }\DataTypeTok{meta =} \DecValTok{33}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

where function \texttt{structure} adds, replaces, or (in case of value \texttt{NULL}) removes
attributes from the object in its first argument.

\hypertarget{dissecting-a-multipolygon}{%
\section*{\texorpdfstring{dissecting a \texttt{MULTIPOLYGON}}{dissecting a MULTIPOLYGON}}\label{dissecting-a-multipolygon}}
\addcontentsline{toc}{section}{dissecting a \texttt{MULTIPOLYGON}}

We can use the above examples to dissect an \texttt{sf} object with
\texttt{MULTIPOLYGON}s into pieces. Suppose we use the \texttt{nc} dataset,

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(sf)}
\KeywordTok{system.file}\NormalTok{(}\StringTok{"gpkg/nc.gpkg"}\NormalTok{, }\DataTypeTok{package =} \StringTok{"sf"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{read_sf}\NormalTok{() ->}\StringTok{ }\NormalTok{nc}
\end{Highlighting}
\end{Shaded}

we can see from the attributes of \texttt{nc},

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{attributes}\NormalTok{(nc)}
\CommentTok{# $names}
\CommentTok{#  [1] "AREA"      "PERIMETER" "CNTY_"     "CNTY_ID"   "NAME"     }
\CommentTok{#  [6] "FIPS"      "FIPSNO"    "CRESS_ID"  "BIR74"     "SID74"    }
\CommentTok{# [11] "NWBIR74"   "BIR79"     "SID79"     "NWBIR79"   "geom"     }
\CommentTok{# }
\CommentTok{# $row.names}
\CommentTok{#   [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15}
\CommentTok{#  [16]  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30}
\CommentTok{#  [31]  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45}
\CommentTok{#  [46]  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60}
\CommentTok{#  [61]  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75}
\CommentTok{#  [76]  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90}
\CommentTok{#  [91]  91  92  93  94  95  96  97  98  99 100}
\CommentTok{# }
\CommentTok{# $class}
\CommentTok{# [1] "sf"         "tbl_df"     "tbl"        "data.frame"}
\CommentTok{# }
\CommentTok{# $sf_column}
\CommentTok{# [1] "geom"}
\CommentTok{# }
\CommentTok{# $agr}
\CommentTok{#      AREA PERIMETER     CNTY_   CNTY_ID      NAME      FIPS }
\CommentTok{#      <NA>      <NA>      <NA>      <NA>      <NA>      <NA> }
\CommentTok{#    FIPSNO  CRESS_ID     BIR74     SID74   NWBIR74     BIR79 }
\CommentTok{#      <NA>      <NA>      <NA>      <NA>      <NA>      <NA> }
\CommentTok{#     SID79   NWBIR79 }
\CommentTok{#      <NA>      <NA> }
\CommentTok{# Levels: constant aggregate identity}
\end{Highlighting}
\end{Shaded}

that the geometry column is named \texttt{geom}. When we take out this column,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nc}\OperatorTok{$}\NormalTok{geom}
\CommentTok{# Geometry set for 100 features }
\CommentTok{# Geometry type: MULTIPOLYGON}
\CommentTok{# Dimension:     XY}
\CommentTok{# Bounding box:  xmin: -84.3 ymin: 33.9 xmax: -75.5 ymax: 36.6}
\CommentTok{# Geodetic CRS:  NAD27}
\CommentTok{# First 5 geometries:}
\CommentTok{# MULTIPOLYGON (((-81.5 36.2, -81.5 36.3, -81.6 3...}
\CommentTok{# MULTIPOLYGON (((-81.2 36.4, -81.2 36.4, -81.3 3...}
\CommentTok{# MULTIPOLYGON (((-80.5 36.2, -80.5 36.3, -80.5 3...}
\CommentTok{# MULTIPOLYGON (((-76 36.3, -76 36.3, -76 36.3, -...}
\CommentTok{# MULTIPOLYGON (((-77.2 36.2, -77.2 36.2, -77.3 3...}
\end{Highlighting}
\end{Shaded}

we see an object that has the following attributes

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{attributes}\NormalTok{(nc}\OperatorTok{$}\NormalTok{geom)}
\CommentTok{# $n_empty}
\CommentTok{# [1] 0}
\CommentTok{# }
\CommentTok{# $crs}
\CommentTok{# Coordinate Reference System:}
\CommentTok{#   User input: NAD27 }
\CommentTok{#   wkt:}
\CommentTok{# GEOGCRS["NAD27",}
\CommentTok{#     DATUM["North American Datum 1927",}
\CommentTok{#         ELLIPSOID["Clarke 1866",6378206.4,294.978698213898,}
\CommentTok{#             LENGTHUNIT["metre",1]]],}
\CommentTok{#     PRIMEM["Greenwich",0,}
\CommentTok{#         ANGLEUNIT["degree",0.0174532925199433]],}
\CommentTok{#     CS[ellipsoidal,2],}
\CommentTok{#         AXIS["geodetic latitude (Lat)",north,}
\CommentTok{#             ORDER[1],}
\CommentTok{#             ANGLEUNIT["degree",0.0174532925199433]],}
\CommentTok{#         AXIS["geodetic longitude (Lon)",east,}
\CommentTok{#             ORDER[2],}
\CommentTok{#             ANGLEUNIT["degree",0.0174532925199433]],}
\CommentTok{#     USAGE[}
\CommentTok{#         SCOPE["Geodesy."],}
\CommentTok{#         AREA["North and central America: Antigua and Barbuda - onshore. Bahamas - onshore plus offshore over internal continental shelf only. Belize - onshore. British Virgin Islands - onshore. Canada onshore - Alberta, British Columbia, Manitoba, New Brunswick, Newfoundland and Labrador, Northwest Territories, Nova Scotia, Nunavut, Ontario, Prince Edward Island, Quebec, Saskatchewan and Yukon - plus offshore east coast. Cuba - onshore and offshore. El Salvador - onshore. Guatemala - onshore. Honduras - onshore. Panama - onshore. Puerto Rico - onshore. Mexico - onshore plus offshore east coast. Nicaragua - onshore. United States (USA) onshore and offshore - Alabama, Alaska, Arizona, Arkansas, California, Colorado, Connecticut, Delaware, Florida, Georgia, Idaho, Illinois, Indiana, Iowa, Kansas, Kentucky, Louisiana, Maine, Maryland, Massachusetts, Michigan, Minnesota, Mississippi, Missouri, Montana, Nebraska, Nevada, New Hampshire, New Jersey, New Mexico, New York, North Carolina, North Dakota, Ohio, Oklahoma, Oregon, Pennsylvania, Rhode Island, South Carolina, South Dakota, Tennessee, Texas, Utah, Vermont, Virginia, Washington, West Virginia, Wisconsin and Wyoming - plus offshore . US Virgin Islands - onshore."],}
\CommentTok{#         BBOX[7.15,167.65,83.17,-47.74]],}
\CommentTok{#     ID["EPSG",4267]]}
\CommentTok{# }
\CommentTok{# $class}
\CommentTok{# [1] "sfc_MULTIPOLYGON" "sfc"             }
\CommentTok{# }
\CommentTok{# $precision}
\CommentTok{# [1] 0}
\CommentTok{# }
\CommentTok{# $bbox}
\CommentTok{#  xmin  ymin  xmax  ymax }
\CommentTok{# -84.3  33.9 -75.5  36.6}
\end{Highlighting}
\end{Shaded}

When we take the \emph{contents} of the fourth list element, we obtain

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nc}\OperatorTok{$}\NormalTok{geom[[}\DecValTok{4}\NormalTok{]]}
\CommentTok{# MULTIPOLYGON (((-76 36.3, -76 36.3, -76 36.3, -76 36.4, -76.1 36.3, -76.2 36.4, -76.2 36.4, -76.2 36.4, -76.3 36.6, -76.1 36.6, -76 36.6, -76 36.5, -76.1 36.5, -76 36.4, -76 36.4, -76 36.4, -76 36.4, -75.9 36.4, -75.9 36.4, -75.8 36.1, -75.8 36.1, -75.9 36.1, -75.9 36.2, -76 36.3, -75.9 36.3, -76 36.3)), ((-76 36.6, -76 36.6, -75.9 36.5, -75.9 36.5, -76 36.5, -76 36.5, -76 36.6)), ((-75.9 36.6, -75.9 36.6, -75.8 36.2, -75.8 36.2, -75.9 36.6)))}
\end{Highlighting}
\end{Shaded}

which is a list,

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{typeof}\NormalTok{(nc}\OperatorTok{$}\NormalTok{geom[[}\DecValTok{4}\NormalTok{]])}
\CommentTok{# [1] "list"}
\end{Highlighting}
\end{Shaded}

with attributes

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{attributes}\NormalTok{(nc}\OperatorTok{$}\NormalTok{geom[[}\DecValTok{4}\NormalTok{]])}
\CommentTok{# $class}
\CommentTok{# [1] "XY"           "MULTIPOLYGON" "sfg"}
\end{Highlighting}
\end{Shaded}

and length

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{length}\NormalTok{(nc}\OperatorTok{$}\NormalTok{geom[[}\DecValTok{4}\NormalTok{]])}
\CommentTok{# [1] 3}
\end{Highlighting}
\end{Shaded}

The length indicates the number of outer rings: a multi-polygon
can consist of more than one polygon. We see that most counties
only have a single polygon:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{lengths}\NormalTok{(nc}\OperatorTok{$}\NormalTok{geom)}
\CommentTok{#   [1] 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1}
\CommentTok{#  [31] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 2 1 1 1}
\CommentTok{#  [61] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1}
\CommentTok{#  [91] 2 1 1 1 2 1 1 1 1 1}
\end{Highlighting}
\end{Shaded}

A multi-polygon is a list with polygons,

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{typeof}\NormalTok{(nc}\OperatorTok{$}\NormalTok{geom[[}\DecValTok{4}\NormalTok{]])}
\CommentTok{# [1] "list"}
\end{Highlighting}
\end{Shaded}

and the \emph{first} polygon of the fourth multi-polygon is again a list,
because polygons have an outer ring \emph{possibly} followed by multiple inner
rings (holes)

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{typeof}\NormalTok{(nc}\OperatorTok{$}\NormalTok{geom[[}\DecValTok{4}\NormalTok{]][[}\DecValTok{1}\NormalTok{]])}
\CommentTok{# [1] "list"}
\end{Highlighting}
\end{Shaded}

we see that it contains only one ring, the exterior ring:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{length}\NormalTok{(nc}\OperatorTok{$}\NormalTok{geom[[}\DecValTok{4}\NormalTok{]][[}\DecValTok{1}\NormalTok{]])}
\CommentTok{# [1] 1}
\end{Highlighting}
\end{Shaded}

and we can print type, the dimension and the first set of coordinates by

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{typeof}\NormalTok{(nc}\OperatorTok{$}\NormalTok{geom[[}\DecValTok{4}\NormalTok{]][[}\DecValTok{1}\NormalTok{]][[}\DecValTok{1}\NormalTok{]])}
\CommentTok{# [1] "double"}
\KeywordTok{dim}\NormalTok{(nc}\OperatorTok{$}\NormalTok{geom[[}\DecValTok{4}\NormalTok{]][[}\DecValTok{1}\NormalTok{]][[}\DecValTok{1}\NormalTok{]])}
\CommentTok{# [1] 26  2}
\KeywordTok{head}\NormalTok{(nc}\OperatorTok{$}\NormalTok{geom[[}\DecValTok{4}\NormalTok{]][[}\DecValTok{1}\NormalTok{]][[}\DecValTok{1}\NormalTok{]])}
\CommentTok{#       [,1] [,2]}
\CommentTok{# [1,] -76.0 36.3}
\CommentTok{# [2,] -76.0 36.3}
\CommentTok{# [3,] -76.0 36.3}
\CommentTok{# [4,] -76.0 36.4}
\CommentTok{# [5,] -76.1 36.3}
\CommentTok{# [6,] -76.2 36.4}
\end{Highlighting}
\end{Shaded}

and we can now for instance change the latitude of the third coordinate by

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nc}\OperatorTok{$}\NormalTok{geom[[}\DecValTok{4}\NormalTok{]][[}\DecValTok{1}\NormalTok{]][[}\DecValTok{1}\NormalTok{]][}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{] =}\StringTok{ }\FloatTok{36.5}
\end{Highlighting}
\end{Shaded}

\bibliography{book.bib,packages.bib}

\end{document}
