[["index.html", "Spatial Data Science with applications in R Preface", " Spatial Data Science with applications in R Edzer Pebesma, Roger Bivand 2021-04-15 Preface Data science is concerned with finding answers to questions on the basis of available data, and communicating that effort. Besides showing the results, this communication involves sharing the data used, but also exposing the path that led to the answers in a comprehensive and reproducible way. It also acknowledges the fact that available data may not be sufficient to answer questions, and that any answers are conditional on the data collection or sampling protocols employed. This book introduces and explains the concepts underlying spatial data: points, lines, polygons, rasters, coverages, geometry attributes, data cubes, reference systems, as well as higher-level concepts including how attributes relate to geometries and how this affects analysis. The relationship of attributes to geometries is known as support, and changing support also changes the characteristics of attributes. Some data generation processes are continuous in space, and may be observed everywhere. Others are discrete, observed in tesselated containers. In modern spatial data analysis, tesellated methods are often used for all data, extending across the legacy partition into point process, geostatistical and lattice models. It is support (and the understanding of support) that underlies the importance of spatial representation. The book aims at data scientists who want to get a grip on using spatial data in their analysis. To exemplify how to do things, it uses R. It is often thought that spatial data boils down to having observations’ longitude and latitude in a dataset, and treating these just like any other variable. This carries the risk of missed opportunities and meaningless analyses. For instance, coordinate pairs really are pairs, and lose much of their meaning when treated independently rather than having point locations, observations are often associated with spatial lines, areas, or grid cells spatial distances between observations are often not well represented by straight-line distances, but by great circle distances, distances through networks, or by measuring the effort it takes getting from A to B We introduce the concepts behind spatial data, coordinate reference systems, spatial analysis, and introduce a number of packages, including sf (Pebesma 2018, @R-sf), stars (E. Pebesma 2021b), s2 (Dunnington, Pebesma, and Rubak 2021) and lwgeom (Pebesma 2020), as well as a number of tidyverse (Wickham 2019) extensions, and a number of spatial analysis and visualisation packages that can be used with these packages, including gstat (E. Pebesma and Graeler 2021), spdep (Bivand 2021), spatstat (Baddeley, Turner, and Rubak 2021), tmap (Tennekes 2021) and mapview (Appelhans et al. 2020). This work is licensed under the Attribution-NonCommercial-NoDerivatives 4.0 International License. References "],["intro.html", "Chapter 1 Getting Started 1.1 A first map 1.2 Coordinate reference systems 1.3 Raster and vector data 1.4 Raster types 1.5 Time series, arrays, data cubes 1.6 Support 1.7 Spatial data science software 1.8 Exercises", " Chapter 1 Getting Started This chapter introduces a number of concepts associated with handling spatial data, and points forward to later sections where they are discussed in more detail. The code sections in this chapter are not explained, but can be unfolded by clicking on the “code” button. They should be easy to follow when understanding R at the level of, say, R for Data Science (Wickham and Grolemund 2017). Detailed explanation of R code starts in Part II of this book. 1.1 A first map The typical way to graph spatial data is by creating a map. Let us consider a simple map, shown in figure 1.1. library(tidyverse) library(sf) system.file(&quot;gpkg/nc.gpkg&quot;, package=&quot;sf&quot;) %&gt;% read_sf() -&gt; nc nc.32119 &lt;- st_transform(nc, 32119) nc.32119 %&gt;% select(BIR74) %&gt;% plot(graticule = TRUE, axes = TRUE) Figure 1.1: a first map A number of graphical elements are present here, in this case: polygons are drawn with a black outline and filled with colors chosen according to a variable BIR74, whose name is in the title a legend key explains the meaning of the colors, and has a certain color palette and color breaks, values at which color changes the background of the map shows curved lines with constant latitude or longitude (graticule) the axis ticks show the latitude and longitude values Polygons are a particular form of geometry; spatial geometries (points, lines, polygons, pixels) are discussed in detail in chapter 3. Polygons consist of sequences of points, connected by straight lines. How point locations of spatial data are expressed, or measured, is discussed in chapter 2. As can be seen from figure 1.1, lines of equal latitude and longitude do not form straight lines, indicating that some form of projection took place before plotting; projections are also discussed in chapter 2 and section 9.1. The color values in figure 1.1 are derived from numeric values of a variable, BIR74, which has a single value associated with each geometry or feature. Chapter 5 discusses such feature attributes, and the way they can relate to feature geometries. In this case, BIR74 refers to birth counts, meaning counts over the region. This implies that the count does not refer to a value associated with every point inside the polygon, which the continuous color might suggest, but rather measures an integral (sum) over the polygon. Before plotting figure 1.1 we had to read the data, in this case from a file (section 7.1). Printing a data summary for the first three records of three attribute variables shows: nc %&gt;% select(AREA, BIR74, SID74) %&gt;% print(n = 3) # Simple feature collection with 100 features and 3 fields # Geometry type: MULTIPOLYGON # Dimension: XY # Bounding box: xmin: -84.3 ymin: 33.9 xmax: -75.5 ymax: 36.6 # Geodetic CRS: NAD27 # # A tibble: 100 x 4 # AREA BIR74 SID74 geom # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;MULTIPOLYGON [°]&gt; # 1 0.114 1091 1 (((-81.5 36.2, -81.5 36.3, -81.6 36.3, -81.6 36.3, -81.7 36… # 2 0.061 487 0 (((-81.2 36.4, -81.2 36.4, -81.3 36.4, -81.3 36.4, -81.3 36… # 3 0.143 3188 5 (((-80.5 36.2, -80.5 36.3, -80.5 36.3, -80.5 36.3, -80.6 36… # # … with 97 more rows The printed output shows the (selected) dataset has 100 features (records) and 3 fields (attributes) the geometry type is MULTIPOLYGON (chapter 3) it has dimension XY, indicating that each point will consist of 2 coordinate values the range of x and y values of the geometry the coordinate reference system (CRS) is geodetic, with coordinates in degrees longitude and latitude associated to the NAD27 datum (chapter 2) the three selected attribute variables are followed by a variable geom of type MULTIPOLYGON with unit degrees that contains the polygon information More complicated plots can involve facet plots with a map in each facet, as shown in figure 1.2. year_labels = c(&quot;SID74&quot; = &quot;1974 - 1978&quot;, &quot;SID79&quot; = &quot;1979 - 1984&quot;) nc.32119 %&gt;% select(SID74, SID79) %&gt;% gather(VAR, SID, -geom) -&gt; nc2 ggplot() + geom_sf(data = nc2, aes(fill = SID)) + facet_wrap(~VAR, ncol = 1, labeller = labeller(VAR = year_labels)) + scale_y_continuous(breaks = 34:36) + scale_fill_gradientn(colors = sf.colors(20)) + theme(panel.grid.major = element_line(color = &quot;white&quot;)) Figure 1.2: ggplot with facet maps An interactive, leaflet-based map is obtained in figure ??. suppressPackageStartupMessages(library(mapview)) nc.32119 %&gt;% mapview(zcol = &quot;BIR74&quot;, legend = TRUE, col.regions = sf.colors) 1.2 Coordinate reference systems In figure 1.1, the grey lines denote the graticule, a grid with lines along constant latitude or longitude. Clearly, these lines are not straight, which indicates that a projection of the data was used for which the x and y axis do not align with longitude and latitude. In figure ?? we see that the north boundary of North Carolina is plotted as a straight line again, indicating that another projection was used. The ellipsoidal coordinates of the graticule of figure 1.1 are associated with a particular datum (here: NAD27), which implicates a set of rules what the shape of the Earth is and how it is attached to the Earth (to which point of the Earth is the origin associated, and how is it directed). If one would measure coordinates with a GPS device (e.g. a mobile phone) it would typically report coordinates associated with the WGS84 datum, which can be around 30 m different from the identical coordinate values when associated with NAD27. Projections describe how we go back and forth between ellipsoidal coordinates which are expressed as degrees latitude and longitude, pointing to locations on a shape approximating the Earth’s shape (an ellipsoid or spheroid), and projected coordinates which are coordinates on a flat, two-dimensional coordinate system, used when plotting maps. Datums transformations are associated with moving from one datum to another. Both topics are covered by spatial reference systems are described in more detail in chapter 2. 1.3 Raster and vector data Polygon, point and line geometries are examples of vector data: point coordinates describe the “exact” locations that can be anywhere. Raster data on the other hand describe data where values are aligned on a raster, meaning on a regularly laid out lattice of usually square pixels. An example is shown in figure 1.3. library(stars) par(mfrow = c(2, 2)) par(mar = rep(1, 4)) tif &lt;- system.file(&quot;tif/L7_ETMs.tif&quot;, package = &quot;stars&quot;) x &lt;- read_stars(tif)[,,,1] image(x, main = &quot;(a)&quot;) image(x[,1:10,1:10], text_values = TRUE, border = &#39;grey&#39;, main = &quot;(b)&quot;) image(x, main = &quot;(c)&quot;) set.seed(131) pts = st_sample(st_as_sfc(st_bbox(x)), 3) plot(pts, add = TRUE, pch = 3, col = &#39;blue&#39;) image(x, main = &quot;(d)&quot;) plot(st_buffer(pts, 500), add = TRUE, pch = 3, border = &#39;blue&#39;, col = NA, lwd = 2) Figure 1.3: raster maps: Landsat-7 blue band, with color values derived from data values (a), the top-left 10x10 sub-image from (a) with numeric values shown (b), and overlayed by two different types of vector data: three sample points (c), and a 500m radius around the points represented as polygons (d) Vector and raster data can be combined in different ways; for instance we can query the raster at the three points of figure 1.3(c), st_extract(x, pts) # Simple feature collection with 3 features and 1 field # Geometry type: POINT # Dimension: XY # Bounding box: xmin: 290000 ymin: 9110000 xmax: 292000 ymax: 9120000 # Projected CRS: UTM Zone 25, Southern Hemisphere # L7_ETMs.tif geometry # 1 80 POINT (290830 9114499) # 2 58 POINT (290019 9119219) # 3 63 POINT (291693 9116038) or compute an aggregate, such as the average, over arbitrary regions such as the circles shown in figure 1.3(d): aggregate(x, st_buffer(pts, 500), FUN = mean) %&gt;% st_as_sf() # Simple feature collection with 3 features and 1 field # Geometry type: POLYGON # Dimension: XY # Bounding box: xmin: 290000 ymin: 9110000 xmax: 292000 ymax: 9120000 # Projected CRS: UTM Zone 25, Southern Hemisphere # V1 geometry # 1 77.2 POLYGON ((291330 9114499, 2... # 2 60.1 POLYGON ((290519 9119219, 2... # 3 71.6 POLYGON ((292193 9116038, 2... Other raster-to-vector conversions are discussed in 7.5 and include converting raster pixels in point values converting raster pixels into small polygons, possibly merging polygons with identical values (“polygonize”) generating lines or polygons that delineate continuous pixel areas with a certain value range (“contour”) plot(st_rasterize(nc[&quot;BIR74&quot;], dx = 0.1), col =sf.colors(), breaks = &quot;equal&quot;) Figure 1.4: The map obtained by rasterizing county total number of births for the period 1974-1979 shown in figure 1.1 Vector-to-raster conversions can be as simple as rasterizing polygons, as shown in figure 1.4. Other, more general vector-to-raster conversions that may involve statistical modelling include interpolation of point values to points on a regular grid (chapter 12) estimating densities of points over a regular grid (chapter 11) area-weighted interpolation of polygon values to grid cells (section 5.3) direct rasterization of points, lines or polygons (section 7.5) 1.4 Raster types Raster dimensions describe how the rows and columns relate to spatial coordinates. Figure 1.5 shows a number of different possibilities. x = 1:5 y = 1:4 d = st_dimensions(x = x, y = y, .raster = c(&quot;x&quot;, &quot;y&quot;)) m = matrix(runif(20),5,4) r1 = st_as_stars(r = m, dimensions = d) r = attr(d, &quot;raster&quot;) r$affine = c(0.2, -0.2) attr(d, &quot;raster&quot;) = r r2 = st_as_stars(r = m, dimensions = d) r = attr(d, &quot;raster&quot;) r$affine = c(0.1, -0.3) attr(d, &quot;raster&quot;) = r r3 = st_as_stars(r = m, dimensions = d) x = c(1, 2, 3.5, 5, 6) y = c(1, 1.5, 3, 3.5) d = st_dimensions(x = x, y = y, .raster = c(&quot;x&quot;, &quot;y&quot;)) r4 = st_as_stars(r = m, dimensions = d) grd = st_make_grid(cellsize = c(10,10), offset = c(-130,10), n= c(8,5), crs=st_crs(4326)) r5 = st_transform(grd, &quot;+proj=laea +lon_0=-70 +lat_0=35&quot;) par(mfrow = c(2,3), mar = c(0.1, 1, 1.1, 1)) r1 = st_make_grid(cellsize = c(1,1), n = c(5,4), offset = c(0,0)) plot(r1, main = &quot;regular&quot;) plot(st_geometry(st_as_sf(r2)), main = &quot;rotated&quot;) plot(st_geometry(st_as_sf(r3)), main = &quot;sheared&quot;) plot(st_geometry(st_as_sf(r4, as_points = FALSE)), main = &quot;rectilinear&quot;) plot(st_geometry((r5)), main = &quot;curvilinear&quot;) Figure 1.5: various raster types Regular rasters like shown in figure 1.5 have a constant, not necessarily square cell size and axes aligned with the x and y (Easting and Northing) axes. Other raster types include those where the axes are no longer aligned with x and y (rotated), where axes are no longer perpendicular (sheared), or where cell size varies along a dimension (rectilinear). Finally, curvilinear rasters have cellsize and/or direction properties that are no longer independent from the other raster dimension. When a raster that is regular in a given coordinate reference system is projected to another raster while keeping each raster cell in tact, it changes shape and may become rectilinear (e.g. when going from ellipsoidal coordinates to Mercator, as in figure ??) or curvilinear (e.g. when going from ellipsoidal coordinates to Lambert Conic Conformal, as in figure 1.1). When reverting this procedure, one can recover the exact original raster. Creating a new, regular grid in the new projection is called raster (or image) reprojection or warping (section 7.7). This process is lossy, irreversible, and may need to be informed whether raster cells should be interpolated, averaged or summed, whether they denote categorical variables, or whether resampling using nearest neighbours should be used; see also section 1.6. 1.5 Time series, arrays, data cubes A lot of spatial data is not just spatial, but in addition temporal. Just like any observation is associated with an observation location, it is associated with an observation time or period. The dataset on the North Carolina counties shown above contains disease cases counted over two time periods, shown in figure 1.2. Although the original dataset has these variables in two different columns, for plotting them these columns had to be stacked first, while repeating the associated geometries - a form called tidy by (Wickham 2014b). When we have longer time series associated with geometries, neither option - distributing time over multiple columns, or stacking columns while repeating geometries - works well, and a more effective way of storing such data would be a matrix or array, where one dimension refers to time, and the other(s) to space. The natural way for image or raster data is already to store them in matrices; time series of rasters then lead to a three-dimensional array. The general term for such data is a (spatiotemporal) data cube, where cube refers to arrays with any number of dimensions. Data cubes can refer to both raster and vector data, examples are given in chapter 6. 1.6 Support When we have spatial data with geometries that are not points but collections of points (multi-points, lines, polygons, pixels), then the attributes associated with these geometries has one of several different relationships to them. Attributes can have a constant value for every point of the geometry a value that is unique to only this geometry, describing its identity a single value that is an aggregate over all points of the geometry An example of a constant is land use or bedrock type of a polygon. An example of an identity is a county name. An example of an aggregate is the number of births over a given period of time, of a county. The area with to which an attribute value refers to is called its support: aggregate properties have “block” (or polygon, or line) support, constant properties have “point” support (they apply to every point). Support matters when we manipulate the data. For instance, figure 1.4 was derived from a variable that has polygon support: the number of births per county. Rasterizing these values gives pixels with values that are associated to counties. The result of the rasterization is a meaningless map: the numeric values (“birth totals”) are not associated with the raster cells, and the county boundaries are no longer present. Totals of birth for the whole state can no longer be recovered from the pixel values. Ignoring support can easily lead to meaningless results. Chapter 5 discusses this further. Raster cell values may have point support, e.g. when the cell records the elevation of the point at the cell centre in a digital elevation model, or cell support, e.g. when a satellite image pixel gives the color value averaged over (an area similar to the) pixel. Most file formats do not provide this information, yet it may be important to know when aggregating, regridding or warping rasters (section 7.7). 1.7 Spatial data science software Although this book largely uses R and R packages for spatial data science, a number of these packages use software libraries that were not developed for R specifically. As an example, the dependency of R package sf on other R packages and system libraries is shown in figure 1.6. Figure 1.6: sf and its dependencies; arrows indicate strong dependency, dashed arrows weak dependency The C or C++ libraries used (GDAL, GEOS, PROJ, liblwgeom, s2geometry, NetCDF, udunits2) are all developed, maintained and used by (spatial) data science communities that are large and mostly different from the R community. By using these libraries, R users share how we understand what we are doing with these other communities. Because R, Python and Julia provide interactive interfaces to this software, many users get closer to these libraries than do users of other software based on these libraries. The first part of this book describes many of the concepts implemented in these libraries, which is relevant to spatial data science in general. 1.7.1 GDAL GDAL (geospatial data abstraction library) can be seen as the Swiss army knive of spatial data; besides for R it is being used in Python, QGIS, PostGIS, and more than 100 other software projects. GDAL is a “library of libraries” – in order to read all these data sources it needs a large number of other libraries. It typically links to over 100 other libraries, each of which provides access to e.g. a particular data file format, database access, or web service. Binary R packages distributed by CRAN contain only statically linked code: CRAN does not want to make any assumptions about presence of third-party libraries on the host system. As a consequence, when the sf package is installed in binary form from CRAN, it includes a copy of all the required external libraries as well as their dependencies, which may amount to 100 Mb. 1.7.2 PROJ PROJ (or PR\\(\\phi\\)J) is a library for cartographic projections and datum transformations: it converts spatial coordinates from one coordinate reference system to another. It comes with a large database of known projections and access to datum grids (high-precision pre-calculated values for datum transformations). It aligns with an international standard for coordinate reference systems (Lott 2015). Chapter 2 deals with coordinate systems, and PROJ. 1.7.3 GEOS and s2geometry GEOS (“Geometry Engine Open Source”) and s2geometry are two libraries for geometric operations. They are used to find measures (length, area, distance), and calculate predicates (do two geometries have any points in common?) or new geometries (which points do these two geometries have in common?). GEOS does this for flat, two-dimensional space (indicated by \\(R^2\\)), s2geometry does this for geometries on the sphere (indicated by \\(S^2\\)). Chapter 2 introduces coordinate reference systems, and chapter 4 discusses more about the differences between working with these two spaces. 1.7.4 NetCDF, udunits2, liblwgeom NetCDF (UCAR 2020) refers to a file format as well as a C library for reading and writing NetCDF files. It allows the definition of arrays of any dimensionality, and is widely used for spatial and spatiotemporal information, especially in the (climate) modelling communities. Udunits2 (UCAR 2014; E. Pebesma et al. 2021) is a database and software library for units of measurement that allows the conversion of units, handles derived units, and supports user-defined units. The liblwgeom “library” is a software component of PostGIS (Obe and Hsu 2015) that contains several routines missing from GDAL or GEOS, including convenient access to GeographicLib routines (Karney 2013) that ship with PROJ. 1.8 Exercises List five differences between raster and vector data. In addition to those listed below figure 1.1, list five further graphical components that are often found on a map. In your own words, why is the numeric information shown in figure 1.4 misleading (or meaningless)? Under which conditions would you expect strong differences when doing geometrical operations on \\(S^2\\), compared to doing them on \\(R^2\\)? References "],["cs.html", "Chapter 2 Coordinates 2.1 Quantities, units, datum 2.2 Ellipsoidal coordinates 2.3 Coordinate Reference Systems 2.4 PROJ and mapping accuracy 2.5 WKT-2 2.6 Exercises", " Chapter 2 Coordinates “Data are not just numbers, they are numbers with a context”; “In data analysis, context provides meaning” (Cobb and Moore 1997) Before we can try to understand geometries like points, lines, polygons, coverage and grids, it is useful to review coordinate systems so that we have an idea what exactly coordinates of a point reflect. For spatial data, the location of observations are characterized by coordinates, and coordinates are defined in a coordinate system. Different coordinate systems can be used for this, and the most important difference is whether coordinates are defined over a 2-dimensional or 3-dimensional space referenced to orthogonal axes (Cartesian coordinates), or using distance and directions (polar coordinates, spherical and ellipsoidal coordinates). Besides a location of observation, all observations are associated with time of observation, and so time coordinate systems are also briefly discussed. First we will briefly review quantities, to learn what units and datum are. 2.1 Quantities, units, datum The VIM (International vocabulary of metrology, BIPM et al. (2012)) defines a quantity as a “property of a phenomenon, body, or substance, where the property has a magnitude that can be expressed as a number and a reference”, where “[a] reference can be a measurement unit, a measurement procedure, a reference material, or a combination of such.” Although one could argue whether all data is constituted of quantities, there is no need to argue that proper data handling requires that numbers (or symbols) are accompanied by information on what they mean, in particular what they refer to. A measurement system consist of base units for base quantities, and derived units for derived quantities. For instance, the SI system of units (Bureau International des Poids et Mesures 2006) consist of the seven base units length (metre, m), mass (kilogram, kg), time (second, s), electric current (ampere, A), thermodynamic temperature (Kelvin, K), amount of substance (mole, mol), and luminous intensity (candela, cd). Derived units are composed of products of integer powers of base units; examples are speed (\\(\\mbox{m}~\\mbox{s}^{-1}\\)), density (\\(\\mbox{kg}~\\mbox{m}^{-3}\\)) and area (\\(\\mbox{m}^2\\)). The special case of unitless measures can refer to either cases where units cancel out (e.g. mass fraction: kg/kg, or angle measured in rad: m/m) or to cases where objects or events were counted (e.g. 5 apples). Adding an angle to a count of apples would not make sense; adding 5 apples to 3 oranges may make sense if the result is reinterpreted as a superclass, e.g. as pieces of fruit. Many data variables have units that are not expressible as SI base units or derived units. Hand (2004) discusses many such measurement scales, e.g. those used to measure intelligence in social sciences, in the context of measurement units. For many quantities, the natural origin of values is zero. This works for amounts, where differences between amounts result in meaningful negative values. For locations and times, differences have a natural zero interpretation: distance and duration. Absolute location (position) and time need a fixed origin, from which we can meaningfully measure other absolute space-time points: we call this a datum. For space, a datum involves more than one dimension. The combination of a datum and a measurement unit (scale) is a reference system. We will now elaborate how spatial locations can be expressed as either ellipsoidal or Cartesian coordinates. The next sections will deal with temporal and spatial reference systems, and how they are handled in R. 2.2 Ellipsoidal coordinates par(mar = rep(0,4)) plot(3, 4, xlim = c(-6,6), ylim = c(-6,6), asp = 1) axis(1, pos = 0, at = 0:6) axis(2, pos = 0, at = -6:6) xd = seq(-5, 5, by = .1) lines(xd, sqrt(25 - xd^2), col = &#39;grey&#39;) lines(xd, -sqrt(25 - xd^2), col = &#39;grey&#39;) arrows(0, 0, 3, 4, col = &#39;red&#39;, length = .15, angle = 20) text(1.5, 2.7, label = &quot;r&quot;, col = &#39;red&#39;) xd = seq(3/5, 1, by = .1) lines(xd, sqrt(1 - xd^2), col = &#39;red&#39;) text(1.2, 0.5, label = parse(text = &quot;phi&quot;), col = &#39;red&#39;) lines(c(3,3), c(0,4), lty = 2, col = &#39;blue&#39;) lines(c(0,3), c(4,4), lty = 2, col = &#39;blue&#39;) text(3.3, 0.3, label = &quot;x&quot;, col = &#39;blue&#39;) text(0.3, 4.3, label = &quot;y&quot;, col = &#39;blue&#39;) Figure 2.1: Two-dimensional polar (red) and Cartesian (blue) coordinates Figure 2.1 shows both polar and Cartesian coordinates for a two-dimensional situation. In Cartesian coordinates, the point shown is \\((x,y) = (3,4)\\), for polar coordinates it is \\((r,\\phi) = (5, \\mbox{arctan}(4/3))\\), where \\(\\mbox{arctan}(4/3)\\) is approximately \\(0.93\\) radians, or \\(53^{\\circ}\\). Note that \\(x\\), \\(y\\) and \\(r\\) all have length units, where \\(\\phi\\) is an angle (a unitless length/length ratio). Converting back and forth between Cartesian and polar coordinates is trivial, as \\[x = r~\\mbox{cos} \\phi,\\] \\[y = r~\\mbox{sin} \\phi,\\] \\[r = \\sqrt{x^2 + y^2}, \\ \\mbox{and}\\] \\[\\phi = \\mbox{atan2}(y, x)\\] where \\(\\mbox{atan2}\\) is used in favor of \\(\\mbox{atan}(y/x)\\) to take care of the right quadrant. 2.2.1 Ellipsoidal coordinates In three dimensions, where Cartesian coordinates are expressed as \\((x,y,z)\\), spherical coordinates are the three-dimensional equivalent of polar coordinates and can be expressed as \\((r,\\lambda,\\phi)\\), where \\(r\\) is the radius of the sphere, \\(\\lambda\\) is the longitude, measured in the \\((x,y)\\) plane counter-clockwise from positive \\(x\\), and \\(\\phi\\) is the latitude, the angle between the vector and the \\((x,y)\\) plane. Figure 2.2 illustrates Cartesian geocentric and ellipsoidal coordinates. suppressPackageStartupMessages(library(sf)) e = cbind(-90:90,0) # equator f1 = rbind(cbind(0, -90:90)) # 0/antimerid. f2 = rbind(cbind(90, -90:90), cbind(270, 90:-90))# +/- 90 eq = st_sfc(st_linestring(e), st_linestring(f1), st_linestring(f2), crs=4326) geoc = st_transform(eq, &quot;+proj=geocent&quot;) cc = rbind(geoc[[1]], NA, geoc[[2]], NA, geoc[[3]]) from3d = function(x, offset, maxz, minz) { x = x[,c(2,3,1)] + offset # move to y right, x up, z backw x[,2] = x[,2] - maxz # shift y to left d = maxz z = x[,3] - minz + offset x[,1] = x[,1] * (d/z) x[,2] = x[,2] * (d/z) x[,1:2] } maxz = max(cc[,3], na.rm = TRUE) minz = min(cc[,3], na.rm = TRUE) offset = 3e7 circ = from3d(cc, offset, maxz, minz) mx = max(cc, na.rm = TRUE) * 1.1 x = rbind(c(0, 0, 0), c(mx, 0, 0)) y = rbind(c(0, 0, 0), c(0, mx, 0)) z = rbind(c(0, 0, 0), c(0, 0, mx)) ll = rbind(x, NA, y, NA, z) l0 = from3d(ll, offset, maxz, minz) mx = max(cc, na.rm = TRUE) * 1.2 x = rbind(c(0, 0, 0), c(mx, 0, 0)) y = rbind(c(0, 0, 0), c(0, mx, 0)) z = rbind(c(0, 0, 0), c(0, 0, mx)) ll = rbind(x, NA, y, NA, z) l = from3d(ll, offset, maxz, minz) par(mfrow = c(1, 2)) par(mar=rep(0,4)) plot.new() plot.window(xlim = c(min(circ[,1],na.rm = TRUE), 3607103*1.02), ylim = c(min(circ[,2],na.rm = TRUE), 2873898*1.1), asp = 1) lines(circ) lines(l0) text(l[c(2,5,8),], labels = c(&quot;x&quot;, &quot;y&quot;, &quot;z&quot;), col = &#39;red&#39;) # add POINT(60 47) p = st_as_sfc(&quot;POINT(60 47)&quot;, crs = 4326) %&gt;% st_transform(&quot;+proj=geocent&quot;) p = p[[1]] pts = rbind(c(0,0,0), c(p[1],0,0), c(p[1],p[2],0), c(p[1],p[2],p[2])) ptsl = from3d(pts, offset, maxz, minz) lines(ptsl, col = &#39;blue&#39;, lty = 2, lwd = 2) points(ptsl[4,1], ptsl[4,2], col = &#39;blue&#39;, cex = 1, pch = 16) plot.new() plot.window(xlim = c(min(circ[,1],na.rm = TRUE), 3607103*1.02), ylim = c(min(circ[,2],na.rm = TRUE), 2873898*1.1), asp = 1) lines(circ) p = st_as_sfc(&quot;POINT(60 47)&quot;, crs = 4326) %&gt;% st_transform(&quot;+proj=geocent&quot;) p = p[[1]] pts = rbind(c(0,0,0), c(p[1],p[2],p[3])) pt = from3d(pts, offset, maxz, minz) lines(pt) points(pt[2,1], pt[2,2], col = &#39;blue&#39;, cex = 1, pch = 16) p0 = st_as_sfc(&quot;POINT(60 0)&quot;, crs = 4326) %&gt;% st_transform(&quot;+proj=geocent&quot;) p0 = p0[[1]] pts = rbind(c(0,0,0), c(p0[1],p0[2],p0[3])) pt = from3d(pts, offset, maxz, minz) lines(pt) p0 = st_as_sfc(&quot;POINT(0 0)&quot;, crs = 4326) %&gt;% st_transform(&quot;+proj=geocent&quot;) p0 = p0[[1]] pts = rbind(c(0,0,0), c(p0[1],p0[2],p0[3])) pt = from3d(pts, offset, maxz, minz) lines(pt) p0 = st_as_sfc(&quot;POINT(0 90)&quot;, crs = 4326) %&gt;% st_transform(&quot;+proj=geocent&quot;) p0 = p0[[1]] pts = rbind(c(0,0,0), c(p0[1],p0[2],p0[3])) pt = from3d(pts, offset, maxz, minz) lines(pt, lty = 2) p0 = st_as_sfc(&quot;POINT(90 0)&quot;, crs = 4326) %&gt;% st_transform(&quot;+proj=geocent&quot;) p0 = p0[[1]] pts = rbind(c(0,0,0), c(p0[1],p0[2],p0[3])) pt = from3d(pts, offset, maxz, minz) lines(pt, lty = 2) f1 = rbind(cbind(0:60, 0)) arc = st_sfc(st_linestring(f1), crs=4326) geoc = st_transform(arc, &quot;+proj=geocent&quot;) cc = rbind(geoc[[1]]) circ = from3d(cc, offset, maxz, minz) lines(circ, col = &#39;red&#39;, lwd = 2, lty = 2) f1 = rbind(cbind(60, 0:47)) arc = st_sfc(st_linestring(f1), crs=4326) geoc = st_transform(arc, &quot;+proj=geocent&quot;) cc = rbind(geoc[[1]]) circ = from3d(cc, offset, maxz, minz) lines(circ, col = &#39;blue&#39;, lwd = 2, lty = 2) text(pt[1,1]+100000, pt[1,2]+50000, labels = expression(phi), col = &#39;blue&#39;) # lat text(pt[1,1]+20000, pt[1,2]-50000, labels = expression(lambda), col = &#39;red&#39;) # lng Figure 2.2: Cartesian geocentric coordinates (left) measure three distances, ellipsoidal coordinates (right) measure two angles, and possibly an ellipsoidal height \\(\\lambda\\) typically varies between \\(-180^{\\circ}\\) and \\(180^{\\circ}\\) (or alternatively from \\(0^{\\circ}\\) to \\(360^{\\circ}\\)), \\(\\phi\\) from \\(-90^{\\circ}\\) to \\(90^{\\circ}\\). When we are only interested in points on a sphere with given radius, we can drop \\(r\\): \\((\\lambda,\\phi)\\) now suffice to identify any point. It should be noted that this is just a definition, one could for instance also choose to measure polar angle, the angle between the vector and \\(z\\), instead of latitude. There is also a long tradition of specifying points as \\((\\phi,\\lambda)\\) but throughout this book we will stick to longitude-latitude, \\((\\lambda,\\phi)\\). The point denoted in figure 2.2 has \\((\\lambda,\\phi)\\) or ellipsoidal coordinates with values p = st_as_sfc(&quot;POINT(60 47)&quot;, crs = 4326) p[[1]] # POINT (60 47) with angles measured in degrees, and geocentric coordinates p = st_as_sfc(&quot;POINT(60 47)&quot;, crs = 4326) %&gt;% st_transform(&quot;+proj=geocent&quot;) p[[1]] # POINT Z (2178844 3773868 4641765) with unit metres. For points on an ellipse, there are two ways in which angle can be expressed (figure 2.3): measured from the center of the ellipse (\\(\\psi\\)), or measured perpendicular to the tangent on the ellipse at the target point (\\(\\phi\\)). par(mar = rep(0,4)) x = 4 y = 5/8 * sqrt(48) plot(x, y, xlim = c(-6,6), ylim = c(-8,8), asp = 1) axis(1, pos = 0, at = 0:9) axis(2, pos = 0, at = -5:5) xd = seq(-8, 8, by = .1) lines(xd, 5/8 * sqrt(64 - xd^2), col = &#39;grey&#39;) lines(xd, 5/8 * -sqrt(64 - xd^2), col = &#39;grey&#39;) arrows(0, 0, x, y, col = &#39;red&#39;, length = .15, angle = 20) b = (x * 25) / (-y * 64) a = y - x * b abline(a, b, col = &#39;grey&#39;) b = -1/b x0 = x - y / b arrows(x0, 0, x, y, col = &#39;blue&#39;, length = .15, angle = 20) text(1.2, 0.5, label = parse(text = &quot;psi&quot;), col = &#39;red&#39;) text(3, 0.5, label = parse(text = &quot;phi&quot;), col = &#39;blue&#39;) Figure 2.3: Angles on an ellipse: geodetic (blue) and geocentric (red) latitude The most commonly used parametric model for the Earth is an ellipsoid of revolution, an ellipsoid with two equal semi-axes (Iliffe and Lott 2008). In effect, this is a flattened sphere (or spheroid): the distance between the poles is (slightly: about 0.33%) smaller than the distance between two opposite points on the equator. Under this model, longitude is always measured along a circle (as in figure 2.2), and latitude along an ellipse (as in figure 2.3). If we think of figure 2.3 as a cross section of the Earth passing through the poles, the geodetic latitude measure \\(\\phi\\) is the one used when no further specification is given. The latitude measure \\(\\psi\\) is called the geocentric latitude. In addition to longitude and latitude we can add altitude or elevation to define points that are not on the ellipsoid, and obtain a three dimensional space again. When defining altitude, we need to choose where zero altitude is: on the ellipsoid, or relative to the surface approximating mean sea level (the geoid)? which direction is positive, and which direction is “straight up”: perpendicular to the ellipsoid surface, or in the direction perpendicular to the surface of the geoid? All these choices may matter, depending on the application area and required measurement accuracies. The shape of the Earth is not a perfect ellipsoid. As a consequence, several ellipsoids with different shape parameters and bound to the Earth in different ways are being used. Such ellipsoids are called datums, and are briefly discussed in section 2.3, along with coordinate reference systems. 2.2.2 Projected coordinates, distances Because paper maps and computer screens are much more abundant and practical than globes, most of the time we look at spatial data we see it projected: drawn on a flat, two-dimensional surface. Computing the locations in a two-dimensional space means that we work with projected coordinates. Projecting ellipsoidal coordinates means that shapes, directions, areas, or even all three, are distorted (Iliffe and Lott 2008). Distances between two points \\(p_i\\) and \\(p_j\\) in Cartesian coordinates are computed as Euclidean distances, in two dimensions by \\[d_{ij} = \\sqrt{(x_i-x_j)^2+(y_i-y_j)^2}\\] with \\(p_i = (x_i,y_i)\\) and in three dimensions by \\[d_{ij} = \\sqrt{(x_i-x_j)^2+(y_i-y_j)^2+(z_i-z_j)^2}\\] with \\(p_i = (x_i,y_i,z_i).\\) These distances represent the length of a straight line between two points \\(i\\) and \\(j\\). For two points on a circle, the length of the arc of two points \\(c_1 = (r,{\\phi}_i)\\) and \\(c_2 = (r, \\phi_2)\\) is \\[s_{ij}=r~|\\phi_1-\\phi_2| = r ~\\theta\\] with \\(\\theta\\) the angle between \\(\\phi_1\\) and \\(\\phi_2\\) in radians. For very small values of \\(\\theta\\), we will have \\(s_{ij} \\approx d_{ij}\\), because a small arc segment is nearly straight. For two points \\(p_1 = (\\lambda_1,\\phi_1)\\) and \\(p_2 = (\\lambda_2,\\phi_2)\\) on a sphere with radius \\(r&#39;\\), the great circle distance is the arc length between \\(p_1\\) and \\(p_2\\) on the circle that passes through \\(p_1\\) and \\(p_2\\) and has the center of the sphere as its center, and is given by \\(s_{12} = r ~ \\theta_{12}\\) with \\[\\theta_{12} = \\arccos(\\sin \\phi_1 \\cdot \\sin \\phi_2 + \\cos \\phi_1 \\cdot \\cos \\phi_2 \\cdot \\cos(|\\lambda_1-\\lambda_2|))\\] the angle between \\(p_1\\) and \\(p_2\\), in radians. Arc distances between two points on a spheroid are more complicated to compute; a good discussion on the topic and an explanation of the method implemented in GeographicLib (part of PROJ) is given in Karney (2013). To show that these distance measures actually give different values, we computed them for the distance Berlin - Paris. Here, gc_ refers to ellipsoidal and spherical great circle distances, straight_ refers to straight line, Euclidean distances between Cartesian geocentric coordinates associated on the WGS84 ellipse and sphere: pts = st_sfc(st_point(c(13.4050, 52.5200)), st_point(c(2.3522, 48.8566)), crs = &#39;EPSG:4326&#39;) sf_use_s2(FALSE) d1 = c(gc_ellipse = st_distance(pts)[1,2]) sf_use_s2(TRUE) # or, without using s2, use st_distance(st_transform(pts, &quot;+proj=longlat +ellps=sphere&quot;)) d2 = c(gc_sphere = st_distance(pts)[1,2]) p = st_transform(pts, &quot;+proj=geocent&quot;) d3 = c(straight_ellipse = units::set_units(sqrt(sum(apply(do.call(cbind, p), 1, diff)^2)), m)) p2 = st_transform(pts, &quot;+proj=longlat +ellps=sphere&quot;) %&gt;% st_transform(&quot;+proj=geocent&quot;) d4 = c(straight_sphere = units::set_units(sqrt(sum(apply(do.call(cbind, p2), 1, diff)^2)), m)) res = c(d1,d3,d2,d4) # print as km, re-add names: res %&gt;% units::set_units(km) %&gt;% setNames(names(res)) %&gt;% print(digits = 5) # Units: [km] # gc_ellipse straight_ellipse gc_sphere straight_sphere # 879.70 879.00 877.46 876.77 2.2.3 Bounded and unbounded spaces Two-dimensional and three-dimensional Euclidean spaces (\\(R^2\\) and \\(R^3\\)) are unbounded: every line in this space has infinite length, distances, areas or volumes are unbounded. In contrast, spaces defined on a circle (\\(S^1\\)) or sphere (\\(S^2\\)) define a bounded set: there may be infinitely many points but the length and area of the circle and the radius, area and volume of a sphere are bound. This may sound trivial, but leads to some interesting findings when handling spatial data. A polygon on \\(R^2\\) has unambiguously an inside and an outside. On a sphere, \\(S^2\\), any polygon divides the sphere in two parts, and which of these two is to be considered inside and which outside is ambiguous and needs to be defined e.g. by the traversal direction. Chapter 4 will further discuss consequences when working with geometries on \\(S^2\\). 2.3 Coordinate Reference Systems We follow Lott (2015) when defining the following concepts (italics indicate literal quoting): a coordinate system is a set of mathematical rules for specifying how coordinates are to be assigned to points a datum is a parameter or set of parameters that define the position of the origin, the scale, and the orientation of a coordinate system, a geodetic datum is a datum describing the relationship of a two- or three-dimensional coordinate system to the Earth, and a coordinate reference system is a coordinate system that is related to an object by a datum; for geodetic and vertical datums, the object will be the Earth. A readable text that further explains these concepts is Iliffe and Lott (2008). The Earth does not follow a regular shape. The topography of the Earth is of course known to vary strongly, but also the surface formed by constant gravity at mean sea level, the geoid, is irregular. A commonly used model that is fit to the geoid is an ellipsoid of revolution, which is an ellipse with two identical minor axes. Fitting such an ellipsoid to the Earth gives a datum. However, fitting it to different areas, or based on different sets of reference points gives different fits, and hence different datums: a datum can for instance be fixed to a particular tectonic plate (like ETRS89), others can be globally fit (like WGS84). More local fits lead to smaller approximation errors. The definitions above imply that coordinates in degrees longitude and latitude only have a meaning, i.e. can only be interpreted unambiguously as Earth coordinates, when the datum they are associated with is given. Note that for projected data, the data that were projected are associated with a reference ellipsoid (datum). Going from one projection to another without changing datum is called coordinate conversion, and passes through the ellipsoidal coordinates associated with the datum involved. This process is lossless and invertible: the parameters and equations associated with a conversion are not empirical. Recomputing coordinates in a new datum is called coordinate transformation, and is approximate: because datums are a result of model fitting, transformations between datums are models too that have been fit; the equations involved are empirical, and multiple transformation paths, based on different model fits and associated with different accuracies, are possible. Plate tectonics imply that within a global datum, fixed objects may have coordinates that change over time, and that transformations from one datum to another may be time-dependent. Earthquakes are a cause of more local and sudden changes in coordinates. 2.4 PROJ and mapping accuracy Very few living people active in open source geospatial software can remember the time before PROJ. PROJ (Evenden 1990) started in the 1970s as a Fortran project, and was released in 1985 as a C library for cartographic projections. It came with command line tools for direct and inverse projections, and could be linked to software to let it support (re)projection directly. Originally, datums were considered implicit, and no datum transformations were allowed. In the early 2000s, PROJ was known as PROJ.4, after its never changing major version number. Amongst others motivated by the rise of GPS, the need for datum transformations increased and PROJ.4 was extended with rudimentary datum support. PROJ definitions for coordinate reference systems would look like this: +proj=utm +zone=33 +datum=WGS84 +units=m +no_defs where key=value pairs are preceded by a + and separated by a space. This form came to be known as “PROJ.4 string”, since the PROJ project stayed at version 4.x for several decades. Other datums would come with a fields like +ellps=bessel +towgs84=565.4,50.3,465.6,-0.399,0.344,-1.877,4.072 indicating another ellipse, as well as the seven (or three) parameters for transforming from this ellipse to WGS84 (the “World Geodetic System 1984” global datum once popularized by GPS), effectively defining the datum in terms of a transformation to WGS84. Along with PROJ.4 came a set of databases with known (registered) projections, from which the best known is the EPSG registry. National mapping agencies would provide (and update over time) their best guesses of +towgs84= parameters for national coordinate reference systems, and distribute it through the EPSG registry, which was part of PROJ distributions. For some transformations, datum grids were available and distributed as part of PROJ.4: such grids are raster maps that provide for every location pre-computed values for the shift in longitude and latitude, or elevation, for a particular datum transformation. library(stars) library(rnaturalearth) countries110 = st_as_sf(countries110) uk = countries110[countries110$admin %in% c(&quot;United Kingdom&quot;),] %&gt;% st_geometry() r = read_stars(&quot;data/uk_os_OSTN15_NTv2_OSGBtoETRS.tif&quot;) # r = read_stars(&quot;/vsicurl/https://cdn.proj.org/uk_os_OSTN15_NTv2_OSGBtoETRS.tif&quot;) hook = function() { plot(uk, border = &quot;orange&quot;, col = NA, add = TRUE) } plot(r[,,,1:2], axes = TRUE, hook = hook, key.pos = 4) Figure 2.4: UK horizontal datum grid, from datum OSGB 1936 (EPSG:4277) to datum ETRS89 (EPSG:4258); units arc-seconds h = read_stars(&quot;data/uk_os_OSGM15_GB.tif&quot;) # h = read_stars(&quot;/vsicurl/https://cdn.proj.org/uk_os_OSGM15_GB.tif&quot;) plot(h, axes = TRUE, reset = FALSE) plot(uk, border = &quot;orange&quot;, col = NA, add = TRUE) Figure 2.5: UK vertical datum grid, from ETRS89 (EPSG:4937) to ODN height (EPSG:5701), units m In PROJ.4, every coordinate transformation had to go through a conversion to and from WGS84; even reprojecting data associated with a datum different from WGS84 had to go through a transformation to and from WGS84. The associated errors of up to 100 m were acceptable for mapping purposes for not too small areas, but applications that need high accuracy transformations, e.g. precision agriculture, planning flights of UAV’s, or object tracking are often more demanding in terms of accuracy. In 2018, after a successful “GDAL Coordinate System Barn Raising” initiative, a number of companies profiting from the open source geospatial software stack supported the development of a more modern, mature coordinate transformation system in PROJ. Over a few years, PROJ.4 evolved through versions 5, 6, 7 and 8 and was hence renamed into PROJ (or PR\\(\\phi\\)J). The most notable changes include: although PROJ.4 strings can still be used to initialize certain coordinate reference systems, they are no longer sufficient to represent all of them; a new format, WKT2 (described in next section) replaces it. WGS84 as a hub datum is dropped: coordinate transformation no longer need to go through a particular datum Multiple conversion or transformation paths (so-called pipelines) to go from CRS A to CRS B are possible, and can be reported along with the associated accuracy; PROJ will by default use the most accurate one but user control is possible transformation pipelines can chain an arbitrary number of elementary transformation operations, including swapping of axes and unit transformations datum grids, of which there are now many more, are no longer distributed with the library but are accessible from a CDN; PROJ allows to enabling and disabling network access to access these grids, and only downloads the section of the grid actually needed, storing it in a cache on the user’s machine for future use coordinate transformations receive support for epochs, time-dependent transformations (and hence: four-dimensional coordinates, including the source and target time) the set of files with registered coordinate reference systems is handled in an SQLite database. instead of always handling axis order (longitude, latitude), when the authority defines differently this is now obeyed (with the most notable example: EPSG:4326 defines axis order to be latitude, longitude.) All these points sound like massive improvements, and accuracies of transformation can be below 1 metre. An interesting point is the last: Where we could safely assume for many decades that spatial data with ellipsoidal coordinates would have axis order (longitude, latitude), this is no longer the case. We will see in section ?? how to deal with this. Examples of a horizontal datum grids, downloaded from cdn.proj.org, are shown in figure 2.4 and for a vertical datum grid in figure 2.5. Datum grids may carry per-pixel accuracy values. 2.5 WKT-2 Lott (2015) describes a standard for encoding coordinate reference systems, as well as transformations between them using well known text; the standard (and format) is referred to informally as WKT-2. As mentioned above, GDAL and PROJ fully support this encoding. An example of WKT2 for CRS OGC:CRS84 is: GEOGCRS[&quot;WGS 84&quot;, DATUM[&quot;World Geodetic System 1984&quot;, ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, LENGTHUNIT[&quot;metre&quot;,1]], ID[&quot;EPSG&quot;,6326]], PRIMEM[&quot;Greenwich&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8901]], CS[ellipsoidal,2], AXIS[&quot;longitude&quot;,east, ORDER[1], ANGLEUNIT[&quot;degree&quot;,0.0174532925199433, ID[&quot;EPSG&quot;,9122]]], AXIS[&quot;latitude&quot;,north, ORDER[2], ANGLEUNIT[&quot;degree&quot;,0.0174532925199433, ID[&quot;EPSG&quot;,9122]]]] This shows a WGS84 ellipsoid, and a coordinate system with the axis order (longitude, latitude) that can be used to replace EPSG:4326 when one wants unambiguously “traditional” (GIS) axis order. A longer introduction on the history and recent changes in PROJ is given in (R. Bivand 2020b), building upon the work of (Knudsen and Evers 2017; Evers and Knudsen 2017) 2.6 Exercises list three geographic measures that do not have a natural zero origin convert the \\((x,y)\\) points \\((10,2)\\), \\((-10,-2)\\), \\((10,-2)\\) and \\((0,10)\\) to polar coordinates convert the polar \\((r,\\phi)\\) points \\((10,45^{\\circ})\\), \\((0,100^{\\circ})\\) and \\((5,359^{\\circ})\\) to Cartesian coordinates assuming the Earth is a sphere with a radius of 6371 km, compute for \\((\\lambda,\\phi)\\) points the great circle distance between \\((10,10)\\) and \\((11,10)\\), between \\((10,80)\\) and \\((11,80)\\), between \\((10,10)\\) and \\((10,11)\\) and between \\((10,80)\\) and \\((10,81)\\) (units: degree). What are the distance units? References "],["geometries.html", "Chapter 3 Geometries 3.1 Simple feature geometries 3.2 Operations on geometries 3.3 Precision 3.4 Coverages: tessellations and rasters 3.5 Networks", " Chapter 3 Geometries Having learned how we describe coordinates systems, we can define how geometries can be described using these coordinate systems. This chapter will explain simple features, a standard that describes point, line and polygon geometries along with operations on them, operations on geometries coverages, subdivisions of larger regions into sub-regions networks Geometries on the sphere are discussed in chapter 4, Rasters and other rectangular subdivisions of space are discussed in chapter 6. 3.1 Simple feature geometries Simple feature geometries are a way to describe the geometries of features. By features we mean things that have a geometry, potentially some time properties, and other attributes that could include a label describing the thing and quantitative measures of it. The main application of simple feature geometries is to describe geometries in two-dimensional space by points, lines, or polygons. The “simple” adjective refers to the fact that the line or polygon geometries are represented by sequences of points connected with straight lines that do not self-intersect. Simple features access is a standard (J. R. Herring 2011, @sfa2, @iso) for describing simple feature geometries that includes a class hierarchy a set of operations binary and text encodings We will first discuss the seven most common simple feature geometry types. 3.1.1 The big seven The most commonly used simple features geometries, used to represent a single feature are: type description POINT single point geometry MULTIPOINT set of points LINESTRING single linestring (two or more points connected by straight lines) MULTILINESTRING set of linestrings POLYGON exterior ring with zero or more inner rings, denoting holes MULTIPOLYGON set of polygons GEOMETRYCOLLECTION set of the geometries above library(sf) par(mfrow = c(2,4)) par(mar = c(1,1,1.2,1)) # 1 p = st_point(0:1) plot(p, pch = 16) title(&quot;point&quot;) box(col = &#39;grey&#39;) # 2 mp = st_multipoint(rbind(c(1,1), c(2, 2), c(4, 1), c(2, 3), c(1,4))) plot(mp, pch = 16) title(&quot;multipoint&quot;) box(col = &#39;grey&#39;) # 3 ls = st_linestring(rbind(c(1,1), c(5,5), c(5, 6), c(4, 6), c(3, 4), c(2, 3))) plot(ls, lwd = 2) title(&quot;linestring&quot;) box(col = &#39;grey&#39;) # 4 mls = st_multilinestring(list( rbind(c(1,1), c(5,5), c(5, 6), c(4, 6), c(3, 4), c(2, 3)), rbind(c(3,0), c(4,1), c(2,1)))) plot(mls, lwd = 2) title(&quot;multilinestring&quot;) box(col = &#39;grey&#39;) # 5 polygon po = st_polygon(list(rbind(c(2,1), c(3,1), c(5,2), c(6,3), c(5,3), c(4,4), c(3,4), c(1,3), c(2,1)), rbind(c(2,2), c(3,3), c(4,3), c(4,2), c(2,2)))) plot(po, border = &#39;black&#39;, col = &#39;#ff8888&#39;, lwd = 2) title(&quot;polygon&quot;) box(col = &#39;grey&#39;) # 6 multipolygon mpo = st_multipolygon(list( list(rbind(c(2,1), c(3,1), c(5,2), c(6,3), c(5,3), c(4,4), c(3,4), c(1,3), c(2,1)), rbind(c(2,2), c(3,3), c(4,3), c(4,2), c(2,2))), list(rbind(c(3,7), c(4,7), c(5,8), c(3,9), c(2,8), c(3,7))))) plot(mpo, border = &#39;black&#39;, col = &#39;#ff8888&#39;, lwd = 2) title(&quot;multipolygon&quot;) box(col = &#39;grey&#39;) # 7 geometrycollection gc = st_geometrycollection(list(po, ls + c(0,5), st_point(c(2,5)), st_point(c(5,4)))) plot(gc, border = &#39;black&#39;, col = &#39;#ff6666&#39;, pch = 16, lwd = 2) title(&quot;geometrycollection&quot;) box(col = &#39;grey&#39;) Figure 3.1: sketches of the main simple feature geometry types Figure 3.1 shows examples of these basic geometry types. The human-readable, “well-known-text” (WKT) representation of the geometries plotted are: p mp ls mls po mpo gc POINT (0 1) MULTIPOINT ((1 1), (2 2), (4 1), (2 3), (1 4)) LINESTRING (1 1, 5 5, 5 6, 4 6, 3 4, 2 3) MULTILINESTRING ((1 1, 5 5, 5 6, 4 6, 3 4, 2 3), (3 0, 4 1, 2 1)) POLYGON ((2 1, 3 1, 5 2, 6 3, 5 3, 4 4, 3 4, 1 3, 2 1), (2 2, 3 3, 4 3, 4 2, 2 2)) MULTIPOLYGON (((2 1, 3 1, 5 2, 6 3, 5 3, 4 4, 3 4, 1 3, 2 1), (2 2, 3 3, 4 3, 4 2, 2 2)), ((3 7, 4 7, 5 8, 3 9, 2 8, 3 7))) GEOMETRYCOLLECTION ( POLYGON ((2 1, 3 1, 5 2, 6 3, 5 3, 4 4, 3 4, 1 3, 2 1), (2 2 , 3 3, 4 3, 4 2, 2 2)), LINESTRING (1 6, 5 10, 5 11, 4 11, 3 9, 2 8), POINT (2 5), POINT (5 4) ) In this representation, coordinates are separated by space, and points by commas. Sets are grouped by parentheses, and separated by commas. Individual points in a geometry contain at least two coordinates: x and y, in that order. If these coordinates refer to ellipsoidal coordinates, x and y usually refer to longitude and latitude, respectively, although sometimes to latitude and longitude (see sections 1.7.2 and ??). 3.1.2 Simple and valid geometries, ring direction Linestrings are called simple when they do not self-intersect: (ls = st_linestring(rbind(c(0,0), c(1,1), c(2,2), c(0,2), c(1,1), c(2,0)))) # LINESTRING (0 0, 1 1, 2 2, 0 2, 1 1, 2 0) c(is_simple = st_is_simple(ls)) # is_simple # FALSE Valid polygons and multipolygons obey all of the following properties: polygon rings are closed (the last point equals the first) polygon holes (inner rings) are inside their exterior ring polygon inner rings maximally touch the exterior ring in single points, not over a line a polygon ring does not repeat its own path in a multipolygon, an external ring maximally touches another exterior ring in single points, not over a line If this is not the case, the geometry concerned is not valid. Invalid geometries typically cause errors when they are processed, but can usually be repaired to make them valid. A further convention is that the outer ring of a polygon is winded counter-clockwise, while the holes are winded clockwise, but polygons for which this is not the case are still considered valid. For polygons on the sphere, the “clockwise” is not very useful: if for instance we take the equator as polygon, is the Northern hemisphere or the Southern hemisphere “inside”? The convention taken here is to consider the area on the left while traversing the polygon is considered the polygon’s inside. 3.1.3 Z and M coordinates In addition to X and Y coordinates, Single points (vertices) of simple feature geometries may have a Z coordinate, denoting altitude, and/or an M value, denoting some “measure” The M attribute shall be a property of the vertex. It sounds attractive to encode a time stamp in it, e.g. to pack movement data (trajectories) in LINESTRINGs. These become however invalid (or “non-simple”) once the trajectory self-intersects, which easily happens when only X and Y are considered for self-intersections. Both Z and M are not found often, and software support to do something useful with them is (still) rare. Their WKT representation are fairly easily understood: st_point(c(1,3,2)) # POINT Z (1 3 2) st_point(c(1,3,2), dim = &quot;XYM&quot;) # POINT M (1 3 2) st_linestring(rbind(c(3,1,2,4), c(4,4,2,2))) # LINESTRING ZM (3 1 2 4, 4 4 2 2) 3.1.4 Empty geometries A very important concept in the feature geometry framework is that of the empty geometry. Empty geometries arise naturally when we do geometrical operations (section 3.2), for instance when we want to know the intersection of POINT (0 0) and POINT (1 1): (e = st_intersection(st_point(c(0,0)), st_point(c(1,1)))) # GEOMETRYCOLLECTION EMPTY and it represents essentially the empty set: when combining (unioning) an empty point it with other non-empty geometries, it vanishes. All geometry types have a special value representing the empty (typed) geometry: st_point() # POINT EMPTY st_linestring(matrix(1,1,3)[0,], dim = &quot;XYM&quot;) # LINESTRING M EMPTY and so on, but they all point to the empty set, differing only in their dimension (section 3.2.2). 3.1.5 Ten further geometry types There are 10 more geometry types which are more rare, but increasingly find implementation: type description CIRCULARSTRING The CIRCULARSTRING is the basic curve type, similar to a LINESTRING in the linear world. A single segment requires three points, the start and end points (first and third) and any other point on the arc. The exception to this is for a closed circle, where the start and end points are the same. In this case the second point MUST be the center of the arc, i.e. the opposite side of the circle. To chain arcs together, the last point of the previous arc becomes the first point of the next arc, just like in LINESTRING. This means that a valid circular string must have an odd number of points greater than 1. COMPOUNDCURVE A compound curve is a single, continuous curve that has both curved (circular) segments and linear segments. That means that in addition to having well-formed components, the end point of every component (except the last) must be coincident with the start point of the following component. CURVEPOLYGON Example compound curve in a curve polygon: CURVEPOLYGON(COMPOUNDCURVE(CIRCULARSTRING(0 0,2 0, 2 1, 2 3, 4 3),(4 3, 4 5, 1 4, 0 0)), CIRCULARSTRING(1.7 1, 1.4 0.4, 1.6 0.4, 1.6 0.5, 1.7 1) ) MULTICURVE A MultiCurve is a 1-dimensional GeometryCollection whose elements are Curves, it can include linear strings, circular strings or compound strings. MULTISURFACE A MultiSurface is a 2-dimensional GeometryCollection whose elements are Surfaces, all using coordinates from the same coordinate reference system. CURVE A Curve is a 1-dimensional geometric object usually stored as a sequence of Points, with the subtype of Curve specifying the form of the interpolation between Points SURFACE A Surface is a 2-dimensional geometric object POLYHEDRALSURFACE A PolyhedralSurface is a contiguous collection of polygons, which share common boundary segments TIN A TIN (triangulated irregular network) is a PolyhedralSurface consisting only of Triangle patches. TRIANGLE A Triangle is a polygon with 3 distinct, non-collinear vertices and no interior boundary CIRCULASTRING, COMPOUNDCURVE and CURVEPOLYGON are not described in the SFA standard, but in the SQL-MM part 3 standard. The descriptions above were copied from the PostGIS manual. 3.1.6 Text and binary encodings Part of the simple feature standard are two encodings: a text and a binary encoding. The well-known text encoding, used above, is human-readable, the well-known binary encoding is machine-readable. Binary encodings are lossless and typically faster to work with than text encoding (and decoding), and are used for instance in all communications between R package sf and the GDAL, GEOS, liblwgeom and s2geometry libraries (figure 1.6). 3.2 Operations on geometries Simple feature geometries can be queried for properties, transformed into new geometries, and combinations of geometries can be queried for properties. This section gives an overview of the operations entirely focusing on geometrical properties. Chapter 5, focuses on the analysis of non-geometrical feature properties, in relationship to their geometries. Some of the material in this section appeared in (Pebesma 2018). We can categorize operations on geometries in terms of what they take as input, and what they return as output. In terms of output we have operations that return predicates: a logical asserting a certain property is TRUE, measures: a quantity (e.g. a numeric value with measurement unit), or transformations: newly generated geometries and in terms of what they operate on, we distinguish operations that are unary when they work on a single geometry binary when they work on pairs of geometries n-ary when they work on sets of geometries 3.2.1 Unary predicates Unary predicates describe a certain property of a geometry. The predicates is_simple, is_valid, and is_empty return respectively whether a geometry is simple, valid or empty. Given a coordinate reference system, is_longlat returns whether the coordinates are geographic or projected. is(geometry, class) checks whether a geometry belongs to a particular class. 3.2.2 Binary predicates and DE-9IM The Dimensionally Extended nine-Intersection Model (DE-9IM, (Clementini, Di Felice, and Oosterom 1993; Egenhofer and Franzosa 1991)) is a model that helps describing the qualitative relation between any two geometries in two-dimensional space (\\(R^2\\)). Any geometry has a dimension value that is 0 for points, 1 for linear geometries, 2 for polygonal geometries, and F (false) for empty geometries. Any geometry also has an inside (I), a boundary (B) and an exterior (E); these roles are obvious for polygons but e.g. for lines the boundary is formed by the end points, and the interior by all non-end points on the line, points have a zero-dimensional inside but no boundary library(sf) polygon = po = st_polygon(list(rbind(c(0,0), c(1,0), c(1,1), c(0,1), c(0,0)))) p0 = st_polygon(list(rbind(c(-1,-1), c(2,-1), c(2,2), c(-1,2), c(-1,-1)))) line = li = st_linestring(rbind(c(.5, -.5), c(.5, 0.5))) s = st_sfc(po, li) par(mfrow = c(3,3)) par(mar = c(1,1,1,1)) # &quot;1020F1102&quot; # 1: 1 plot(s, col = c(NA, &#39;darkgreen&#39;), border = &#39;blue&#39;, main = expression(paste(&quot;I(pol)&quot;,intersect(),&quot;I(line) = 1&quot;))) lines(rbind(c(.5,0), c(.5,.495)), col = &#39;red&#39;, lwd = 2) points(0.5, 0.5, pch = 1) # 2: 0 plot(s, col = c(NA, &#39;darkgreen&#39;), border = &#39;blue&#39;, main = expression(paste(&quot;I(pol)&quot;,intersect(),&quot;B(line) = 0&quot;))) points(0.5, 0.5, col = &#39;red&#39;, pch = 16) # 3: 2 plot(s, col = c(NA, &#39;darkgreen&#39;), border = &#39;blue&#39;, main = expression(paste(&quot;I(pol)&quot;,intersect(),&quot;E(line) = 2&quot;))) plot(po, col = &#39;#ff8888&#39;, add = TRUE) plot(s, col = c(NA, &#39;darkgreen&#39;), border = &#39;blue&#39;, add = TRUE) # 4: 0 plot(s, col = c(NA, &#39;darkgreen&#39;), border = &#39;blue&#39;, main = expression(paste(&quot;B(pol)&quot;,intersect(),&quot;I(line) = 0&quot;))) points(.5, 0, col = &#39;red&#39;, pch = 16) # 5: F plot(s, col = c(NA, &#39;darkgreen&#39;), border = &#39;blue&#39;, main = expression(paste(&quot;B(pol)&quot;,intersect(),&quot;B(line) = F&quot;))) # 6: 1 plot(s, col = c(NA, &#39;darkgreen&#39;), border = &#39;blue&#39;, main = expression(paste(&quot;B(pol)&quot;,intersect(),&quot;E(line) = 1&quot;))) plot(po, border = &#39;red&#39;, col = NA, add = TRUE, lwd = 2) # 7: 1 plot(s, col = c(NA, &#39;darkgreen&#39;), border = &#39;blue&#39;, main = expression(paste(&quot;E(pol)&quot;,intersect(),&quot;I(line) = 1&quot;))) lines(rbind(c(.5, -.5), c(.5, 0)), col = &#39;red&#39;, lwd = 2) # 8: 0 plot(s, col = c(NA, &#39;darkgreen&#39;), border = &#39;blue&#39;, main = expression(paste(&quot;E(pol)&quot;,intersect(),&quot;B(line) = 0&quot;))) points(.5, -.5, col = &#39;red&#39;, pch = 16) # 9: 2 plot(s, col = c(NA, &#39;darkgreen&#39;), border = &#39;blue&#39;, main = expression(paste(&quot;E(pol)&quot;,intersect(),&quot;E(line) = 2&quot;))) plot(p0 / po, col = &#39;#ff8888&#39;, add = TRUE) plot(s, col = c(NA, &#39;darkgreen&#39;), border = &#39;blue&#39;, add = TRUE) Figure 3.2: DE-9IM: intersections between the interior, boundary and exterior of a polygon (rows) and of a linestring (columns) indicated by red Figure 3.2 shows the intersections between the I, B and E components of a polygon and a linestring indicated by red; the sub-plot title gives the dimension of these intersections (0, 1, 2 or F). The relationship between the two geometries is the concatenation of these dimensions: st_relate(polygon, line) # [,1] # [1,] &quot;1020F1102&quot; Using the ability to express relationships, we can also query pairs of geometries about particular conditions expressed in a mask string; e.g. the string \"*0*******\" would evaluate TRUE when the second geometry has one or more boundary points in common with the interior of the first geometry; the symbol * standing for “any dimensionality” (0, 1, 2 or F). The mask string \"T********\" matches pairs of geometry with intersecting interiors. Here, the symbol T stands for any non-empty intersection (of dimensionality 0, 1 or 2). Binary predicates are further described using normal-language verbs, using DE-9IM definitions. For instance, the predicate equals corresponds to the relationship \"T*F**FFF*\". If any two geometries obey this relationship, they are (topologically) equal, but may have a different ordering of nodes. A list of binary predicates is predicate meaning inverse of equals Two geometries A and B are topologically identical contains None of the points of A are outside B within contains_properly A contains B and B has no points in common with the boundary of A covers No points of B lie in the exterior of A covered_by covered_by inverse of covers crosses A and B have some but not all interior points in common disjoint A and B have no points in common intersects equals A and B are geometrically equal; node order number of nodes may differ; identical to A contains B AND A within B equals_exact A and B are geometrically equal, and have identical node order intersects A and B are not disjoint disjoint is_within_distance A is closer to B than a given distance within None of the points of B are outside A contains touches A and B have at least one boundary point in common, but no interior points overlaps A and B have some points in common; the dimension of these is identical to that of A and B relate given a mask pattern, return whether A and B adhere to this pattern The Wikipedia DE-9IM page provides the relate patterns for each of these verbs. They are important to check out; for instance covers and contains (and their inverses) are often not completely intuitive: if A contains B, B has no points in common with the exterior or boundary of A if A covers B, B has no points in common with the exterior of A 3.2.3 Unary Measures Unary measures return a measure or quantity that describes a property of the geometry: measure returns dimension 0 for points, 1 for linear, 2 for polygons, possibly NA for empty geometries area the area of a geometry length the length of a linear geometry 3.2.4 Binary Measures distance returns the distance between pairs of geometries. The qualitative measure relate (without mask) gives the relation pattern, a description of the geometrical relationship between two geometries explained in section 3.2.2. 3.2.5 Unary Transformers Unary transformations work on a per-geometry basis, and for each geometry return a new geometry. transformer returns a geometry … centroid of type POINT with the geometry’s centroid buffer that is this larger (or smaller) than the input geometry, depending on the buffer size jitter that was moved in space a certain amount, using a bivariate uniform distribution wrap_dateline cut into pieces that do no longer cover the dateline boundary with the boundary of the input geometry convex_hull that forms the convex hull of the input geometry (figure 3.3) line_merge after merging connecting LINESTRING elements of a MULTILINESTRING into longer LINESTRINGs. make_valid that is valid node with added nodes to linear geometries at intersections without a node; only works on individual linear geometries point_on_surface with a (arbitrary) point on a surface polygonize of type polygon, created from lines that form a closed ring segmentize a (linear) geometry with nodes at a given density or minimal distance simplify simplified by removing vertices/nodes (lines or polygons) split that has been split with a splitting linestring transform transformed or convert to a new coordinate reference system (chapter 2) triangulate with triangulated polygon(s) voronoi with the Voronoi tessellation of an input geometry (figure 3.3) zm with removed or added Z and/or M coordinates collection_extract with subgeometries from a GEOMETRYCOLLECTION of a particular type cast that is converted to another type + that is shifted over a given vector * that is multiplied by a scalar or matrix par(mar = rep(0,4), mfrow = c(1, 2)) plot(st_geometry(nc)[1], col = NA, border = &#39;black&#39;) plot(st_convex_hull(st_geometry(nc)[1]), add = TRUE, col = NA, border = &#39;red&#39;) box() set.seed(131) mp = st_multipoint(matrix(runif(20), 10)) plot(mp) plot(st_voronoi(mp), add = TRUE, col = NA, border = &#39;red&#39;) box() Figure 3.3: left: convex hull (red) around a polygon (black); right: Voronoi diagram (red) from a MULTIPOINT (black) 3.2.6 Binary Transformers Binary transformers are functions that return a geometry based on operating on a pair of geometries. They include function returns infix operator intersection the overlapping geometries for pair of geometries &amp; union the combination of the geometries; removes internal boundaries and duplicate points, nodes or line pieces | difference the geometries of the first after removing the overlap with the second geometry / sym_difference the combinations of the geometries after removing where they overlap %/% 3.2.7 N-ary Transformers N-ary transformers operate on sets of geometries. union can be applied to a set of geometries to return its geometrical union. Otherwise, any set of geometries can be combined into a MULTI-type geometry when they have equal dimension, or else into a GEOMETRYCOLLECTION. Without unioning, this may lead to a geometry that is not valid, e.g. because two polygon rings have a boundary line in common. N-ary intersection and difference take a single argument, but operate (sequentially) on all pairs, triples, quadruples etc. Consider the plot in figure 3.4: how do we identify the area where all three boxes overlap? Using binary intersections gives us intersections for all pairs: 1-1, 1-2, 1-3, 2-1, 2-2, 2-3, 3-1, 3-2, 3-3, but does not let us identify areas where more than two geometries intersect. Figure @(fig:boxes) (right) shows the n-ary intersection: the 7 unique, non-overlapping geometries originating from intersection of one, two, or more geometries. par(mar = rep(.1, 4), mfrow = c(1, 2)) sq = function(pt, sz = 1) st_polygon(list(rbind(c(pt - sz), c(pt[1] + sz, pt[2] - sz), c(pt + sz), c(pt[1] - sz, pt[2] + sz), c(pt - sz)))) x = st_sf(box = 1:3, st_sfc(sq(c(0,0)), sq(c(1.7, -0.5)), sq(c(0.5, 1)))) plot(st_geometry(x), col = NA, border = sf.colors(3, categorical=TRUE), lwd = 3) plot(st_intersection(st_geometry(x)), col = sf.colors(7, categorical=TRUE, alpha = .5)) Figure 3.4: left: three overlapping boxes – how do we identify the small box where all three overlap? right: unique, non-overlapping n-ary intersections Similarly, one can compute an n-ary difference from a set \\(\\{s_1, s_2, s_3, ...\\}\\) by creating differences \\(\\{s_1, s_2-s_1, s_3-s_2-s_1, ...\\}\\). This is shown in figure 3.5, left for the original set, right for the set after reversing its order to make clear that the result here depends on the ordering of the input geometries. Again, resulting geometries do not overlap. par(mar = rep(.1, 4), mfrow = c(1, 2)) xg = st_geometry(x) plot(st_difference(xg), col = sf.colors(3, alpha = .5, categorical=TRUE)) plot(st_difference(xg[3:1]), col = sf.colors(3, alpha = .5, categorical=TRUE)) Figure 3.5: difference between subsequent boxes, left: in original order; right: in reverse order 3.3 Precision Geometrical operations, such as finding out whether a certain point is on a line, may fail when coordinates are represented by double precision floating point numbers, such as 8-byte doubles used in R. An often chosen remedy is to limit the precision of the coordinates before the operation. For this, a precision model is adopted; the most common is to choose a factor \\(p\\) and compute rounded coordinates \\(c&#39;\\) from original coordinates \\(c\\) by \\[c&#39; = \\mbox{round}(p \\cdot c) / p\\] Rounding of this kind brings the coordinates to points on a regular grid with spacing \\(1/p\\), which is beneficial for geometric computations. Of course, it also affects all computations like areas and distances, and may turn valid geometries into invalid ones. Which precision values are best for which application is often a matter of common sense combined with trial and error. 3.4 Coverages: tessellations and rasters The Open Geospatial Consortium defines a coverage as a ``feature that acts as a function to return values from its range for any direct position within its spatiotemporal domain’’ (Baumann, Hirschorn, and Masó 2017). Having a function implies that for every “point”, i.e. every combination of spatial point and a moment in time of the spatiotemporal domain, we have single value for the range. This is a very common situation for spatiotemporal phenomena, a few examples can be given: boundary disputes aside, every point in a region (domain) belongs to a single administrative unit (range) at any given moment in time, every point in a region (domain) has a certain land cover type (range) every point in an area (domain) has a single elevation (range), e.g. measured with respect to a given mean sea level surface every spatiotemporal point in a three-dimensional body of air (domain) has single value for temperature (range) A caveat here is that because observation or measurement always takes time and requires space, measured values are always an average over a spatiotemporal volume, and hence range variables can rarely be measured for true, zero-volume “points”; for many practical cases however the measured volume is small enough to be considered a “point”; for a variable like land cover type the volume needs to be chosen such that the types distinguished make sense with respect to the measured units. In the first two of the given examples the range variable is categorical, in the last two the range variable is continuous. For categorical range variables, if large connected areas have a constant range value, an efficient way to represent these data is by storing the boundaries of the areas with constant value, such as country boundaries. Although this can be done (and is often done) by a set of simple feature geometries (polygons or multipolygons), but this brings along some challenges: it is hard to guarantee for such a set of simple feature polygons that they do not overlap, or that there are no gaps between them, simple features have no way of assigning points on the boundary of two adjacent polygons uniquely to a single polygon, which introduces ambiguity in terms the interpretation as coverage. 3.4.1 Topological models A data model that guarantees no inadvertent gaps or overlaps of polygonal coverages is the topological model, examples of which are found in geographic information systems (GIS) like GRASS GIS or ArcGIS. Topological models store boundaries between polygons only once, and register which polygonal area is on either side of a boundary. Deriving the set of (multi)polygons for each area with a constant range value from a topological model is straightforward; the other way around: reconstructing topology from a set of polygons typically involves setting thresholds on errors and handling gaps or overlaps. 3.4.2 Raster tessellations A tessellation is a subdivision of a space (area, volume) into smaller elements by ways of polygons. A regular tessellation does this with regular polygons: triangles, squares or hexagons. Tessellations using squares are very commonly used for spatial data, and are called raster data. Raster data tessellate each spatial dimension \\(d\\) into regular cells, formed e.g. by left-closed and right-open intervals \\(d_i\\): \\[\\begin{equation} d_i = d_0 + [i \\times \\delta, (i+1) \\times \\delta) \\end{equation}\\] with \\(d_0\\) an offset, \\(\\delta\\) the interval (cell or pixel) size, and where the cell index \\(i\\) is an arbitrary but consecutive set of integers. The \\(\\delta\\) value is often taken negative for the \\(y\\)-axis (Northing), indicating that raster row numbers increasing Southwards correspond to \\(y\\)-coordinates increasing Northwards. Where in arbitrary polygon tessellations the assignment of points to polygons is ambiguous for points falling on a boundary shared by two polygons, using left-closed “[” and right-open “)” intervals in regular tessellations removes this ambiguity. This means that for rasters with negative \\(\\delta\\) values for the \\(y\\)-coordinate and positive for the \\(x\\)-coordinate, only the top-left corner point is part of each raster cell. An artifact resulting from this is shown in figure 3.6. library(stars) ls = st_sf(a = 1:2, st_sfc(st_linestring(rbind(c(0.1, 0), c(1, .9))))) # Warning in data.frame(..., check.names = FALSE): row names were found from a # short variable and have been discarded grd = st_as_stars(st_bbox(ls), nx = 10, ny = 10, xlim = c(0, 1.0), ylim = c(0, 1), values = -1) r = st_rasterize(ls, grd, options = &quot;ALL_TOUCHED=TRUE&quot;) r[r == -1] = NA plot(st_geometry(st_as_sf(grd)), border = &#39;orange&#39;, col = NA, reset = FALSE, key.pos=NULL) plot(r, axes = TRUE, add = TRUE) # ALL_TOUCHED=FALSE; # Warning in plot.stars(r, axes = TRUE, add = TRUE): breaks=&quot;quantile&quot; leads to a # single class; maybe try breaks=&quot;equal&quot; instead? plot(ls, add = TRUE, col = &quot;red&quot;, lwd = 2) Figure 3.6: rasterization artifact: as only top-left corners are part of the raster cell, only cells below the diagonal line are rasterized Tessellating the time dimension with left-closed, right-open intervals is very common, and reflects the implicit assumption underlying time series software such as the xts package in R, where time stamps indicate the start of time intervals. Different models can be combined: one could use simple feature polygons to tessellate space, and combine this with a regular tessellation of time in order to cover a space-time vector data cube. Raster and vector data cubes are discussed in chapter ??. As mentioned above, besides square cells the other two shapes that can lead to regular tessellations of \\(R^2\\) are triangles and hexagons. On the sphere, there are few more, including cube, octahedron, icosahedron and dodecahedron. A spatial index that builds on the cube is s2geometry, the H3 library uses the icosahedron and densifies that with (mostly) hexagons. Mosaics that cover the entire Earth are also called discrete global grids. 3.5 Networks Spatial networks are typically composed of linear (LINESTRING) elements, but possess further topological properties describing the network coherence: start and endpoints of a linestring may be connected to other linestring start or end points, forming a set of nodes and edges edges may be directed, to only allow for connection (flow, transport) in one way. Several R packages (osmar, stplanr, sfnetworks) have functionality available for constructing network objects, and working with them, e.g. computing shortest or fastest routes through a network. References "],["spherical.html", "Chapter 4 Spherical Geometries 4.1 Straight lines 4.2 Ring direction 4.3 Full polygon 4.4 Bounding box, rectangle, and cap 4.5 Validity on the sphere", " Chapter 4 Spherical Geometries The previous chapter discussed geometries defined on the plane, \\(R^2\\). This chapter discusses what changes when we consider geometries not on the plane, but on the sphere (\\(S^2\\)). Although we learned in chapter 2 that the shape of the Earth is usually approximated by an ellipsoid, none of the libraries shown in green in figure 1.6 provide access to a comprehensive set of functions using ellipsoidal geometries, whereas the s2geometry (Dunnington, Pebesma, and Rubak 2021; Veach et al. 2020) library does provide it using a sphere rather than an ellipsoid. However, when compared to using a flat (projected) space we did in the previous chapter, a sphere is a much better approximation to an ellipsoid. 4.1 Straight lines The basic premise of simple features of chapter 3 is that geometries are represented by sequences of points connected by straight lines. On \\(R^2\\) (or any Cartesian space), this is is trivial, but on a sphere straight lines do not exist. The shortest line connecting two points is an arc of the circle through both points and the center of the sphere, also called a great circle segment. A consequence is that “the” shortest distance line connecting two points on opposing sides of the sphere does not exist, as any great circle segment connecting them has equal length. 4.2 Ring direction Any polygon on the sphere divides the sphere surface in two parts with finite area: the inside and the outside. Using the “counter clockwise rule” as was done for \\(R^2\\) will not work, because the direction interpretation depends on what is defined as inside. The convention here is to define the inside as the left (or right) side of the polygon boundary when traversing its points in sequence. Reversal of the node order then switches inside and outside. 4.3 Full polygon In addition to empty polygons, on a sphere one can define the full polygon, which comprises its entire surface. This is useful, for instance for computing the oceans as the geometric difference between the full polygon and those of the land mass. 4.4 Bounding box, rectangle, and cap Where in \\(R^2\\) one can easily define bounding boxes as the range of the \\(x\\) and \\(y\\) coordinates, for ellipsoidal coordinates these ranges are not of much use when geometries cross the antimeridian (longitude +/- 180) or one of the poles. The assumption in \\(R^2\\) that lower \\(x\\) values are Westwards of higher ones does not hold when crossing the antimeridian. An alternative to delineating an area on a sphere that is more natural is the bounding cap, defined by its center coordinates and a radius. For Antarctica, as depicted in figure 4.1 (a) and (c), the bounding box formed by coordinate ranges is library(sf) suppressPackageStartupMessages(library(maps)) # maps: par(mfrow = c(2,2)) par(mar = c(1,1.2,1,1)) m = st_as_sf(map(fill=TRUE, plot=FALSE)) a = m[m$ID == &quot;Antarctica&quot;, ] st_bbox(a) # xmin ymin xmax ymax # -180.0 -85.2 179.6 -60.5 which clearly does not contain the region (ymin being -90 and xmax 180). Two geometries that do contain the region are the bounding cap: library(s2) s2_bounds_cap(a) # lng lat angle # 1 0 -90 29.5 and the bounding rectangle: s2_bounds_rect(a) # lng_lo lat_lo lng_hi lat_hi # 1 -180 -90 180 -60.5 For an area spanning the antimeridian, here the Fiji island country, the bounding box st_bbox(m[m$ID == &quot;Fiji&quot;,]) # xmin ymin xmax ymax # -179.9 -21.7 180.2 -12.5 seems to span most of the Earth, as opposed to the bounding rectangle s2_bounds_rect(m[m$ID == &quot;Fiji&quot;,]) # lng_lo lat_lo lng_hi lat_hi # 1 175 -21.7 -178 -12.5 where a value lng_lo larger than lng_hi indicates that the bounding rectangle spans the antimeridian. This property could not be inferred from the coordinate ranges. 4.5 Validity on the sphere Many global datasets are given in ellipsoidal coordinates but are prepared in a way that they “work” when interpreted on the \\(R^2\\) space [-180,180] \\(\\times\\) [-90,90]. This means that geometries crossing the antimeridian (longitude +/- 180) are cut in halves, such that they no longer cross it (but nearly touch each other) geometries including a pole, like Antarctica, are cut at +/- 180 and make an excursion through -180,-90 and 180,90 (both representing the Geographic South Pole). Figure 4.1 shows two different representation of Antarctica, plotted with ellipsoidal coordinates taken as \\(R^2\\) (top) and in a Polar Stereographic projection (bottom), without (left) and with (right) an excursion through the Geographic South Pole. In the projections as plotted, polygons (b) and (c) are valid; polygon (a) is not valid as it self-intersects, polygon (d) is not valid because it traverses the same edge to the South Pole twice. On the sphere (\\(S^2\\)), polygon (a) is valid but (b) is not, for the same reason as (d) is not valid. library(sf) suppressPackageStartupMessages(library(maps)) # maps: par(mfrow = c(2,2)) par(mar = c(1,1.2,1,1)) m = st_as_sf(map(fill=TRUE, plot=FALSE)) m = m[m$ID == &quot;Antarctica&quot;, ] plot(st_geometry(m), asp = 2) title(&quot;a (not valid)&quot;) # ne: library(rnaturalearth) ne = ne_countries(returnclass = &quot;sf&quot;) ne = ne[ne$region_un == &quot;Antarctica&quot;, &quot;region_un&quot;] plot(st_geometry(ne), asp = 2) title(&quot;b (valid)&quot;) # 3031 m %&gt;% st_geometry() %&gt;% st_transform(3031) %&gt;% plot() title(&quot;c (valid)&quot;) ne %&gt;% st_geometry() %&gt;% st_transform(3031) %&gt;% plot() title(&quot;d (not valid)&quot;) Figure 4.1: different representations of Antarctica, (a, c): with a polygon not passing through (-180 -90); (b, d): with a polygon passing through (-180 -90) and (180 -90) References "],["featureattributes.html", "Chapter 5 Attributes and Support 5.1 Attribute-geometry relationships and support 5.2 Aggregating and Summarising 5.3 Area-weighted interpolation 5.4 Up- and Downscaling 5.5 Exercises", " Chapter 5 Attributes and Support Feature attributes refer to the properties of features (“things”) that do not describe the feature’s geometry. Feature attributes can be derived from geometry (e.g. length of a LINESTRING, area of a POLYGON) but they can also refer to non-derived properties, such as the name of a street or a county, the number of people living in a country, the type of a road the soil type in a polygon from a soil map. the opening hours of a shop the body weight or heart beat rate of an animal the NO\\(_2\\) concentration measured at an air quality monitoring station In some cases, time properties can be seen as attributes of features, e.g. the date of birth of a person or the construction year of a road. When an attribute such as for instance air quality is a function of both space and time, time is best handled on equal footing with geometry (e.g. in a data cube, see chapter 6). Spatial data science software implementing simple features typically organizes data in tables that contain both geometries and attributes for features; this is true for geopandas in Python, PostGIS tables in PostgreSQL, and sf objects in R. The geometric operations described in section 3.2 operate on geometries only, and may occasionally yield new attributes (predicates, measures or transformations), but do not operate on attributes present. When, while manipulating geometries, attribute values are retained unmodified, support problems may arise. If we look into a simple case of replacing a county polygon with the centroid of that polygon on a dataset that has attributes, we see that R package sf issues a warning: library(sf) library(dplyr) system.file(&quot;gpkg/nc.gpkg&quot;, package=&quot;sf&quot;) %&gt;% read_sf() %&gt;% st_transform(32119) %&gt;% select(BIR74, SID74, NAME) %&gt;% st_centroid() -&gt; x # Warning in st_centroid.sf(.): st_centroid assumes attributes are constant over # geometries of x The reason for this is that the dataset contains variables with values that are associated with entire polygons – in this case: population counts – meaning they are not associated with a POINT geometry replacing the polygon. In section 1.6 we already described that for non-point geometries (lines, polygons), feature attribute values either have point support, meaning that the value applies to every point, or they have block support, meaning that the value summarizes all points in the geometry. (More complex options, e.g. in between these two extremes, may also occur.) This chapter will describe different ways in which an attribute may relate to the geometry, its consequences on analysing such data, and ways to derive attribute data for different geometries (up- and downscaling). 5.1 Attribute-geometry relationships and support Changing the feature geometry without changing the feature attributes does change the feature, since the feature is characterised by the combination of geometry and attributes. Can we, ahead of time, predict whether the resulting feature will still meaningfully relate to the attribute value when we replace all geometries for instance with their convex hull or centroid? It depends. Take the example of a road, represented by a LINESTRING, which has an attribute property road width equal to 10 m. What can we say about the road width of an arbitrary subsection of this road? That depends on whether the attribute road length describes, for instance the road width everywhere, meaning that road width is constant along the road, or whether it describes an aggregate property, such as minimum or average road width. In case of the minimum, for an arbitrary subsection of the road one could still argue that the minimum road width must be at least as large as the minimum road width for the whole segment, but it may no longer be the minimum for that subsection. This gives us two “types” for the attribute-geometry relationship (AGR): constant the attribute value is valid everywhere in or over the geometry; we can think of the feature as consisting of an infinite number of points that all have this attribute value; in the geostatistical literature this is known as a variable with point support; aggregate the attribute is an aggregate, a summary value over the geometry; we can think of the feature as a single observation with a value that is associated with the entire geometry; this is also known as as a variable having block support. For polygon data, typical examples of constant AGR (point support) variables are land use for a land use polygon rock units or geologic strata in a geological map soil type in a soil map elevation class in a elevation map that shows elevation as classes climate zone in a climate zone map A typical property of such variables is that they have geometries that are not man-made and also not associated with a sensor device (such as remote sensing image pixel boundaries). Instead, the geometry follows from the variable observed. Examples for the aggregate AGR (block support) variables are population, either as number of persons or as population density other socio-economic variables, summarised by area average reflectance over a remote sensing pixel total emission of pollutants by region block mean NO\\(_2\\) concentrations, as e.g. obtained by block kriging over square blocks or a dispersion model that predicts areal means A typical property of such variables is that associated geometries come for instance from legislation, observation devices or analysis choices, but not intrinsically from the observed variable. A third type of AGR arises when an attribute identifies a feature geometry; we call an attribute an identity variable when the associated geometry uniquely identifies the variable’s value (there are no other geometries with the same value). An example is county name: the name identifies the county, and is still the county for any sub-area (point support), but for arbitrary sub-areas, the attributes loses the identity property to become a constant attribute. An example is: an arbitrary point (or region) inside a county is still part of the county and must have the same value for county name, but it does not longer identify the (entire) geometry corresponding to that county. The challenge here is that spatial information (ignoring time for simplicity) belongs to different phenomena types (e.g. Scheider et al. 2016), including fields: where over continuous space, every location corresponds to a single value, e.g. elevation, air quality, or land use, objects: found at a discrete set of locations, e.g. houses or persons, aggregates: e.g. sums, totals, averages of fields, counts or densities of objects, associated with lines or regions, but that different spatial geometry types (points, lines, polygons, raster cells) have no simple mapping to these phenomena types: points may refer to sample locations of observations on fields (air quality) or to locations of objects, lines may be used for objects (roads, rivers), contours of a field, or administrative borders, raster pixels and polygons may reflect fields of a categorical variable such as land use (coverage), but also aggregates such as population density. Properly specifying attribute-geometry relationships, and warning against their absence or cases when change in geometry (change of support) implies a change of information can help avoiding a large class of common spatial data analysis mistakes (Stasch et al. 2014) associated with the support of spatial data. 5.2 Aggregating and Summarising Aggregating records in a table (or data.frame) involves two steps grouping records based on a grouping predicate, and applying an aggregation function to the attribute values of a group to summarize them into a single number. In SQL, this looks for instance like SELECT GroupID, SUM(population) FROM table GROUP BY GroupID; indicating the aggregation function (SUM) and the grouping predicate (GroupID). R package dplyr for instance uses two steps to accomplish this: function group_by specifies the group membership of records, summarize computes data summaries (such as sum or mean) for each of the groups. R (base) function aggregate does both in a single function that takes the data table, the grouping predicate(s) and the aggregation function. An example for the North Carolina counties is shown in figure 5.1. Here, we grouped counties by their position (according to the quadrant in which the county centroid is with respect to ellipsoidal coordinate POINT(-79, 35.5)) and counted the number of disease cases per group. The result shows that the geometries of the resulting groups have been unioned (section 3.2.6): this is necessary because the MULTIPOLYGON formed by just putting all the county geometries together would have many duplicate boundaries, and hence not be valid (section 3.1.2). nc &lt;- read_sf(system.file(&quot;gpkg/nc.gpkg&quot;, package=&quot;sf&quot;)) #plot(nc[&quot;SID74&quot;], axes = TRUE) sf_use_s2(TRUE) # encode quadrant by two logicals: nc$lng = st_coordinates(st_centroid(st_geometry(nc)))[,1] &gt; -79 nc$lat = st_coordinates(st_centroid(st_geometry(nc)))[,2] &gt; 35.5 nc.grp = aggregate(nc[&quot;SID74&quot;], list(nc$lng, nc$lat), sum) plot(nc.grp[&quot;SID74&quot;], axes = TRUE) Figure 5.1: top: SID74 counts by county quadrant, with county polygons unioned by county quadrant Plotting collated county polygons is technically not a problem, but for this case would raise the wrong suggestion that the group sums relate to the counties, and not the group of counties. One particular property of aggregation in this way is that each record is assigned to a single group; this has the advantage that the sum of the group-wise sums equals the sum of the ungrouped data: for variables that reflect amount, nothing gets lost and nothing is added. The newly formed geometry is the result of unioning the geometries of the records. nc &lt;- st_transform(nc, 2264) gr = st_sf( label = apply(expand.grid(1:10, LETTERS[10:1])[,2:1], 1, paste0, collapse = &quot; &quot;), geom = st_make_grid(nc)) plot(st_geometry(nc), reset = FALSE, border = &#39;grey&#39;) plot(st_geometry(gr), add = TRUE) Figure 5.2: Example target blocks layed out over North Carolina counties When we need an aggregate for a new area that is not a union of the geometries for a group of records, and we use a spatial predicate then single records may be matched to multiple groups. When taking the rectangles of figure 5.2 as the target areas, and summing for each rectangle the disease cases of the counties that intersect with the rectangles of figure 5.2, the sum of these will be much larger: a = aggregate(nc[&quot;SID74&quot;], gr, sum) c(sid74_sum_counties = sum(nc$SID74), sid74_sum_rectangles = sum(a$SID74, na.rm = TRUE)) # sid74_sum_counties sid74_sum_rectangles # 667 2621 Choosing another predicate, e.g. contains or covers would on the contrary result in much smaller values, because many counties are not contained by any the target geometries. However, there are a few cases where this approach might be good or satisfactory: when we want to aggregate POINT geometries by a set of polygons, and all points are contained by a single polygon. If points fall on a shared boundary than they are assigned to both polygons (this is the case for DE-9IM-based GEOS library; the s2geometry library has the option to define polygons as “semi-open”, which implies that points are assigned to single polygons when the polygons form a coverage) when aggregating many very small polygons or raster pixels over larger areas, e.g. averaging altitude from 30 m resolution raster over North Carolina counties, the error made by multiple matches may be insignificant. A more comprehensive approach to aggregating spatial data associated to areas to larger, arbitrary shaped areas is by using area-weighted interpolation. 5.3 Area-weighted interpolation When we want to combine geometries and attributes of two datasets such that we get attribute values of a source dataset summarised for the geometries of a target, where source and target geometries are unrelated, area-weighted interpolation may be a simple approach. In effect, it considers the area of overlap of the source and target geometries, and uses that to weight the source attribute values into the target value (Goodchild and Lam 1980; Do, Thomas-Agnan, and Vanhems 2015a, 2015b). Here, we follow the notation of (Do, Thomas-Agnan, and Vanhems 2015b). Area-weighted interpolation computes for each of \\(q\\) spatial target areas \\(T_j\\) a weighted average from the values \\(Y_i\\) corresponding to the \\(p\\) spatial source areas \\(S_i\\), \\[\\begin{equation} \\hat{Y}_j(T_j) = \\sum_{i=1}^p w_{ij} Y_i(S_i) \\tag{5.1} \\end{equation}\\] where the \\(w_{ij}\\) depend on the amount of overlap of \\(T_j\\) and \\(S_i\\), \\(A_{ij} = T_j \\cap S_i\\). Different options exist for choosing weights, including methods using external variables (e.g. dasymetric mapping, (Mennis 2003)). Two simple approaches for computing weights that do not use external variables arise, depending on whether the variable \\(Z\\) is intensive or extensive. 5.3.1 Spatially extensive and intensive variables An example of an extensive variable is population count. It is associated with an area, and if that area is cut into smaller areas, the population count is split accordingly: not necessary proportional to area, because population is rarely uniform, but split in such a way that the sum of the population count for the smaller areas equals that of the total. An example of a related variable that is intensive is population density. If an an area is split into smaller areas, population density is not split similarly: the sum of the population densities for the smaller areas is a meaningless measure, as opposed to the average of the population densities which will be similar to the density of the total area. Extensive variables correspond to amounts, associated with a physical size (length, area, volume); for spatially extensive variables, if the area a value corresponds to is cut in parts, the values associated with the sub-area are split accordingly. In other words: the value is proportional to the support. Intensive variables are variables that do not have values proportional to support: if the area is split, values may vary but on average remain the same. The corresponding example of an intensive variable is population density: when we split an area into sub-areas, the sub-areas either have identical population densities (in case population is uniformly distributed) or, more realistically, have varying population densities that by necessity are both higher and lower than the density of the total area. When we assume that the extensive variable \\(Y\\) is uniformly distributed over space, the value \\(Y_{ij}\\), derived from \\(Y_i\\) for a sub-area of \\(S_i\\), \\(A_{ij} = T_j \\cap S_i\\) of \\(S_i\\) is \\[\\hat{Y}_{ij}(A_{ij}) = \\frac{|A_{ij}|}{|S_i|} Y_i(S_i)\\] where \\(|\\cdot|\\) denotes the spatial area. For estimating \\(Y_j(T_j)\\) we sum all the elements over area \\(T_j\\): \\[\\begin{equation} \\hat{Y}_j(T_j) = \\sum_{i=1}^p \\frac{|A_{ij}|}{|S_i|} Y_i(S_i) \\tag{5.2} \\end{equation}\\] For an intensive variable, under the assumption that the variable has a constant value over each area \\(S_i\\), the estimate for a sub-area equals that of the total, \\[\\hat{Y}_{ij} = Y_i(S_i)\\] and we can estimate the value of \\(Y\\) for a new spatial unit \\(T_j\\) by an area-weighted average of the source values: \\[\\begin{equation} \\hat{Y}_j(T_j) = \\sum_{i=1}^p \\frac{|A_{ij}|}{|T_j|} Y_i(S_i) \\tag{5.3} \\end{equation}\\] 5.3.2 Dasymetric mapping Dasymetric mapping distributes variables, such as population, known at a course spatial aggregation level over finer spatial units by using other variables that are associated with population distribution, such as land use, building density, or road density. The simplest approach to dasymetric mapping is obtained for extensive variables, where the ratio \\(|A_{ij}| / |S_i|\\) in (5.2) is replaced by the ratio of another extensive variable \\(X_{ij}(S_{ij})/X_i(S_i)\\), which has to be known for both the intersecting regions \\(S_{ij}\\) and the source regions \\(S_i\\). (Do, Thomas-Agnan, and Vanhems 2015b) discuss several alternatives for intensive \\(Y\\) and/or \\(X\\), and cases where \\(X\\) is known for other areas. 5.4 Up- and Downscaling Up- and downscaling refers in general to obtaining high-resolution information from low-resolution data (downscaling) or obtaining low-resolution information from high-resolution data (upscaling). Both are activities involve attributes’ relation to geometries and both change support. They are synonymous with aggregation (upscaling) and disaggregation (downscaling). The simplest form of downscaling is sampling (or extracting) polygon, line or grid cell values at point locations. This works well for variables with point-support (“constant” AGR), but is at best approximate when the values are aggregates. Challenging applications for downscaling include high-resolution prediction of variables obtained by low-resolution weather prediction models or climate change models, and the high-resolution prediction of satellite image derived variables based on the fusion of sensors with different spatial and temporal resolutions. The application of areal interpolation using (5.1) with its realisations for extensive (5.2) and intensive (5.3) variables allows moving information from any source area \\(S_i\\) to any target area \\(T_j\\) as long as the two areas have some overlap. This means that one can go arbitrarily to much larger units (aggregation) or to much smaller units (disaggregation). Of course this makes only sense to the extent that the assumptions hold: over the source regions extensive variables need to be uniformly distributed and intensive variables need to have constant value. The ultimate disaggregation involves retrieving (extracting) point values from line or area data. For this, we cannot work with equations (5.2) or (5.3) because \\(|A_{ij}| = 0\\) for points, but under the assumption of having a constant value over the geometry, for intensive variables the value \\(Y_i(S_i)\\) can be assigned to points as long as all points can be uniquely assigned to a single source area \\(S_i\\). For polygon data, this implies that \\(Y\\) needs to be a coverage variable (section 3.4). In cases where values associated with areas are aggregate values over the area, the assumptions made by area-weighted interpolation or dasymetric mapping – uniformity or constant values over the source areas – are highly unrealistic. In such cases, these simple approaches still be reasonable approximations, for instance when the source and target area are nearly identical the variability inside source units is very small, and the variable is nearly uniform or constant. In other cases, results obtained using these methods are merely consequences of unjustified assumptions. Statistical aggregation methods that can estimate quantities for larger regions from points or smaller regions include design-based methods, which require that a probability sample is available from the target region, with known inclusion probabilities (D. J. Brus 2021a), and model-based methods, which assume a random field model with spatially correlated values (block kriging, section 12.5). Alternative disaggregation methods include deterministic, smoothing-based approaches such as kernel- or spline-based smoothing methods (Tobler 1979; Martin 1989) statistical, model-based approaches: area-to-area and area-to-point kriging (Kyriakidis 2004), see section 12.10 5.5 Exercises References "],["datacube.html", "Chapter 6 Data Cubes 6.1 A four-dimensional data cube 6.2 Dimensions, attributes, and support 6.3 Operations on data cubes 6.4 Aggregating raster to vector cubes 6.5 Switching dimension with attributes 6.6 Other dynamic data 6.7 Exercises", " Chapter 6 Data Cubes Data cubes arise naturally when we observe properties of a set of geometries repeatedly over time. Time information may sometimes be considered as an attribute of a feature, e.g. when we register the year of construction of a building, or the date of birth of a person (chapter 5). In other cases it may refer to the time of observing an attribute, or the time for which a prediction of an attribute has been made. In these cases, time is on equal footing with space, and time and space together describe the physical dimensions over which we observe, model, make predictions or make forecasts. One way of considering our world is that of a four-dimensionsal space, with three space and one time dimension. In that view, events become “things” or “objects” that have as duration their size on the time dimension (Galton 2004). Although such a view does not align well with how we experience and describe the world, from a data analytic perspective, four numbers, along with their reference systems, suffice to describe space and time coordinates of an observation associated with a point location and time instance. We define data cubes as array data with one or more array dimensions associated with space and/or time (Lu, Appel, and Pebesma 2018). This implies that raster data, features with attributes, and time series data are all special cases of data cubes. Since we don’t restrict to three-dimensionsal structures, we actually mean hypercubes rather than cubes, and as the cube extent of the different dimensions does not have to be identical, or have comparable units, the better term would be hyperrectangle. For simplicity, we talk about data cubes instead. A cannonical form of a data cube is shown in figure 6.1: it shows in a perspective plot a set of raster layers for the same region that were collected (observed, or modelled) at different time steps. The three cube dimensions, longitude, latitude and time are thought of as being orthogonal. Arbitrary two-dimensional cube slices are obtained by fixing one of the dimensions at a particular value, one-dimensional slices are obtained by fixing two of the dimensions at a particular value, and a scalar is obtained by fixing three dimensions at a particular value. # (C) 2019, Edzer Pebesma, CC-BY-SA set.seed(1331) suppressPackageStartupMessages(library(stars)) library(colorspace) tif = system.file(&quot;tif/L7_ETMs.tif&quot;, package = &quot;stars&quot;) r = read_stars(tif) nrow = 5 ncol = 8 m = r[[1]][1:nrow,1:ncol,1] dim(m) = c(x = nrow, y = ncol) # named dim s = st_as_stars(m) # s attr(s, &quot;dimensions&quot;)[[1]]$delta = 3 attr(s, &quot;dimensions&quot;)[[2]]$delta = -.5 attr(attr(s, &quot;dimensions&quot;), &quot;raster&quot;)$affine = c(-1.2, 0.0) plt = function(x, yoffset = 0, add, li = TRUE) { attr(x, &quot;dimensions&quot;)[[2]]$offset = attr(x, &quot;dimensions&quot;)[[2]]$offset + yoffset l = st_as_sf(x, as_points = FALSE) pal = sf.colors(10) if (li) pal = lighten(pal, 0.3 + rnorm(1, 0, 0.1)) if (! add) plot(l, axes = FALSE, breaks = &quot;equal&quot;, pal = pal, reset = FALSE, border = grey(.75), key.pos = NULL, main = NULL, xlab = &quot;time&quot;) else plot(l, axes = TRUE, breaks = &quot;equal&quot;, pal = pal, add = TRUE, border = grey(.75)) u = st_union(l) # print(u) plot(st_geometry(u), add = TRUE, col = NA, border = &#39;black&#39;, lwd = 2.5) } pl = function(s, x, y, add = TRUE, randomize = FALSE) { attr(s, &quot;dimensions&quot;)[[1]]$offset = x attr(s, &quot;dimensions&quot;)[[2]]$offset = y m = r[[1]][y + 1:nrow,x + 1:ncol,1] if (randomize) m = m[sample(y + 1:nrow),x + 1:ncol] dim(m) = c(x = nrow, y = ncol) # named dim s[[1]] = m plt(s, 0, add) plt(s, 1, TRUE) plt(s, 2, TRUE) plt(s, 3, TRUE) plt(s, 4, TRUE) plt(s, 5, TRUE) plt(s, 6, TRUE) plt(s, 7, TRUE) plt(s, 8, TRUE, FALSE) } plot.new() par(mar = rep(0.5,4)) plot.window(xlim = c(-12,15), ylim = c(-5,10), asp=1) pl(s, 0, 0) # box() text(-10, 0, &quot;time&quot;, srt = -90, col = &#39;black&#39;) text(-5, 6.5, &quot;latitude&quot;, srt = 25, col = &#39;black&#39;) text( 5, 8.5, &quot;longitude&quot;, srt = 0, col = &#39;black&#39;) Figure 6.1: Raster data cube with dimensions latitude, longitude and time 6.1 A four-dimensional data cube # (C) 2021, Jonathan Bahlmann, CC-BY-SA # https://github.com/Open-EO/openeo.org/tree/datacube_doc/documentation/1.0/datacubes/.scripts # based on work by Edzer Pebesma, 2019, here: https://gist.github.com/edzer/5f1b0faa3e93073784e01d5a4bb60eca # plotting runs via a dummy stars object with x, y dimensions (no bands) # to not be overly dependent on an input image, time steps and bands # are displayed by replacing the matrix contained in the dummy stars object # every time something is plotted # packages, read input ---- set.seed(1331) library(stars) suppressPackageStartupMessages(library(colorspace)) suppressPackageStartupMessages(library(scales)) # make color palettes ---- blues &lt;- sequential_hcl(n = 20, h1 = 211, c1 = 80, l1 = 40, l2 = 100, p1 = 2) greens &lt;- sequential_hcl(n = 20, h1 = 134, c1 = 80, l1 = 40, l2 = 100, p1 = 2) reds &lt;- sequential_hcl(n = 20, h1 = 360, c1 = 80, l1 = 40, l2 = 100, p1 = 2) purples &lt;- sequential_hcl(n = 20, h1 = 299, c1 = 80, l1 = 40, l2 = 100, p1 = 2) greys &lt;- sequential_hcl(n = 20, h1 = 0, c1 = 0, l1 = 40, l2 = 100, p1 = 2) # matrices from raster ---- # make input matrices from an actual raster image input &lt;- read_stars(&quot;data/iceland_delta_cutout_2.tif&quot;) # this raster needs approx 6x7 format # if the input raster is changed, every image where a pixel value is written as text needs to be checked and corrected accordingly input &lt;- input[,,,1:4] warped &lt;- st_warp(input, crs = st_crs(input), cellsize = 200) # warp to approx. 6x7 pixel # these are only needed for resampling warped_highres &lt;- st_warp(warped, crs = st_crs(warped), cellsize = 100) # with different input, cellsize must be adapted # this is a bit of a trick, because 3:4 is different format than 6:7 # when downsampling, the raster of origin isn&#39;t so important anyway warped_lowres &lt;- st_warp(warped_highres[,1:11,,], crs = st_crs(warped), cellsize = 390) # plot(warped_lowres) # image(warped[,,,1], text_values = TRUE) t1 &lt;- floor(matrix(runif(42, -30, 150), ncol = 7)) # create timesteps 2 and 3 randomly t2 &lt;- floor(matrix(runif(42, -250, 50), ncol = 7)) # create dummy stars object ---- make_dummy_stars &lt;- function(x, y, d1, d2, aff) { m = warped_highres[[1]][1:x,1:y,1] # underlying raster doesn&#39;t matter because it&#39;s just dummy construct dim(m) = c(x = x, y = y) # named dim dummy = st_as_stars(m) attr(dummy, &quot;dimensions&quot;)[[1]]$delta = d1 attr(dummy, &quot;dimensions&quot;)[[2]]$delta = d2 attr(attr(dummy, &quot;dimensions&quot;), &quot;raster&quot;)$affine = c(aff, 0.0) return(dummy) } s &lt;- make_dummy_stars(6, 7, 2.5, -.5714286, -1.14) # mainly used, perspective f &lt;- make_dummy_stars(6, 7, 1, 1, 0) # flat highres &lt;- make_dummy_stars(12, 14, 1.25, -.2857143, -.57) # for resampling lowres &lt;- make_dummy_stars(3, 4, 5, -1, -2) # for resampling # matrices from image ---- make_matrix &lt;- function(image, band, n = 42, ncol = 7, t = 0) { # this is based on an input image with &gt;= 4 input bands # n is meant to cut off NAs, ncol is y, t is random matrix for time difference return(matrix(image[,,,band][[1]][1:n], ncol = ncol) - t) # before: b3 &lt;- matrix(warped[,,,1][[1]][1:42], ncol = 7) - t2 } # now use function: b1 &lt;- make_matrix(warped, 1) b2 &lt;- make_matrix(warped, 1, t = t1) b3 &lt;- make_matrix(warped, 1, t = t2) g1 &lt;- make_matrix(warped, 2) g2 &lt;- make_matrix(warped, 2, t = t1) g3 &lt;- make_matrix(warped, 2, t = t2) r1 &lt;- make_matrix(warped, 3) r2 &lt;- make_matrix(warped, 3, t = t1) r3 &lt;- make_matrix(warped, 3, t = t2) n1 &lt;- make_matrix(warped, 4) n2 &lt;- make_matrix(warped, 4, t = t1) n3 &lt;- make_matrix(warped, 4, t = t2) # plot functions ---- plt = function(x, yoffset = 0, add, li = TRUE, pal, print_geom = TRUE, border = .75, breaks = &quot;equal&quot;) { # pal is color palette attr(x, &quot;dimensions&quot;)[[2]]$offset = attr(x, &quot;dimensions&quot;)[[2]]$offset + yoffset l = st_as_sf(x, as_points = FALSE) if (li) pal = lighten(pal, 0.2) # + rnorm(1, 0, 0.1)) if (! add) plot(l, axes = FALSE, breaks = breaks, pal = pal, reset = FALSE, border = grey(border), key.pos = NULL, main = NULL, xlab = &quot;time&quot;) else plot(l, axes = TRUE, breaks = breaks, pal = pal, add = TRUE, border = grey(border)) u = st_union(l) # print(u) if(print_geom) { plot(st_geometry(u), add = TRUE, col = NA, border = &#39;black&#39;, lwd = 2.5) } else { # not print geometry } } pl_stack = function(s, x, y, add = TRUE, nrM, imgY = 7, inner = 1) { # nrM is the timestep {1, 2, 3}, cause this function # prints all 4 bands at once attr(s, &quot;dimensions&quot;)[[1]]$offset = x attr(s, &quot;dimensions&quot;)[[2]]$offset = y # m = r[[1]][y + 1:nrow,x + 1:ncol,1] m &lt;- eval(parse(text=paste0(&quot;n&quot;, nrM))) s[[1]] = m[,c(imgY:1)] # turn around to have same orientation as flat plot plt(s, 0, TRUE, pal = purples) m &lt;- eval(parse(text=paste0(&quot;r&quot;, nrM))) s[[1]] = m[,c(imgY:1)] plt(s, 1*inner, TRUE, pal = reds) m &lt;- eval(parse(text=paste0(&quot;g&quot;, nrM))) s[[1]] = m[,c(imgY:1)] plt(s, 2*inner, TRUE, pal = greens) m &lt;- eval(parse(text=paste0(&quot;b&quot;, nrM))) s[[1]] = m[,c(imgY:1)] plt(s, 3*inner, TRUE, pal = blues) # li FALSE deleted } # flat plot function # prints any dummy stars with any single matrix to position pl = function(s, x, y, add = TRUE, randomize = FALSE, pal, m, print_geom = TRUE, border = .75, breaks = &quot;equal&quot;) { # m is matrix to replace image with # m &lt;- t(m) attr(s, &quot;dimensions&quot;)[[1]]$offset = x attr(s, &quot;dimensions&quot;)[[2]]$offset = y # print(m) s[[1]] = m plt(s, 0, add = TRUE, pal = pal, print_geom = print_geom, border = border, breaks = breaks) #plot(s, text_values = TRUE) } print_segments &lt;- function(x, y, seg, by = 1, lwd = 4, col = &quot;black&quot;) { seg = seg * by seg[,1] &lt;- seg[,1] + x seg[,3] &lt;- seg[,3] + x seg[,2] &lt;- seg[,2] + y seg[,4] &lt;- seg[,4] + y segments(seg[,1], seg[,2], seg[,3], seg[,4], lwd = lwd, col = col) } # time series ---- # from: cube1_ts_6x7_bigger.png offset = 26 plot.new() #par(mar = c(3, 2, 7, 2)) par(mar = c(0, 0, 0, 0)) #plot.window(xlim = c(10, 50), ylim = c(-3, 10), asp = 1) plot.window(xlim = c(-15, 75), ylim = c(-3, 10), asp = 1) pl_stack(s, 0, 0, nrM = 3) pl_stack(s, offset, 0, nrM = 2) pl_stack(s, 2 * offset, 0, nrM = 1) # po &lt;- matrix(c(0,-8,7,0,15,3.5, 0,1,1,5,5,14), ncol = 2) heads &lt;- matrix(c(3.5, 3.5 + offset, 3.5 + 2*offset, 14,14,14), ncol = 2) points(heads, pch = 16) # 4 or 16 segments(c(-8, 7, 0, 15), c(-1,-1,3,3), 3.5, 14) # first stack pyramid segments(c(-8, 7, 0, 15) + offset, c(-1,-1,3,3), 3.5 + offset, 14) # second stack pyramid segments(c(-8, 7, 0, 15) + 2*offset, c(-1,-1,3,3), 3.5 + 2*offset, 14) # third stack pyramid arrows(-13, 14, 72, 14, angle = 20, lwd = 2) # timeline text(7.5, 3.8, &quot;x&quot;, col = &quot;black&quot;) text(-10, -2.5, &quot;bands&quot;, srt = 90, col = &quot;black&quot;) text(-4.5, 1.8, &quot;y&quot;, srt = 27.5, col = &quot;black&quot;) y = 15.8 text(69, y, &quot;time&quot;, col = &quot;black&quot;) text(3.5, y, &quot;2020-10-01&quot;, col = &quot;black&quot;) text(3.5 + offset, y, &quot;2020-10-13&quot;, col = &quot;black&quot;) text(3.5 + 2*offset, y, &quot;2020-10-25&quot;, col = &quot;black&quot;) Figure 6.2: Four-dimensional raster data cube with dimensions x, y, bands and time Figure 6.2 depicts a four-dimensional raster data cube, where three-dimensional raster data cubes with a spectral dimension (“bands”) are organised along a fourth dimension, a time axis. Color image data always has three bands (Blue, Green, Red), and this example has a fourth band (near infrared, NIR), which is commonly found in spectral remote sensing data. Figure 6.3 shows exactly the same data, but layed out flat as a facet plot (or scatterplot matrix), where two dimensions (\\(x\\) and \\(y\\)) are aligned with (or nested within) the dimensions bands and time, respectively. # flat ---- xlabels &lt;- seq(attr(warped, &quot;dimensions&quot;)[[1]]$offset + attr(warped, &quot;dimensions&quot;)[[1]]$delta / 2, length.out = attr(warped, &quot;dimensions&quot;)[[1]]$to, by = attr(warped, &quot;dimensions&quot;)[[1]]$delta) ylabels &lt;- seq(attr(warped, &quot;dimensions&quot;)[[2]]$offset + attr(warped, &quot;dimensions&quot;)[[2]]$delta / 2, length.out = attr(warped, &quot;dimensions&quot;)[[2]]$to, by = attr(warped, &quot;dimensions&quot;)[[2]]$delta) print_labels &lt;- function(x, y, off, lab, horizontal, cex = 1) { if(horizontal) { # x for(i in 0:(length(lab)-1)) { text(x + i*off, y, lab[i+1], cex = cex, srt = 90) } } else { # y lab &lt;- lab[length(lab):0] for(i in 0:(length(lab)-1)) { text(x, y + i*off, lab[i+1], cex = cex) } } } # before: width=1000, xlim(-2, 33), date labels x=31 plot.new() # par(mar = c(0,0,0,0)) par(mar = c(3,0,0,0)) plot.window(xlim = c(-2, 40), ylim = c(0, 25), asp = 1) pl(f, 7, 0, pal = blues, m = b1) pl(f, 7, 10, pal = blues, m = b2) pl(f, 7, 20, pal = blues, m = b3) pl(f, 14, 0, pal = greens, m = g1) pl(f, 14, 10, pal = greens, m = g2) pl(f, 14, 20, pal = greens, m = g3) pl(f, 21, 0, pal = reds, m = r1) pl(f, 21, 10, pal = reds, m = r2) pl(f, 21, 20, pal = reds, m = r3) pl(f, 28, 0, pal = purples, m = n1) pl(f, 28, 10, pal = purples, m = n2) pl(f, 28, 20, pal = purples, m = n3) print_labels(28.5, -2, 1, xlabels, horizontal = TRUE, cex = 0.7) print_labels(36, 0.5, 1, ylabels, horizontal = FALSE, cex = 0.7) # arrows(6, 27, 6, 0, angle = 20, lwd = 2) # text(5, 14, &quot;time&quot;, srt = 90, col = &quot;black&quot;) text(10, 28, &quot;blue&quot;, col = &quot;black&quot;) text(17, 28, &quot;green&quot;, col = &quot;black&quot;) text(24, 28, &quot;red&quot;, col = &quot;black&quot;) text(31, 28, &quot;nir&quot;, col = &quot;black&quot;) text(3, 23.5, &quot;2020-10-01&quot;, col = &quot;black&quot;) text(3, 13.5, &quot;2020-10-13&quot;, col = &quot;black&quot;) text(3, 3.5, &quot;2020-10-25&quot;, col = &quot;black&quot;) Figure 6.3: Four-dimensional raster data cube layed out flat over two dimensions 6.2 Dimensions, attributes, and support Phenomena in space and time can be thought of as functions with domain space and time, and with range one or more observed attributes. For clearly identifiable discrete events or objects, the range is typically discrete, and precise delineation involves describing the precise coordinates where a thing starts or stops, which is best suited by vector geometries. For continuous phenomena, variables that take on a value everywhere such as air temperature or land use type, there are infinitely many values to represent and a common approach is to discretize space and time regularly over the spatiotemporal domain (extent) of interest. This leads to a number of familiar data structures: time series, depicted as time lines for functions of time, image or raster data for two-dimensional spatial data, and time sequences of images for dynamic spatial data. The third form of this, where a variable \\(Z\\) depends on \\(x\\), \\(y\\) and \\(t\\), as in \\[Z = f(x, y, t)\\] is the archetype of a spatiotemporal array or data cube: the shape of the volume where points regularly discretizing the domain forms a cube. We call the variables that form the range (here: \\(x, y, t\\)) the cube dimensions. Data cubes may have multiple attributes, as in \\[\\{Z_1,Z_2,...,Z_p\\} = f(x, y, t)\\] and if \\(Z\\) is functional, e.g. reflectance values measured over the electromagnetic spectrum, the spectral wavelengths \\(\\lambda\\) may form an additional dimension, as in \\(Z = f(x,y,t,\\lambda)\\); section 6.5 discusses the alternative of representing color bands as attributes. Multiple time dimensions arise for instance when making forecasts for different times in the future \\(t&#39;\\) at different times \\(t\\), or when time is split into multiple dimensions (e.g. year, day-of-year, hour-of-day). The most general definition of a data cube is a functional mapping from \\(n\\) dimensions to \\(p\\) attributes: \\[\\{Z_1,Z_2,...,Z_p\\} = f(D_1,D_2,...,D_n)\\] Here, we will consider any dataset with one or more space dimensions and zero or more time dimensions a data cubes. That includes simple features (section 3.1), time series for sets of features, raster data, multi-spectral raster data (images), or time series of multi-spectral raster data (video). 6.2.1 Regular dimensions, GDAL’s geotransform Data cubes are usually stored in multi-dimensional arrays, and the usual relationship between 1-based array index \\(i\\) and an associated regularly discretized dimension variable \\(x\\) is \\[x = o_x + (i-1) d_x\\] with \\(o_x\\) the origin, and \\(d_x\\) the grid spacing for this dimension. For more general cases like those in figure 1.5b-c, the relation between \\(x\\) and \\(y\\) and array indexes \\(i\\) and \\(j\\) is \\[x = o_x + (i-1) d_x + (j-1) a_1\\] \\[y = o_y + (i-1) a_2 + (j-1) d_y\\] With two affine parameters \\(a_1\\) and \\(a_2\\); this is the so-called geotransform as used in GDAL. When \\(a_1=a_2=0\\), this reduces to the regular raster of figure 1.5a. For integer indexes, the coordinates are that of the starting edge of a grid cell, and the cell area (pixel) spans a range corresponding to index values ranging from \\(i\\) (inclusive) to \\(i+1\\) (exclusive). For most common imagery formats, \\(d_y\\) is negative, indicating that image row index increases with decreasing \\(y\\) values (southward). To get the \\(x\\)- and \\(y\\)-coordinate of the grid cell center of the top left grid cell (in case of a negative \\(d_y\\)), we use \\(i=1.5\\) and \\(j=1.5\\). For rectilinear rasters, a table that maps array index to dimension values is needed. NetCDF files for instance always stores all values of spatial dimension variables that correspond to the center of spatial grid cells, and may in addition store grid cell boundaries (which is needed to define rectlinear dimensions unambiguously). For curvilinear rasters an array that maps every combination of \\(i,j\\) into \\(x,y\\) pairs is needed, or a parametric function that does this (e.g. an projection or its inverse). NetCDF files often provide both, when available. 6.2.2 Support along cube dimensions Section 5.1 defined spatial support of an attribute variable as the size (length, area, volume) of a geometry a particular observation or prediction is associated with. The same notion applies to temporal support. Although time is rarely reported by explicit time periods having a start- en end-time, in many cases either the time stamp implies a period (e.g. ISO-8601 indications like “2021” for a full year, “2021-01” for full month) or the time period is taken as the period from the time stamp of the current record up to but not including the time stamp of the next record. An example is MODIS satellite imagery, where vegetation indexes (NDVI and EVI) are available as 16-day composites, meaning that over 16-day periods all available imagery is aggregated into a single image; such composites have temporal “block support”. Sentinel-2 or Landsat-8 data on the other hand are “snapshot” images and have temporal “point support”. When temporally aggregating data with temporal point support e.g. to monthly values one would select all images falling in the target time interval. when aggregating temporal block support imagery such as the MODIS 16-day composite, one might weigh images, e.g. according to the amount of overlap of the 16-day composite period and the target period, similar to area-weighted interpolation but over the time dimension. 6.3 Operations on data cubes 6.3.1 slicing a cube: filter Data cubes can be sliced into subcubes by fixing a dimension at a particular value. Figure 6.4 shows the sub-cubes obtained by doing with each of the dimensions. In this figure, the spatial filtering does not happen by fixing a single spatial dimension at a particular value, but by selecting a particular subregion, which is a more common operation. Fixing \\(x\\) or \\(y\\) would give a sub-cube along a transect of constant \\(x\\) or \\(y\\), which can be used to show a Hovmöller diagram, where an attribute is plotted (colored) in the space of one space and one time dimension. # filter ---- # mask &lt;- matrix(c(rep(NA, 26), 1,NA,1,NA,1,1,1, rep(NA, 9)), ncol = 7) mask &lt;- matrix(c(NA,NA,NA,NA,NA,NA, NA,NA,NA,NA,NA,NA, NA,NA,NA, 1, 1, 1, NA,NA, 1, 1, 1,NA, NA,NA,NA, 1, 1,NA, NA,NA,NA,NA,NA,NA, NA,NA,NA,NA,NA,NA), ncol = 7) print_grid &lt;- function(x, y) { pl(f, 0+x, 0+y, pal = blues, m = b1) pl(f, 0+x, 10+y, pal = blues, m = b2) pl(f, 0+x, 20+y, pal = blues, m = b3) pl(f, 7+x, 0+y, pal = greens, m = g1) pl(f, 7+x, 10+y, pal = greens, m = g2) pl(f, 7+x, 20+y, pal = greens, m = g3) pl(f, 14+x, 0+y, pal = reds, m = r1) pl(f, 14+x, 10+y, pal = reds, m = r2) pl(f, 14+x, 20+y, pal = reds, m = r3) pl(f, 21+x, 0+y, pal = purples, m = n1) pl(f, 21+x, 10+y, pal = purples, m = n2) pl(f, 21+x, 20+y, pal = purples, m = n3) } print_alpha_grid &lt;- function(x,y, alp = 0.2, geom = FALSE) { pl(f, 0+x, 0+y, pal = alpha(blues, alp), print_geom = geom, m = b1, border = 1) pl(f, 0+x, 10+y, pal = alpha(blues, alp), print_geom = geom, m = b2, border = 1) pl(f, 0+x, 20+y, pal = alpha(blues, alp), print_geom = geom, m = b3, border = 1) pl(f, 7+x, 0+y, pal = alpha(greens, alp), print_geom = geom, m = g1, border = 1) pl(f, 7+x, 10+y, pal = alpha(greens, alp), print_geom = geom, m = g2, border = 1) pl(f, 7+x, 20+y, pal = alpha(greens, alp), print_geom = geom, m = g3, border = 1) pl(f, 14+x, 0+y, pal = alpha(reds, alp), print_geom = geom, m = r1, border = 1) pl(f, 14+x, 10+y, pal = alpha(reds, alp), print_geom = geom, m = r2, border = 1) pl(f, 14+x, 20+y, pal = alpha(reds, alp), print_geom = geom, m = r3, border = 1) pl(f, 21+x, 0+y, pal = alpha(purples, alp), print_geom = geom, m = n1, border = 1) pl(f, 21+x, 10+y, pal = alpha(purples, alp), print_geom = geom, m = n2, border = 1) invisible(pl(f, 21+x, 20+y, pal = alpha(purples, alp), print_geom = geom, m = n3, border = 1)) } print_grid_filter &lt;- function(x, y) { pl(f, 0+x, 0+y, pal = blues, m = matrix(b1[mask == TRUE], ncol = 7)) pl(f, 0+x, 10+y, pal = blues, m = matrix(b2[mask == TRUE], ncol = 7)) pl(f, 0+x, 20+y, pal = blues, m = matrix(b3[mask == TRUE], ncol = 7)) pl(f, 7+x, 0+y, pal = greens, m = matrix(g1[mask == TRUE], ncol = 7)) pl(f, 7+x, 10+y, pal = greens, m = matrix(g2[mask == TRUE], ncol = 7)) pl(f, 7+x, 20+y, pal = greens, m = matrix(g3[mask == TRUE], ncol = 7)) pl(f, 14+x, 0+y, pal = reds, m = matrix(r1[mask == TRUE], ncol = 7)) pl(f, 14+x, 10+y, pal = reds, m = matrix(r2[mask == TRUE], ncol = 7)) pl(f, 14+x, 20+y, pal = reds, m = matrix(r3[mask == TRUE], ncol = 7)) pl(f, 21+x, 0+y, pal = purples, m = matrix(n1[mask == TRUE], ncol = 7)) pl(f, 21+x, 10+y, pal = purples, m = matrix(n2[mask == TRUE], ncol = 7)) pl(f, 21+x, 20+y, pal = purples, m = matrix(n3[mask == TRUE], ncol = 7)) } print_grid_time_filter &lt;- function(x, y) { # 3x1, 28x7 pl(f, 0+x, 10+y, pal = blues, m = b3) pl(f, 7+x, 10+y, pal = greens, m = g3) pl(f, 14+x, 10+y, pal = reds, m = r3) pl(f, 21+x, 10+y, pal = purples, m = n3) } print_grid_bands_filter &lt;- function(x, y, pal = greys) { # 1x3 6x27 pl(f, 0+x, 0+y, pal = pal, m = n1) pl(f, 0+x, 10+y, pal = pal, m = n2) pl(f, 0+x, 20+y, pal = pal, m = n3) } # build exactly like reduce plot.new() par(mar = c(3,3,3,3)) x = 120 y = 100 down = 0 plot.window(xlim = c(0, x), ylim = c(0, y), asp = 1) print_grid(x/2-28/2,y-27) print_alpha_grid((x/3-28)/2, 0-down) # alpha grid print_grid_time_filter((x/3-28)/2, -10-down) # select 3rd print_alpha_grid(x/3+((x/3-6)/2) -10.5, 0-down) # alpha grid print_grid_bands_filter(x/3+((x/3-12.4)), 0-down, pal = purples) print_alpha_grid(2*(x/3)+((x/3-28)/2), 0-down) # alpha grid print_grid_filter(2*(x/3)+((x/3-28)/2), 0-down) text(3, 13.5-down, &quot;time&quot;, srt = 90, col = &quot;black&quot;) text(43, 13.5-down, &quot;time&quot;, srt = 90, col = &quot;black&quot;) text(83, 13.5-down, &quot;time&quot;, srt = 90, col = &quot;black&quot;) text(20, 30, &quot;bands&quot;, col = &quot;black&quot;) text(60, 30, &quot;bands&quot;, col = &quot;black&quot;) text(100, 30, &quot;bands&quot;, col = &quot;black&quot;) arrows(x/2-28/2,y-30, x/6,32, angle = 20, lwd = 2) arrows(x/2,y-30, x/2,32, angle = 20, lwd = 2) arrows(x/2+28/2,y-30, 100, 32, angle = 20, lwd = 2) # points(seq(1,120,10), seq(1,120,10)) text(28.5,49, &quot;filter temporally&quot;, srt = 55.5, col = &quot;black&quot;, cex = 0.8) text(57,49, &quot;filter bands&quot;, srt = 90, col = &quot;black&quot;, cex = 0.8) text(91.5,49, &quot;filter spatially&quot;, srt = -55.5, col = &quot;black&quot;, cex = 0.8) print_labels(x = x/2-28/2 + 3, y = y+4, off = 7, lab = c(&quot;blue&quot;, &quot;green&quot;, &quot;red&quot;, &quot;nir&quot;), horizontal = TRUE, cex = 0.6) print_labels(x = x/2-28/2 - 9, y = y-23, off = 10, lab = c(&quot;2020-10-01&quot;, &quot;2020-10-13&quot;, &quot;2020-10-25&quot;), horizontal = FALSE, cex = 0.6) print_labels(x = x/2-28/2 + 21.5, y = y-30, off = 1, lab = xlabels, horizontal = TRUE, cex = 0.3) print_labels(x = x/2-28/2 + 30, y = y-26.5, off = 1, lab = ylabels, horizontal = FALSE, cex = 0.3) Figure 6.4: Data cube filtering by time, band or spatially 6.3.2 Applying functions to dimensions A common analysis involves applying a function over one or more cube dimensions. Simple cases arise where a function such as abs, sin or sqrt is applied to all values in the cube, or when a function takes all values in the cube and returns a single scalar, e.g. when one computes the mean or maximum value over the entire cube. Other options include applying the function to selected dimensions, e.g. applying a temporal low-pass filter to every individual (pixel/band) time series as shown in figure 6.6, or applying a spatial low-pass filter to every spatial slice (i.e. every band/time combination), shown in figure 6.5. # apply ---- print_text = function(s, x, y, m) { # m &lt;- t(m) # transverse for correct order # print(m) r &lt;- rep(seq(0.5, 5.5, 1), 7) r &lt;- r + x # consider offsets u &lt;- c(rep(0.5, 6), rep(1.5, 6), rep(2.5, 6), rep(3.5, 6), rep(4.5, 6), rep(5.5, 6), rep(6.5, 6)) u &lt;- u + y # offset tab &lt;- matrix(c(r,u), nrow = 42, byrow = FALSE) # make point table for (i in 1:42) { #text(tab[i, 1], tab[i, 2], labels = paste0(&quot;&quot;, m[i]), cex = 1.1) text(tab[i, 1], tab[i, 2], labels = paste0(&quot;&quot;, m[i]), cex = 0.8) } } abs_brks &lt;- seq(-500,500, 50) abs_pal &lt;- sequential_hcl(n = 20, h1 = 211, c1 = 80, l1 = 30, l2 = 100, p1 = 1.2) #png(&quot;exp_apply_unary.png&quot;, width = 2400, height = 1000, pointsize = 24) #plot.new() #par(mar = c(2,2,2,2)) #x = 30 #y = 7.5 #plot.window(xlim = c(0, x), ylim = c(0, y), asp = 1) #pl(f, 3, 2, pal = abs_pal, m = b3 - 200, breaks = abs_brks) #pl(f, 1.5, .5, pal = abs_pal, m = b2 - 200, breaks = abs_brks) #pl(f, 0, -1, pal = abs_pal, m = b1 - 200, breaks = abs_brks) #print_text(s, 0, -1, m = b1 - 200) #pl(f, 23, 2, pal = abs_pal, m = abs(b3 - 200), breaks = abs_brks) #pl(f, 21.5, 0.5, pal = abs_pal, m = abs(b2 - 200), breaks = abs_brks) #pl(f, 20, -1, pal = abs_pal, m = abs(b1 - 200), breaks = abs_brks) #print_text(s, 20, -1, m = abs(b1 - 200)) #arrows(11, 4, 17.5, 4, lwd = 3) #text(14.3, 3.5, &quot;absolute()&quot;, cex = 1.4) #dev.off() vNeumann_seg &lt;- matrix(c(c(0,0,1,1,2,2,1,1,0,0,-1,-1), c(0,-1,-1,0,0,1,1,2,2,1,1,0), c(0,1,1,2,2,1,1,0,0,-1,-1,0), c(-1,-1,0,0,1,1,2,2,1,1,0,0)), ncol = 4) apply_filter &lt;- function(input, pad = TRUE, padValue = 1) { ras &lt;- raster::focal(raster::raster(input), w = matrix(c(0,0.2,0, 0.2,0.2,0.2, 0,0.2,0), ncol = 3), pad = pad, padValue = padValue) ras &lt;- raster::as.matrix(ras) ras[ras == &quot;NaN&quot;] &lt;- -999 return(floor(ras)) } brks &lt;- seq(0,1000, 50) plot.new() par(mar = c(0,2,0,0)) x = 30 y = 7.5 plot.window(xlim = c(0, x), ylim = c(0, y), asp = 1) pl(f, 3, 2, pal = blues, m = b3, breaks = brks) pl(f, 1.5, .5, pal = blues, m = b2, breaks = brks) pl(f, 0, -1, pal = blues, m = b1, breaks = brks) print_text(s, 0, -1, m = b1) # print text on left first stack print_segments(2, 3, seg = vNeumann_seg, lwd = 3) pl(f, 23, 2, pal = blues, m = apply_filter(b3), breaks = brks) pl(f, 21.5, 0.5, pal = blues, m = apply_filter(b2), breaks = brks) pl(f, 20, -1, pal = blues, m = apply_filter(b1), breaks = brks) print_text(s, 20, -1, m = apply_filter(b1)) # set pad = FALSE for -99 print_segments(22, 3, seg = vNeumann_seg, lwd = 3) arrows(11, 4, 17.5, 4, lwd = 3) text(14.3, 3.5, &quot;apply_kernel()&quot;, cex = 1.4) print_segments(13.8, 1, seg = vNeumann_seg, lwd = 3) cex = .8 text(14.3, 1.5, &quot;0.2&quot;, cex = cex) text(13.3, 1.5, &quot;0.2&quot;, cex = cex) text(15.3, 1.5, &quot;0.2&quot;, cex = cex) text(14.3, 2.5, &quot;0.2&quot;, cex = cex) text(14.3, .5, &quot;0.2&quot;, cex = cex) Figure 6.5: Low pass filtering of spatial slices time_arrow_seg &lt;- matrix(c(c(-1.0, 0.3, 1.5, 2.7, 3.9, 5.1), c(-1.0, 0.3, 1.5, 2.7, 3.9, 5.1), c(-0.5, 0.7, 1.9, 3.1, 4.3, 5.6), c(-0.5, 0.7, 1.9, 3.1, 4.3, 5.6)), ncol = 4) time_arrow_flag_seg &lt;- matrix(c(c(-1.0, 1.5, 2.7, 3.9), c(-1.0, 1.5, 2.7, 3.9), c(0.7, 1.9, 3.1, 5.6), c(0.7, 1.9, 3.1, 5.6)), ncol = 4) b11 &lt;- b2 - t1 b12 &lt;- b1 - t2 + t1 # png(&quot;exp_apply_ts.png&quot;, width = 2400, height = 1000, pointsize = 24) plot.new() par(mar = c(2,2,2,2)) x = 30 y = 10 # 7.5 plot.window(xlim = c(0, x), ylim = c(0, y), asp = 1) pl(f, 4.8, 3.8, pal = blues, m = b3, breaks = brks) print_text(s, 4.8, 3.8, m = b3) pl(f, 3.6, 2.6, pal = blues, m = b11, breaks = brks) print_text(s, 3.6, 2.6, m = b11) pl(f, 2.4, 1.4, pal = blues, m = b12, breaks = brks) print_text(s, 2.4, 1.4, m = b12) pl(f, 1.2, .2, pal = blues, m = b2, breaks = brks) print_text(s, 1.2, .2, m = b2) pl(f, 0, -1, pal = blues, m = b1, breaks = brks) print_text(s, 0, -1, m = b1) # print text on left first stack pl(f, 24.8, 3.8, pal = alpha(greys, 0.1), m = matrix(rep(&quot;NA&quot;, 42), ncol = 7)) pl(f, 23.6, 2.6, pal = blues, m = (b12 + b11 + b3) / 3, breaks = brks) print_text(s, 23.6, 2.6, m = floor((b12 + b11 + b3) / 3)) pl(f, 22.4, 1.4, pal = blues, m = (b2 + b12 + b11) / 3, breaks = brks) print_text(s, 22.4, 1.4, m = floor((b2 + b12 + b11) / 3)) pl(f, 21.2, .2, pal = blues, m = (b1 + b2 + b12) / 3, breaks = brks) print_text(s, 21.2, .2, m = floor((b1 + b2 + b12) / 3)) pl(f, 20, -1, pal = alpha(greys, 0.1), m = matrix(rep(&quot;NA&quot;, 42), ncol = 7)) print_segments(5.7, 1.7, seg = time_arrow_seg, col = &quot;forestgreen&quot;) arrows(12.5, 9, 20, 9, lwd = 2) cex = .9 text(16.3, 8.3, &quot;apply_dimension(dimension = &#39;t&#39;)&quot;, cex = cex) print_segments(9.7, 1.7, time_arrow_seg, col = &quot;forestgreen&quot;) # draw ma explanation text(-0.5 + 10, -0.5 + 2, &quot;496&quot;, cex = cex) text(.7 + 10, .7 + 2, &quot;363&quot;, cex = cex) text(1.9 + 10, 1.9 + 2, &quot;658&quot;, cex = cex) text(3.1 + 10, 3.1 + 2, &quot;230&quot;, cex = cex) text(4.3 + 10, 4.3 + 2, &quot;525&quot;, cex = cex) t_formula &lt;- expression(&quot;t&quot;[n]*&quot; = (t&quot;[n-1]*&quot; + t&quot;[n]*&quot; + t&quot;[n+1]*&quot;) / 3&quot;) # text(13.8, 3, t_formula, srt = 45, cex = 1.2) text(14.4, 3.6, &quot;calculate moving average&quot;, srt = 45, cex = cex) arrows(15, 5.7, 18, 5.7, lwd = 2) print_segments(15.4, 1.7, seg = time_arrow_seg, col = &quot;forestgreen&quot;) # draw ma explanation text(-0.5 + 15.7, -0.5 + 2, &quot;NA&quot;, cex = cex) text(.7 + 15.7, .7 + 2, &quot;505&quot;, cex = cex) text(1.9 + 15.7, 1.9 + 2, &quot;417&quot;, cex = cex) text(3.1 + 15.7, 3.1 + 2, &quot;471&quot;, cex = cex) text(4.3 + 15.7, 4.3 + 2, &quot;NA&quot;, cex = cex) print_segments(25.7, 1.7, seg = time_arrow_seg, col = &quot;forestgreen&quot;) Figure 6.6: Low pass filtering of time series 6.3.3 Reducing dimensions When applying function mean to an entire data cube, all dimensions vanish: the resulting “data cube” has dimensionality zero. We can also apply functions to a limited set of dimensions such that selected dimensions vanish, or are reduced. We already saw that filtering is a special case of this, but more in general we could for instance compute the maximum of every time series, the mean over every spatial slice, or a band index such as NDVI that summarizes different spectral values into a single new “band” with the index value. Figure 6.7 illustrates these options. # reduce ---- # calc mean over time timeB &lt;- (b1 + b2 + b3) / 3 timeG &lt;- (g1 + g2 + g3) / 3 timeR &lt;- (r1 + r2 + r3) / 3 timeN &lt;- (n1 + n2 + n3) / 3 print_grid_time &lt;- function(x, y) { # 3x1, 28x7 pl(f, 0+x, 10+y, pal = blues, m = timeB) pl(f, 7+x, 10+y, pal = greens, m = timeG) pl(f, 14+x, 10+y, pal = reds, m = timeR) pl(f, 21+x, 10+y, pal = purples, m = timeN) } # calc ndvi ndvi1 &lt;- (n1 - r1) / (n1 + r1) ndvi2 &lt;- (n2 - r2) / (n2 + r2) ndvi3 &lt;- (n3 - r3) / (n3 + r3) print_grid_bands &lt;- function(x, y, pal = greys) { # 1x3 6x27 pl(f, 0+x, 0+y, pal = pal, m = ndvi1) pl(f, 0+x, 10+y, pal = pal, m = ndvi2) pl(f, 0+x, 20+y, pal = pal, m = ndvi3) } plte = function(s, x, y, add = TRUE, randomize = FALSE, pal, m) { attr(s, &quot;dimensions&quot;)[[1]]$offset = x attr(s, &quot;dimensions&quot;)[[2]]$offset = y # m = r[[1]][y + 1:nrow,x + 1:ncol,1] # dim(m) = c(x = nrow, y = ncol) # named dim # s[[1]] = m # me &lt;- floor(mean(s[[1]])) me &lt;- floor(mean(m)) if (me[1] &gt; 100) { # in case non-artificial grids with very high me &lt;- m / 10 # numbers are used, make them smaller me &lt;- floor(mean(me)) } text(x,y,me,cex = 0.8) } print_grid_spat &lt;- function(x, y) { x = x + 3 y = y + 3.5 plte(s, 0+x, 0+y, pal = blues, m = b1) plte(s, 0+x, 10+y, pal = blues, m = b2) plte(s, 0+x, 20+y, pal = blues, m = b3) plte(s, 7+x, 0+y, pal = greens, m = g1) plte(s, 7+x, 10+y, pal = greens, m = g2) plte(s, 7+x, 20+y, pal = greens, m = g3) plte(s, 14+x, 0+y, pal = reds, m = r1) plte(s, 14+x, 10+y, pal = reds, m = r2) plte(s, 14+x, 20+y, pal = reds, m = r3) plte(s, 21+x, 0+y, pal = purples, m = n1) plte(s, 21+x, 10+y, pal = purples, m = n2) plte(s, 21+x, 20+y, pal = purples, m = n3) } # png(&quot;exp_reduce.png&quot;, width = 1200, height = 1000, pointsize = 32) plot.new() #par(mar = c(3,3,3,3)) par(mar = c(3,0,2,0)) x = 120 y = 100 plot.window(xlim = c(0, x), ylim = c(0, y), asp = 1) print_grid(x/2-28/2,y-27) # print_alpha_grid((x/3-28)/2, 0) # alpha grid print_grid_time((x/3-28)/2, 0) # off = 5.5 # print_alpha_grid(x/3+((x/3-6)/2) -10.5, 0) # alpha grid print_grid_bands(x/3+((x/3-6)/2), 0) print_alpha_grid(2*(x/3)+((x/3-28)/2), 0, alp = 0, geom = TRUE) # alpha grid print_grid_spat(2*(x/3)+((x/3-28)/2), 0) text(3, 13.5, &quot;time&quot;, srt = 90, col = &quot;black&quot;) #segments(3.6, 8, 3.7, 19, col = &quot;red&quot;, lwd=3) segments(3.4, 8, 3.4, 19, col = &quot;red&quot;, lwd=3) text(43, 13.5, &quot;time&quot;, srt = 90, col = &quot;black&quot;) text(83, 13.5, &quot;time&quot;, srt = 90, col = &quot;black&quot;) text(20, 30, &quot;bands&quot;, col = &quot;black&quot;) text(60, 30, &quot;bands&quot;, col = &quot;black&quot;) segments(53,29.8, 67,29.8, col = &quot;red&quot;, lwd=3) text(100, 30, &quot;bands&quot;, col = &quot;black&quot;) text(30, 7, &quot;x&quot;, col = &quot;black&quot;) text(36, 13, &quot;y&quot;, col = &quot;black&quot;) text(60, -3, &quot;x&quot;, col = &quot;black&quot;) text(66, 3, &quot;y&quot;, col = &quot;black&quot;) text(110, -3, &quot;x&quot;, col = &quot;black&quot;) text(116, 3, &quot;y&quot;, col = &quot;black&quot;) segments(108,-2.4, 112,-3.2, col = &quot;red&quot;, lwd=3) segments(114,3.2, 118,2.4, col = &quot;red&quot;, lwd=3) text(60, y+4, &quot;bands&quot;, col = &quot;black&quot;) # dim names on main text(43, y-14, &quot;time&quot;, srt = 90, col = &quot;black&quot;) text(x/2-28/2 + 24, y-30, &quot;x&quot;, col = &quot;black&quot;) text(x/2-28/2 + 30, y-24, &quot;y&quot;, col = &quot;black&quot;) arrows(x/2-28/2,y-30, x/6,32, angle = 20, lwd = 2) arrows(x/2,y-30, x/2,32, angle = 20, lwd = 2) arrows(x/2+28/2,y-30, 100, 32, angle = 20, lwd = 2) # points(seq(1,120,10), seq(1,120,10)) text(28.5,49, &quot;reduce temporally&quot;, srt = 55.5, col = &quot;black&quot;, cex = 0.8) text(57,49, &quot;reduce bands&quot;, srt = 90, col = &quot;black&quot;, cex = 0.8) text(91.5,49, &quot;reduce spatially&quot;, srt = -55.5, col = &quot;black&quot;, cex = 0.8) Figure 6.7: Reducing data cube dimensions 6.4 Aggregating raster to vector cubes Figure 6.8 illustrates how a four-dimensional raster data cube can be aggregated to a three-dimensional vector data cube. Pixels in the raster are grouped by spatial intersection with a set of vector geometries, and each group is then reduced to a single value by an aggregation function such as mean or max. In the example, the two spatial dimensions \\(x\\) and \\(y\\) reduce to a single dimension, the one-dimensional sequence of feature geometries, with geometries that are defined in the space of \\(x\\) and \\(y\\). Grouping geometries can also be POINT geometries, in which case the aggregation function is obsolete as single values at the POINT locations are extracted, e.g. by querying a pixels value or by interpolating from the nearest pixels. # aggregate ---- mask_agg &lt;- matrix(c(NA,NA,NA,NA,NA,NA, NA, 1, 1,NA,NA,NA, 1, 1,NA,NA, 1,NA, 1,NA,NA, 1, 1,NA, 1,NA,NA, 1, 1,NA, 1,NA,NA,NA, 1,NA, NA,NA,NA,NA,NA,NA), ncol = 7) pl_stack_agg = function(s, x, y, add = TRUE, nrM, imgY = 7, inner = 1) { # pl_stack that masks the added matrices # nrM is the timestep {1, 2, 3}, cause this function # prints all 4 bands at once attr(s, &quot;dimensions&quot;)[[1]]$offset = x attr(s, &quot;dimensions&quot;)[[2]]$offset = y # m = r[[1]][y + 1:nrow,x + 1:ncol,1] m &lt;- eval(parse(text=paste0(&quot;n&quot;, nrM))) m &lt;- matrix(m[mask_agg == TRUE], ncol = 7) s[[1]] = m[,c(imgY:1)] # turn around to have same orientation as flat plot plt(s, 0, TRUE, pal = purples) m &lt;- eval(parse(text=paste0(&quot;r&quot;, nrM))) m &lt;- matrix(m[mask_agg == TRUE], ncol = 7) s[[1]] = m[,c(imgY:1)] plt(s, 1*inner, TRUE, pal = reds) m &lt;- eval(parse(text=paste0(&quot;g&quot;, nrM))) m &lt;- matrix(m[mask_agg == TRUE], ncol = 7) s[[1]] = m[,c(imgY:1)] plt(s, 2*inner, TRUE, pal = greens) m &lt;- eval(parse(text=paste0(&quot;b&quot;, nrM))) m &lt;- matrix(m[mask_agg == TRUE], ncol = 7) s[[1]] = m[,c(imgY:1)] plt(s, 3*inner, TRUE, pal = blues) # li FALSE } polygon_1 &lt;- matrix(c(c(0.0, 5.1, 4.9,-2.3), c(0.0, 2.4, 3.1, 1.8), c(5.1, 4.9,-2.3, 0.0), c(2.4, 3.1, 1.8, 0.0)), ncol = 4) a &lt;- make_dummy_stars(6, 7, 5, -1.14, -2.28) print_vector_content &lt;- function(x, y, cex = 0.8) { vec &lt;- floor(rnorm(8, 250, 100)) text( 0 + x,12 + y, vec[1], cex = cex) text( 0 + x, 8 + y, vec[2], cex = cex) text( 0 + x, 4 + y, vec[3], cex = cex) text( 0 + x, 0 + y, vec[4], cex = cex) text( 12 + x,12 + y, vec[5], cex = cex) text( 12 + x, 8 + y, vec[6], cex = cex) text( 12 + x, 4 + y, vec[7], cex = cex) text( 12 + x, 0 + y, vec[8], cex = cex) } print_ts &lt;- function(off2, yoff2) { pl_stack(s, 0 + off2, yoff2, nrM = 3) # input 2 pl_stack(s, off + off2, yoff2, nrM = 2) pl_stack(s, 2 * off + off2, yoff2, nrM = 1) arrows(-13 + off2, 14 + yoff2, 72 + off2, 14 + yoff2, angle = 20, lwd = 2) # timeline heads &lt;- matrix(c(3.5+off2, 3.5 + off + off2, 3.5 + 2*off + off2, 14+yoff2,14+yoff2,14+yoff2), ncol = 2) points(heads, pch = 16) # 4 or 16 segments(c(-8, 7, 0, 15)+off2, c(-1,-1,3,3)+yoff2, 3.5+off2, 14+yoff2) # first stack pyramid segments(c(-8, 7, 0, 15) + off + off2, c(-1,-1,3,3)+yoff2, 3.5 + off + off2, 14+yoff2) # second stack pyramid segments(c(-8, 7, 0, 15) + 2*off + off2, c(-1,-1,3,3)+yoff2, 3.5 + 2*off + off2, 14+yoff2) # third stack pyramid text(7.5+off2, 4.3+yoff2, &quot;x&quot;, col = &quot;black&quot;, cex = secText) text(-9.5+off2, -2.5+yoff2, &quot;bands&quot;, srt = 90, col = &quot;black&quot;, cex = secText) text(-4.5+off2, 2+yoff2, &quot;y&quot;, srt = 27.5, col = &quot;black&quot;, cex = secText) text(69+off2, 15.5+yoff2+1, &quot;time&quot;, col = &quot;black&quot;) text(3.5+off2, 15.5+yoff2, &quot;2020-10-01&quot;, col = &quot;black&quot;) text(3.5 + off + off2, 15.5+yoff2, &quot;2020-10-13&quot;, col = &quot;black&quot;) text(3.5 + 2*off + off2, 15.5+yoff2, &quot;2020-10-25&quot;, col = &quot;black&quot;) } secText = 0.8 # secondary text size (dimension naming) off = 26 # image stacks are always 26 apart x = 72 # png X y = 48 # png Y yoff = 30 plot.new() par(mar = c(5,3,3,3)) plot.window(xlim = c(-1, x), ylim = c(0, y), asp = 1) print_ts(5, yoff) col = &#39;#ff5555&#39; print_segments(10.57, yoff-.43, seg = polygon_1, col = col) x = 3 segments(c(2, 0, -1.3)+x, c(-.2, 0, 1)+yoff, c(0, -1.3, 1)+x, c(0, 1, 2)+yoff, lwd = 4, col = col) print_segments(10.57+off, yoff-.43, seg = polygon_1, col = col) x = 3 + off segments(c(2, 0, -1.3)+x, c(-.2, 0, 1)+yoff, c(0, -1.3, 1)+x, c(0, 1, 2)+yoff, lwd = 4, col = col) print_segments(10.57+2*off, yoff-.43, seg = polygon_1, col = col) x = 3 + 2 * off segments(c(2, 0, -1.3)+x, c(-.2, 0, 1)+yoff, c(0, -1.3, 1)+x, c(0, 1, 2)+yoff, lwd = 4, col = col) # old 75 28 poly 86.25, 27.15 pl_stack_agg(a, 5, 5, nrM = 1, inner = 3) # print masked enlargement print_segments(16.25, 7.15, seg = polygon_1, by = 2, col = col) x &lt;- 1 y &lt;- 8 segments(c(4, 0, -2.6)+x, c(-.4, 0, 2)+y, c(0, -2.6, 2)+x, c(0, 2, 4)+y, lwd = 4, col = col) # line in large segments(-3, 25, -7, 9, lwd = 3, col = &#39;grey&#39;) segments(21, 29, 27.5, 14, lwd = 3, col = &#39;grey&#39;) text(10, 20, &quot;1. Group by geometry&quot;, cex = 1.3) vecM &lt;- matrix(rep(1,8), ncol = 2) text(57, 21, &quot;2. Reduce to vector cube&quot;, cex = 1.3) b &lt;- make_dummy_stars(2, 4, 12, 4, 0) pl(b, 48, -5, m = vecM, pal = alpha(&quot;white&quot;, 0.9), border = 0) print_vector_content(54, -3) pl(b, 46.5, -3.5, m = vecM, pal = alpha(&quot;white&quot;, 0.9), border = 0) print_vector_content(52.5, -1.5) pl(b, 45, -2, m = vecM, pal = alpha(&quot;white&quot;, 0.9), border = 0) print_vector_content(51, 0) text(51.5, 15, &quot;Line_1&quot;, col = col) text(63, 15, &quot;Polygon_1&quot;, col = col) text(57, 17.5, &quot;Geometries&quot;, cex = 1.1) text(42, 12, &quot;blue&quot;) text(42, 8, &quot;green&quot;) text(42, 4, &quot;red&quot;) text(42, 0, &quot;nir&quot;) text(38, 6, &quot;Bands&quot;, srt = 90, cex = 1.1) # arrows(13.5, -2, 13.5, -6, angle = 20, lwd = 3) text(72, 15.5, &quot;time&quot;, srt = 315, cex = 1.1) arrows(69.5, 15, 72.5, 12, angle = 20, lwd = 2) Figure 6.8: Aggregating a raster data cube to a vector data cube # print_segments(30, 35, seg = arrow_seg) Further examples of vector data cubes include be air quality data, where we could have \\(PM_{10}\\) measurements over two dimensions: a sequence of monitoring stations, and a sequence of time intervals or where we consider time series of demographic or epidemiological data, consisting of (population, disease) counts, with number of persons by region, for a sequence of \\(n\\) regions by age class, for \\(m\\) age classes, and by year, for \\(p\\) years. which forms an array with \\(n m p\\) elements. For spatial data science, support of vector and raster data cubes is extremely useful, because many variables are both spatially and temporaly varying, and because we often want to either change dimensions or aggregate them out, but in a fully flexible manner and order. Examples of changing dimensions are interpolating air quality measurements to values on a regular grid (raster; chapter 12) estimating density maps from points or lines, e.g. with the number of flights passing by per week within a range of 1 km (chapter 11 aggregating climate model predictions to summary indicators for administrative regions combining Earth observation data from different sensors, e.g. Modis (250 m pixels, every 16 days) with Sentinel-2 (10 m, every 5 days). Examples of aggregating one ore more full dimensions are assessments of which air quality monitoring stations indicate unhealthy conditions (time), which region has the highest increase in disease incidence (space, time), global warming (e.g. in degrees per year). 6.5 Switching dimension with attributes When we accept that a dimension can also reflect an unordered, categorical variable, then one can easily swap a set of attributes for a single dimension, by replacing \\[\\{Z_1,Z_2,...,Z_p\\} = f(D_1,D_2,...,D_n)\\] with \\[Z = f(D_1,D_2,...,D_n, D_{n+1})\\] where \\(D_{n+1}\\) has cardinality \\(p\\) and has as labels (the names of) \\(Z_1,Z_2,...,Z_p\\). Figure 6.9 shows a vector data cube for air quality stations where one cube dimension reflects air quality parameters. When the \\(Z_i\\) have incompatible measurement units, as in figure 6.9, one would have to take care when reducing the “parameter” dimension \\(D_{n+1}\\): numeric functions like mean or max would be meaningless. Counting the number of variables that exceed their respective threshold values may however be meaningful. set.seed(1331) library(stars) library(colorspace) tif = system.file(&quot;tif/L7_ETMs.tif&quot;, package = &quot;stars&quot;) r = read_stars(tif) nrow = 5 ncol = 8 #m = matrix(runif(nrow * ncol), nrow = nrow, ncol = ncol) m = r[[1]][1:nrow,1:ncol,1] dim(m) = c(x = nrow, y = ncol) # named dim s = st_as_stars(m) # s attr(s, &quot;dimensions&quot;)[[1]]$delta = 3 attr(s, &quot;dimensions&quot;)[[2]]$delta = -.5 attr(attr(s, &quot;dimensions&quot;), &quot;raster&quot;)$affine = c(-1.2, 0.0) plt = function(x, yoffset = 0, add, li = TRUE) { attr(x, &quot;dimensions&quot;)[[2]]$offset = attr(x, &quot;dimensions&quot;)[[2]]$offset + yoffset l = st_as_sf(x, as_points = FALSE) pal = sf.colors(10) if (li) pal = lighten(pal, 0.3 + rnorm(1, 0, 0.1)) if (! add) plot(l, axes = FALSE, breaks = &quot;equal&quot;, pal = pal, reset = FALSE, border = grey(.75), key.pos = NULL, main = NULL, xlab = &quot;time&quot;) else plot(l, axes = TRUE, breaks = &quot;equal&quot;, pal = pal, add = TRUE, border = grey(.75)) u = st_union(l) plot(st_geometry(u), add = TRUE, col = NA, border = &#39;black&#39;, lwd = 2.5) } pl = function(s, x, y, add = TRUE, randomize = FALSE) { attr(s, &quot;dimensions&quot;)[[1]]$offset = x attr(s, &quot;dimensions&quot;)[[2]]$offset = y m = r[[1]][y + 1:nrow,x + 1:ncol,1] if (randomize) m = m[sample(y + 1:nrow),x + 1:ncol] dim(m) = c(x = nrow, y = ncol) # named dim s[[1]] = m plt(s, 0, add) plt(s, 1, TRUE) plt(s, 2, TRUE) plt(s, 3, TRUE) plt(s, 4, TRUE) plt(s, 5, TRUE) plt(s, 6, TRUE) plt(s, 7, TRUE) plt(s, 8, TRUE, FALSE) } # point vector data cube: plot.new() par(mar = c(5, 0, 5, 0)) plot.window(xlim = c(-10, 16), ylim = c(-2,12), asp = 1) library(spacetime) data(air) de = st_geometry(st_normalize(st_as_sf(DE))) # pl(s, 0, 0, TRUE, randomize = TRUE) de = de * 6 + c(-7, 9) plot(de, add = TRUE, border = grey(.5)) text(-10, 0, &quot;time&quot;, srt = -90, col = &#39;black&#39;) text(-5, 7.5, &quot;sensor location&quot;, srt = 25, col = &#39;black&#39;) text( 7, 10.5, &quot;air quality parameter&quot;, srt = 0, col = &#39;black&#39;) text( 1.5, 8.5, expression(PM[10]), col = &#39;black&#39;, cex = .75) text( 4.5, 8.5, expression(NO[x]), col = &#39;black&#39;, cex = .75) text( 8, 8.5, expression(SO[4]), col = &#39;black&#39;, cex = .75) text( 11, 8.5, expression(O[3]), col = &#39;black&#39;, cex = .75) text( 14, 8.5, expression(CO), col = &#39;black&#39;, cex = .75) # location points: p = st_coordinates(s[,1]) p[,1] = p[,1]-1.4 p[,2] = p[,2] + 8.2 points(p, col = grey(.7), pch = 16) # centroids: set.seed(131) cent = st_coordinates(st_sample(de, 8)) points(cent, col = grey(.7), pch = 16) cent = cent[rev(order(cent[,1])),] seg = cbind(p, cent[1:8,]) segments(seg[,1], seg[,2], seg[,3], seg[,4], col = &#39;grey&#39;) Figure 6.9: Vector data cube with air quality time series Being able to swap dimensions to attributes flexibly and vice-versa leads to extremely flexible analysis possibilities, as e.g. shown by the array database SciDB (Brown 2010). 6.6 Other dynamic data We have seen several dynamic raster and vector data examples that match the data cube structure well. Other data examples do less so: in particular spatiotemporal point patterns (chapter 11) and trajectories (movement data) are often more straightforward to not handle as a data cube. Spatiotemporal point patterns are the sets of spatiotemporal coordinates of events or objects: accidents, disease cases, traffic jams, lightning strikes, and so on. Trajectory data are time sequences of spatial locations of moving objects (persons, cars, satellites, animals). For such data, the primary information is in the coordinates, and shifting these to a limited set of regularly discretized grid cells covering the space may help some analysis, e.g. to quickly explore patterns in areas of higher densities, but the loss of the exact coordinates also hinders a number of analysis approaches involving distance, direction or speed calculations. Nevertheless, for such data often the first computational steps involves generation of data cube representations by aggregating to a time-fixed spatial and/or space-fixed temporal discretization. Using sparse array representations of data cubes to represent point pattern or trajectory data, as e.g. offered by SciDB (Brown 2010) or TileDB (Papadopoulos et al. 2016), may strongly limit the loss of coordinate accuracy by choosing dimensions that represent an extremely dense grid, and storing only those grid cell that contain data points. For trajectory data, such represenations would need to add a grouping dimension to identify individuals, or individual sequences of consecutive movement observations. 6.7 Exercises Why is it difficult to represent trajectories, sequences of \\((x,y,t)\\) obtained by tracking moving objects, by data cubes as described in this chapter? In a socio-economic vector data cube with variables population, life expectancy, and gross domestic ordered by dimensions country and year, which variables have block support for the spatial dimension, and which have block support for the temporal dimension? The Sentinel-2 satellites collect images in 12 spectral bands; list advantages and disadvantages to represent them as (i) different data cubes, (ii) a data cube with 12 attributes, one for each band, and (iii) a single attribute data cube with a spectral dimension. Explain why a curvilinear raster as shown in figure 1.5 can be considered a special case of a data cube. Explain how the following problems can be solved with data cube operatsion filter, applying functions to dimensions, or reducing dimensions, or aggregating dimensions: from hourly \\(PM_{10}\\) measurements for a set of air quality monitoring stations, compute per station the amount of days per year that the average daily \\(PM_{10}\\) value exceeds 50 \\(\\mu g/m^3\\) for a sequence of aerial images of an oil spill, find the time at which the oil spill had its largest extent, and the corresponding extent from a 10-year period with global daily sea surface temperature (SST) raster maps, find the area with the 10% largest and 10% smallest temporal trends in SST values. References "],["sf.html", "Chapter 7 Introduction to sf and stars 7.1 Package sf 7.2 Spatial joins 7.3 Package stars 7.4 Vector data cube examples 7.5 raster-to-vector, vector-to-raster 7.6 Coordinate transformations and conversions 7.7 transforming and warping rasters 7.8 Exercises", " Chapter 7 Introduction to sf and stars This chapter introduces R packages sf and stars. sf provides a table format for simple features, where feature geometries are carried in a list-column. R package stars was written to support raster and vector datacubes (Chapter 6), and has raster data stacks and feature time series as special cases. sf first appeared on CRAN in 2016, stars in 2018. Development of both packages received support from the R Consortium as well as strong community engagement. The packages were designed to work together. All functions operating on sf or stars objects start with st_, making it easy to recognize them or to search for them when using command line completion. 7.1 Package sf Intended to succeed and replace R packages sp, rgeos and the vector parts of rgdal, R Package sf (Pebesma 2018) was developed to move spatial data analysis in R closer to standards-based approaches seen in the industry and open source projects, to build upon more modern versions of the open source geospatial software stack (figure 1.6), and to allow for integration of R spatial software with the tidyverse if desired. To do so, R package sf provides simple features access (J. Herring and others 2011), natively, to R. It provides an interface to several tidyverse packages, in particular to ggplot2, dplyr and tidyr. It can read and write data through GDAL, execute geometrical operations using GEOS (for projected coordinates) or s2geometry (for ellipsoidal coordinates), and carry out coordinate transformations or conversions using PROJ. External C++ libraries are interfaced using Rcpp (Eddelbuettel 2013). Package sf represents sets of simple features in sf objects, a sub-class of a data.frame or tibble. sf objects contain at least one geometry list-column of class sfc, which for each element contains the geometry as an R object of class sfg. A geometry list-column acts as a variable in a data.frame or tibble, but has a more complex structure than e.g. numeric or character variables. Following the convention of PostGIS, all operations (functions, method) that operate on sf objects or related start with st_. An sf object has the following meta-data: the name of the (active) geometry column, held in attribute sf_column for each non-geometry variable, the attribute-geometry relationships (section 5.1), held in attribute agr An sfc geometry list-column has the following meta-data: the coordinate reference system held in attribute crs the bounding box held in attribute bbox the precision held in attribute precision the number of empty geometries held in attribute n_empty These attributes may best be accessed or set by using functions like st_bbox, st_crs, st_set_crs, st_agr, st_set_agr, st_precision, and st_set_precision. 7.1.1 Creation One could create an sf object from scratch e.g. by library(sf) p1 = st_point(c(7.35, 52.42)) p2 = st_point(c(7.22, 52.18)) p3 = st_point(c(7.44, 52.19)) sfc = st_sfc(list(p1, p2, p3), crs = &#39;OGC:CRS84&#39;) st_sf(elev = c(33.2, 52.1, 81.2), marker = c(&quot;Id01&quot;, &quot;Id02&quot;, &quot;Id03&quot;), geom = sfc) # Simple feature collection with 3 features and 2 fields # Geometry type: POINT # Dimension: XY # Bounding box: xmin: 7.22 ymin: 52.2 xmax: 7.44 ymax: 52.4 # Geodetic CRS: WGS 84 # elev marker geom # 1 33.2 Id01 POINT (7.35 52.4) # 2 52.1 Id02 POINT (7.22 52.2) # 3 81.2 Id03 POINT (7.44 52.2) (ref:sfcomp) components of an sf object Figure 7.1: (ref:sfcomp) Figure 7.1 gives an explanation of the components printed. Rather than creating objects from scratch, spatial data in R are typically read from an external source, which can be an external file, a request to a web service, or a dataset held in some form in another R package. The next section introduces reading from files; section 8.1 discusses handling of datasets too large to fit into working memory. 7.1.2 Reading and writing Reading datasets from an exteral “data source” (file, web service, or even string) is done using st_read: library(sf) (file = system.file(&quot;gpkg/nc.gpkg&quot;, package=&quot;sf&quot;)) # [1] &quot;/home/edzer/R/x86_64-pc-linux-gnu-library/4.0/sf/gpkg/nc.gpkg&quot; nc = st_read(file) # Reading layer `nc.gpkg&#39; from data source # `/home/edzer/R/x86_64-pc-linux-gnu-library/4.0/sf/gpkg/nc.gpkg&#39; # using driver `GPKG&#39; # Simple feature collection with 100 features and 14 fields # Geometry type: MULTIPOLYGON # Dimension: XY # Bounding box: xmin: -84.3 ymin: 33.9 xmax: -75.5 ymax: 36.6 # Geodetic CRS: NAD27 Here, the file name and path file is read from the sf package, which has a different path on every machine, and hence is guaranteed to be present on every sf installation. Command st_read has two arguments: the data set name (dsn) and the layer. In the example above, the geopackage file contains only a single layer that is being read. If it had contained multiple layers, then the first layer would have been read and a warning would have been emitted. The available layers of a data set can be queried by st_layers(file) # Driver: GPKG # Available layers: # layer_name geometry_type features fields # 1 nc.gpkg Multi Polygon 100 14 Simple feature objects can be written with st_write, as in (file = tempfile(fileext = &quot;.gpkg&quot;)) # [1] &quot;/tmp/RtmpNqCwUi/file657b74dd35dcd.gpkg&quot; st_write(nc, file, layer = &quot;layer_nc&quot;) # Writing layer `layer_nc&#39; to data source # `/tmp/RtmpNqCwUi/file657b74dd35dcd.gpkg&#39; using driver `GPKG&#39; # Writing 100 features with 14 fields and geometry type Multi Polygon. where the file format (GPKG) is derived from the file name extension. 7.1.3 Subsetting A very common operation is to subset objects; base R can use [ for this. The rules that apply to data.frame objects also apply to sf objects, e.g. that records 2-5 and columns 3-7 are selected by nc[2:5, 3:7] but with a few additional features, in particular the drop argument is by default FALSE meaning that the geometry column is always selected, and an sf object is returned; when it is set to TRUE and the geometry column not selected, it is dropped and a data.frame is returned; selection with a spatial (sf, sfc or sfg) object as first argument leads to selection of the features that spatially intersect with that object (see next section); other predicates than intersects can be chosen by setting parameter op to a function such as st_covers or or any other binary predicate function listed in section 3.2.2 7.1.4 Binary predicates Binary predicates like st_intersects, st_covers etc (section 3.2.2) take two sets of features or feature geometries and return for all pairs whether the predicate is TRUE or FALSE. For large sets this would potentially result in a huge matrix, typically filled mostly with FALSE values and for that reason a sparse representation is returned by default: nc5 = nc[1:5, ] nc7 = nc[1:7, ] (i = st_intersects(nc5, nc7)) # Sparse geometry binary predicate list of length 5, where the predicate # was `intersects&#39; # 1: 1, 2 # 2: 1, 2, 3 # 3: 2, 3 # 4: 4, 7 # 5: 5, 6 # Warning in st_centroid.sf(nc7): st_centroid assumes attributes are constant over # geometries of x Figure 7.2: First seven North Carolina counties Figure 7.2 shows how the intersections of the first five with the first seven counties can be understood. We can transform the sparse logical matrix into a dense matrix by as.matrix(i) # [,1] [,2] [,3] [,4] [,5] [,6] [,7] # [1,] TRUE TRUE FALSE FALSE FALSE FALSE FALSE # [2,] TRUE TRUE TRUE FALSE FALSE FALSE FALSE # [3,] FALSE TRUE TRUE FALSE FALSE FALSE FALSE # [4,] FALSE FALSE FALSE TRUE FALSE FALSE TRUE # [5,] FALSE FALSE FALSE FALSE TRUE TRUE FALSE The number of countries that each of nc5 intersects with is lengths(i) # [1] 2 3 2 2 2 and the other way around, the number of counties in nc5 that intersect with each of the counties in nc7 is lengths(t(i)) # [1] 2 3 2 1 1 1 1 The object i is of class sgbp (sparse geometrical binary predicate), and is a list of integer vectors, with each element representing a row in the logical predicate matrix holding the column indices of the TRUE values for that row. It further holds some metadata like the predicate used, and the total number of columns. Methods available for sgbp objects include methods(class = &quot;sgbp&quot;) # [1] as.data.frame as.matrix dim Ops print # [6] t # see &#39;?methods&#39; for accessing help and source code where the only Ops method available is !, the negation operation. 7.1.5 tidyverse The tidyverse is a collection of data science packages that work together, described e.g. in (Wickham and Grolemund 2017; Wickham et al. 2019). Package sf has tidyverse-style read and write functions, read_sf and write_sf, which return a tibble rather than a data.frame, do not print any output, and overwrite existing data by default. Further tidyverse generics with methods for sf objects include filter, select, group_by, ungroup, mutate, transmute, rowwise, rename, slice and summarise, distinct, gather, spread, nest, unnest, unite, separate, separate_rows, sample_n, and sample_frac. Most of these methods simply manage the metadata of sf objects, and make sure the geometry remains present. In case a user wants the geometry to be removed, one can use st_set_geometry(nc, NULL) or simply coerce to a tibble before selecting: library(tidyverse) nc %&gt;% as_tibble() %&gt;% select(BIR74) %&gt;% head(3) # # A tibble: 3 x 1 # BIR74 # &lt;dbl&gt; # 1 1091 # 2 487 # 3 3188 The summarise method for sf objects has two special arguments: do_union (default TRUE) determines whether grouped geometries are unioned on return, so that they form a valid geometry; is_coverage (default FALSE) in case the geometries grouped form a coverage (do not have overlaps), setting this to TRUE speeds up the unioning The distinct method selects distinct records, where st_equals is used to evaluate distinctness of geometries. filter can be used with the usual predicates; when one wants to use it with a spatial predicate, e.g. to select all counties less than 50 km away from Orange county, one could use sf_use_s2(TRUE) orange &lt;- nc %&gt;% filter(NAME == &quot;Orange&quot;) wd = st_is_within_distance(nc, orange, units::set_units(50, km)) o50 &lt;- nc %&gt;% filter(lengths(wd) &gt; 0) nrow(o50) # [1] 17 Figure 7.3 shows the results of this analysis, and in addition a buffer around the county borders; note that this buffer serves for illustration, it was not used to select the counties. Figure 7.3: Orange county (orange), counties within a 50 km radius (black), a 50 km buffer around Orange county (brown), and remaining counties (grey) 7.2 Spatial joins In regular (left, right or inner) joins, joined records from a pair of tables are reported when one or more selected attributes match (are identical) in both tables. A spatial join is similar, but the criterion to join records is not equality of attributes but a spatial predicate. This leaves a wide variety of options in order to define spatially matching records, using binary predicates listed in section 3.2.2. The concepts of “left”, “right”, “inner” or “full” joins remain identical to the non-spatial join as the options for handling records that have no spatial match. When using spatial joins, each record may have several matched records, yielding a large result table. A way to reduce this complexity may be to select from the matching records the one with the largest overlap with the target geometry. An example of this is shown (visually) in figure 7.4; this is done using st_join with argument largest = TRUE. # example of largest = TRUE: nc &lt;- st_transform(read_sf(system.file(&quot;shape/nc.shp&quot;, package=&quot;sf&quot;)), 2264) gr = st_sf( label = apply(expand.grid(1:10, LETTERS[10:1])[,2:1], 1, paste0, collapse = &quot; &quot;), geom = st_make_grid(nc)) gr$col = sf.colors(10, categorical = TRUE, alpha = .3) # cut, to check, NA&#39;s work out: gr = gr[-(1:30),] suppressWarnings(nc_j &lt;- st_join(nc, gr, largest = TRUE)) # the two datasets: opar = par(mfrow = c(2,1), mar = rep(0,4)) plot(st_geometry(nc_j)) plot(st_geometry(gr), add = TRUE, col = gr$col) text(st_coordinates(st_centroid(st_geometry(gr))), labels = gr$label) # the joined dataset: plot(st_geometry(nc_j), border = &#39;black&#39;, col = nc_j$col) text(st_coordinates(st_centroid(st_geometry(nc_j))), labels = nc_j$label, cex = .8) plot(st_geometry(gr), border = &#39;green&#39;, add = TRUE) Figure 7.4: example of st_join with largest = TRUE: the label of the polygon in the top figure with the largest intersection with polygons in the bottom figure is assigned to the polygons of the bottom figure. par(opar) Another way to reduce the result set is to use aggregate after a join, all matching records, and union their geometries; see Section 5.4. 7.2.1 sampling, gridding, interpolating Several convenience functions are available in package sf, some of which will be discussed here. Function st_sample generates a sample of points randomly sampled from target geometries, where target geometries can be point, line or polygon geometries. Sampling strategies can be (completely) random, regular or (with polygons) triangular. Chapter 11 explains how spatial sampling (or point pattern simulation) methods available in package spatstat are interfaced through st_sample. Function st_make_grid creates a square, rectangular or hexagonal grid over a region, or points with the grid centers or corners. It was used to create the rectangular grid in figure 7.4. Function st_interpolate_aw “interpolates” area values to new areas, as explained in section 5.3, both for intensive and extensive variables. 7.3 Package stars Athough package sp has always had limited support for raster data, over the last decade R package raster (Hijmans 2020) has clearly been dominant as the prime package for powerful, flexible and scalable raster analysis. The raster data model of package raster (and its successor, terra (Hijmans 2021)) is that of a 2D regular raster, or a set of raster layers (a “raster stack”). This aligns with the classical static “GIS world view”, where the world is modelled as a set of layers, each representing a different theme. A lot of data available today however is dynamic, and comes as time series of rasters or raster stacks. A raster stack does not meaningfully reflect this, requiring the user to keep a register of which layer represents what. Also, the raster package, and its successer terra do an excellent job in scaling computations up to datasizes no larger than the local storage (the computer’s hard drives), and doing this fast. Recent datasets however, including satellite imagery, climate model or weather forecasting data, often no longer fit in local storage (chapter 8). Package spacetime (Pebesma 2012) addresses the analysis of time series of vector geometries or raster grid cells, but does not extend to higher-dimensional arrays. Here, we introduce package stars for analysing raster and vector data cubes. The package allows for representing dynamic (time varying) raster stacks, aims at being scalable, also beyond local disk size, provides a strong integration of raster functions in the GDAL library in addition to regular grids handles rotated, sheared, rectilinear and curvilinear rasters (figure 1.5), provides a tight integration with package sf, also handles array data with non-raster spatial dimensions, the vector data cubes, and follows the tidyverse design principles. Vector data cubes include for instance time series for simple features, or spatial graph data such as potentially dynamic origin-destination matrices. The concept of spatial vector and raster data cubes was explained in chapter 6. 7.3.1 Reading and writing raster data Raster data typically are read from a file. We use a dataset containing a section of Landsat 7 scene, with the 6 30m-resolution bands (bands 1-5 and 7) for a region covering the city of Olinda, Brazil. We can read the example GeoTIFF file holding a regular, non-rotated grid from the package stars: tif = system.file(&quot;tif/L7_ETMs.tif&quot;, package = &quot;stars&quot;) library(stars) (r = read_stars(tif)) # stars object with 3 dimensions and 1 attribute # attribute(s): # Min. 1st Qu. Median Mean 3rd Qu. Max. # L7_ETMs.tif 1 54 69 68.9 86 255 # dimension(s): # from to offset delta refsys point values x/y # x 1 349 288776 28.5 UTM Zone 25, S... FALSE NULL [x] # y 1 352 9120761 -28.5 UTM Zone 25, S... FALSE NULL [y] # band 1 6 NA NA NA NA NULL where we see the offset, cellsize, coordinate reference system, and dimensions. The dimension table reports the following for each dimension: from: the starting index to: the ending index offset: the dimension value at the start (edge) of the first pixel delta: the cell size; negative delta values indicate that pixel index increases with decreasing dimension values refsys: the reference system point: boolean, indicating whether cell values have point support or cell support values: a list with values or labels associated with each of the dimension’s values x/y: an indicator whether a dimension is associated with a spatial raster x- or y-axis For regular, rotated or sheared grids or other regularly discretized dimensions (e.g. time), offset and delta are not NA and values and values is NULL; for other cases, offset and delta are NA and values contains one of: the sequence of values, or intervals, in case of a rectlinear spatial raster or irregular time dimension in case of a vector data cube, geometries associated with the spatial dimension in case of a curvilinear raster, the matrix with coordinate values for each raster cell in case of a discrete dimension, the band names or labels associated with the dimension values The object r is of class stars and is a simple list of length one, holding a three-dimensional array: length(r) # [1] 1 class(r[[1]]) # [1] &quot;array&quot; dim(r[[1]]) # x y band # 349 352 6 and in addition holds an attribute with a dimensions table with all the metadata required to know what the array dimensions refer to, obtained by st_dimensions(r) We can get the spatial extent of the array by st_bbox(r) # xmin ymin xmax ymax # 288776 9110729 298723 9120761 Raster data can be written to local disk using write_stars: tf = tempfile(fileext = &quot;.tif&quot;) write_stars(r, tf) where again the data format (in this case, GeoTIFF) is derived from the file extension. As for simple features, reading and writing uses the GDAL library; the list of available drivers for raster data is obtained by st_drivers(&quot;raster&quot;) 7.3.2 Subsetting stars data cubes Data cubes can be subsetted using [, or using tidyverse verbs. The first options, [ uses for the arguments: attributes first, followed by dimension. This means that r[1:2, 101:200, , 5:10] selects from r attributes 1-2, index 101-200 for dimension 1, and index 5-10 for dimension 3; omitting dimension 2 means that no subsetting takes place. For attributes, attributes names or logical vectors can be used. For dimensions, logical vectors are not supported; Selecting discontinuous ranges supported when it it is a regular sequence. By default, drop is FALSE, when set to TRUE dimensions with a single value are dropped: r[,1:100, seq(1, 250, 5), 4] %&gt;% dim() # x y band # 100 50 1 r[,1:100, seq(1, 250, 5), 4, drop = TRUE] %&gt;% dim() # x y # 100 50 For selecting particular ranges of dimension values, one can use filter (after loading dplyr): library(dplyr, warn.conflicts = FALSE) filter(r, x &gt; 289000, x &lt; 290000) # stars object with 3 dimensions and 1 attribute # attribute(s): # Min. 1st Qu. Median Mean 3rd Qu. Max. # L7_ETMs.tif 5 51 63 64.3 75 242 # dimension(s): # from to offset delta refsys point values x/y # x 1 35 289004 28.5 UTM Zone 25, S... FALSE NULL [x] # y 1 352 9120761 -28.5 UTM Zone 25, S... FALSE NULL [y] # band 1 6 1 1 NA NA NULL which changes the offset of the \\(x\\) dimension. Particular cube slices can also be obtained with slice, e.g. slice(r, band, 3) # stars object with 2 dimensions and 1 attribute # attribute(s): # Min. 1st Qu. Median Mean 3rd Qu. Max. # L7_ETMs.tif 21 49 63 64.4 77 255 # dimension(s): # from to offset delta refsys point values x/y # x 1 349 288776 28.5 UTM Zone 25, S... FALSE NULL [x] # y 1 352 9120761 -28.5 UTM Zone 25, S... FALSE NULL [y] which drops the singular dimension. mutate can be used on stars objects to add new arrays as functions of existing ones, transmute drops existing ones. 7.3.3 Cropping Further subsetting can be done using spatial objects of class sf, sfc or bbox, e.g. when using the sample raster, b = st_bbox(r) %&gt;% st_as_sfc() %&gt;% st_centroid() %&gt;% st_buffer(units::set_units(500, m)) r[b] # stars object with 3 dimensions and 1 attribute # attribute(s): # Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s # L7_ETMs.tif 22 54 66 67.7 78.2 174 2184 # dimension(s): # from to offset delta refsys point values x/y # x 157 193 288776 28.5 UTM Zone 25, S... FALSE NULL [x] # y 159 194 9120761 -28.5 UTM Zone 25, S... FALSE NULL [y] # band 1 6 NA NA NA NA NULL selects the circular center region with a diameter of 500 metre, for the first band shown in figure 7.5, Figure 7.5: circular center region of the Landsat 7 scene (band 1) where we see that pixels outside the spatial object are assigned NA values. This object still has dimension indexes relative to the offset and delta values of r; we can reset these to a new offset with r[b] %&gt;% st_normalize() %&gt;% st_dimensions() # from to offset delta refsys point values x/y # x 1 37 293222 28.5 UTM Zone 25, S... FALSE NULL [x] # y 1 36 9116258 -28.5 UTM Zone 25, S... FALSE NULL [y] # band 1 6 NA NA NA NA NULL By default, the resulting raster is cropped to the extent of the selection object; an object with the same dimensions as the input object is obtained with r[b, crop = FALSE] # stars object with 3 dimensions and 1 attribute # attribute(s): # Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s # L7_ETMs.tif 22 54 66 67.7 78.2 174 731280 # dimension(s): # from to offset delta refsys point values x/y # x 1 349 288776 28.5 UTM Zone 25, S... FALSE NULL [x] # y 1 352 9120761 -28.5 UTM Zone 25, S... FALSE NULL [y] # band 1 6 NA NA NA NA NULL Cropping a stars object can alternatively be done directly with st_crop, as in st_crop(r, b) 7.3.4 redimensioning stars objects Package stars uses package abind (Plate and Heiberger 2016) for a number of array manipulations. One of them is aperm which transposes an array by permuting it. A method for stars objects is provided, and aperm(r, c(3, 1, 2)) # stars object with 3 dimensions and 1 attribute # attribute(s): # Min. 1st Qu. Median Mean 3rd Qu. Max. # L7_ETMs.tif 1 54 69 68.9 86 255 # dimension(s): # from to offset delta refsys point values x/y # band 1 6 NA NA NA NA NULL # x 1 349 288776 28.5 UTM Zone 25, S... FALSE NULL [x] # y 1 352 9120761 -28.5 UTM Zone 25, S... FALSE NULL [y] permutes the order of dimensions of the resulting object. Attributes and dimensions can be swapped, using merge and split: (rs = split(r)) # stars object with 2 dimensions and 6 attributes # attribute(s): # Min. 1st Qu. Median Mean 3rd Qu. Max. # X1 47 67 78 79.1 89 255 # X2 32 55 66 67.6 79 255 # X3 21 49 63 64.4 77 255 # X4 9 52 63 59.2 75 255 # X5 1 63 89 83.2 112 255 # X6 1 32 60 60.0 88 255 # dimension(s): # from to offset delta refsys point values x/y # x 1 349 288776 28.5 UTM Zone 25, S... FALSE NULL [x] # y 1 352 9120761 -28.5 UTM Zone 25, S... FALSE NULL [y] merge(rs) # stars object with 3 dimensions and 1 attribute # attribute(s): # Min. 1st Qu. Median Mean 3rd Qu. Max. # X 1 54 69 68.9 86 255 # dimension(s): # from to offset delta refsys point values x/y # x 1 349 288776 28.5 UTM Zone 25, S... FALSE NULL [x] # y 1 352 9120761 -28.5 UTM Zone 25, S... FALSE NULL [y] # attributes 1 6 NA NA NA NA X1,...,X6 split distributes the band dimension over 6 attributes of a 2-dimensional array, merge reverses this operation. st_redimension can be used for more generic operations, such as splitting a single array dimension over two new dimensions: st_redimension(r, c(349, 352, 3, 2)) # stars object with 4 dimensions and 1 attribute # attribute(s): # Min. 1st Qu. Median Mean 3rd Qu. Max. # L7_ETMs.tif 1 54 69 68.9 86 255 # dimension(s): # from to offset delta refsys point values # X1 1 349 288776 28.5 UTM Zone 25, S... FALSE NULL # X2 1 352 9120761 -28.5 UTM Zone 25, S... FALSE NULL # X3 1 3 NA NA NA NA NULL # X4 1 2 NA NA NA NA NULL 7.3.5 extracting point samples, aggregating A very common use case for raster data cube analysis is the extraction of values at certain locations, or computing aggregations over certain geometries. st_extract extracts point values. We will do this for a few randomly sampled points over the bounding box of r: pts = st_bbox(r) %&gt;% st_as_sfc() %&gt;% st_sample(20) (e = st_extract(r, pts)) # stars object with 2 dimensions and 1 attribute # attribute(s): # Min. 1st Qu. Median Mean 3rd Qu. Max. # L7_ETMs.tif 12 41.8 63 61 80.5 145 # dimension(s): # from to offset delta refsys point # geometry 1 20 NA NA UTM Zone 25, S... TRUE # band 1 6 NA NA NA NA # values # geometry POINT (293002 9115516),...,POINT (290941 9114128) # band NULL which results in a vector data cube with 4 points and 6 bands. 7.3.6 Predictive models The typical model prediction workflow in R works as follows: use a data.frame with response and predictor variables (covariates) create a model object based on this data.frame call predict with this model object and the data.frame with target predictor variable values Package stars provides a predict method for stars objects that essentially wraps the last step, by creating the data.frame, calling the predict method for that, and reconstructing a stars object with the predicted values. We will illustrate this with a trivial two-class example mapping land from sea in the example Landsat data set, using the sample points extracted above, shown in figure 7.6. plot(r[,,,1], reset = FALSE) col = rep(&quot;green&quot;, 20) col[c(8, 14, 15, 18, 19)] = &quot;red&quot; st_as_sf(e) %&gt;% st_coordinates() %&gt;% text(labels = 1:20, col = col) Figure 7.6: randomly chosen sample locations for training data; red: water, green: land From this figure, we read “by eye” that the points 8, 14, 15, 18 and 19 are on water, the others on land. Using a linear discriminant (“maximum likelihood”) classifier, we find model predictions as shown in figure 7.7. rs = split(r) trn = st_extract(rs, pts) trn$cls = rep(&quot;land&quot;, 20) trn$cls[c(8, 14, 15, 18, 19)] = &quot;water&quot; model = MASS::lda(cls ~ ., st_set_geometry(trn, NULL)) pr = predict(rs, model) Here, we used the MASS:: prefix to avoid loading MASS, as that would mask select from dplyr. Another way would be to load MASS and unload it later on with detach(). plot(pr[1], key.pos = 4, key.width = lcm(3.5), key.length = lcm(2)) Figure 7.7: Linear discriminant classifier for land/water, based on training data of figure 7.6 We also see that the layer plotted in figure 7.7 is a factor variable, with class labels. 7.3.7 GDAL utils 7.3.8 Plotting raster data We can use the base plot method for stars objects, where the plot created with plot(r) is shown in figure 7.8. plot(r) Figure 7.8: 6 30m Landsat bands downsampled to 90m for Olinda, Br. is shown in figure 7.8. The default color scale uses grey tones, and stretches these such that color breaks correspond to data quantiles over all bands (“histogram equalization”). A more familiar view is the RGB or false color composite shown in figure 7.9. par(mfrow = c(1, 2)) plot(r, rgb = c(3,2,1), reset = FALSE, main = &quot;RGB&quot;) # rgb plot(r, rgb = c(4,3,2), main = &quot;False color (NIR-R-G)&quot;) # false color Figure 7.9: Two color composites Further details and options are given in Chapter 9. 7.3.9 Analysing raster data Element-wise mathematical functions (section 6.3.2) on stars objects are just passed on to the arrays. This means that we can call functions and create expressions: log(r) # stars object with 3 dimensions and 1 attribute # attribute(s): # Min. 1st Qu. Median Mean 3rd Qu. Max. # L7_ETMs.tif 0 3.99 4.23 4.12 4.45 5.54 # dimension(s): # from to offset delta refsys point values x/y # x 1 349 288776 28.5 UTM Zone 25, S... FALSE NULL [x] # y 1 352 9120761 -28.5 UTM Zone 25, S... FALSE NULL [y] # band 1 6 NA NA NA NA NULL r + 2 * log(r) # stars object with 3 dimensions and 1 attribute # attribute(s): # Min. 1st Qu. Median Mean 3rd Qu. Max. # L7_ETMs.tif 1 62 77.5 77.1 94.9 266 # dimension(s): # from to offset delta refsys point values x/y # x 1 349 288776 28.5 UTM Zone 25, S... FALSE NULL [x] # y 1 352 9120761 -28.5 UTM Zone 25, S... FALSE NULL [y] # band 1 6 NA NA NA NA NULL or even mask out certain values: r2 = r r2[r &lt; 50] = NA r2 # stars object with 3 dimensions and 1 attribute # attribute(s): # Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s # L7_ETMs.tif 50 64 75 79 90 255 149170 # dimension(s): # from to offset delta refsys point values x/y # x 1 349 288776 28.5 UTM Zone 25, S... FALSE NULL [x] # y 1 352 9120761 -28.5 UTM Zone 25, S... FALSE NULL [y] # band 1 6 NA NA NA NA NULL or un-mask areas: r2[is.na(r2)] = 0 r2 # stars object with 3 dimensions and 1 attribute # attribute(s): # Min. 1st Qu. Median Mean 3rd Qu. Max. # L7_ETMs.tif 0 54 69 63 86 255 # dimension(s): # from to offset delta refsys point values x/y # x 1 349 288776 28.5 UTM Zone 25, S... FALSE NULL [x] # y 1 352 9120761 -28.5 UTM Zone 25, S... FALSE NULL [y] # band 1 6 NA NA NA NA NULL Dimension-wise, we can apply functions to selected array dimensions (section 6.3.3) of stars objects similar to how apply does this to arrays. For instance, we can compute for each pixel the mean of the 6 band values by st_apply(r, c(&quot;x&quot;, &quot;y&quot;), mean) # stars object with 2 dimensions and 1 attribute # attribute(s): # Min. 1st Qu. Median Mean 3rd Qu. Max. # mean 25.5 53.3 68.3 68.9 82 255 # dimension(s): # from to offset delta refsys point values x/y # x 1 349 288776 28.5 UTM Zone 25, S... FALSE NULL [x] # y 1 352 9120761 -28.5 UTM Zone 25, S... FALSE NULL [y] A more meaningful function would e.g. compute the NDVI (normalized differenced vegetation index): ndvi = function(b1, b2, b3, b4, b5, b6) (b4 - b3)/(b4 + b3) st_apply(r, c(&quot;x&quot;, &quot;y&quot;), ndvi) # stars object with 2 dimensions and 1 attribute # attribute(s): # Min. 1st Qu. Median Mean 3rd Qu. Max. # ndvi -0.753 -0.203 -0.0687 -0.0643 0.187 0.587 # dimension(s): # from to offset delta refsys point values x/y # x 1 349 288776 28.5 UTM Zone 25, S... FALSE NULL [x] # y 1 352 9120761 -28.5 UTM Zone 25, S... FALSE NULL [y] Alternatively, one could have defined ndvi2 = function(x) (x[4]-x[3])/(x[4]+x[3]) which is more convenient if the number of bands is large, but which is also much slower than ndvi as it needs to be called for every pixel whereas ndvi can be called once for all pixels, or for large chunks of pixels. The mean for each band over the whole image is computed by as.data.frame(st_apply(r, c(&quot;band&quot;), mean)) # band mean # 1 1 79.1 # 2 2 67.6 # 3 3 64.4 # 4 4 59.2 # 5 5 83.2 # 6 6 60.0 the result of which is small enough to be printed here as a data.frame. In these two examples, entire dimensions disappear. Sometimes, this does not happen (section 6.3.2); we can for instance compute the three quartiles for each band st_apply(r, c(&quot;band&quot;), quantile, c(.25, .5, .75)) # stars object with 2 dimensions and 1 attribute # attribute(s): # Min. 1st Qu. Median Mean 3rd Qu. Max. # L7_ETMs.tif 32 60.8 66.5 69.8 78.8 112 # dimension(s): # from to offset delta refsys point values # quantile 1 3 NA NA NA NA 25%, 50%, 75% # band 1 6 NA NA NA NA NULL and see that this creates a new dimension, quantile, with three values. Alternatively, the three quantiles over the 6 bands for each pixel are obtained by st_apply(r, c(&quot;x&quot;, &quot;y&quot;), quantile, c(.25, .5, .75)) # stars object with 3 dimensions and 1 attribute # attribute(s): # Min. 1st Qu. Median Mean 3rd Qu. Max. # L7_ETMs.tif 4 55 69.2 67.2 81.2 255 # dimension(s): # from to offset delta refsys point values x/y # quantile 1 3 NA NA NA NA 25%, 50%, 75% # x 1 349 288776 28.5 UTM Zone 25, S... FALSE NULL [x] # y 1 352 9120761 -28.5 UTM Zone 25, S... FALSE NULL [y] 7.3.10 Curvilinear rasters There are several reasons why non-regular rasters occur (figure 1.5). For one, when the data is Earth-bound, a regular raster does not fit the Earth’s surface, which is curved. Other reasons are: when we convert or transform a regular raster data into another coordinate reference system, it will become curvilinear unless we resample (warp; section 7.7); warping always goes at the cost of some loss of data and is not reversible. observation may lead to irregular rasters; e.g. for satellite swaths, we may have a regular raster in the direction of the satellite (not aligned with \\(x\\) or \\(y\\)), and rectilinear in the direction perpendicular to that (e.g. if the sensor discretizes the viewing angle in equal sections) 7.4 Vector data cube examples 7.4.1 Example: aggregating air quality time series Air quality data from package spacetime were obtained from the airBase European air quality data base. Daily average PM\\(_{10}\\) values were downloaded for rural background stations in Germany, 1998-2009. We can create a stars object from the air matrix, the dates Date vector and the stations SpatialPoints objects by library(spacetime) data(air) # this loads several datasets in .GlobalEnv dim(air) # space time # 70 4383 d = st_dimensions(station = st_as_sfc(stations), time = dates) (aq = st_as_stars(list(PM10 = air), dimensions = d)) # stars object with 2 dimensions and 1 attribute # attribute(s): # Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s # PM10 0 9.92 14.8 17.7 22 274 157659 # dimension(s): # from to offset delta refsys point # station 1 70 NA NA +proj=longlat ... TRUE # time 1 4383 1998-01-01 1 days Date FALSE # values # station POINT (9.59 53.7),...,POINT (9.45 49.2) # time NULL We can see from figure 7.10 that the time series are quite long, but also have large missing value gaps. Figure 7.11 shows the spatial distribution measurement stations and mean PM\\(_{10}\\) values. image(aperm(log(aq), 2:1), main = &quot;NA pattern (white) in PM10 station time series&quot;) Figure 7.10: space-time diagram of PM\\(_{10}\\) measurements by time and station plot(st_as_sf(st_apply(aq, 1, mean, na.rm = TRUE)), reset = FALSE, pch = 16, ylim = st_bbox(DE)[c(2,4)]) # Warning in sp::proj4string(obj): CRS object has comment, which is lost in output plot(DE, add=TRUE) Figure 7.11: locations of PM\\(_{10}\\) measurement stations, showing mean values We can aggregate these station time series to area means, mostly as a simple exercise. For this, we use the aggregate method for stars objects (a = aggregate(aq, st_as_sf(DE_NUTS1), mean, na.rm = TRUE)) # stars object with 2 dimensions and 1 attribute # attribute(s): # Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s # PM10 1.08 10.9 15.3 17.9 21.8 172 25679 # dimension(s): # from to offset delta refsys point # geometry 1 16 NA NA +proj=longlat ... FALSE # time 1 4383 1998-01-01 1 days Date FALSE # values # geometry MULTIPOLYGON (((9.65 49.8, ...,...,MULTIPOLYGON (((10.8 51.6, ... # time NULL and we can now for instance show the maps for six arbitrarily chosen days (figure 7.12), using library(tidyverse) a %&gt;% filter(time &gt;= &quot;2008-01-01&quot;, time &lt; &quot;2008-01-07&quot;) %&gt;% plot(key.pos = 4) Figure 7.12: areal mean PM\\(_{10}\\) values, for six days or create a time series plot of mean values for a single state (figure 7.13) by suppressPackageStartupMessages(library(xts)) plot(as.xts(a)[,4], main = DE_NUTS1$NAME_1[4]) Figure 7.13: areal mean PM\\(_{10}\\) values, for six days 7.4.2 Example: Bristol origin-destination datacube The data used for this example come from (Lovelace, Nowosad, and Muenchow 2019), and concern origin-destination (OD) counts: the number of persons going from region A to region B, by transportation mode. We have feature geometries for the 102 origin and destination regions, shown in figure 7.14. library(spDataLarge) plot(st_geometry(bristol_zones), axes = TRUE, graticule = TRUE) plot(st_geometry(bristol_zones)[33], col = &#39;red&#39;, add = TRUE) Figure 7.14: Origin destination data zones for Bristol, UK, with zone 33 (E02003043) colored red and the OD counts come in a table with OD pairs as records, and transportation mode as variables: head(bristol_od) # # A tibble: 6 x 7 # o d all bicycle foot car_driver train # &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 E02002985 E02002985 209 5 127 59 0 # 2 E02002985 E02002987 121 7 35 62 0 # 3 E02002985 E02003036 32 2 1 10 1 # 4 E02002985 E02003043 141 1 2 56 17 # 5 E02002985 E02003049 56 2 4 36 0 # 6 E02002985 E02003054 42 4 0 21 0 We see that many combinations of origin and destination are implicit zeroes, otherwise these two numbers would have been similar: nrow(bristol_zones)^2 # all combinations # [1] 10404 nrow(bristol_od) # non-zero combinations # [1] 2910 We will form a three-dimensional vector datacube with origin, destination and transportation mode as dimensions. For this, we first “tidy” the bristol_od table to have origin (o), destination (d), transportation mode (mode), and count (n) as variables, using gather: # create O-D-mode array: bristol_tidy &lt;- bristol_od %&gt;% select(-all) %&gt;% gather(&quot;mode&quot;, &quot;n&quot;, -o, -d) head(bristol_tidy) # # A tibble: 6 x 4 # o d mode n # &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; # 1 E02002985 E02002985 bicycle 5 # 2 E02002985 E02002987 bicycle 7 # 3 E02002985 E02003036 bicycle 2 # 4 E02002985 E02003043 bicycle 1 # 5 E02002985 E02003049 bicycle 2 # 6 E02002985 E02003054 bicycle 4 Next, we form the three-dimensional array a, filled with zeroes: od = bristol_tidy %&gt;% pull(&quot;o&quot;) %&gt;% unique() nod = length(od) mode = bristol_tidy %&gt;% pull(&quot;mode&quot;) %&gt;% unique() nmode = length(mode) a = array(0L, c(nod, nod, nmode), dimnames = list(o = od, d = od, mode = mode)) dim(a) # [1] 102 102 4 We see that the dimensions are named with the zone names (o, d) and the transportation mode name (mode). Every row of bristol_tidy denotes an array entry, and we can use this to to fill the non-zero entries of the bristol_tidy table with their appropriate value (n): a[as.matrix(bristol_tidy[c(&quot;o&quot;, &quot;d&quot;, &quot;mode&quot;)])] = bristol_tidy$n To be sure that there is not an order mismatch between the zones in bristol_zones and the zone names in bristol_tidy, we can get the right set of zones by: order = match(od, bristol_zones$geo_code) # it happens this equals 1:102 zones = st_geometry(bristol_zones)[order] (It happens that the order is already correct, but it is good practice to not assume this). Next, with zones and modes we can create a stars dimensions object: library(stars) (d = st_dimensions(o = zones, d = zones, mode = mode)) # from to offset delta refsys point # o 1 102 NA NA WGS 84 FALSE # d 1 102 NA NA WGS 84 FALSE # mode 1 4 NA NA NA FALSE # values # o MULTIPOLYGON (((-2.51 51.4,...,...,MULTIPOLYGON (((-2.55 51.5,... # d MULTIPOLYGON (((-2.51 51.4,...,...,MULTIPOLYGON (((-2.55 51.5,... # mode bicycle,...,train and finally build or stars object from a and d: (odm = st_as_stars(list(N = a), dimensions = d)) # stars object with 3 dimensions and 1 attribute # attribute(s): # Min. 1st Qu. Median Mean 3rd Qu. Max. # N 0 0 0 4.8 0 1296 # dimension(s): # from to offset delta refsys point # o 1 102 NA NA WGS 84 FALSE # d 1 102 NA NA WGS 84 FALSE # mode 1 4 NA NA NA FALSE # values # o MULTIPOLYGON (((-2.51 51.4,...,...,MULTIPOLYGON (((-2.55 51.5,... # d MULTIPOLYGON (((-2.51 51.4,...,...,MULTIPOLYGON (((-2.55 51.5,... # mode bicycle,...,train We can take a single slice through from this three-dimensional array, e.g. for zone 33 (figure 7.14), by odm[,,33], and plot it with plot(odm[,,33] + 1, logz = TRUE) # Warning in st_as_sf.stars(x): working on the first sfc dimension only # Warning in st_bbox.dimensions(st_dimensions(obj), ...): returning the bounding # box of the first geometry dimension # Warning in st_bbox.dimensions(st_dimensions(obj), ...): returning the bounding # box of the first geometry dimension Figure 7.15: OD matrix sliced for destination zone 33, by transportation mode the result of which is shown in figure 7.15. Subsetting this way, we take all attributes (there is only one: N) since the first argument is empty, we take all origin regions (second argument empty), we take destination zone 33 (third argument), and all transportation modes (fourth argument empty, or missing). We plotted this particular zone because it has the largest number of travelers as its destination. We can find this out by summing all origins and travel modes by destination: d = st_apply(odm, 2, sum) which.max(d[[1]]) # [1] 33 Other aggregations we can carry out include: total transportation by OD (102 x 102): st_apply(odm, 1:2, sum) # stars object with 2 dimensions and 1 attribute # attribute(s): # Min. 1st Qu. Median Mean 3rd Qu. Max. # sum 0 0 0 19.2 19 1434 # dimension(s): # from to offset delta refsys point # o 1 102 NA NA WGS 84 FALSE # d 1 102 NA NA WGS 84 FALSE # values # o MULTIPOLYGON (((-2.51 51.4,...,...,MULTIPOLYGON (((-2.55 51.5,... # d MULTIPOLYGON (((-2.51 51.4,...,...,MULTIPOLYGON (((-2.55 51.5,... Origin totals, by mode: st_apply(odm, c(1,3), sum) # stars object with 2 dimensions and 1 attribute # attribute(s): # Min. 1st Qu. Median Mean 3rd Qu. Max. # sum 1 57.5 214 490 771 2903 # dimension(s): # from to offset delta refsys point # o 1 102 NA NA WGS 84 FALSE # mode 1 4 NA NA NA FALSE # values # o MULTIPOLYGON (((-2.51 51.4,...,...,MULTIPOLYGON (((-2.55 51.5,... # mode bicycle,...,train Destination totals, by mode: st_apply(odm, c(2,3), sum) # stars object with 2 dimensions and 1 attribute # attribute(s): # Min. 1st Qu. Median Mean 3rd Qu. Max. # sum 0 13 104 490 408 12948 # dimension(s): # from to offset delta refsys point # d 1 102 NA NA WGS 84 FALSE # mode 1 4 NA NA NA FALSE # values # d MULTIPOLYGON (((-2.51 51.4,...,...,MULTIPOLYGON (((-2.55 51.5,... # mode bicycle,...,train Origin totals, summed over modes: o = st_apply(odm, 1, sum) Destination totals, summed over modes (we had this): d = st_apply(odm, 2, sum) We plot o and d together after joining them by x = (c(o, d, along = list(od = c(&quot;origin&quot;, &quot;destination&quot;)))) plot(x, logz = TRUE) Figure 7.16: total commutes, summed by origin (left) or destination (right). the result of which is shown in figure 7.16. There is something to say for the argument that such maps give the wrong message, as both amount (color) and polygon size give an impression of amount. To take out the amount in the count, we can compute densities (count / km\\(^2\\)), by library(units) # udunits database from /usr/share/xml/udunits/udunits2.xml a = set_units(st_area(st_as_sf(o)), km^2) o$sum_km = o$sum / a d$sum_km = d$sum / a od = c(o[&quot;sum_km&quot;], d[&quot;sum_km&quot;], along = list(od = c(&quot;origin&quot;, &quot;destination&quot;))) plot(od, logz = TRUE) Figure 7.17: total commutes per square km, by area of origin (left) or destination (right) shown in figure 7.17. Another way to normalize these totals would be to divide them by population size. 7.4.3 Tidy array data The tidy data paper (Wickham 2014b) may suggest that such array data should be processed not as an array, but in a long table where each row holds (region, class, year, value), and it is always good to be able to do this. For primary handling and storage however, this is often not an option, because a lot of array data are collected or generated as array data, e.g. by imagery or other sensory devices, or e.g. by climate models it is easier to derive the long table form from the array than vice versa the long table form requires much more memory, since the space occupied by dimension values is \\(O(nmp)\\), rather than \\(O(n+m+p)\\) when missing-valued cells are dropped, the long table form loses the implicit indexing of the array form To put this argument to the extreme, consider for instance that all image, video and sound data are stored in array form; few people would make a real case for storing them in a long table form instead. Nevertheless, R packages like tsibble take this approach, and have to deal with ambiguous ordering of multiple records with identical time steps for different spatial features and index them, which is solved for both automatically by using the array form – at the cost of using dense arrays, in package stars. Package stars tries to follow the tidy manifesto to handle array sets, and has particularly developed support for the case where one or more of the dimensions refer to space, and/or time. 7.5 raster-to-vector, vector-to-raster Section 1.3 already showed some examples of raster-to-vector and vector-to-raster conversions, this section will add some code details and examples. 7.5.1 vector-to-raster st_as_stars is meant as a method to transform objects into stars objects. However, not all stars objects are raster objects, and the method for sf objects creates a vector data cube with the geometry as its spatial (vector) dimension, and attributes as attributes. When given a feature geometry (sfc) object, st_as_stars will rasterize it, as shown in section 7.7, and in figure 7.18. sf_use_s2(TRUE) (file = system.file(&quot;gpkg/nc.gpkg&quot;, package=&quot;sf&quot;)) # [1] &quot;/home/edzer/R/x86_64-pc-linux-gnu-library/4.0/sf/gpkg/nc.gpkg&quot; read_sf(file) %&gt;% st_geometry() %&gt;% st_as_stars() %&gt;% plot() Figure 7.18: rasterizing vector geometry using st_as_stars Here, st_as_stars can be parameterized to control cell size, number of cells, and/or extent. The cell values returned are 0 for cells with center point outside the geometry and 1 for cell with center point inside or on the border of the geometry. Rasterizing existing features is done using st_rasterize, as also shown in figure 1.4: library(dplyr) read_sf(file) %&gt;% mutate(name = as.factor(NAME)) %&gt;% select(SID74, SID79, name) %&gt;% st_rasterize() # stars object with 2 dimensions and 3 attributes # attribute(s): # SID74 SID79 name # Min. : 0 Min. : 0 Sampson : 655 # 1st Qu.: 3 1st Qu.: 3 Columbus: 648 # Median : 5 Median : 6 Robeson : 648 # Mean : 8 Mean :10 Bladen : 604 # 3rd Qu.:10 3rd Qu.:13 Wake : 590 # Max. :44 Max. :57 (Other) :30952 # NA&#39;s :30904 NA&#39;s :30904 NA&#39;s :30904 # dimension(s): # from to offset delta refsys point values x/y # x 1 461 -84.3239 0.0192484 NAD27 FALSE NULL [x] # y 1 141 36.5896 -0.0192484 NAD27 FALSE NULL [y] Similarly, line and point geometries can be rasterized, as shown in figure 7.19. read_sf(file) %&gt;% st_cast(&quot;MULTILINESTRING&quot;) %&gt;% select(CNTY_ID) %&gt;% st_rasterize() %&gt;% plot() Figure 7.19: rasterization of North Carolina boundaries 7.6 Coordinate transformations and conversions 7.6.1 st_crs Spatial objects of class sf or stars contain a coordinate reference system that can be get or replaced with st_crs, or be set or replaced in a pipe with st_set_crs. Coordinate reference systems can be set with an EPSG code, like st_crs(4326) which will be converted to st_crs('EPSG:4326'), or with a PROJ.4 string like \"\"+proj=utm +zone=25 +south\", a name like “WGS84”, or a name preceded by an authority like “OGC:CRS84”; alternatives include a coordinate reference system definition in WKT, WKT2 (section 2.5) or PROJJSON. The object returned contains two fields, wkt with the WKT2 representation, and input with the user input, if any, or else a copy of the wkt field. Note that PROJ.4 strings can be used to define some coordinate reference systems, but they cannot be used to represent coordinate reference systems. Conversion of a WKT2 in a crs object to a proj4string using the $proj4string method, as in x = st_crs(&quot;OGC:CRS84&quot;) x$proj4string # [1] &quot;+proj=longlat +datum=WGS84 +no_defs&quot; may succeed but is not in general lossless or invertible. 7.6.2 st_transform, sf_project Coordinate transformations or conversions (section 1.7.2) for sf or stars objects are carried out with st_transform, which takes as its first argument a spatial object of class sf or stars that has a coordinate reference system set, and as a second argument with an crs object (or something that can be converted to it with st_crs). When PROJ finds more than one possibility to transform or convert from the source crs to the target crs, it chooses the one with the highest declared accuracy. More fine-grained control over the options is explained in section 7.6.5. A lower-level function to transform or convert coordinates not in sf or stars objects is sf_project: it takes a matrix with coordinates and a source and target crs, and returns transformed or converted coordinates. 7.6.3 sf_proj_info Function sf_proj_info can be used to query available projections, ellipsoids, units and prime meridians available in the PROJ software. It takes a single parameter, type, which can have the following values: type = \"proj\" lists the short and long names of available projections; short names can be used in a “+proj=name” string. type = \"ellps\" lists available ellipses, with name, long name, and ellipsoidal parameters. type = \"units\" lists the available length units, with conversion constant to meters type = \"prime_meridians\" lists the prime meridians with their position with respect to the Greenwich meridian. 7.6.4 proj.db, datum grids, cdn.proj.org, local cache Datum grids (section 1.7.2) can be installed locally, or be read from the PROJ datum grid CDN at https://cdn.proj.org/. If installed locally, they are read from the PROJ search path, which is shown by sf_proj_search_paths() # [1] &quot;/home/edzer/.local/share/proj&quot; &quot;/usr/share/proj&quot; The main PROJ database is proj.db, an sqlite3 database typically found at paste0(tail(sf_proj_search_paths(), 1), .Platform$file.sep, &quot;proj.db&quot;) # [1] &quot;/usr/share/proj/proj.db&quot; which can be read. The version of the snapshot of the EPSG database included in each PROJ release is stated in the \"metadata\" table of proj.db; the version of the PROJ runtime used by sf is shown by sf_extSoftVersion()[&quot;PROJ&quot;] # PROJ # &quot;7.2.1&quot; If for a particular coordinate transformation datum grids are not locally found, PROJ will search for online datum grids in the PROJ CDN when sf_proj_network() # [1] FALSE returns TRUE. By default it is set to FALSE, but sf_proj_network(TRUE) # [1] &quot;https://cdn.proj.org&quot; sets it to TRUE and returns the URL of the network resource used; this resource can also be set to another resource, that may be faster or less limited. After querying a datum grid on the CDN, PROJ writes the portion of the grid queried (not, by default, the entire grid) to a local cache, which is another sqlite3 database found locally in a user directory, e.g. at list.files(sf_proj_search_paths()[1], full.names = TRUE) # [1] &quot;/home/edzer/.local/share/proj/cache.db&quot; that will be searched first in subsequent datum grid queries. 7.6.5 transformation pipelines Internally, PROJ uses a so-called coordinate operation pipeline, to represent the sequence of operations to get from a source CRS to a target CRS. Given multiple options to go from source to target, st_transform chooses the one with highest accuracy. We can query the options available by (p = sf_proj_pipelines(&quot;EPSG:4326&quot;, &quot;EPSG:22525&quot;)) # Candidate coordinate operations found: 5 # Strict containment: FALSE # Axis order auth compl: FALSE # Source: EPSG:4326 # Target: EPSG:22525 # Best instantiable operation has accuracy: 2 m # Description: axis order change (2D) + Inverse of Corrego Alegre 1970-72 to # WGS 84 (2) + UTM zone 25S # Definition: +proj=pipeline +step +proj=unitconvert +xy_in=deg +xy_out=rad # +step +inv +proj=hgridshift # +grids=br_ibge_CA7072_003.tif +step +proj=utm # +zone=25 +south +ellps=intl and see that pipeline with the highest accuracy is summarised; we can see that it specifies use of a datum grid. Had we not switched on the network search, we would have obtained a different result: sf_proj_network(FALSE) # character(0) sf_proj_pipelines(&quot;EPSG:4326&quot;, &quot;EPSG:22525&quot;) # Candidate coordinate operations found: 5 # Strict containment: FALSE # Axis order auth compl: FALSE # Source: EPSG:4326 # Target: EPSG:22525 # Best instantiable operation has accuracy: 5 m # Description: axis order change (2D) + Inverse of Corrego Alegre 1970-72 to # WGS 84 (4) + UTM zone 25S # Definition: +proj=pipeline +step +proj=unitconvert +xy_in=deg +xy_out=rad # +step +proj=push +v_3 +step +proj=cart # +ellps=WGS84 +step +proj=helmert +x=206.05 # +y=-168.28 +z=3.82 +step +inv +proj=cart # +ellps=intl +step +proj=pop +v_3 +step +proj=utm # +zone=25 +south +ellps=intl # Operation 4 is lacking 1 grid with accuracy 2 m # Missing grid: br_ibge_CA7072_003.tif # URL: https://cdn.proj.org/br_ibge_CA7072_003.tif and a report that a datum grid is missing The object returned by sf_proj_pipelines is a sub-classed data.frame, with columns names(p) # [1] &quot;id&quot; &quot;description&quot; &quot;definition&quot; &quot;has_inverse&quot; &quot;accuracy&quot; # [6] &quot;axis_order&quot; &quot;grid_count&quot; &quot;instantiable&quot; &quot;containment&quot; and we can list for instance the accuracies by p$accuracy # [1] 5 5 8 2 NA Here, NA refers to “ballpark accuracy”, which may be anything in the 30-100 m range: p[is.na(p$accuracy),] # Candidate coordinate operations found: 1 # Strict containment: FALSE # Axis order auth compl: FALSE # Source: EPSG:4326 # Target: EPSG:22525 # Best instantiable operation has only ballpark accuracy # Description: axis order change (2D) + Ballpark geographic offset from WGS 84 # to Corrego Alegre 1970-72 + UTM zone 25S # Definition: +proj=pipeline +step +proj=unitconvert +xy_in=deg +xy_out=rad # +step +proj=utm +zone=25 +south +ellps=intl The default, most accurate pipeline chosen by st_transform can be overriden by specifying pipeline argument, as selected from the set of options in p$definition. 7.7 transforming and warping rasters When using st_transform on a raster data set, as e.g. in tif = system.file(&quot;tif/L7_ETMs.tif&quot;, package = &quot;stars&quot;) read_stars(tif) %&gt;% st_transform(4326) # stars object with 3 dimensions and 1 attribute # attribute(s): # Min. 1st Qu. Median Mean 3rd Qu. Max. # L7_ETMs.tif 1 54 69 68.9 86 255 # dimension(s): # from to offset delta refsys point values x/y # x 1 349 NA NA WGS 84 FALSE [349x352] -34.9165,...,-34.8261 [x] # y 1 352 NA NA WGS 84 FALSE [349x352] -8.0408,...,-7.94995 [y] # band 1 6 NA NA NA NA NULL # curvilinear grid we see that a curvilinear is created, which means that for every grid cell the coordinates are computed in the new CRS, which no longer form a regular grid. Plotting such data is extremely slow, as small polygons are computed for every grid cell and then plotted. The advantage of this is that no information is lost: grid cell values remain identical after the projection. When we start with a raster on a regular grid and want to obtain a regular grid in a new coordinate reference system, we need to warp the grid: we need to recreate a grid at new locations, and use some rule to assign values to new grid cells. Rules can involve using the nearest value, or using some form of interpolation. This operation is not lossless and not invertible. The best options for warping is to specify the target grid as a stars object. When only a target CRS is specified, default options for the target grid are picked that may be completely inappropriate for the problem at hand. An example workflow that uses only a target CRS is read_stars(tif) %&gt;% st_warp(crs = st_crs(4326)) %&gt;% st_dimensions() # from to offset delta refsys point values x/y # x 1 350 -34.9166 0.000259243 WGS 84 NA NULL [x] # y 1 352 -7.94982 -0.000259243 WGS 84 NA NULL [y] # band 1 6 NA NA NA NA NULL which creates a pretty close raster, but then the transformation is also relatively modest. For a workflow that creates a target raster first, here with exactly the same number of rows and columns as the original raster one could use: r = read_stars(tif) grd = st_bbox(r) %&gt;% st_as_sfc() %&gt;% st_transform(4326) %&gt;% st_bbox() %&gt;% st_as_stars(nx = dim(r)[&quot;x&quot;], ny = dim(r)[&quot;y&quot;]) st_warp(r, grd) # stars object with 3 dimensions and 1 attribute # attribute(s): # Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s # L7_ETMs.tif 1 54 69 68.9 86 255 6180 # dimension(s): # from to offset delta refsys point values x/y # x 1 349 -34.9166 0.000259666 WGS 84 NA NULL [x] # y 1 352 -7.94982 -0.000258821 WGS 84 NA NULL [y] # band 1 6 NA NA NA NA NULL 7.8 Exercises NDVI, normalized differenced vegetation index, is coputed as (NIR-R)/(NIR+R), with NIR the near infrared and R the red band. Read the L7_ETMs.tif file into object x, and distribute the band dimensions over attributes by split(x, \"band\"). Then, compute NDVI by using an expression that uses the NIR (band 4) and R (band 3) attributes directly. Compute NDVI for the S2 image, using st_apply and an a function ndvi = function(x) (x[4]-x[3])/(x[4]+x[3]). Plot the result, and write the result to a GeoTIFF. Explain the difference in runtime between plotting and writing. Use st_transform to transform the stars object read from L7_ETMs.tif to EPSG 4326. Print the object. Is this a regular grid? Plot the first band using arguments axes=TRUE and border=NA, and explain why this takes such a long time. Use st_warp to warp the L7_ETMs.tif object to EPSG 4326, and plot the resulting object with axes=TRUE. Why is the plot created much faster than after st_transform? References "],["large.html", "Chapter 8 Large data sets 8.1 Vector data: sf 8.2 Raster data: stars 8.3 Very large data cubes", " Chapter 8 Large data sets This chapter describes how large spatial and spatiotemporal datasets can be handled with R, with a focus on packages sf and stars. For practical use, we classify large data sets as either too large to fit in working memory, or also too large to fit on the local hard drive, or also too large to download it to own compute infrastructure these three categories correspond very roughly to Gigabyte-, Terabyte- and Petabyte-sized data sets. 8.1 Vector data: sf 8.1.1 Reading from disk Function st_read reads vector data from disk, using GDAL, and then keeps the data read in working memory. In case the file is too large to be read in working memory, several options exist to read parts of the file. The first is to set argument wkt_filter with a WKT text string containing a geometry; only geometries from the target file that intersect with this geometry will be returned. An example is library(sf) (file = system.file(&quot;gpkg/nc.gpkg&quot;, package=&quot;sf&quot;)) # [1] &quot;/home/edzer/R/x86_64-pc-linux-gnu-library/4.0/sf/gpkg/nc.gpkg&quot; bb = &quot;POLYGON ((-81.7 36.2, -80.4 36.2, -80.4 36.5, -81.7 36.5, -81.7 36.2))&quot; nc.1 = st_read(file, wkt_filter = bb) # Reading layer `nc.gpkg&#39; from data source # `/home/edzer/R/x86_64-pc-linux-gnu-library/4.0/sf/gpkg/nc.gpkg&#39; # using driver `GPKG&#39; # Simple feature collection with 8 features and 14 fields # Geometry type: MULTIPOLYGON # Dimension: XY # Bounding box: xmin: -81.9 ymin: 36 xmax: -80 ymax: 36.6 # Geodetic CRS: NAD27 The second option is to use the query argument to st_read, which can be any query in “OGR SQL” dialect, which can be used to select features from a layer, and limit fields. An example is: q = paste(&quot;select BIR74,SID74,geom from &#39;nc.gpkg&#39; where BIR74 &gt; 1500&quot;) nc.2 = st_read(file, query = q) # Reading layer `nc.gpkg&#39; from data source # `/home/edzer/R/x86_64-pc-linux-gnu-library/4.0/sf/gpkg/nc.gpkg&#39; # using driver `GPKG&#39; # Simple feature collection with 61 features and 2 fields # Geometry type: MULTIPOLYGON # Dimension: XY # Bounding box: xmin: -83.3 ymin: 33.9 xmax: -76.1 ymax: 36.6 # Geodetic CRS: NAD27 note that nc.pgkg is the layer name, which can be obtained from file using st_layers. Sequences of records can be read using LIMIT and OFFSET, to read records 51-60 use q = paste(&quot;select BIR74,SID74,geom from &#39;nc.gpkg&#39; LIMIT 10 OFFSET 50&quot;) nc.2 = st_read(file, query = q) # Reading layer `nc.gpkg&#39; from data source # `/home/edzer/R/x86_64-pc-linux-gnu-library/4.0/sf/gpkg/nc.gpkg&#39; # using driver `GPKG&#39; # Simple feature collection with 10 features and 2 fields # Geometry type: MULTIPOLYGON # Dimension: XY # Bounding box: xmin: -84 ymin: 35.2 xmax: -75.5 ymax: 36.2 # Geodetic CRS: NAD27 Further query options include selection on geometry type, polygon area. When the dataset queried is a spatial database, then the query is passed on to the database and not interpreted by GDAL; this means that more powerful features will be available. Further information is found in the GDAL documentation under “OGR SQL dialect”. Very large file files or directories that are zipped can be read without the need to unzip them, using the /vsizip (for zip), /vsigzip (for gzip) or /vsitar (for tar files) prefix to files; this is followed by the path to the zip file, and then followed by the file inside this zip file. Reading files this way may come at some computatational cost. 8.1.2 Reading from databases, dbplyr Although GDAL has support for several spatial databases, and as mentioned above it passes on SQL in the query argument to the database, it is sometimes beneficial to directly read and to a spatial database using the R database drivers for this. An example of this is: pg &lt;- DBI::dbConnect( RPostgres::Postgres(), host = &quot;localhost&quot;, dbname = &quot;postgis&quot;) st_read(pg, query = &quot;select BIR74,wkb_geometry from nc limit 3&quot;) # Simple feature collection with 3 features and 1 field # Geometry type: MULTIPOLYGON # Dimension: XY # Bounding box: xmin: -81.7 ymin: 36.2 xmax: -80.4 ymax: 36.6 # Geodetic CRS: NAD27 # bir74 wkb_geometry # 1 1091 MULTIPOLYGON (((-81.5 36.2,... # 2 487 MULTIPOLYGON (((-81.2 36.4,... # 3 3188 MULTIPOLYGON (((-80.5 36.2,... A spatial query might look like q = &quot;SELECT BIR74,wkb_geometry FROM nc WHERE \\ ST_Intersects(wkb_geometry, &#39;SRID=4267;POINT (-81.49826 36.4314)&#39;);&quot; st_read(pg, query = q) # Simple feature collection with 1 feature and 1 field # Geometry type: MULTIPOLYGON # Dimension: XY # Bounding box: xmin: -81.7 ymin: 36.2 xmax: -81.2 ymax: 36.6 # Geodetic CRS: NAD27 # bir74 wkb_geometry # 1 1091 MULTIPOLYGON (((-81.5 36.2,... Here, the intersection is done in the database, and uses the spatial index typically present. The same mechanism works when using dplyr with a database backend: library(dplyr, warn.conflicts = FALSE) nc_db = tbl(pg, &quot;nc&quot;) Spatial queries can be formulated and are passed on to the database: nc_db %&gt;% filter(ST_Intersects(wkb_geometry, &#39;SRID=4267;POINT (-81.49826 36.4314)&#39;)) %&gt;% collect() # # A tibble: 1 x 16 # ogc_fid area perimeter cnty_ cnty_id name fips fipsno cress_id bir74 sid74 # &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 1 0.114 1.44 1825 1825 Ashe 37009 37009 5 1091 1 # # … with 5 more variables: nwbir74 &lt;dbl&gt;, bir79 &lt;dbl&gt;, sid79 &lt;dbl&gt;, # # nwbir79 &lt;dbl&gt;, wkb_geometry &lt;pq_gmtry&gt; nc_db %&gt;% filter(ST_Area(wkb_geometry) &gt; 0.1) %&gt;% head(3) # # Source: lazy query [?? x 16] # # Database: postgres [edzer@localhost:5432/postgis] # ogc_fid area perimeter cnty_ cnty_id name fips fipsno cress_id bir74 sid74 # &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 1 0.114 1.44 1825 1825 Ashe 37009 37009 5 1091 1 # 2 3 0.143 1.63 1828 1828 Surry 37171 37171 86 3188 5 # 3 5 0.153 2.21 1832 1832 North… 37131 37131 66 1421 9 # # … with 5 more variables: nwbir74 &lt;dbl&gt;, bir79 &lt;dbl&gt;, sid79 &lt;dbl&gt;, # # nwbir79 &lt;dbl&gt;, wkb_geometry &lt;pq_gmtry&gt; It should be noted that PostGIS’ ST_Area computes the same area as the AREA field in nc, which is the meaningless value obtained by assuming the coordinates are projected, although they are ellipsoidal. 8.1.3 Reading from online resources or web services GDAL drivers support reading from online recourses, by prepending /vsicurl/ before the URL starting with e.g. https://. A number of similar drivers specialized for particular clouds include /vsis3 for Amazon S3, /vsigs for Google Cloud Storage, /vsiaz for Azure, /vsioss for Alibaba Cloud, or /vsiswift for OpenStack Swift Object Storage. These prepositions can be combined e.g. with /vsizip/ to read a zipped online resource. Depending on the file format used, reading information this way may always involve reading the entire file, or reading it multiple times, and may not always be the most efficient way of handling resources. A format like “cloud-optimized geotiff” (COG) has been specially designed to be efficient and resource-friendly in many cases, e.g. for only reading the metadata, or for only reading overviews (low-resolutions versions of the full imagery) or spatial segments. COGs can also be created using the GeoTIFF driver of GDAL, and setting the right dataset creation options in a write_stars call. 8.1.4 API’s, OpenStreetMap Although online resource do not have to be stored files but could be created server-side on the fly, typical web services for geospatial data create data on the fly, and give access to this through an API. As an example, data from OpenStreetMap can be bulk downloaded and read locally, e.g. using the GDAL vector driver, but more typical a user wants to obtain a small subset of the data or use the data for a small query. Several R pacages exist that query openstreetmap data: Package OpenStreetMap downloads data as raster tiles, typically used as backdrop or reference for plotting other features Package osmdata downloads vector data as points, lines or polygons in sf or sp format Package osmar returns vector data, but in addition the network topology (as an igraph object) that contains how road elements form a network, and has functions that compute the shortest route. When provided with a correctly formulated API call in the URL the highly configurable GDAL OSM driver (in st_read) can read an “.osm” file (xml) and returns a dataset with five layers: points that have significant tags, lines with non-area “way” features, multilinestrings with “relation” features, multipolygons with “relation” features and other_relations. A simple and very small bounding box query to OpenStreetMap could look like download.file( &quot;https://openstreetmap.org/api/0.6/map?bbox=7.595,51.969,7.598,51.970&quot; &quot;data/ms.osm&quot;, method = &quot;auto&quot;) and from this file we can read the layer lines, and plot its first attribute by o = read_sf(&quot;data/ms.osm&quot;, &quot;lines&quot;) p = read_sf(&quot;data/ms.osm&quot;, &quot;multipolygons&quot;) bb = st_bbox(c(xmin=7.595, ymin = 51.969, xmax = 7.598, ymax = 51.970), crs = 4326) plot(st_as_sfc(bb), axes = TRUE, lwd = 2, lty = 2, cex.axis = .5) plot(o[,1], lwd = 2, add = TRUE) plot(st_geometry(p), border = NA, col = &#39;#88888888&#39;, add = TRUE) Figure 8.1: OpenStreetMap vector data the result of which is shown in figure 8.1. The overpass API provides a more generic and powerful query functionality to OpenStreetMap data. 8.2 Raster data: stars A common challenge with raster datasets is not only that they come in large files (single Sentinel-2 tiles are around 1 Gb), but that many of these files, potentially thousands, are needed to address the area and time period of interest. At time of writing this, the Copernicus program that runs all Sentinel satellites publishes 160 Tb of images per day. This means that a classic pattern in using R, consisting of downloading data to local disc, loading the data in memory, analysing it is not going to work. Cloud-based Earth Observation processing platforms like Google Earth Engine (Gorelick et al. 2017) or Sentinel Hub recognize this and let users work with datasets up to the petabyte range rather easily and with a great deal of interactivity. They share the following properties: computations are posponed as long as possible (lazy evaluation), only the data you ask for are being computed and returned, and nothing more, storing intermediate results is avoided in favour of on-the-fly computations, maps with useful results are generated and shown quickly to allow for interactive model development. This is similar to the dbplyr interface to databases and cloud-based analytics environments, but differs in the aspect of what we want to see quickly: rather than the first \\(n\\) records of a dbplyr table, we want a quick overview of the results, in the form of a map covering the whole area, or part of it, but at screen resolution rather than native (observation) resolution. If for instance we want to “see” results for the United States on screen with 1000 x 1000 pixels, we only need to compute results for this many pixels, which corresponds roughly to data on a grid with 3000 m x 3000 m grid cells. For Sentinel-2 data with 10 m resolution, this means we can subsample with a factor 300, giving 3 km x 3 km resolution. Processing, storage and network requirements then drop a factor \\(300^2 \\approx 10^5\\), compared to working on the native 10 m x 10 m resolution. On the platforms mentioned, zooming in the map triggers further computations on a finer resolution and smaller extent. A simple optimisation that follows these lines is how stars’ plot method works: in case of plotting large rasters, it subsamples the array before it plots, drastically saving time. The degree of subsampling is derived from the plotting region size and the plotting resolution (pixel density). For vector devices, such as pdf, R sets plot resolution to 75 dpi, corresponding to 0.3 mm per pixel. Enlarging plots may reveal this, but replotting to an enlarged devices will create a plot at target density. 8.2.1 stars proxy objects To handle datasets that are too large to fit in memory, stars provides stars_proxy objects. To demonstrate its use, we will use the starsdata package, an R data package with larger datasets (around 1 Gb total). It can be installed by install.packages(&quot;starsdata&quot;, repos = &quot;http://pebesma.staff.ifgi.de&quot;, type = &quot;source&quot;) We can “load” a Sentinel-2 image from it by f = &quot;sentinel/S2A_MSIL1C_20180220T105051_N0206_R051_T32ULE_20180221T134037.zip&quot; granule = system.file(file = f, package = &quot;starsdata&quot;) file.size(granule) # [1] 7.69e+08 base_name = strsplit(basename(granule), &quot;.zip&quot;)[[1]] s2 = paste0(&quot;SENTINEL2_L1C:/vsizip/&quot;, granule, &quot;/&quot;, base_name, &quot;.SAFE/MTD_MSIL1C.xml:10m:EPSG_32632&quot;) (p = read_stars(s2, proxy = TRUE)) # stars_proxy object with 1 attribute in 1 file(s): # $`MTD_MSIL1C.xml:10m:EPSG_32632` # [1] &quot;[...]/MTD_MSIL1C.xml:10m:EPSG_32632&quot; # # dimension(s): # from to offset delta refsys point values x/y # x 1 10980 3e+05 10 WGS 84 / UTM z... NA NULL [x] # y 1 10980 6e+06 -10 WGS 84 / UTM z... NA NULL [y] # band 1 4 NA NA NA NA B4,...,B8 object.size(p) # 11160 bytes and we see that this does not actually load any of the pixel values, but keeps the reference to the dataset and fills the dimensions table. (The convoluted s2 name is needed to point GDAL to the right file inside the .zip file containing 115 files in total). The idea of a proxy object is that we can build expressions like p2 = p * 2 but that the computations for this are postponed. Only when we really need the data, e.g. because we want to plot it, is p * 2 evaluated. We need data when either we want to plot data, or we want to write an object to disk, with write_stars, or we want to explicitly load an object in memory, with st_as_stars In case the entire object does not fit in memory, plot and write_stars choose different strategies to deal with this: plot fetches only the pixels that can be seen, rather than all pixels available, and write_stars reads, processes, and writes data chunk by chunk. Downsampling and chunking is implemented for spatially dense images, not e.g. for dense time series, or other dense dimensions. As an example, the output of plot(p), shown in figure 8.2 plot(p) Figure 8.2: Plot of downsampled 10 m bands of a Sentinel-2 scene only fetches the pixels that can be seen on the plot device, rather than the 10980 x 10980 pixels available in each band. The downsampling ratio taken is (ds = floor(sqrt(prod(dim(p)) / prod(dev.size(&quot;px&quot;))))) # [1] 19 meaning that for every 19 \\(\\times\\) 19 sub-image in the original image, only one pixel is read, and plotted. This value is still a bit too low as it ignores the white space and space for the key on the plotting device. 8.2.2 Operations on proxy objects A few dedicated methods are available for stars_proxy objects: methods(class = &quot;stars_proxy&quot;) # [1] [ [[&lt;- [&lt;- adrop aggregate # [6] aperm as.data.frame c coerce dim # [11] droplevels filter initialize is.na mapView # [16] Math merge mutate Ops plot # [21] predict print pull replace_na select # [26] show slice slotsFromS3 split st_apply # [31] st_as_sf st_as_stars st_crop st_mosaic st_redimension # [36] st_sample st_set_bbox transmute write_stars # see &#39;?methods&#39; for accessing help and source code We have seen plot and print in action; dim reads out the dimension from the dimensions metadata table. The three methods that actually fetch data are st_as_stars, plot and write_stars. st_as_stars reads the actual data into a stars object, its argument downsample controls the downsampling rate. plot does this too, choosing an appropriate downsample value from the device resolution, and plots the object. write_stars writes a star_proxy object to disc. All other methods for stars_proxy objects do not actually operate on the raster data but add the operations to a to do list, attached to the object. Only when actual raster data are fetched, e.g. by calling plot or st_as_stars, the commands in this list are executed. st_crop limits the extent (area) of the raster that will be read. c combines stars_proxy objects, but still doesn’t read any data. adrop drops empty dimensions, aperm changes dimension order. write_stars reads and processes its input chunk-wise; it has an argument chunk_size that lets users control the size of spatial chunks. 8.3 Very large data cubes At some stage, data sets need to be analysed that are so large that downloading them is no longer feasible; even when local storage would be sufficient, network bandwidth may become limiting. Examples are satellite image archives such as those from Landsat and Copernicus (Sentinel-x), or model computations such as the ERA5 (Hersbach et al. 2020), a model reanalysis of the global atmosphere, land surface and ocean waves from 1950 onwards. In such cases it may be most helpful to either gain access to (typically: rent) virtual machines in a cloud where these data are available and nearby (i.e., the data should be stored in the same data center as where the virtual machine is), or to use a system that lets the user carry out such computations without having to worry about virtual machines. Both options will be discussed. 8.3.1 Finding and processing assets When working on a virtual machine on a cloud, a first task is usally to find the assets (files) to work on. It looks attractive to obtain a file listing, and then parse file names such as S2A_MSIL1C_20180220T105051_N0206_R051_T32ULE_20180221T134037.zip for their metadata including the date of acquisition and the code of the spatial tile covered. Obtaining such a file listing however is usually computationally very demanding, as is the processing of the result, when the number of tiles runs in the many millions. A solution to this is to use a catalogue. The recently developed and increasingly deployed STAC, short for spatiotemporal asset catalogue, provides an API that can be used to query image collections by properties like bounding box, date, band, and cloud coverage. The R package rstac (Brazil Data Cube Team 2021) provides an R interface to create queries, and manage the information returned. Processing the resulting files may involve creating a data cube at a lower spatial and/or temporal resolution, from images that may span a range of coordinate reference systems (e.g., several UTM zones). An R package that can do that is gdalcubes (Appel 2020; Appel and Pebesma 2019). 8.3.2 Processing data: GEE, openEO Platforms that do not require the management and programming of virtual machines in the cloud but provide direct access to the imagery managed include GEE, openEO, and the climate data store. Google Earth Engine (GEE) is a cloud platform that allows users to compute on large amounts of Earth Observation data as well as modelling products (Gorelick et al. 2017). It has powerful analysis capabilities, including most of the data cube operations explained in section 6.3. It has an IDE where scripts can be written in JavaScript, and a Python interface to the same functionality. The code of GEE is not open source, and cannot be extended by arbitrary user-defined functions in languages like Python or R. R package rgee (Aybar 2021) provides an R client interface to GEE. Cloud-based data cube processing platforms built entirely around open source software are arising, several of which using the openEO API (Schramm et al. 2021). This API has room for user-defined functions (UDFs) written in Python or R that are being passed on through the API and executed at the pixel level, e.g. to aggregate or reduce dimensions. UDFs in R represent the data chunk to be processed as stars object, in Python xarray objects are used. Other platforms include the Copernicus climate data store (Raoult et al. 2017) or atmosphere data store, which allow processing of atmospheric or climate data from ECMWF, including ERA5. An R package with an interface to both data stores is ecmwfr (Hufkens 2020). References "],["plotting.html", "Chapter 9 Plotting spatial data 9.1 Every plot is a projection 9.2 Plotting points, lines, polygons, grid cells 9.3 Base plot 9.4 Maps with ggplot2 9.5 Maps with tmap 9.6 Interactive maps: leaflet, mapview, tmap", " Chapter 9 Plotting spatial data Together with timelines, maps belong to the most powerful graphs, perhaps because we can immediately relate where we are, or have been, on the space of the plot. Two recent books on visualisation (Healy 2018; Wilke 2019) contain chapters on visualising geospatial data or maps. Here, we will not try to preach the do’s and don’ts of maps, but rather point out a number of possibilities how to do things, point out challenges along the way and ways to mitigate them. 9.1 Every plot is a projection The world is round, but plotting devices are flat. As mentioned in section 2.2.2, any time we visualise, in any way, the world on a flat device, we project: we convert ellipsoidal coordinates into Cartesian coordinates. This includes the cases where we think we “do nothing” (figure 9.1, left), or where show the world “as it is”, e.g. as seen from space (figure 9.1, right). Figure 9.1: Earth country boundaries; left: mapping long/lat linearly to \\(x\\) and \\(y\\) (plate carrée); right: ``as seen from space’’ (orthographic) The left plot of figure 9.1 was obtained by library(sf) library(rnaturalearth) w &lt;- ne_countries(scale = &quot;medium&quot;, returnclass = &quot;sf&quot;) plot(st_geometry(w)) and we see that this is the default projection for data with ellipsoidal coordinates, as indicated by st_is_longlat(w) # [1] TRUE The projection taken in figure 9.1 (left) is the equirectangular (or equidistant cylindrical) projection, which maps longitude and latitude linearly to the \\(x\\) and \\(y\\) axis, keeping an aspect ratio of 1. Were we to do this for smaller areas not on the equator, it makes sense to choose a plot ratio such that one distance unit E-W equals one distance unit N-S on the center of the plotted area, and this is the default behaviour of the plot() method for unprojected sf or stars datasets, as well as the default for ggplot2::geom_sf() (section @rer(geomsf)). We can also carry out this projection before plotting. Say we want to plot Germany, then after loading the (rough) country outline, we use st_transform to project: DE = st_geometry(ne_countries(country = &quot;germany&quot;, returnclass = &quot;sf&quot;)) DE.eqc = st_transform(DE, &quot;+proj=eqc +lat_ts=51.14 +lon_0=90w&quot;) The eqc refers to the “equidistant cylindrical” projection of PROJ; the projection parameter here is lat_ts, the latitude of true scale (i.e., one length unit N-S equals one length unit E-W), which was here chosen as the middle of the bounding box latitudes print(mean(st_bbox(DE)[c(&quot;ymin&quot;, &quot;ymax&quot;)]), digits = 4) # [1] 51.14 When we now plot both maps (figure 9.2), they look identical up to the values along the axes: degrees for ellipsoidal (left), and metres for projected (Cartesian) coordinates. par(mfrow = c(1, 2)) plot(DE, axes = TRUE) plot(DE.eqc, axes = TRUE) Figure 9.2: Germany in equirectangular projection: with axis units degrees (left) and metres in the equidistant cylindrical projection (right) 9.1.1 What is a good projection for my data? There is unfortunately no silver bullet here. Projections that maintain all distances do not exist; only globes do. The most used projections try to preserve areas (equal area), directions (conformal, e.g. Mercator), some properties of distances (e.g. equirectangular preserves distances along meridians, azimuthal equidistant preserves distances to a central point) or some compromise of these. Parameters of projections decide what is shown in the center of a map and what on the fringes, which areas are up and which are down, and which areas are most enlarged. All these choices are in the end political decisions. It is often entertaining and at times educational to play around with the different projections and understand their consequences. When the primary purpose of the map however is not to entertain or educate projection varieties, it may be preferrable to choose a well-known or less surprising projection, and move the discussion which projection should be preferred to a decision process on its own. 9.2 Plotting points, lines, polygons, grid cells Since maps are just a special form of plots of statistical data, the usual rules hold. Frequently occuring challenges include: polygons may be very small, and vanish when plotted, depending on the data, polygons for different features may well overlap, and be visible only partially; using transparent fill colors may help indentify them when points are plotted with symbols, they may easily overlap and be hidden; density maps (chapter 11) may be more helpful lines may be hard to read when coloured and may overlap regardless line width 9.2.1 Colors When plotting polygons filled with colors, one has the choice to plot polygon boundaries, or to suppress these. If polygon boundaries draw too much attention, an alternative is to colour them in a grey tone, or another color that doesn’t interfere with the fill colors. When suppressing boundaries entirely, polygons with (nearly) identical colors will no longer be visually distinguishable. If the property indicating the fill color is constant over the region, such as land cover type, then this is not a problem but if the property is an aggregation, the region over which it was aggregated gets lost, and by that the proper interpretation: especially for extensive variables, e.g. the amount of people living in a polygon, this strongly misleads. But even with polygon boundaries, using filled polygons for such variables may not be a good idea. The use of continuous color scales that have no noticable color breaks for continuously varying variables may look attractive, but is often more fancy than useful: it impracticle to match a color on the map with a legend value colors ramps often stretch non-linearly over the value range, making it hard to convey magnitude Only for cases where the identification of values is less important than the continuity of the map, such as the coloring of a high resolution digital terrain model, it does serve its goal. Good colors scales are e.g. found in packages RColorBrewer (Neuwirth 2014), viridis (Garnier 2018) or colorspace (Ihaka et al. 2020; Zeileis et al. 2020). 9.2.2 Color breaks: classInt When plotting continuous geometry attributes using a limited set of colors (or symbols), classes need to be made from the data. R package classInt (R. Bivand 2020a) provides a number of methods to do so. The default method is “quantile”: library(classInt) # set.seed(1) needed ? r = rnorm(100) (cI &lt;- classIntervals(r)) # style: quantile # one of 1.49e+10 possible partitions of this variable into 8 classes # [-2.61,-1.26) [-1.26,-0.356) [-0.356,-0.131) [-0.131,0.091) [0.091,0.433) # 13 12 13 12 12 # [0.433,0.623) [0.623,1.11) [1.11,2.76] # 13 12 13 cI$brks # [1] -2.612 -1.257 -0.356 -0.131 0.091 0.433 0.623 1.113 2.755 it takes argument n for the number of intervals, and a style that can be one of “fixed”, “sd”, “equal”, “pretty”, “quantile”, “kmeans”, “hclust”, “bclust”, “fisher” or “jenks”. Style “pretty” may not obey n; if n is missing, ‘nclass.Sturges’ is used; two other methods are available for choosing n automatically. If the number of observations is greater than 3000, a 10% sample is used to create the breaks for “fisher” and “jenks”. 9.2.3 Graticules and other navigation aids Graticules are lines on a map that follow constant latitude or longitude values. On figure 9.1 left they are drawn in grey. Graticules are often drawn in maps to give reference where something is. In our first map in figure 1.1 we can read that the area plotted is near 35\\(^o\\) North and 80\\(^o\\) West. Had we plotted the lines in the projected coordinate system, they would have been straight and their actual numbers would not have been very informative, apart from giving an interpretation of size or distances when the unit is known, and familiar to the map reader. Graticules, by that, also shed light on which projection was used: equirectangular or Mercator projections will have straight vertical and horizontal lines, conic projections have straight but diverging meridians, equal area may have curved meridians The real navigation aid on figure 9.1 and most other maps are geographical features like the state outline, country outlines, coast lines, rivers, roads, railways and so on. If these are added sparsely and sufficiently, a graticule can as well be omitted. In such cases, maps look good without axes, tics, and labels, leaving up a lot of plotting space to be filled with actual map data. 9.3 Base plot The plot method for sf and stars objects try to make quick, useful, exploratory plots; for production quality, alternatives with more control and/or better defaults are offered by packages ggplot2 (Wickham et al. 2020) or tmap (Tennekes 2021, 2018). By default, the plot method tries to plot “all” it is given. This means that given a geometry only (sfc), the geometry is plotted, without colors, given a geometry and an attribute, the geometry is colored according to the values of the attribute, using a qualitative color scale for factor or logical attributes and a continuous scale otherwise, given multiple attributes, multiple maps are plotted, each with a color scale but a legend is omitted by default, as color assignment is done on a per sub-map basis, for stars objects with multiple attributes, only the first attribute is plotted; for three-dimensional raster cubes, all slices over the third dimension are plotted. 9.3.1 Adding to plots with legends The plot methods for stars and sf objects may show a color key on one of the sides (e.g., figure 1.1). To do this with base::plot, the plot region is split in two and two plots are created: one with the map, and one with the legend. By default, the plot function resets the graphics device (using layout(matrix(1)) so that subsequent plots are not hindered by the device being split in two. If one wants to add to an existing plot having a color legend, this is however what is needed, and resetting the plotting device needs to be prevented by setting argument reset = FALSE, and use add = TRUE in a subsequent call to plot, an example is sf_use_s2(TRUE) library(sf) nc = read_sf(system.file(&quot;gpkg/nc.gpkg&quot;, package=&quot;sf&quot;)) plot(nc[&quot;BIR74&quot;], reset = FALSE, key.pos = 4) plot(st_buffer(nc[1,1], units::set_units(10, km)), col = &#39;NA&#39;, border = &#39;red&#39;, lwd = 2, add = TRUE) Figure 9.3: Annotating base plots that have a legend which is shown in figure 9.3. Annotating stars plots can be done in the same way when a single stars layer is shown. Annotating stars plots with multiple cube slices can be done by adding a “hook” function that will be called on every slice shown, as in library(stars) r = read_stars(system.file(&quot;tif/L7_ETMs.tif&quot;, package = &quot;stars&quot;)) circ = st_bbox(r) %&gt;% st_as_sfc() %&gt;% st_sample(5) %&gt;% st_buffer(300) hook = function() plot(circ, col = NA, border = &#39;yellow&#39;, add = TRUE) plot(r, hook = hook, key.pos = 4) Figure 9.4: annotated multi-slice stars plot and as shown in figure 9.4. Base plot methods have access to the resolution of the screen device and hence the base plot method for stars and stars_proxy object will downsample dense rasters and only plot pixels at a density that makes sense for the device available. 9.3.2 Projections in base plots The base plot method plots data with ellipsoidal coordinates using the equirectangular projection, using a latitude parameter equal to the middle latitude of the data bounding box (figure 9.2). To control this parameter, either a projection to another equirectangular can be applied before plotting, or the parameter asp can be set to override, e.g. asp=1 would lead to plate carrée (figure 9.1 left). Subsequent plots need to be in the same coordinate reference system in order to make sense with overplotting, this is not being checked. 9.3.3 Colors and color breaks In base plots, nbreaks can be used to set the number of color breaks, and breaks either to the numeric vector with actual breaks, or to a value for the style argument in classInt::classIntervals. 9.4 Maps with ggplot2 Package ggplot2 (Wickham et al. 2020; Wickham 2016) can be used to create more complex an nicer looking graphs; it has a geometry geom_sf that was developed in conjunction with the development of sf, and helps creating beautiful maps; an introduction to this is found in (Moreno and Basille 2018), a first example is shown in figure ??. The code used for this plot is: suppressPackageStartupMessages(library(tidyverse)) nc.32119 = st_transform(nc, 32119) year_labels = c(&quot;SID74&quot; = &quot;1974 - 1978&quot;, &quot;SID79&quot; = &quot;1979 - 1984&quot;) nc.32119 %&gt;% select(SID74, SID79) %&gt;% gather(VAR, SID, -geom) -&gt; nc2 ggplot() + geom_sf(data = nc2, aes(fill = SID)) + facet_wrap(~VAR, ncol = 1, labeller = labeller(VAR = year_labels)) + scale_y_continuous(breaks = 34:36) + scale_fill_gradientn(colors = sf.colors(20)) + theme(panel.grid.major = element_line(color = &quot;white&quot;)) where we see that two attributes had to be stacked (gather) before plotting them as facets: this is the idea of “tidy” data, and the gather method for sf objects automatically stacks the geometry column too. Because ggplot2 creates graphics objects before plotting them, it can control the cooordinate reference system of all elements involved, and will transform or convert all subsequent objects to the coordinate reference system of the first. It will also draw a graticule for the (default) thin white lines on a grey background, and uses a datum (by default: WGS84) for this. geom_sf() can be combined with For package stars, a geom_stars has, at the moment of writing this, very limited scope: it wraps geom_sf for vector data cubes, and calls geom_raster for regular rasters and geom_rect for rectilinear rasters. It downsamples if the user specifies a downsampling rate, but has no access to the screen dimensions to automatically do this. This may be just enough, for instance figure 9.5 is created by the following commands: library(ggplot2) library(stars) r = read_stars(system.file(&quot;tif/L7_ETMs.tif&quot;, package = &quot;stars&quot;)) ggplot() + geom_stars(data = r) + facet_wrap(~band) + coord_equal() + theme_void() + scale_x_discrete(expand=c(0,0)) + scale_y_discrete(expand=c(0,0)) + scale_fill_viridis_c() Figure 9.5: Simple raster plot with ggplot2 More elaborate ggplot2-based plots with stars objects may be obtained using package ggspatial (Dunnington 2021); non-compatible but nevertheless ggplot2-style plots can be created with tmap. 9.5 Maps with tmap Package tmap (Tennekes 2021, 2018) takes a fresh look on plotting spatial data in R; it resembles ggplot2 in the sense that it composes graphics objects before printing, by building on the grid package, and by concatenating map elements with a + between them, but otherwise it is entirely independent from, and incompatible with, ggplot2. It has a number of options that allow for highly professional looking maps, and many defaults have been carefully chosen. To recreate figure ??, for instance, we use library(tmap) tm_shape(nc.32119) + tm_polygons(c(&quot;SID74&quot;, &quot;SID79&quot;)) Figure 9.6: tmap: using … with two attributes to create figure 9.6 and tm_shape(nc2) + tm_polygons(&quot;SID&quot;) + tm_facets(by = &quot;VAR&quot;) Figure 9.7: tmap: using … to create figure 9.7. Package tmap also has support for stars objects, an example created with tm_shape(r) + tm_raster(&quot;L7_ETMs.tif&quot;) # Warning: col specification in tm_raster is ignored, since stars object contains # a 3rd dimension, where its values are used to create facets Figure 9.8: Simple raster plot with tmap is shown in figure 9.8. More examples of the use of tmap are given in sectiion 13. 9.6 Interactive maps: leaflet, mapview, tmap Interactive maps as shown in figure ?? can be created with R packages leaflet, mapview or tmap. mapview adds a number of capabilities to leaflet including a map legend, configurable pop-up windows when clicking features, support for raster data, and scalable maps with very large feature sets using the filegeobuf file format, as well as facet maps that react synchronously to zoom and pan actions. Package tmap has the nice option that after giving tmap_mode(&quot;view&quot;) all usual tmap commands are applied to an interactive html/leaflet widget, whereas after tmap_mode(&quot;plot&quot;) again all output is sent to R own graphics device. References "],["statistical-modelling-of-spatial-data.html", "Chapter 10 Statistical modelling of spatial data 10.1 Design-based and model-based inference 10.2 Predictive models with coordinates 10.3 Model-based inference", " Chapter 10 Statistical modelling of spatial data Spatial data almost always (and everywhere) has the property that it is spatially structured: observations done closeby in space tend to be more similar than observations done at larger distance from each other. This phenomenon, in the geography domain attributed to Waldo Tobler (as in “Waldo Tobler’s first law of geography”) was already noted by (Fisher and others 1937) and was a motivation for developing randomized block design in agricultural experiments: allocating treatments randomly to blocks avoids that spatial structure gets mixed up (or: confounds) with a signal caused by the treatment. The often heard argument that spatially structured data means that the data is spatially correlated, which would exclude estimation methods that assume independent observations is false. Correlation is a property of two random variables, and there are different ways in which spatial data can be approached with random variables: either the observation locations are random (leading to design-based inference) or the observed values are random (leading to model-based inference). The next section points out the difference between these two. 10.1 Design-based and model-based inference Statistical inference means the action of estimating parameters about a population from sample data. Suppose we denote the variable of interest with \\(z(s)\\), where \\(z\\) is the attribute value measured at location \\(s\\), and we are interested in estimating the mean value of \\(z(s)\\) over a domain \\(D\\), \\[z(s)=\\frac{1}{|D|} \\int_{ u \\in D} z(u)du,\\] with \\(|D|\\) the area of \\(D\\), from sample data \\(z(s_1),...,z(s_n)\\). Then, there are two possibilities to proceed: model-based, or design-based. A model-based approach considers \\(z(s)\\) to be a realisation of a superpopulation \\(Z(s)\\) (using capital letters to indicate random variables), and could for instance postulate a model for its spatial variability in the form of \\[Z(s) = m + e(s), \\ \\ \\mbox{E}(e(s)) = 0, \\ \\ \\mbox{Cov(e(s))} = \\Sigma(\\theta)\\] which would require choosing the covariance function \\(\\Sigma()\\) and estimating its parameters \\(\\theta\\) form \\(z(s)\\), and then computing a block kriging prediction \\(\\hat{Z}(D)\\) (section 12.5). This approach makes no assumptions about the sample \\(z(s)\\), but of course it should allow for choosing the covariance function and estimating its parameters; inference is conditional to the validity of the postulated model. Rather than assuming a superpopulation model, the design-based approach (De Gruijter and Ter Braak 1990; D. J. Brus 2021a; Breidt, Opsomer, and others 2017) assumes randomness in the locations, which is justified (only) when using random sampling. It requires that the sample data were obtained by probability sampling, meaning that some form of spatial random sampling was used where all elements of \\(z(s)\\) had a known and positive probability of being included in the sample obtained. The random process is that of sampling: \\(z(s_1)\\) is a realisation of the random process \\(z(S_1)\\), the first observation taken over repeated random sampling. Design-based estimaters only need these inclusion probabilities to estimate mean values with standard errors. This means that for instance given a simple random sample, the unweighted sample mean is used to estimate the population mean, and no model parameters need to be fit. The misconception here, as explained in [brus2021], is that this is only the case when working under model-based approaches: \\(Z(s_1)\\) and \\(Z(s_2)\\) may well be correlated (“model-dependent”), but although in a particular random sampling (realisation) \\(z(s_1)\\) and \\(z(s_2)\\) may be close in space, the corresponding random variables \\(z(S_1)\\) and \\(z(S_2)\\) considered over repeated random sampling are not close together, and are design-independent. Both situations can co-exist without contradiction, and are a consequence of choosing to work under one inference framework or the other. The choice whether to work under a design-based or model-based framework depends on the purpose of the study and the data collection process. The model-based framework lends itself best for cases * where predictions are required for individual locations, or for areas too small to be sampled * when the available data were not collected using a known random sampling scheme (i.e., the inclusion probabilities are unknown, or are zero over particular areas or/and times) Design-based approaches are most suitable when * observations were collected using a spatial random sampling process * aggregated properties of the entire sample region (or sub-region) are needed. * estimates are required that are not sensitive to potential model misspecification, e.g. when needed for regulatory purposes. In case a sampling procedure is to be planned (De Gruijter et al. 2006), some form of spatial random sampling is definitely worth considering since it opens up the possibility of following both inference frameworks. 10.2 Predictive models with coordinates In data science projects, coordinates may be seen as features in a larger set of predictors (or features, or covariates) and treated accordingly. There are some catches with doing so. As usual when working with predictors, it is good to choose predictive methods that are not sensitive to shifts in origin or shifts in unit (scale). Assuming a two-dimensional problem, predictive models should also not be sensitive to arbitrary rotations of the x- and y- or latitude and longitude axes. For projected (2D, Cartesian) coordinates this can be assured e.g. by using polynomials of order \\(n\\) as \\((x+y)^n\\), rather than \\((x)^n + (y)^n\\); for a second order polynomial this involves including the term \\(xy\\), so that an ellipsoidal-shape trend surface does not have to be aligned with the \\(x-\\) or \\(y-\\)axis. For a GAM model with spline components, one would use a spline in two dimensions rather than two independent splines in \\(x\\) and \\(y\\). An exception to this “rule” is when e.g. a pure latitude effect is desired, for instance to account for solar energy influx. When the area covered by the data is large, the difference between using ellipsoidal coordinates and projected coordinates will automatically become larger, and hence choosing one of both will have an effect on predictive modelling. For very large extents, e.g. global models, polynomials or splines in latitude and longitude will not make sense as they ignore the circular nature of longitude and the coordinate singularities at the poles. Here, spherical harmonics, base functions that are continuous on the sphere with increasing spatial frequencies can replace polynomials or be used as spline base functions. In many cases, the spatial coordinates over which samples were collected also define the space over which predictions are made, setting them apart from other features. Many simple predictive approaches, including most machine learning methods, assume sample data to be independent. When samples are collected by spatially random sampling over the spatial target area, this assumption may be justified when working under a design-based context (D. J. Brus 2021b). This context however treats the coordinate space as the variable over which we randomize, which affords predicting values for a new randomly chosen location but rules out making predictions for fixed locations; this implies that averages over areas over which samples were collected can be obtained, but not spatial interpolations. In case predictions for fixed locations are required, or in case data were not collected by spatial random sampling, a model-based approach (as taken in chapter 12) is needed and typically some form of spatial and/or temporal autocorrelation of residuals must be assumed. A common case is where sample data are collected opportunistically (“whatever could be found”), and are then used in a predictive framework that does not weigh them. This has a consequence that the resulting model may be biased towards over-represented areas (in predictor space and/or in spatial coordinates space), and that simple (random) cross validation statistics may be over-optimistic when taken as performance measures for spatial prediction (Meyer and Pebesma 2020). Adaptive cross validation measures, e.g. spatial cross validation may help getting more relevant measures for predictive performance. 10.3 Model-based inference Wikle/Zambione/Cressie: (Wikle, Zammit-Mangion, and Cressie 2019) Geostatistics: (Gräler, Pebesma, and Heuvelink 2016) scalable method comparison: (Heaton et al. 2018) Point patterns: (Stoyan et al. 2017) (Gabriel, Rowlingson, and Diggle 2013) Spatstat book: (Baddeley, Rubak, and Turner 2015) R-INLA: (Blangiardo et al. 2013), (Blangiardo and Cameletti 2015) (Gómez-Rubio 2020) R-INLA-SPDF: Advanced Spatial Modeling with Stochastic Partial Differential Equations Using R and INLA: (Krainski et al. 2018) Possibly: M. Cameletti: Stem: Spatio-temporal models in R Estimation of the parameters of a spatio-temporal model using the EM algorithm, estimation of the parameter standard errors using a spatio-temporal parametric bootstrap, spatial mapping. Stcos: https://onlinelibrary.wiley.com/doi/abs/10.1002/sta4.94 , CRAN, https://arxiv.org/abs/1904.12092 (Raim et al. 2020) References "],["pointpatterns.html", "Chapter 11 Point Pattern Analysis", " Chapter 11 Point Pattern Analysis Basics PP, beyond counting; basic steps in PPA sf &amp; stars - spatstat interface; which classes? mention st_sample and examples of it using spatstat spatial sampling methods; explain that n is not guaranteed to be n? "],["interpolation.html", "Chapter 12 Interpolation and Geostatistics 12.1 Preparing the air quality dataset 12.2 Sample variogram 12.3 Fitting variogram models 12.4 Kriging interpolation 12.5 Areal means: block kriging 12.6 Conditional simulation 12.7 Trend models 12.8 Multivariable geostatistics 12.9 Spatiotemporal interpolation 12.10 Area-to-point kriging", " Chapter 12 Interpolation and Geostatistics Geostatistics is concerned with the modelling, prediction and simulation of spatially continuous phenomena. The typical problem is a missing value problem: we observe a property of a phenomenon \\(Z(s)\\) at a limited number of sample locations \\(s_i, i = 1,...,n\\), and are interested in the property value at all locations \\(s_0\\), so we have to predict it for unobserved locations. This is also called kriging, or Gaussian process prediction. In case \\(Z(s)\\) contains a white noise component \\(\\epsilon\\), as in \\(Z(s)=S(s)+\\epsilon(s)\\), an alternative but similar goal is to predict \\(S(s)\\), which may be called filtering or smoothing. In this chapter we will show simple approaches for handling geostatistical data, will demonstrate simple interpolation methods, explore modelling spatial correlation, spatial prediction and simulation. We will use package gstat (E. Pebesma and Graeler 2021, @gstatcg), which offers a fairly wide palette of models and options for geostatistical analysis. 12.1 Preparing the air quality dataset The dataset we work with is an air quality dataset obtained from the European Environmental Agency (EEA). European member states report air quality measurements to this Agency. So-called validated data are quality controlled by member states, and are reported on a yearly basis. They form the basis for policy compliancy evaluations. The EEA’s air quality e-reporting website gives access to the data reported by European member states. We decided to download hourly (time series) data, which is the data primarily measured. A web form helps convert simple selection criteria into an http request. The following URL https://fme.discomap.eea.europa.eu/fmedatastreaming/AirQualityDownload/AQData_Extract.fmw?CountryCode=DE&amp;CityName=&amp;Pollutant=8&amp;Year_from=2017&amp;Year_to=2017&amp;Station=&amp;Samplingpoint=&amp;Source=E1a&amp;Output=TEXT&amp;UpdateDate= was created to select all validated (Source=E1a) \\(NO_2\\) (Pollutant=8) data for 2017 (Year_from, Year_to) from Germany (CountryCode=DE). It returns a text file with a set of URLs to CSV files, each containing the hourly values for the whole period for a single measurement station. These files were downloaded and converted to the right encoding using the dos2unix command line utility. In the following, we will read all the files into a list, files = list.files(&quot;aq&quot;, pattern = &quot;*.csv&quot;, full.names = TRUE) r = lapply(files[-1], function(f) read.csv(f)) then convert the time variable into a POSIXct variable, and time order them Sys.setenv(TZ = &quot;UTC&quot;) # make sure times are not interpreted as DST r = lapply(r, function(f) { f$t = as.POSIXct(f$DatetimeBegin) f[order(f$t), ] } ) and we deselect smaller datasets that do not contain hourly data: r = r[sapply(r, nrow) &gt; 1000] names(r) = sapply(r, function(f) unique(f$AirQualityStationEoICode)) length(r) == length(unique(names(r))) # [1] TRUE and then combine all files using xts::cbind, so that they are matched based on time: library(xts) r = lapply(r, function(f) xts(f$Concentration, f$t)) aq = do.call(cbind, r) We can now select those stations for which we have 75% of the hourly values measured, i.e. drop those with more than 25% hourly values missing: # remove stations with more than 75% missing values: sel = apply(aq, 2, function(x) sum(is.na(x)) &lt; 0.75 * 365 * 24) aqsel = aq[, sel] # stations are in columns Next, the station metadata was read and filtered for rural background stations in Germany by library(tidyverse) read.csv(&quot;aq/AirBase_v8_stations.csv&quot;, sep = &quot;\\t&quot;, stringsAsFactors = FALSE) %&gt;% as_tibble %&gt;% filter(country_iso_code == &quot;DE&quot;, station_type_of_area == &quot;rural&quot;, type_of_station == &quot;Background&quot;) -&gt; a2 These stations contain coordinates, and an sf object with (static) station metadata is created by library(sf) library(stars) a2.sf = st_as_sf(a2, coords = c(&quot;station_longitude_deg&quot;, &quot;station_latitude_deg&quot;), crs = 4326) We now subset the air quality data to stations that are of type rural background: sel = colnames(aqsel) %in% a2$station_european_code aqsel = aqsel[, sel] We can compute station means, and join these to stations locations by tb = tibble(NO2 = apply(aqsel, 2, mean, na.rm = TRUE), station_european_code = colnames(aqsel)) crs = 32632 right_join(a2.sf, tb) %&gt;% st_transform(crs) -&gt; no2.sf # Joining, by = &quot;station_european_code&quot; # load German boundaries data(air, package = &quot;spacetime&quot;) de &lt;- st_transform(st_as_sf(DE_NUTS1), crs) ggplot() + geom_sf(data = de) + geom_sf(data = no2.sf, mapping = aes(col = NO2)) 12.2 Sample variogram In order to make spatial predictions using geostatistical methods, we first need to identify a model for the mean and for the spatial correlation. In the simplest model, \\(Z(s) = m + e(s)\\), the mean is an unknown constant \\(m\\), and in this case the spatial correlation can be modelled using the variogram, \\(\\gamma(h) = 0.5 E (Z(s)-Z(s+h))^2\\). For processes with a finite variance \\(C(0)\\), the variogram is related to the covariogram or covariance function through \\(\\gamma(h) = C(0)-C(h)\\). The sample variogram is obtained by computing estimates of \\(\\gamma(h)\\) for distance intervals, \\(h_i = [h_{i,0},h_{i,1}]\\): \\[ \\hat{\\gamma}(h_i) = \\frac{1}{2N(h_i)}\\sum_{j=1}^{N(h_i)}(z(s_i)-z(s_i+h&#39;))^2, \\ \\ h_{i,0} \\le h&#39; &lt; h_{i,1} \\] with \\(N(h_i)\\) the number of sample pairs available for distance interval \\(h_i\\). Function gstat::variogram computes sample variograms. library(gstat) v = variogram(NO2~1, no2.sf) plot(v, plot.numbers = TRUE) This chooses default maximum distance (cutoff: one third of the length of the bounding box diagonal) and (constant) interval widths (width: cutoff divided by 15). These defaults can be changed, e.g. by library(gstat) v0 = variogram(NO2~1, no2.sf, cutoff = 100000, width = 10000) plot(v0, plot.numbers = TRUE) Note that the formula NO2~1 is used to select the variable of interest from the data file (NO2), and to specify the mean model: ~1 refers to an intercept-only (unknown, constant mean) model. 12.3 Fitting variogram models In order to progress toward spatial predictions, we need a variogram model \\(\\gamma(h)\\) for (potentially) all distances \\(h\\), rather than the set of estimates derived above: in case we would for instance connect these estimates with straight lines, or assume they reflect constant values over their respective distance intervals, this would lead to statisical models with non-positive covariance matrices, which is a complicated way of expressing that you are in a lot of trouble. To avoid these troubles we fit parametric models \\(\\gamma(h)\\) to the estimates \\(\\hat{\\gamma}(h_i)\\), where we take \\(h_i\\) as the mean value of all the \\(h&#39;\\) values involved in estimating \\(\\hat{\\gamma}(h_i)\\). For this, when we fit a model like the exponential variogram, v.m = fit.variogram(v, vgm(1, &quot;Exp&quot;, 50000, 1)) plot(v, v.m, plot.numbers = TRUE) the fitting is done by minimizing \\(\\sum_{i=1}^{n}w_i(\\gamma(h_i)-\\hat{\\gamma}(h_i))^2\\), with \\(w_i\\) by default equal to \\(N(h_i)/h^2\\), other fitting schemes are available through argument fit.method. 12.4 Kriging interpolation Kriging involves the prediction of \\(Z(s_0)\\) at arbitrary locations \\(s_0\\). Typically, when we interpolate a variable, we do that on points on a regular grid covering the target area. We first create a stars object with a raster covering the target area, and NA’s outside it: # build a grid over Germany: st_bbox(de) %&gt;% st_as_stars(dx = 10000) %&gt;% st_set_crs(crs) %&gt;% st_crop(de) -&gt; grd grd # stars object with 2 dimensions and 1 attribute # attribute(s): # Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s # values 0 0 0 0 0 0 2076 # dimension(s): # from to offset delta refsys point values x/y # x 1 65 280741 10000 WGS 84 / UTM z... NA NULL [x] # y 1 87 6101239 -10000 WGS 84 / UTM z... NA NULL [y] Then, we can krige by using gstat::krige, with the model for the trend, the data, the prediction grid, and the variogram model: k = krige(NO2~1, no2.sf, grd, v.m) # [using ordinary kriging] ggplot() + geom_stars(data = k, aes(fill = var1.pred, x = x, y = y)) + geom_sf(data = st_cast(de, &quot;MULTILINESTRING&quot;)) + geom_sf(data = no2.sf) 12.5 Areal means: block kriging Computing areal means can be done in several ways. The simples is to take the average of point samples falling inside the target polygons: a = aggregate(no2.sf[&quot;NO2&quot;], by = de, FUN = mean) A more complicated way is to use block kriging (Journel and Huijbregts 1978), which uses all the data to estimate the mean of the variable over the target area. With krige, this can be done by giving the target areas (polygons) as the newdata argument: b = krige(NO2~1, no2.sf, de, v.m) # [using ordinary kriging] we can now compile the two maps together in order to create a single plot: b$sample = a$NO2 b$kriging = b$var1.pred b %&gt;% select(sample, kriging) %&gt;% gather(var, NO2, -geometry) -&gt; b2 ggplot() + geom_sf(data = b2, mapping = aes(fill = NO2)) + facet_wrap(~var) + scale_fill_gradientn(colors = sf.colors(20)) We see that the signal is similar, but that the simple means are more variable than the block kriging values; this may be due to the smoothing effect of kriging: data points outside the target area are weighted, too. To compare the standard errors of means, for the sample mean we can get a rough guess of the standard error by \\(\\sqrt{(\\sigma^2/n)}\\): SE = function(x) sqrt(var(x)/length(x)) a = aggregate(no2.sf[&quot;NO2&quot;], de, SE) which would have been the actual estimate in design-based inference if the sample was obtained by spatially random sampling. The block kriging variance is the model-based estimate, and is a by-product of kriging. We combine and rename the two: b$sample = a$NO2 b$kriging = sqrt(b$var1.var) b %&gt;% select(sample, kriging) %&gt;% gather(var, NO2, -geometry) -&gt; b2 ggplot() + geom_sf(data = b2, mapping = aes(fill = NO2)) + facet_wrap(~var) + scale_fill_gradientn(colors = sf.colors(20)) where we see that the simple approach gives clearly more variability in prediction errors for the areal means. 12.6 Conditional simulation In case one or more conditional realisation of the field \\(Z(s)\\) are needed rather than their conditional mean, we can obtain this by conditional simulation. A reason for wanting this may be the need to estimate areal mean values of \\(g(Z(s))\\) with \\(g(\\cdot)\\) a non-linear function; a simple example is the areal fraction where \\(Z(s)\\) exceeds a threshold. The standard approach used by gstat is to use the sequential simulation algorithm for this. This is a simple algorithm that randomly steps through the prediction locations and at each location carries out a kriging prediction draws a random variable from the normal distribution with mean and variance equal to the kriging variance adds this value to the conditioning dataset finds a new random simulation location until all locations have been visited. This is caried out by gstat::krige when nsim is set to a positive value. In addition, it is good to constrain nmax, the (maximum) number of nearest neigbours to include in kriging estimation, because the dataset grows each step, leading otherwise quickly to very long computing times and large memory requirements. library(viridis) # Loading required package: viridisLite # # Attaching package: &#39;viridis&#39; # The following object is masked from &#39;package:scales&#39;: # # viridis_pal s = krige(NO2~1, no2.sf, grd, v.m, nmax = 30, nsim = 10) # drawing 10 GLS realisations of beta... # [using conditional Gaussian simulation] g = ggplot() + coord_equal() + scale_fill_viridis() + theme_void() + scale_x_discrete(expand=c(0,0)) + scale_y_discrete(expand=c(0,0)) g + geom_stars(data = s[,,,1:6]) + facet_wrap(~sample) Alternative methods for conditional simulation have recently been added to gstat, and include krigeSimCE implementing the circular embedding method (Davies and Bryant 2013), and krigeSTSimTB implementing the turning bands method (Schlather 2011). These are of particular of interest for larger datasets or conditional simulations of spatiotemporal data. 12.7 Trend models Kriging and conditional simulation, as used so far in this chapter, assume that all spatial variability is a random process, characterized by a spatial covariance model. In case we have other variables that are meaningfully correlated with the target variable, we can use them in a linear regression model for the trend, \\[ Z(s) = \\sum_{j=0}^p \\beta_j X_p(s) + e(s) \\] with \\(X_0(s) = 1\\) and \\(\\beta_0\\) an intercept, but with the other \\(\\beta_j\\) regression coefficients. This typically reduces both the spatial correlation in the residual \\(e(s)\\), as well as its variance, and leads to more accurate predictions and more similar conditional simulations. 12.7.1 A population grid As a potential predictor for NO2 in the air, we use population density. NO2 is mostly caused by traffic, and traffic is stronger in more densely populated areas. Population density is obtained from the 2011 census, and is downloaded as a csv file with the number of inhabitants per 100 m grid cell. We can aggregate these data to the target grid cells by summing the inhabitants: v = vroom::vroom(&quot;aq/pop/Zensus_Bevoelkerung_100m-Gitter.csv&quot;) # Rows: 35,785,840 # Columns: 4 # Delimiter: &quot;;&quot; # chr [1]: Gitter_ID_100m # dbl [3]: x_mp_100m, y_mp_100m, Einwohner # # Use `spec()` to retrieve the guessed column specification # Pass a specification to the `col_types` argument to quiet this message v %&gt;% filter(Einwohner &gt; 0) %&gt;% select(-Gitter_ID_100m) %&gt;% st_as_sf(coords = c(&quot;x_mp_100m&quot;, &quot;y_mp_100m&quot;), crs = 3035) %&gt;% st_transform(st_crs(grd)) -&gt; b a = aggregate(b, st_as_sf(grd, na.rm = FALSE), sum) Now we have the population counts per grid cell in a. To get to population density, we need to find the area of each cell; for cells crossing the country border, this will be less than 10 x 10 km: grd$ID = 1:prod(dim(grd)) # so we can find out later which grid cell we have ii = st_intersects(grd[&quot;ID&quot;], st_cast(st_union(de), &quot;MULTILINESTRING&quot;)) # Warning in st_intersects.stars(grd[&quot;ID&quot;], st_cast(st_union(de), # &quot;MULTILINESTRING&quot;)): as_points is NA: assuming here that raster cells are small # polygons, not points grd_sf = st_as_sf(grd[&quot;ID&quot;], na.rm = FALSE)[lengths(ii) &gt; 0,] iii = st_intersection(grd_sf, st_union(de)) # Warning: attribute variables are assumed to be spatially constant throughout all # geometries grd$area = st_area(grd)[[1]] + units::set_units(grd$values, m^2) # NA&#39;s grd$area[iii$ID] = st_area(iii) Instead of doing the two-stage procedure above: first finding cells that have a border crossing it, then computing its area, we could also directly use st_intersection on all cells, but that takes considerably longer. From the counts and areas we can compute densities, and verify totals: grd$pop_dens = a$Einwohner / grd$area sum(grd$pop_dens * grd$area, na.rm = TRUE) # verify # 80323301 [1] sum(b$Einwohner) # [1] 80324282 g + geom_stars(data = grd, aes(fill = sqrt(pop_dens), x = x, y = y)) We need to divide the number of inhabitants by the number of 100 m grid points contributing to it, in order to convert population counts into population density. To obtain population density values at monitoring network stations, we can use (a = aggregate(grd[&quot;pop_dens&quot;], no2.sf, mean)) # stars object with 1 dimensions and 1 attribute # attribute(s): # Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s # pop_dens 3.37e-06 4.98e-05 8.93e-05 0.000195 0.000237 0.00224 1 # dimension(s): # from to offset delta refsys point # geometry 1 74 NA NA WGS 84 / UTM z... TRUE # values # geometry POINT (545414 5930802),...,POINT (835252 5630738) no2.sf$pop_dens = st_as_sf(a)[[1]] summary(lm(NO2~sqrt(pop_dens), no2.sf)) # # Call: # lm(formula = NO2 ~ sqrt(pop_dens), data = no2.sf) # # Residuals: # Min 1Q Median 3Q Max # -7.96 -2.15 -0.50 1.60 8.10 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 4.561 0.697 6.54 8.0e-09 *** # sqrt(pop_dens) 325.006 49.927 6.51 9.2e-09 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 3.15 on 71 degrees of freedom # (1 observation deleted due to missingness) # Multiple R-squared: 0.374, Adjusted R-squared: 0.365 # F-statistic: 42.4 on 1 and 71 DF, p-value: 9.19e-09 and the corresponding scatterplot is shown in 12.1. Figure 12.1: Scatter plot of 2017 annual mean NO2 concentration against population density, for rural background air quality stations Prediction under this new model involves first modelling a residual variogram, by no2.sf = no2.sf[!is.na(no2.sf$pop_dens),] vr = variogram(NO2~sqrt(pop_dens), no2.sf) vr.m = fit.variogram(vr, vgm(1, &quot;Exp&quot;, 50000, 1)) plot(vr, vr.m, plot.numbers = TRUE) and subsequently, kriging prediction is done by kr = krige(NO2~sqrt(pop_dens), no2.sf, grd[&quot;pop_dens&quot;], vr.m) # [using universal kriging] k$kr1 = k$var1.pred k$kr2 = kr$var1.pred st_redimension(k[c(&quot;kr1&quot;, &quot;kr2&quot;)], along = list(what = c(&quot;kriging&quot;, &quot;residual kriging&quot;))) %&gt;% setNames(&quot;NO2&quot;) -&gt; km g + geom_stars(data = km, aes(fill = NO2, x = x, y = y)) + geom_sf(data = st_cast(de, &quot;MULTILINESTRING&quot;)) + geom_sf(data = no2.sf) + facet_wrap(~what) # Coordinate system already present. Adding new coordinate system, which will replace the existing one. where, critically, the pop_dens values are now available for prediction locations in object grd. We see some clear differences: the map with population density in the trend follows the extremes of the population density rather than those of the measurement stations, and has a range that extends that of the former. It should be taken with a large grain of salt however, since the stations used were filtered for the category “rural background”, indicating that they represent conditions of lower populations density. The scatter plot of Figure 12.1 reveals that the the population density at the locations of stations is much more limited than that in the population density map, and hence the right-hand side map is based on strongly extrapolating the relationship shown in 12.1. 12.8 Multivariable geostatistics Multivariable geostatics involves the joint modelling, prediction and simulation of multiple variables, \\[Z_1(s) = X_1 \\beta_1 + e_1(s)\\] \\[...\\] \\[Z_n(s) = X_n \\beta_n + e_n(s).\\] In addition to having observations, trend models, and variograms for each variable, the cross variogram for each pair of residual variables, describing the covariance of \\(e_i(s), e_j(s+h)\\), is required. If this cross covariance is non-zero, knowledge of \\(e_j(s+h)\\) may help predict (or simulate) \\(e_i(s)\\). This is especially true if \\(Z_j(s)\\) is more densely sample than \\(Z_i(s)\\). Prediction and simulation under this model are called cokriging and cosimulation. Examples using gstat are found when running the demo scripts library(gstat) demo(cokriging) demo(cosimulation) and are illustrated and discussed in (Roger S. Bivand, Pebesma, and Gomez-Rubio 2013). In case the different variables considered are observed at the same set of locations, for instance different air quality parameters, then the statistical gain of using cokriging as opposed to direct (univariable) kriging is often modest, when not negligible. A gain may however be that the prediction is truly multivariable: in addition to the prediction vector \\(\\hat{Z(s_0)}=(\\hat{Z}_1(s_0),...,\\hat{Z}_n(s_0))\\) we get the full covariance matrix of the prediction error (Ver Hoef and Cressie 1993). This means for instance that if we are interested in some linear combination of \\(\\hat{Z}(s_0)\\), such as \\(\\hat{Z}_2(s_0) - \\hat{Z}_1(s_0)\\), that we can get the standard error of that combination because we have the correlations between the prediction errors. Although sets of direct and cross variograms can be computed and fitted automatically, multivariable geostatistical modelling becomes quickly hard to manage when the number of variables gets large, because the number of direct and cross variograms required is \\(n(n+1)/2\\). In case different variables refer to the same variable take at different time steps, one could use a multivariable (cokriging) prediction approach, but this would not allow for e.g. interpolation between two time steps. For this, and for handling the case of having data observed at many time instances, one can also model its variation as a function of continuous space and time, as of \\(Z(s,t)\\), which we will do in the next section. 12.9 Spatiotemporal interpolation Spatiotemporal geostatistical processes are modelled as variables having a value everywhere in space and time, \\(Z(s,t)\\), with \\(s\\) and \\(t\\) the continuously indext space and time index. Given observations \\(Z(s_i,t_j)\\) and a variogram (covariance) model \\(\\gamma(s,t)\\) we can predict \\(Z(s_0,t_0)\\) at arbitrary space/time locations \\((s_0,t_0)\\) using standard Gaussian process theory. Several books have been written recently about modern approaches to handling and modelling spatiotemporal geostatistical data, including (Wikle, Zammit-Mangion, and Cressie 2019) and (Blangiardo and Cameletti 2015). Here, we will use (Gräler, Pebesma, and Heuvelink 2016) and give some simple examples building upon the dataset used throughout this chapter. 12.9.1 A spatiotemporal variogram model Starting with the spatiotemporal matrix of NO\\(_2\\) data in aq constructed at the beginning of this chapter, we will first select the rural background stations: aqx = aq[ , colnames(aq) %in% a2$station_european_code] sfc = st_geometry(a2.sf)[match(colnames(aqx), a2.sf$station_european_code)] st_as_stars(NO2 = as.matrix(aqx)) %&gt;% st_set_dimensions(names = c(&quot;time&quot;, &quot;station&quot;)) %&gt;% st_set_dimensions(&quot;time&quot;, index(aqx)) %&gt;% st_set_dimensions(&quot;station&quot;, sfc) -&gt; no2.st v.st = variogramST(NO2~1, no2.st[,1:(24*31)], tlags = 0:48, cores = getOption(&quot;mc.cores&quot;, 2)) v1 = plot(v.st) v2 = plot(v.st, map = FALSE) print(v1, split = c(1,1,2,1), more = TRUE) print(v2, split = c(2,1,2,1), more = FALSE) # product-sum prodSumModel &lt;- vgmST(&quot;productSum&quot;, space=vgm(150, &quot;Exp&quot;, 200, 0), time= vgm(20, &quot;Sph&quot;, 40, 0), k=2) StAni = estiStAni(v.st, c(0,200000)) (fitProdSumModel &lt;- fit.StVariogram(v.st, prodSumModel, fit.method = 7, stAni = StAni, method = &quot;L-BFGS-B&quot;, control = list(parscale = c(1,10,1,1,0.1,1,10)), lower = rep(0.0001, 7))) # space component: # model psill range # 1 Nug 26.3 0 # 2 Exp 140.5 432 # time component: # model psill range # 1 Nug 1.21 0.0 # 2 Sph 15.99 40.1 # k: 0.0322469094848839 plot(v.st, fitProdSumModel, wireframe=FALSE, all=TRUE, scales=list(arrows=FALSE), zlim=c(0,150)) which can also be plotted as wireframes, by plot(v.st, model=fitProdSumModel, wireframe=TRUE, all=TRUE, scales=list(arrows=FALSE), zlim=c(0,185)) Hints about the fitting strategy and alternative models for spatiotemporal variograms are given in (Gräler, Pebesma, and Heuvelink 2016). With this fitted model, and given the observations, we can carry out kriging or simulation at arbitrary points in space and time. For instance, we could estimate (or simulate) values in the time series that are now missing: this occurs regularly, and in section 12.4 we used means over time series based on simply ignoring up to 25% of the observations: substituting these with estimated or simulated values based on neigbouring (in space and time) observations before computing yearly mean values seems a more reasonable approach. More in general, we can estimate at arbitrary locations and time points, and we will illustrate this with predicting time series at particular locations, and and predicting spatial slices (Gräler, Pebesma, and Heuvelink 2016). We can create a stars object to denote two points and all time instances with set.seed(123) pt = st_sample(de, 2) t = st_get_dimension_values(no2.st, 1) st_as_stars(list(pts = matrix(1, length(t), length(pt)))) %&gt;% st_set_dimensions(names = c(&quot;time&quot;, &quot;station&quot;)) %&gt;% st_set_dimensions(&quot;time&quot;, t) %&gt;% st_set_dimensions(&quot;station&quot;, pt) -&gt; new_pt and obtain the corresponding spatiotemporal predictions using krigeST with no2.st &lt;- st_transform(no2.st, crs) new_ts &lt;- krigeST(NO2~1, data = no2.st[&quot;NO2&quot;], newdata = new_pt, nmax = 50, stAni = StAni, modelList = fitProdSumModel, progress = FALSE) plot(as.xts(new_ts[2])) Alternatively, we can create spatiotemporal predictions for a set of time-stamped raster maps, created by t4 = t[(1:4 - 0.5) * (3*24*30)] d = dim(grd) st_as_stars(pts = array(1, c(d[1], d[2], time=length(t4)))) %&gt;% st_set_dimensions(&quot;time&quot;, t4) %&gt;% st_set_dimensions(&quot;x&quot;, st_get_dimension_values(grd, &quot;x&quot;)) %&gt;% st_set_dimensions(&quot;y&quot;, st_get_dimension_values(grd, &quot;y&quot;)) %&gt;% st_set_crs(crs) -&gt; grd.st and the subsequent predictions are obtained by new_int &lt;- krigeST(NO2~1, data = no2.st[&quot;NO2&quot;], newdata = grd.st, nmax = 200, stAni = StAni, modelList = fitProdSumModel, progress = FALSE) names(new_int)[2] = &quot;NO2&quot; g + geom_stars(data = new_int, aes(fill = NO2, x = x, y = y)) + facet_wrap(~time) + geom_sf(data = st_cast(de, &quot;MULTILINESTRING&quot;)) + geom_sf(data = no2.sf, col = &#39;grey&#39;, cex = .5) # Coordinate system already present. Adding new coordinate system, which will replace the existing one. and shown in figure ??. A larger value for nmax was needed here to decrease the visible disturbance (sharp edges) caused by discrete neighbourhood selections, which are now done in space and time. 12.10 Area-to-point kriging T.b.d. References "],["area.html", "Chapter 13 Area Data and Spatial Autocorrelation 13.1 Spatial autocorrelation 13.2 Spatial weight matrices 13.3 Measures of spatial autocorrelation", " Chapter 13 Area Data and Spatial Autocorrelation Areal units of observation are very often used when simultaneous observations are aggregated within non-overlapping boundaries. The boundaries may be those of administrative entities, and may be related to underlying spatial processes, such as commuting flows, but are usually arbitrary. If they do not match the underlying and unobserved spatial processes in one or more variables of interest, proximate areal units will contain parts of the underlying processes, engendering spatial autocorrelation. This is at least in part because the aggregated observations are driven by autocorrelated factors which may or may not themselves have been observed. With support of data we mean the physical size (lenth, area, volume) associated with an individual observational unit (measurement). It is possible to represent the support of areal data by a point, despite the fact that the data have polygonal support. The centroid of the polygon may be taken as a representative point, or the centroid of the largest polygon in a multi-polygon object. When data with intrinsic point support are treated as areal data, the change of support goes the other way, from the known point to a non-overlapping tesselation such as a Voronoi diagram or Dirichlet tessellation or Thiessen polygons often through a Delaunay triangulation and using a Euclidean plane (projected coordinates). Here, different metrics may also be chosen, or distances measured on a network rather than on the plane. There is also a literature using weighted Voronoi diagrams in local spatial analysis (see for example Boots and Okabe 2007; Okabe et al. 2008; She et al. 2015). When the intrinsic support of the data is as points, but the underlying process is between proximate observations rather than driven chiefly by distance however measured between observations, the data may be aggregate counts or totals (polling stations, retail turnover) or represent a directly observed characteristic of the observation (opening hours of the polling station). Obviously, the risk of mis-representing the footprint of the underlying spatial processes remains in all of these cases, not least because the observations are taken as encompassing the entirety of the underlying process in the case of tesselation of the whole area of interest. This is distinct from the geostatistical setting in which observations are rather samples taken using some scheme within the area of interest. It is also partly distinct from the practice of taking areal sample plots within the area of interest but covering only a small proportion of the area, typically used in ecological and environmental research. This chapter then considers a subset of the methods potentially available for exploring spatial autocorrelation in areal data, or data being handled as areal, where the spatial processes are considered as working through proximity understood in the first instance as contiguity, as a graph linking observations taken as neighbours. This graph is typically undirected and unweighted, but may be directed and/or weighted in certain settings, which then leads to further issues with regard to symmetry. In principle, proximity would be expected to operate symmetrically in space, that is that the influence of \\(i\\) on \\(j\\) and of \\(j\\) on \\(i\\) based on their relative positions should be equivalent. Edge effects are not considered in standard treatments. 13.1 Spatial autocorrelation When analysing areal data, it has long been recognised that, if present, spatial autocorrelation changes how we may infer, relative to the default position of independent observations. In the presence of spatial autocorrelation, we can predict the values of observation \\(i\\) from the values observed at \\(j \\in N_i\\), the set of its proximate neighbours. Early results (Moran 1948; Geary 1954), entered into research practice gradually, for example the social sciences (Duncan, Cuzzort, and Duncan 1961). These results were then collated and extended to yield a set of basic tools of analysis (Cliff and Ord 1973, 1981). Cliff and Ord (1973) generalised and extended the expression of the spatial weights matrix representation as part of the framework for establishing the distribution theory for join count, Moran’s \\(I\\) and Geary’s \\(C\\) statistics. This development of what have become known as global measures, returning a single value of autocorrelation for the total study area, has been supplemented by local measures returning values for each areal unit (Getis and Ord 1992; Anselin 1995). 13.2 Spatial weight matrices Handling spatial autocorrelation using relationships to neighbours on a graph takes the graph as given, chosen by the analyst. This differs from the geostatistical approach in which the analyst chooses the binning of the empirical variogram and function used, and then the way the fitted variogram is fitted. Both involve a priori choices, but represent the underlying correlation in different ways (Wall 2004). In Bavaud (1998) and work citing his contribution, attempts have been made to place graph-based neighbours in a broader context. One issue arising in the creation of objects representing neighbourhood relationships is that of no-neighbour areal units (Bivand and Portnov 2004). Islands or units separated by rivers may not be recognised as neighbours when the units have areal support and when using topological relationships such as shared boundaries. In some settings, for example mrf (Markov Random Field) terms in mgcv::gam() and similar model fitting functions that require undirected connected graphs, a requirement is violated when there are disconnected subgraphs. No-neighbour observations can also occur when a distance threshold is used between points, where the threshold is smaller than the maximum nearest neighbour distance. Shared boundary contiguities are not affected by using geographical, unprojected coordinates, but all point-based approaches use distance in one way or another, and need to calculate distances in an appropriate way. The spdep package provides an nb class for neighbours, a list of length equal to the number of observations, with integer vector components. No-neighbours are encoded as an integer vector with a single element 0L, and observations with neighbours as sorted integer vectors containing values in 1L:n pointing to the neighbouring observations. This is a typical row-oriented sparse representation of neighbours. spdep provides many ways of constructing nb objects, and the representation and construction functions are widely used in other packages. spdep builds on the nb representation (undirected or directed graphs) with the listw object, a list with three components, an nb object, a matching list of numerical weights, and a single element character vector containing the single letter name of the way in which the weights were calculated. The most frequently used approach in the social sciences is calculating weights by row standardization, so that all the non-zero weights for one observation will be the inverse of the cardinality of its set of neighbours (1/card(nb[[i]])). We will be using election data from the 2015 Polish Presidential election in this chapter, with 2495 municipalities and Warsaw boroughs (see Figure 13.1 for a tmap map (section 9.5) of the municipality types) , and complete count data from polling stations aggregated to these areal units. The data are an sf sf object: library(sf) data(pol_pres15, package=&quot;spDataLarge&quot;) head(pol_pres15[, c(1, 4, 6)]) # Simple feature collection with 6 features and 3 fields # Geometry type: MULTIPOLYGON # Dimension: XY # Bounding box: xmin: 235000 ymin: 367000 xmax: 281000 ymax: 413000 # Projected CRS: ETRS89 / Poland CS92 # TERYT name types geometry # 1 020101 BOLESŁAWIEC Urban MULTIPOLYGON (((261089 3855... # 2 020102 BOLESŁAWIEC Rural MULTIPOLYGON (((254150 3837... # 3 020103 GROMADKA Rural MULTIPOLYGON (((275346 3846... # 4 020104 NOWOGRODZIEC Urban/rural MULTIPOLYGON (((251770 3770... # 5 020105 OSIECZNICA Rural MULTIPOLYGON (((263424 4060... # 6 020106 WARTA BOLESŁAWIECKA Rural MULTIPOLYGON (((267031 3870... library(tmap) tm_shape(pol_pres15) + tm_fill(&quot;types&quot;) Figure 13.1: Polish municipality types 2015 Between early 2002 and April 2019, spdep contained functions for constructing and handling neighbour and spatial weights objects, tests for spatial autocorrelation, and model fitting functions. The latter have been split out into spatialreg, and will be discussed in the next chapter. spdep now accommodates objects represented using sf classes and sp classes directly. library(spdep) # Loading required package: sp # Loading required package: spData 13.2.1 Contiguous neighbours The poly2nb() function in spdep takes the boundary points making up the polygon boundaries in the object passed as the pl= argument, and for each observation checks whether at least one (queen=TRUE, default), or at least two (rook, queen=FALSE) points are within snap= distance units of each other. The distances are planar in the raw coordinate units, ignoring geographical projections. Once the required number of sufficiently close points is found, the search is stopped. args(poly2nb) # function (pl, row.names = NULL, snap = sqrt(.Machine$double.eps), # queen = TRUE, useC = TRUE, foundInBox = NULL, small_n = 500) # NULL Two other arguments are also worth discussing, foundInBox= and small_n=. The first, foundInBox=, accepted the output of the rgeos gUnarySTRtreeQuery() function to list candidate neighbours, that is polygons whose bounding boxes intersect the bounding boxes of other polygons. From spdep 1.1-7, the GEOS interface of the sf package is used within poly2nb() if foundInBox=NULL and the number of observations is greater than small_n=, to find the candidate neighbours and populate foundInBox internally. In this case, this use of spatial indexing (STRtree queries) in GEOS through sf is the default, as the number of observations is greater than small_n: system.time(nb_q &lt;- poly2nb(pol_pres15, queen=TRUE)) # user system elapsed # 0.859 0.000 0.860 The print method shows the summary structure of the neighbour object: nb_q # Neighbour list object: # Number of regions: 2495 # Number of nonzero links: 14242 # Percentage nonzero weights: 0.229 # Average number of links: 5.71 Raising small_n= above the observation count, we see that the processing time is increased a little; the benefits of indexing are more apparent with larger data sets. system.time(nb_q_legacy &lt;- poly2nb(pol_pres15, queen=TRUE, small_n=2500)) # user system elapsed # 0.998 0.000 0.998 The output objects are identical: all.equal(nb_q, nb_q_legacy, check.attributes=FALSE) # [1] TRUE Much of the work involved in finding contiguous neighbours is spent on finding candidate neighbours with intersecting bounding boxes. Note that nb objects record both symmetric neighbour relationships, because these objects admit asymmetric relationships as well, but these duplications are not needed for object construction. Most of the spdep functions for constructing neighbour objects take a row.names= argument, the value of which is stored as a region.id attribute. If not given, the values are taken from row.names() of the first argument. These can be used to check that the neighbours object is in the same order as data. If nb objects are subsetted, the indices change to continue to be within 1:length(subsetted_nb), but the region.id attribute values point back to the object from which it was constructed. We can also check that this undirected graph is connected using the n.comp.nb() function; while some model estimation techniques do not support graphs that are not connected, it is helpful to be aware of possible problems (Freni-Sterrantino, Ventrucci, and Rue 2018): n.comp.nb(nb_q)$nc # [1] 1 Neighbour objects may be exported and imported in GAL format for exchange with other software, using write.nb.gal() and read.gal(): tf &lt;- tempfile(fileext=&quot;.gal&quot;) write.nb.gal(nb_q, tf) 13.2.2 Graph-based neighbours If areal units are an appropriate representation, but only points have been observed, contiguity relationships may be approximated using graph-based neighbours. In this case, the imputed boundaries tesselate the plane such that points closer to one observation than any other fall within its polygon. The simplest form is by using triangulation, here using the deldir() function in the deldir package. Because the function returns from and to identifiers, it is easy to construct a long representation of a listw object, as used in the S-Plus SpatialStats module and the sn2listw() function internally to construct an nb object (ragged wide representation). Alternatives often fail to return sufficient information to permit the neighbours to be identified. The soi.graph() function takes triangulated neighbours and prunes off neighbour relationships represented by unusually long edges, especially around the convex hull, but may render the output object asymmetric. Other graph-based approaches include relativeneigh() and gabrielneigh(). The output of these functions is then converted to the nb representation using graph2nb(), with the possible use of the sym= argument to coerce to symmetry. We take the centroids of the largest component polygon for each observation as the point representation; population-weighted centroids might have been a better choice if they were available: coords &lt;- st_centroid(st_geometry(pol_pres15), of_largest_polygon=TRUE) suppressMessages(nb_tri &lt;- tri2nb(coords)) nb_tri # Neighbour list object: # Number of regions: 2495 # Number of nonzero links: 14930 # Percentage nonzero weights: 0.24 # Average number of links: 5.98 The average number of neighbours is similar to the Queen boundary contiguity case, but if we look at the distribution of edge lengths using nbdists(), we can see that although the upper quartile is about 15 km, the maximum is almost 300 km, an edge along much of one side of the convex hull. The short minimum distance is also of interest, as many centroids of urban municipalities are very close to the centroids of their surrounding rural counterparts. summary(unlist(nbdists(nb_tri, coords))) # Min. 1st Qu. Median Mean 3rd Qu. Max. # 247 9847 12151 13485 14994 296974 Triangulated neighbours also yield a connected graph: n.comp.nb(nb_tri)$nc # [1] 1 The sphere of influence graph trims a neighbour object such as nb_tri to remove edges that seem long in relation to typical neighbours (Avis and Horton 1985). nb_soi &lt;- graph2nb(soi.graph(nb_tri, coords)) nb_soi # Neighbour list object: # Number of regions: 2495 # Number of nonzero links: 12792 # Percentage nonzero weights: 0.205 # Average number of links: 5.13 Unpicking the triangulated neighbours does however remove the connected character of the underlying graph: n_comp &lt;- n.comp.nb(nb_soi) n_comp$nc # [1] 16 The SoI algorithm has stripped out longer edges leading to urban and rural municipality pairs where their centroids are very close to each other because the rural ones completely surround the urban, giving 15 pairs of neighbours unconnected to the main graph: table(n_comp$comp.id) # # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 2465 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 The largest length edges along the convex hull have been removed, but “holes” have appeared where the unconnected pairs of neighbours have appeared. The differences between nb_tri and nb_soi are shown in orange in Figure 13.2. opar &lt;- par(mar=c(0,0,0,0)+0.5) plot(st_geometry(pol_pres15), border=&quot;grey&quot;, lwd=0.5) plot(nb_soi, coords=st_coordinates(coords), add=TRUE, points=FALSE, lwd=0.5) plot(diffnb(nb_tri, nb_soi), coords=st_coordinates(coords), col=&quot;orange&quot;, add=TRUE, points=FALSE, lwd=0.5) Figure 13.2: Triangulated (orange + black) and sphere of influence neighbours (black) par(opar) 13.2.3 Distance-based neighbours Distance-based neighbours can be constructed using dnearneigh(), with a distance band with lower d1= and upper d2= bounds controlled by the bounds= argument. If spherical coordinates are used and either specified in the coordinates object x or with x as a two column matrix and longlat=TRUE, great circle distances in km will be calculated assuming the WGS84 reference ellipsoid. From spdep 1.1-7, two arguments have been added, to use functionality in the dbscan package for finding neighbours using a kd-tree in two or three dimensions by default, and not to test the symmetry of the output neighbour object. The knearneigh() function for \\(k\\)-nearest neighbours returns a knn object, converted to an nb object using knn2nb(). It can also use great circle distances, not least because nearest neighbours may differ when uprojected coordinates are treated as planar. k= should be a small number. For projected coordinates, the dbscan package is used to compute nearest neighbours more efficiently. Note that nb objects constructed in this way are most unlikely to be symmetric, hence knn2nb() has a sym= argument to permit the imposition of symmetry, which will mean that all units have at least k= neighbours, not that all units will have exactly k= neighbours. The nbdists() function returns the length of neighbour relationship edges in the units of the coordinates if the coordinates are projected, in km otherwise. In order to set the upper limit for distance bands, one may first find the maximum first nearest neighbour distance, using unlist() to remove the list structure of the returned object. k1 &lt;- knn2nb(knearneigh(coords)) k1dists &lt;- unlist(nbdists(k1, coords)) summary(k1dists) # Min. 1st Qu. Median Mean 3rd Qu. Max. # 247 6663 8538 8275 10124 17979 Here the largest first nearest neighbour distance is just under 18 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour: system.time(nb_d18 &lt;- dnearneigh(coords, 0, 18000)) # user system elapsed # 0.168 0.000 0.168 system.time(nb_d18a &lt;- dnearneigh(coords, 0, 18000, use_kd_tree=FALSE)) # user system elapsed # 0.152 0.000 0.152 all.equal(nb_d18, nb_d18a, check.attributes=FALSE) # [1] TRUE nb_d18 # Neighbour list object: # Number of regions: 2495 # Number of nonzero links: 20358 # Percentage nonzero weights: 0.327 # Average number of links: 8.16 However, even though there are no no-neighbour observations (their presence is reported by the print method for nb objects), the graph is not connected, as a pair of observations are each others’ only neighbours. n_comp &lt;- n.comp.nb(nb_d18) n_comp$nc # [1] 2 table(n_comp$comp.id) # # 1 2 # 2493 2 Adding 300 m to the threshold gives us a neighbour object with no no-neighbour units, and all units can be reached from all others across the graph. nb_d183 &lt;- dnearneigh(coords, 0, 18300) nb_d183 # Neighbour list object: # Number of regions: 2495 # Number of nonzero links: 21086 # Percentage nonzero weights: 0.339 # Average number of links: 8.45 n_comp &lt;- n.comp.nb(nb_d183) n_comp$nc # [1] 1 One characteristic of distance-based neighbours is that more densely settled areas, with units which are smaller in terms of area (Warsaw boroughs are much smaller on average, but have almost 30 neighbours). Having many neighbours smooths the neighbour relationship across more neighbours. For use later, we also construct a neighbour object with no-neighbour units, using a threshold of 16 km: nb_d16 &lt;- dnearneigh(coords, 0, 16000) nb_d16 # Neighbour list object: # Number of regions: 2495 # Number of nonzero links: 15850 # Percentage nonzero weights: 0.255 # Average number of links: 6.35 # 7 regions with no links: # 569 1371 1522 2374 2385 2473 2474 It is possible to control the numbers of neighbours directly using \\(k\\)-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry: knn_k6 &lt;- knearneigh(coords, k=6) knn2nb(knn_k6) # Neighbour list object: # Number of regions: 2495 # Number of nonzero links: 14970 # Percentage nonzero weights: 0.24 # Average number of links: 6 # Non-symmetric neighbours list nb_k6s &lt;- knn2nb(knn_k6, sym=TRUE) nb_k6s # Neighbour list object: # Number of regions: 2495 # Number of nonzero links: 16810 # Percentage nonzero weights: 0.27 # Average number of links: 6.74 Here the size of k= is sufficient to ensure connectedness, although the graph is not planar as edges cross at locations other than nodes, which is not the case for contiguous or graph-based neighbours. n_comp &lt;- n.comp.nb(nb_k6s) n_comp$nc # [1] 1 13.2.4 Weights specification Once neighbour objects are available, further choices need to made in specifying the weights objects. The nb2listw() function is used to create a listw weights object with an nb object, a matching list of weights vectors, and a style specification. Because handling no-neighbour observations now begins to matter, the zero.policy= argument is introduced. By default, this is FALSE, indicating that no-neighbour observations will cause an error, as the spatially lagged value for an observation with no neighbours is not available. By convention, zero is substituted for the lagged value, as the cross product of a vector of zero-valued weights and a data vector, hence the name of zero.policy. args(nb2listw) # function (neighbours, glist = NULL, style = &quot;W&quot;, zero.policy = NULL) # NULL We will be using the helper function spweights.constants() below to show some consequences of varing style choices. It returns constants for a listw object, \\(n\\) is the number of observations, n1 to n3 are \\(n-1, \\ldots\\), nn is \\(n^2\\) and \\(S_0\\), \\(S_1\\) and \\(S_2\\) are constants, \\(S_0\\) being the sum of the weights. There is a full discussion of the constants in Bivand and Wong (2018). args(spweights.constants) # function (listw, zero.policy = NULL, adjust.n = TRUE) # NULL The \"B\" binary style gives a weight of unity to each neighbour relationship, and typically upweights units with no boundaries on the edge of the study area. lw_q_B &lt;- nb2listw(nb_q, style=&quot;B&quot;) unlist(spweights.constants(lw_q_B)) # n n1 n2 n3 nn S0 S1 S2 # 2495 2494 2493 2492 6225025 14242 28484 357280 The \"W\" row-standardized style upweights units around the edge of the study area that necessarily have fewer neighbours. This style first gives a weight of unity to each neighbour relationship, then divides these weights by the per unit sums of weights. Naturally this leads to division by zero where there are no neighbours, a not-a-number result, unless the chosen policy is to permit no-neighbour observations. We can see that \\(S_0\\) is now equal to \\(n\\). lw_q_W &lt;- nb2listw(nb_q, style=&quot;W&quot;) unlist(spweights.constants(lw_q_W))[c(1,6:8)] # n S0 S1 S2 # 2495 2495 958 10406 Inverse distance weights are used in a number of scientific fields. Some use dense inverse distance matrices, but many of the inverse distances are close to zero, so have little practical contribution, especially as the spatial process matrix is itself dense. Inverse distance weights may be constructed by taking the lengths of edges, changing units to avoid most weights being too large or small (here from m to km), taking the inverse, and passing through the glist= argument to nb2listw(): gwts &lt;- lapply(nbdists(nb_d183, coords), function(x) 1/(x/1000)) lw_d183_idw_B &lt;- nb2listw(nb_d183, glist=gwts, style=&quot;B&quot;) unlist(spweights.constants(lw_d183_idw_B))[c(1,6:8)] # n S0 S1 S2 # 2495 1841 534 7265 No-neighbour handling is by default to prevent the construction of a weights object, making the analyst take a position on how to proceed. try(lw_d16_B &lt;- nb2listw(nb_d16, style=&quot;B&quot;)) # Error in nb2listw(nb_d16, style = &quot;B&quot;) : Empty neighbour sets found Use can be made of the zero.policy= argument to many functions used with nb and listw objects. lw_d16_B &lt;- nb2listw(nb_d16, style=&quot;B&quot;, zero.policy=TRUE) unlist(spweights.constants(lw_d16_B, zero.policy=TRUE))[c(1,6:8)] # n S0 S1 S2 # 2488 15850 31700 506480 Note that by default the adjust.n= argument to spweights.constants() is set by default to TRUE, subtracting the count of no-neighbour observations from the observation count, so \\(n\\) is smaller with possible consequences for inference. The complete count can be retrieved by changing the argument. 13.3 Measures of spatial autocorrelation Measures of spatial autocorrelation unfortunately pick up other mis-specifications in the way that we model data (Schabenberger and Gotway 2005; McMillen 2003). For reference, Moran’s \\(I\\) is given as (Cliff and Ord 1981, 17): \\[ I = \\frac{n \\sum_{(2)} w_{ij} z_i z_j}{S_0 \\sum_{i=1}^{n} z_i^2} \\] where \\(x_i, i=1, \\ldots, n\\) are \\(n\\) observations on the numeric variable of interest, \\(z_i = x_i - \\bar{x}\\), \\(\\bar{x} = \\sum_{i=1}^{n} x_i / n\\), \\(\\sum_{(2)} = \\stackrel{\\sum_{i=1}^{n} \\sum_{j=1}^{n}}{i \\neq j}\\), \\(w_{ij}\\) are the spatial weights, and \\(S_0 = \\sum_{(2)} w_{ij}\\). First we test a random variable using the Moran test, here under the normality assumption (argument randomisation=FALSE, default TRUE): set.seed(1) x &lt;- rnorm(nrow(pol_pres15)) mt &lt;- moran.test(x, lw_q_B, randomisation=FALSE, alternative=&quot;two.sided&quot;) Inference is made on the statistic \\(Z(I) = \\frac{I - E(I)}{\\sqrt{\\mathrm{Var}(I)}}\\), the z-value compared with the Normal distribution for \\(E(I)\\) and \\(\\mathrm{Var}(I)\\) for the chosen assumptions; this x does not show spatial autocorrelation with these spatial weights: glance_htest &lt;- function(ht) c(ht$estimate, &quot;Std deviate&quot;=unname(ht$statistic), &quot;p.value&quot;=unname(ht$p.value)) glance_htest(mt) # Moran I statistic Expectation Variance Std deviate # -0.004772 -0.000401 0.000140 -0.369320 # p.value # 0.711889 The test however fails to detect a missing trend in the data as a missing variable problem, finding spatial autocorrelation instead: beta &lt;- 0.15e-02 t &lt;- st_coordinates(coords)[,1]/1000 x_t &lt;- x + beta*t mt &lt;- moran.test(x_t, lw_q_B, randomisation=FALSE, alternative=&quot;two.sided&quot;) glance_htest(mt) # Moran I statistic Expectation Variance Std deviate # 0.043403 -0.000401 0.000140 3.701491 # p.value # 0.000214 If we test the residuals of a linear model including the trend, the apparent spatial autocorrelation disappears: lmt &lt;- lm.morantest(lm(x_t ~ t), lw_q_B, alternative=&quot;two.sided&quot;) glance_htest(lmt) # Observed Moran I Expectation Variance Std deviate # -0.004777 -0.000789 0.000140 -0.337306 # p.value # 0.735886 A comparison of implementations of measures of spatial autocorrelation shows that a wide range of measures is available in R in a number of packages, chiefly in the spdep package, and that differences from other implementations can be attributed to design decisions (Bivand and Wong 2018). The spdep package also includes the only implementations of exact and Saddlepoint approximations to global and local Moran’s I for regression residuals (Tiefelsdorf 2002; Bivand, Müller, and Reder 2009). 13.3.1 Global measures We will begin by examining join count statistics, where joincount.test() takes a factor vector of values fx= and a listw object, and returns a list of htest (hypothesis test) objects defined in the stats package, one htest object for each level of the fx= argument. The observed counts are of neighbours with the same factor levels, known as same-colour joins. args(joincount.test) # function (fx, listw, zero.policy = NULL, alternative = &quot;greater&quot;, # sampling = &quot;nonfree&quot;, spChk = NULL, adjust.n = TRUE) # NULL The function takes an alternative= argument for hypothesis testing, a sampling= argument showing the basis for the construction of the variance of the measure, where the default \"nonfree\" choice corresponds to analytical permutation; the spChk= argument is retained for backward compatibility. For reference, the counts of factor levels for the type of municipality or Warsaw borough are: table(pol_pres15$types) # # Rural Urban Urban/rural Warsaw Borough # 1563 303 611 18 Since there are four levels, we re-arrange the list of htest objects to give a matrix of estimated results. The observed same-colour join counts are tabulated with their expectations based on the counts of levels of the input factor, so that few joins would be expected between for example Warsaw boroughs, because there are very few of them. The variance calculation uses the underlying constants of the chosen listw object and the counts of levels of the input factor. The z-value is obtained in the usual way by dividing the difference between the observed and expected join counts by the square root of the variance. The join count test was subsequently adapted for multi-colour join counts (Upton and Fingleton 1985). The implementation as joincount.mult() in spdep returns a table based on nonfree sampling, and does not report p-values. joincount.multi(pol_pres15$types, listw=lw_q_B) # Joincount Expected Variance z-value # Rural:Rural 3087.000 2793.920 1126.534 8.73 # Urban:Urban 110.000 104.719 93.299 0.55 # Urban/rural:Urban/rural 656.000 426.526 331.759 12.60 # Warsaw Borough:Warsaw Borough 41.000 0.350 0.347 68.96 # Urban:Rural 668.000 1083.941 708.209 -15.63 # Urban/rural:Rural 2359.000 2185.769 1267.131 4.87 # Urban/rural:Urban 171.000 423.729 352.190 -13.47 # Warsaw Borough:Rural 12.000 64.393 46.460 -7.69 # Warsaw Borough:Urban 9.000 12.483 11.758 -1.02 # Warsaw Borough:Urban/rural 8.000 25.172 22.354 -3.63 # Jtot 3227.000 3795.486 1496.398 -14.70 So far, we have used binary weights, so the sum of join counts multiplied by the weight on that join remains integer. If we change to row standardised weights, where the weights are not unity in all cases, the counts, expectations and variances change, but there are few major changes in the z-values. Using an inverse distance based listw object does, however, change the z-values markedly, because closer centroids are upweighted relatively strongly: joincount.multi(pol_pres15$types, listw=lw_d183_idw_B) # Joincount Expected Variance z-value # Rural:Rural 3.46e+02 3.61e+02 4.93e+01 -2.10 # Urban:Urban 2.90e+01 1.35e+01 2.23e+00 10.39 # Urban/rural:Urban/rural 4.65e+01 5.51e+01 9.61e+00 -2.79 # Warsaw Borough:Warsaw Borough 1.68e+01 4.53e-02 6.61e-03 206.38 # Urban:Rural 2.02e+02 1.40e+02 2.36e+01 12.73 # Urban/rural:Rural 2.25e+02 2.83e+02 3.59e+01 -9.59 # Urban/rural:Urban 3.65e+01 5.48e+01 8.86e+00 -6.14 # Warsaw Borough:Rural 5.65e+00 8.33e+00 1.73e+00 -2.04 # Warsaw Borough:Urban 9.18e+00 1.61e+00 2.54e-01 15.01 # Warsaw Borough:Urban/rural 3.27e+00 3.25e+00 5.52e-01 0.02 # Jtot 4.82e+02 4.91e+02 4.16e+01 -1.38 The implementation of Moran’s \\(I\\) in spdep in the moran.test() function has similar arguments to those of joincount.test(), but sampling= is replaced by randomisation= to indicate the underlying analytical approach used for calculating the variance of the measure. It is also possible to use ranks rather than numerical values (Cliff and Ord 1981, 46). The drop.EI2= agrument may be used to reproduce results where the final component of the variance term is omitted as found in some legacy software implementations. args(moran.test) # function (x, listw, randomisation = TRUE, zero.policy = NULL, # alternative = &quot;greater&quot;, rank = FALSE, na.action = na.fail, # spChk = NULL, adjust.n = TRUE, drop.EI2 = FALSE) # NULL The default for the randomisation= argument is TRUE, but here we will simply show that the test under normality is the same as a test of least squares residuals with only the intercept used in the mean model. The spelling of randomisation is that of Cliff and Ord (1973). mt &lt;- moran.test(pol_pres15$I_turnout, listw=lw_q_B, randomisation=FALSE) glance_htest(mt) # Moran I statistic Expectation Variance Std deviate # 0.691434 -0.000401 0.000140 58.461349 # p.value # 0.000000 The lm.morantest() function also takes a resfun= argument to set the function used to extract the residuals used for testing, and clearly lets us model other salient features of the response variable (Cliff and Ord 1981, 203). To compare with the standard test, we are only using the intercept here, and as can be seen, the results are the same. ols &lt;- lm(I_turnout ~ 1, pol_pres15) lmt &lt;- lm.morantest(ols, listw=lw_q_B) glance_htest(lmt) # Observed Moran I Expectation Variance Std deviate # 0.691434 -0.000401 0.000140 58.461349 # p.value # 0.000000 The only difference between tests under normality and randomisation is that an extra term is added if the kurtosis of the variable of interest indicates a flatter or more peaked distribution, where the measure used is the classical measure of kurtosis. Under the default randomisation assumption of analytical randomisation, the results are largely unchanged. mtr &lt;- moran.test(pol_pres15$I_turnout, listw=lw_q_B) glance_htest(mtr) # Moran I statistic Expectation Variance Std deviate # 0.691434 -0.000401 0.000140 58.459835 # p.value # 0.000000 Of course, from the very beginning, interest was shown in Monte Carlo testing, also known as a Hope-type test and as a permutation bootstrap. By default, moran.mc() returns a \"htest\" object, but may simply use boot::boot() internally and return a \"boot\" object when return_boot=TRUE. In addition the number of simulations of the variable of interest by permutation, that is shuffling the values across the observations at random, needs to be given as nsim=. set.seed(1) mmc &lt;- moran.mc(pol_pres15$I_turnout, listw=lw_q_B, nsim=999, return_boot = TRUE) The bootstrap permutation retains the outcomes of each of the random permutations, reporting the observed value of the statistic, here Moran’s \\(I\\), the difference between this value and the mean of the simulations under randomisation (equivalent to \\(E(I)\\)), and the standard deviation of the simulations under randomisation. If we compare the Monte Carlo and analytical variances of \\(I\\) under randomisation, we typically see few differences, arguably rendering Monte Carlo testing unnecessary. c(&quot;Permutation bootstrap&quot;=var(mmc$t), &quot;Analytical randomisation&quot;=unname(mtr$estimate[3])) # Permutation bootstrap Analytical randomisation # 0.000144 0.000140 Geary’s global \\(C\\) is implemented in geary.test() largely following the same argument structure as moran.test(). The Getis-Ord \\(G\\) test includes extra arguments to accommodate differences between implementations, as Bivand and Wong (2018) found multiple divergences from the original definitions, often to omit no-neighbour observations generated when using distance band neighbours. It is given by (Getis and Ord 1992, 194). For \\(G_*\\), the \\(\\sum_{(2)}\\) constraint is relaxed by including \\(i\\) as a neighbour of itself (thereby also removing the no-neighbour problem, because all observations have at least one neighbour). Finally, the empirical Bayes Moran’s \\(I\\) takes account of the denominator in assessing spatial autocorrelation in rates data (Assunção and Reis 1999). Until now, we have considered the proportion of valid votes cast in relation to the numbers entitled to vote by spatial entity, but using EBImoran.mc() we can try to accommodate uncertainty in extreme rates in entities with small numbers entitled to vote. There is, however, little impact on the outcome in this case. Global measures of spatial autocorrelation using spatial weights objects based on graphs of neighbours are, as we have seen, rather blunt tools, which for interpretation depend critically on a reasoned mean model of the variable in question. If the mean model is just the intercept, the global measures will respond to all kinds of mis-specification, not only spatial autocorrelation. A key source of mis-specification will typically also include the choice of entities for aggregation of data. 13.3.2 Local measures Building on insights from the weaknesses of global measures, local indicators of spatial association began to appear in the first half of the 1990s (Anselin 1995; Getis and Ord 1992, 1996). In addition, the Moran plot was introduced, plotting the values of the variable of interest against their spatially lagged values, typically using row-standardised weights to make the axes more directly comparable (Anselin 1996). The moran.plot() function also returns an influence measures object used to label observations exerting more than propotional influence on the slope of the line representing global Moran’s \\(I\\). In Figure 13.3, we can see that there are many spatial entities exerting such influence. These pairs of observed and lagged observed values make up in aggregate the global measure, but can also be explored in detail. The quadrants of the Moran plot also show low-low pairs in the lower left quadrant, high-high in the upper right quadrant, and fewer low-high and high-low pairs in the upper left and lower right quadrants. infl_W &lt;- moran.plot(pol_pres15$I_turnout, listw=lw_q_W, labels=pol_pres15$TERYT, cex=1, pch=&quot;.&quot;, xlab=&quot;I round turnout&quot;, ylab=&quot;lagged turnout&quot;) Figure 13.3: Moran plot of I round turnout, row standardised weights If we extract the hat value influence measure from the returned object, Figure 13.4 suggests that some edge entities exert more than proportional influence (perhaps because of row standardisation), as do entities in or near larger urban areas. pol_pres15$hat_value &lt;- infl_W$hat tm_shape(pol_pres15) + tm_fill(&quot;hat_value&quot;) Figure 13.4: Moran plot hat values, row standardised neighbours Bivand and Wong (2018) discuss issues impacting the use of local indicators, such as local Moran’s \\(I\\) and local Getis-Ord \\(G\\). Some issues affect the calculation of the local indicators, others inference from their values. Because \\(n\\) statistics may be being calculated from the same number of observations, there are multiple comparison problems that need to be addressed. Although the apparent detection of hotspots from values of local indicators has been quite widely adopted, it remains fraught with difficulty because adjustment of the inferential basis to accommodate multiple comparisons is not often chosen, and as in the global case, mis-specification also remains a source of confusion. Further, interpreting local spatial autocorrelation in the presence of global spatial autocorrelation is challenging (Ord and Getis 2001; Tiefelsdorf 2002; Bivand, Müller, and Reder 2009). The mlvar= and adjust.x= arguments to localmoran() are discussed in Bivand and Wong (2018), and permit matching with other implementations. The p.adjust.method= argument uses an untested speculation that adjustment should only take into account the cardinality of the neighbour set of each observation when adjusting for multiple comparisons; using stats::p.adjust() is preferable. Taking \"two.sided\" p-values because these local indicators when summed and divided by the sum of the spatial weights, and thus positive and negative local spatial autocorrelation may be present, we obtain: locm &lt;- localmoran(pol_pres15$I_turnout, listw=lw_q_W, alternative=&quot;two.sided&quot;) all.equal(sum(locm[,1])/Szero(lw_q_W), unname(moran.test(pol_pres15$I_turnout, lw_q_W)$estimate[1])) # [1] TRUE Using stats::p.adjust() to adjust for multiple comparisons, we see that almost 29% of the local measures have p-values &lt; 0.05 if no adjustment is applied, but only 12% using Bonferroni adjustment, with two other choices also shown: pvs &lt;- cbind(&quot;none&quot;=locm[,5], &quot;bonferroni&quot;=p.adjust(locm[,5], &quot;bonferroni&quot;), &quot;fdr&quot;=p.adjust(locm[,5],&quot;fdr&quot;), &quot;BY&quot;=p.adjust(locm[,5], &quot;BY&quot;)) apply(pvs, 2, function(x) sum(x &lt; 0.05)) # none bonferroni fdr BY # 715 297 576 424 In the global measure case, bootstrap permutations could be used as an alternative to analytical methods for possible inference. In the local case, conditional permutation may be used, retaining the value at observation \\(i\\) and randomly sampling from the remaining \\(n-1\\) values to find randomised values at neighbours, and is provided as localmoran_perm(), which will use multiple nodes to sample in parallel if provided, and permits the setting of a seed for the random number generator across the compute nodes: library(parallel) set.coresOption(ifelse(detectCores() == 1, 1, detectCores()-1L)) # NULL system.time(locm_p &lt;- localmoran_perm(pol_pres15$I_turnout, listw=lw_q_W, nsim=499, alternative=&quot;two.sided&quot;, iseed=1)) # user system elapsed # 0.599 1.506 0.767 The outcome is that almost 32% of observations have two sided p-values &lt; 0.05 without multiple comparison adjustment, and under 3% with Bonferroni adjustment. pv &lt;- locm_p[,5] pvsp &lt;- cbind(&quot;none&quot;=pv, &quot;bonferroni&quot;=p.adjust(pv, &quot;bonferroni&quot;), &quot;fdr&quot;=p.adjust(pv, &quot;fdr&quot;), &quot;BY&quot;=p.adjust(pv, &quot;BY&quot;)) apply(pvsp, 2, function(x) sum(x &lt;= 0.05)) # none bonferroni fdr BY # 797 76 463 161 We can see what is happening by tabulating counts of the standard deviate of local Moran’s \\(I\\), where the two-sided \\(\\alpha=0.05\\) bounds would be \\(0.025\\) and \\(0.975\\), but Bonferroni adjustment is close to \\(0.00001\\) and \\(0.99999\\). Without adjustment, almost 800 observations are significant, with Bonferroni adjustment, only 68 in the conditional permutation case: brks &lt;- qnorm(c(0, 0.00001, 0.0001, 0.001, 0.01, 0.025, 0.5, 0.975, 0.99, 0.999, 0.9999, 0.99999, 1)) (tab &lt;- table(cut(locm_p[,4], brks))) # # (-Inf,-4.26] (-4.26,-3.72] (-3.72,-3.09] (-3.09,-2.33] (-2.33,-1.96] # 0 0 1 4 5 # (-1.96,0] (0,1.96] (1.96,2.33] (2.33,3.09] (3.09,3.72] # 459 1239 195 316 145 # (3.72,4.26] (4.26, Inf] # 55 76 sum(tab[c(1:5, 8:12)]) # [1] 797 sum(tab[c(1, 12)]) # [1] 76 pol_pres15$locm_Z &lt;- locm[,4] pol_pres15$locm_p_Z &lt;- locm_p[,4] tm_shape(pol_pres15) + tm_fill(c(&quot;locm_Z&quot;, &quot;locm_p_Z&quot;), breaks=brks, midpoint=0, title=&quot;Standard deviates of\\nLocal Moran&#39;s I&quot;) + tm_facets(free.scales=FALSE) + tm_layout(panel.labels=c(&quot;Analytical&quot;, &quot;Conditional permutation&quot;)) Figure 13.5: Analytical and conditional permutation standard deviates of local Moran’s I for first round turnout, row-standardised neighbours Figure 13.5 shows that conditional permutation scales back the proportion of standard deviate values taking extreme values, especially positive values. As we will see below, the analytical standard deviates of local Moran’s \\(I\\) should probably not be used if alternatives are available. In presenting local Moran’s \\(I\\), use is often made of “hotspot” maps. Because \\(I_i\\) takes high values both for strong positive autocorrelation of low and high values of the input variable, it is hard to show where “clusters” of similar neighbours with low or high values of the input variable occur. The quadrants of the Moran plot are used, by creating a categorical quadrant variable interacting the input variable and its spatial lag split at their means. The quadrant categories are then set to NA if, for the chosen standard deviate and adjustment, \\(I_i\\) would be considered insignificant. Here, for the conditional permutation standard deviates, Bonferroni adjusted, 14 observations belong to “Low-Low clusters”, and 54 to “High-High clusters”: quadr &lt;- interaction(cut(infl_W$x, c(-Inf, mean(infl_W$x), Inf), labels=c(&quot;Low X&quot;, &quot;High X&quot;)), cut(infl_W$wx, c(-Inf, mean(infl_W$wx), Inf), labels=c(&quot;Low WX&quot;, &quot;High WX&quot;)), sep=&quot; : &quot;) a &lt;- table(quadr) pol_pres15$hs_an_q &lt;- quadr is.na(pol_pres15$hs_an_q) &lt;- !(pol_pres15$locm_Z &lt; brks[6] | pol_pres15$locm_Z &gt; brks[8]) b &lt;- table(pol_pres15$hs_an_q) pol_pres15$hs_cp_q &lt;- quadr is.na(pol_pres15$hs_cp_q) &lt;- !(pol_pres15$locm_p_Z &lt; brks[2] | pol_pres15$locm_p_Z &gt; brks[12]) c &lt;- table(pol_pres15$hs_cp_q) t(rbind(&quot;Moran plot quadrants&quot;=a, &quot;Unadjusted analytical&quot;=b, &quot;Bonferroni cond. perm.&quot;=c)) # Moran plot quadrants Unadjusted analytical # Low X : Low WX 1040 370 # High X : Low WX 264 3 # Low X : High WX 213 0 # High X : High WX 978 342 # Bonferroni cond. perm. # Low X : Low WX 18 # High X : Low WX 0 # Low X : High WX 0 # High X : High WX 58 tm_shape(pol_pres15) + tm_fill(c(&quot;hs_an_q&quot;, &quot;hs_cp_q&quot;), colorNA=&quot;grey95&quot;, textNA=&quot;Not significant&quot;, title=&quot;Turnout hotspot status\\nLocal Moran&#39;s I&quot;) + tm_facets(free.scales=FALSE) + tm_layout(panel.labels=c(&quot;Unadjusted analytical&quot;, &quot;Cond. perm. with Bonferroni&quot;)) Figure 13.6: Local Moran’s I hotspot maps \\(\\alpha = 0.05\\): left panel: unadjusted analytical standard deviates; right panel: Bonferroni adjusted conditional permutation standard deviates, first round turnout, row-standardised neighbours Figure 13.6 shows the impact of using analytical or conditional permutation standard deviates, and no or Bonferroni adjustment, reducing the counts of observations in “Low-Low clusters” from 370 to 14, and “High-High clusters” from 342 to 54; the “High-High clusters” are metropolitan areas. The local Getis-Ord \\(G\\) measure is reported as a standard deviate, and may also take the \\(G^*\\) form where self-neighbours are inserted into the neighbour object using include.self(). The observed and expected values of local \\(G\\) with their analytical variances may also be returned if return_internals=TRUE. system.time(locG &lt;- localG(pol_pres15$I_turnout, lw_q_W)) # user system elapsed # 0.014 0.000 0.013 system.time(locG_p &lt;- localG_perm(pol_pres15$I_turnout, lw_q_W, nsim=499, iseed=1)) # user system elapsed # 0.123 0.762 0.788 Once again we face the problem of multiple comparisons, with the count of areal unit p-values &lt; 0.05 being reduced by an order of magnitude when employing Bonferroni correction: pv &lt;- 2 * pnorm(abs(c(locG)), lower.tail = FALSE) pvsp &lt;- cbind(pv, p.adjust(pv, &quot;bonferroni&quot;), p.adjust(pv, &quot;fdr&quot;), p.adjust(pv, &quot;BY&quot;)) colnames(pvsp) &lt;- c(&quot;none&quot;, &quot;bonferroni&quot;, &quot;fdr&quot;, &quot;BY&quot;) apply(pvsp, 2, function(x) sum(x &lt; 0.05)) # none bonferroni fdr BY # 789 69 468 156 In the \\(G_i\\) case, however, there is no systematic differencce between the analytical and conditional permutation standard deviates. library(ggplot2) p1 &lt;- ggplot(data.frame(Zi=locm[,4], Zi_perm=locm_p[,4])) + geom_point(aes(x=Zi, y=Zi_perm), alpha=0.2) + xlab(&quot;Analytical&quot;) + ylab(&quot;Conditional permutation&quot;) + coord_fixed() + ggtitle(&quot;Local Moran&#39;s I&quot;) p2 &lt;- ggplot(data.frame(Zi=c(locG), Zi_perm=c(locG_p))) + geom_point(aes(x=Zi, y=Zi_perm), alpha=0.2) + xlab(&quot;Analytical&quot;) + ylab(&quot;Conditional permutation&quot;) + coord_fixed() + ggtitle(&quot;Local G&quot;) gridExtra::grid.arrange(p1, p2, nrow=1) Figure 13.7: Plots of analytical against conditional permutation standard deviates; left: local Moran’s I; right: local G; first round turnout, row-standardised neighbours Figure 13.7 shows that, keeping fixed aspect in both panels, conditional permutation changes the range and distribution of the standard deviate values for \\(I_i\\), but that for \\(G_i\\), the two sets of standard deviates are equivalent. pol_pres15$locG_Z &lt;- c(locG) pol_pres15$hs_G &lt;- cut(c(locG), c(-Inf, brks[2], brks[12], Inf), labels=c(&quot;Low&quot;, &quot;Not significant&quot;, &quot;High&quot;)) table(pol_pres15$hs_G) # # Low Not significant High # 14 2426 55 m1 &lt;- tm_shape(pol_pres15) + tm_fill(c(&quot;locG_Z&quot;), midpoint=0, title=&quot;Standard\\ndeviate&quot;) m2 &lt;- tm_shape(pol_pres15) + tm_fill(c(&quot;hs_G&quot;), title=&quot;Bonferroni\\nhotspot status&quot;) tmap_arrange(m1, m2, nrow=1) Figure 13.8: Left: analytical standard deviates of local G; right: Bonferroni hotspots; first round turnout, row-standardised neighbours As can be seen from Figure 13.7, we do not need to contrast the two estimation methods, and showing the mapped standard deviate is as informative as the “hotspot” status for the chosen adjustment (Figure 13.8). In the case of \\(G_i\\), the values taken by the measure reflect the values of the input variable, so a “High cluster” is found for observations with high values of the input variable, here high turnout in metropolitan areas. Very recently, Geoda has been wrapped for R as rgeoda (Li and Anselin 2021), and will provide very similar functionalities for the exploration of spatial autocorrelation in areal data as spdep. The active objects are kept as pointers to a compiled code workspace; using compiled code for all operations (as in Geoda itself) makes rgeoda perform fast, but leaves less flexible when modifications or enhancements are desired. The contiguity neighbours it constructs are the same as those found by poly2nb(), as almost are the \\(I_i\\) measures. The difference is as established by Bivand and Wong (2018), that localmoran() calculates the input variable variance divinding by \\(n\\), but Geoda uses \\((n-1)\\), as can be reproduced by setting mlvar=FALSE: library(rgeoda) # Loading required package: digest # # Attaching package: &#39;rgeoda&#39; # The following object is masked from &#39;package:spdep&#39;: # # skater system.time(Geoda_w &lt;- queen_weights(pol_pres15)) # here # user system elapsed # 0.111 0.004 0.114 summary(Geoda_w) # name value # 1 number of observations: 2495 # 2 is symmetric: TRUE # 3 sparsity: 0.00228786229774178 # 4 # min neighbors: 1 # 5 # max neighbors: 13 # 6 # mean neighbors: 5.70821643286573 # 7 # median neighbors: 6 # 8 has isolates: FALSE system.time(lisa &lt;- local_moran(Geoda_w, pol_pres15[&quot;I_turnout&quot;], cpu_threads=ifelse(parallel::detectCores() == 1, 1, parallel::detectCores()-1L), permutations=499, seed=1)) # user system elapsed # 0.288 0.009 0.079 all.equal(card(nb_q), lisa_num_nbrs(lisa), check.attributes=FALSE) # [1] TRUE all.equal(lisa_values(lisa), localmoran(pol_pres15$I_turnout, listw=lw_q_W, mlvar=FALSE)[,1], check.attributes=FALSE) # [1] TRUE References "],["spatial-regression.html", "Chapter 14 Spatial Regression 14.1 Spatial regression with spatial weights 14.2 Estimators 14.3 Implementation details 14.4 Markov random field and multilevel models with spatial weights", " Chapter 14 Spatial Regression Even though it may be tempting to focus on interpreting the map pattern of an area support response variable of interest, the pattern may largely derive from covariates (and their functional forms), as well as the respective spatial footprints of the variables in play. Spatial autoregressive models in two dimensions began without covariates and with clear links to time series (Whittle 1954). Extensions included tests for spatial autocorrelation in linear model residuals, and models applying the autoregressive component to the response or the residuals, where the latter matched the tests for residuals (Cliff and Ord 1972, 1973). These “lattice” models of areal data typically express the dependence between observations using a graph of neighbours in the form of a contiguity matrix. A division has grown up, possibly unhelpfully, between scientific fields using conditional autoregressive (CAR) models (Besag 1974), and simultaneous autoregressive models (SAR) (Ord 1975; Hepple 1976). Although CAR and SAR models are closely related, these fields have found it difficult to share experience of applying similar models, often despite referring to key work summarising the models (Ripley 1981, 1988; Cressie 1993). Ripley gives the SAR variance as (1981, 89): \\[ {\\rm Var}_S = \\sigma^ 2(I-\\lambda W_S)^{-1} (I-\\lambda W_S^{\\rm T})^{-1} \\] where \\(\\lambda\\) is a spatial autocorrelation parameter and \\(W_S\\) is a nonsingular matrix that represents spatial dependence. The CAR variance is: \\[ {\\rm Var}_C = \\sigma^ 2(I-\\lambda W_C)^{-1} \\] where and \\(W_C\\) is a symmetric and strictly positive definite matrix More recent books expounding the theoretical bases for modelling with areal data simply point out the similarities in relevant chapters (Gaetan and Guyon 2010; Lieshout 2019); the interested reader is invited to consult these sources for background information and examples using the functions described below. Of course, handling a spatial correlation structure in a generalised least squares model or a (generalised) linear or nonlinear mixed effects model such as those provided in the nlme and many other packages does not have to use a graph of neighbours (Pinheiro and Bates 2000). These models are also spatial regression models, using functions of the distance between observations, and fitted variograms to model the spatial autocorrelation present; such models have been held to yield a clearer picture of the underlying processes (Wall 2004), building on geostatistics. For example, the glmmTMB package successfully uses this approach to spatial regression (Brooks et al. 2017). Here we will only consider spatial regression using spatial weights, chiefly as implemented in the spatialreg package recently split out of the spdep package which had grown unnecessarily large, covering too many aspects of spatial dependence. 14.1 Spatial regression with spatial weights Spatial autoregression models using spatial weights matrices were described in some detail using maximum likelihood estimation some time ago (Cliff and Ord 1973, 1981). A family of models were elaborated in spatial econometric terms extending earlier work, and in many cases using the simultaneous autoregressive framework and row standardization of spatial weights (Anselin 1988). The simultaneous and conditional autoregressive frameworks can be compared, and both can be supplemented using case weights to reflect the relative importance of different observations (Waller and Gotway 2004). Here we shall use the Boston housing data set, which has been restructured and furnished with census tract boundaries (R. Bivand 2017). The original data set used 506 census tracts and a hedonic model to try to estimate willingness to pay for clean air. The response was constructed from counts of ordinal answers to a 1970 census question about house value; the response is left and right censored in the census source. The key covariate was created from a calibrated meteorological model showing the annual nitrogen oxides (NOX) level for a smaller number of model output zones. The numbers of houses responding also varies by tract and model output zone. There are several other covariates, some measured at the tract level, some by town only, where towns broadly correspond to the air pollution model output zones. library(sf) library(spatialreg) # Loading required package: Matrix # # Attaching package: &#39;Matrix&#39; # The following objects are masked from &#39;package:tidyr&#39;: # # expand, pack, unpack # # Attaching package: &#39;spatialreg&#39; # The following objects are masked from &#39;package:spdep&#39;: # # as_dgRMatrix_listw, as_dsCMatrix_I, as_dsCMatrix_IrW, # as_dsTMatrix_listw, as.spam.listw, can.be.simmed, cheb_setup, # create_WX, do_ldet, eigen_pre_setup, eigen_setup, eigenw, # errorsarlm, get.ClusterOption, get.coresOption, get.mcOption, # get.VerboseOption, get.ZeroPolicyOption, GMargminImage, GMerrorsar, # griffith_sone, gstsls, Hausman.test, impacts, intImpacts, # Jacobian_W, jacobianSetup, l_max, lagmess, lagsarlm, lextrB, # lextrS, lextrW, lmSLX, LU_prepermutate_setup, LU_setup, # Matrix_J_setup, Matrix_setup, mcdet_setup, MCMCsamp, ME, mom_calc, # mom_calc_int2, moments_setup, powerWeights, sacsarlm, # SE_classic_setup, SE_interp_setup, SE_whichMin_setup, # set.ClusterOption, set.coresOption, set.mcOption, # set.VerboseOption, set.ZeroPolicyOption, similar.listw, spam_setup, # spam_update_setup, SpatialFiltering, spautolm, spBreg_err, # spBreg_lag, spBreg_sac, stsls, subgraph_eigenw, trW boston_506 &lt;- st_read(system.file(&quot;shapes/boston_tracts.shp&quot;, package=&quot;spData&quot;)[1]) # Reading layer `boston_tracts&#39; from data source # `/home/edzer/R/x86_64-pc-linux-gnu-library/4.0/spData/shapes/boston_tracts.shp&#39; # using driver `ESRI Shapefile&#39; # Simple feature collection with 506 features and 36 fields # Geometry type: POLYGON # Dimension: XY # Bounding box: xmin: -71.5 ymin: 42 xmax: -70.6 ymax: 42.7 # Geodetic CRS: NAD27 nb_q &lt;- spdep::poly2nb(boston_506) lw_q &lt;- spdep::nb2listw(nb_q, style=&quot;W&quot;) We can start by reading in the 506 tract data set from spData, and creating a contiguity neighbour object and from that again a row standardized spatial weights object. If we examine the median house values, we find that they have been assigned as missing values, and that 17 tracts are affected. table(boston_506$censored) # # left no right # 2 489 15 summary(boston_506$median) # Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s # 5600 16800 21000 21749 24700 50000 17 Next, we can subset to the remaining 489 tracts with non-censored house values, and the neighbour object to match. The neighbour object now has one observation with no neighbours. boston_489 &lt;- boston_506[!is.na(boston_506$median),] nb_q_489 &lt;- spdep::poly2nb(boston_489) lw_q_489 &lt;- spdep::nb2listw(nb_q_489, style=&quot;W&quot;, zero.policy=TRUE) The NOX_ID variable specifies the upper level aggregation, letting us aggregate the tracts to air pollution model output zones. We can create aggregate neighbour and row standardized spatial weights objects, and aggregate the NOX variable taking means, and the CHAS Charles River dummy variable for observations on the river. agg_96 &lt;- list(as.character(boston_506$NOX_ID)) boston_96 &lt;- aggregate(boston_506[, &quot;NOX_ID&quot;], by=agg_96, unique) nb_q_96 &lt;- spdep::poly2nb(boston_96) lw_q_96 &lt;- spdep::nb2listw(nb_q_96) boston_96$NOX &lt;- aggregate(boston_506$NOX, agg_96, mean)$x boston_96$CHAS &lt;- aggregate(as.integer(boston_506$CHAS)-1, agg_96, max)$x The response is aggregated using the weightedMedian() function in matrixStats, and midpoint values for the house value classes. Counts of houses by value class were punched to check the published census values, which can be replicated using weightedMedian() at the tract level. Here we find two output zones with calculated weighted medians over the upper census question limit of USD 50,000, and remove them subsequently as they also are affected by not knowing the appropriate value to insert for the top class by value. nms &lt;- names(boston_506) ccounts &lt;- 23:31 for (nm in nms[c(22, ccounts, 36)]) { boston_96[[nm]] &lt;- aggregate(boston_506[[nm]], agg_96, sum)$x } br2 &lt;- c(3.50, 6.25, 8.75, 12.50, 17.50, 22.50, 30.00, 42.50, 60.00)*1000 counts &lt;- as.data.frame(boston_96)[, nms[ccounts]] f &lt;- function(x) matrixStats::weightedMedian(x=br2, w=x, interpolate=TRUE) boston_96$median &lt;- apply(counts, 1, f) is.na(boston_96$median) &lt;- boston_96$median &gt; 50000 summary(boston_96$median) # Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s # 9009 20417 23523 25263 30073 49496 2 Before subsetting, we aggregate the remaining covariates by weighted mean using the tract population counts punched from the census (R. Bivand 2017). boston_94 &lt;- boston_96[!is.na(boston_96$median),] nb_q_94 &lt;- spdep::subset.nb(nb_q_96, !is.na(boston_96$median)) lw_q_94 &lt;- spdep::nb2listw(nb_q_94, style=&quot;W&quot;) We now have two data sets each at the lower, census tract level and the upper, air pollution model output zone level, one including the censored observations, the other excluding them. The original model related the log of median house values by tract to the square of NOX values, including other covariates usually related to house value by tract, such as aggregate room counts, aggregate age, ethnicity, social status, distance to downtown and to the nearest radial road, a crime rate, and town-level variables reflecting land use (zoning, industry), taxation and education (R. Bivand 2017). This structure will be used here to exercise issues raised in fitting spatial regression models, including the presence of multiple levels. form &lt;- formula(log(median) ~ CRIM + ZN + INDUS + CHAS + I((NOX*10)^2) + I(RM^2) + AGE + log(DIS) + log(RAD) + TAX + PTRATIO + I(BB/100) + log(I(LSTAT/100))) Before moving to presentations of issues raised in fitting spatial regression models, it is worth making a few further points. A recent review of spatial regression in a spatial econometrics setting is given by Kelejian and Piras (2017); note that their usage is to call the spatial coefficient of the lagged response \\(\\lambda\\) and that of the lagged residuals \\(\\rho\\), the reverse of other usage (Anselin 1988; LeSage and Pace 2009); here we use \\(\\rho_{\\mathrm{Lag}}\\) for the spatial coefficient in the spatial lag model, and \\(\\rho_{\\mathrm{Err}}\\) for the spatial error model. One interesting finding is that relatively dense spatial weights matrices may downweight model estimates, suggesting that sparser weights are preferable (Smith 2009). Another useful finding is that the presence of residual spatial autocorrelation need not bias the estimates of variance of regression coefficients, provided that the covariates themselves do not exhibit spatial autocorrelation (Smith and Lee 2012). In general, however, the footprints of the spatial processes of the response and covariates may not be aligned, and if covariates and the residual are autocorrelated, it is likely that the estimates of variance of regression coefficients will be biassed downwards if attempts are not made to model the spatial processes. In trying to model these spatial processes, we may choose to model the spatial autocorrelation in the residual with a spatial error model (SEM). \\[ {\\mathbf y} = {\\mathbf X}{\\mathbf \\beta} + {\\mathbf u}, \\qquad {\\mathbf u} = \\rho_{\\mathrm{Err}} {\\mathbf W} {\\mathbf u} + {\\mathbf \\varepsilon}, \\] where \\({\\mathbf y}\\) is an \\((N \\times 1)\\) vector of observations on a response variable taken at each of \\(N\\) locations, \\({\\mathbf X}\\) is an \\((N \\times k)\\) matrix of covariates, \\({\\mathbf \\beta}\\) is a \\((k \\times 1)\\) vector of parameters, \\({\\mathbf u}\\) is an \\((N \\times 1)\\) spatially autocorrelated disturbance vector, \\({\\mathbf \\varepsilon}\\) is an \\((N \\times 1)\\) vector of independent and identically distributed disturbances and \\(\\rho_{\\mathrm{Err}}\\) is a scalar spatial parameter. If the processes in the covariates and the response match, we should find little difference between the coefficients of a least squares and a SEM, but very often they diverge, suggesting that a Hausman test for this condition should be employed (Pace and LeSage 2008). This may be related to earlier discussions of a spatial equivalent to the unit root and cointegration where spatial processes match (Fingleton 1999). A model with a spatial process in the response only is termed a spatial lag model (SLM, often SAR - spatial autoregressive) (LeSage and Pace 2009). \\[ {\\mathbf y} = \\rho_{\\mathrm{Lag}} {\\mathbf W}{\\mathbf y} + {\\mathbf X}{\\mathbf \\beta} + {\\mathbf \\varepsilon}, \\] where \\(\\rho_{\\mathrm{Lag}}\\) is a scalar spatial parameter. Work reviewed by Mur and Angulo (2006) on the Durbin model; the Durbin model adds the spatially lagged covariates to the covariates included in the spatial lag model, giving a spatial Durbin model (SDM) with different processes in the response and covariates: \\[ {\\mathbf y} = \\rho_{\\mathrm{Lag}} {\\mathbf W}{\\mathbf y} + {\\mathbf X}{\\mathbf \\beta} + {\\mathbf W}{\\mathbf X}{\\mathbf \\gamma} + {\\mathbf \\varepsilon}, \\] where \\({\\mathbf \\gamma}\\) is a \\((k&#39; \\times 1)\\) vector of parameters. \\(k&#39;\\) defines the subset of the intercept and covariates, often \\(k&#39; = k-1\\) when using row standardised spatial weights and omitting the spatially lagged intercept. This permits the spatial processes to be viewed and tested for as a Common Factor (Burridge 1981; Bivand 1984). The inclusion of spatially lagged covariates lets us check whether the same spatial process is manifest in the response and the covariates (SEM), whether they are different processes, or whether no process is detected. The Common Factor is present when \\({\\mathbf \\gamma} = - \\rho_{\\mathrm{Lag}} {\\mathbf \\beta}\\): \\[ {\\mathbf y} = \\rho_{\\mathrm{Lag}} {\\mathbf W}{\\mathbf y} + {\\mathbf X}{\\mathbf \\beta} - \\rho_{\\mathrm{Lag}} {\\mathbf W}{\\mathbf X} {\\mathbf \\beta} + {\\mathbf \\varepsilon}, \\qquad ({\\mathbf I} - \\rho_{\\mathrm{Lag}} {\\mathbf W}){\\mathbf y} = ({\\mathbf I} - \\rho_{\\mathrm{Lag}} {\\mathbf W}) {\\mathbf X}{\\mathbf \\beta} + {\\mathbf \\varepsilon}, \\] where \\({\\mathbf I}\\) is the \\(N \\times N\\) identity matrix, and \\({\\mathbf I} - \\rho_{\\mathrm{Lag}} {\\mathbf W}\\) is the Common Factor: \\[ \\qquad {\\mathbf y} = {\\mathbf X}{\\mathbf \\beta} + ({\\mathbf I} - \\rho_{\\mathrm{Lag}} {\\mathbf W})^{-1} {\\mathbf \\varepsilon}, \\qquad {\\mathbf y} = {\\mathbf X}{\\mathbf \\beta} + {\\mathbf u}, \\qquad {\\mathbf u} = \\rho_{\\mathrm{Err}} {\\mathbf W} {\\mathbf u} + {\\mathbf \\varepsilon}, \\] If we extend this family with processes in the covariates and the residual, we get a spatial error Durbin model (SDEM). If it is chosen to admit a spatial process in the residuals in addition to a spatial process in the response, again two models are formed, a general nested model (GNM) nesting all the others, and a model without spatially lagged covariates (SAC, also known as SARAR - Spatial AutoRegressive-AutoRegressive model). If neither the residuals nor the residual are modelled with spatial processes, spatially lagged covariates may be added to a linear model, as a spatially lagged X model (SLX) (Elhorst 2010; Bivand 2012; LeSage 2014; Halleck Vega and Elhorst 2015). We can write the general nested model (GNM) as: \\[ {\\mathbf y} = \\rho_{\\mathrm{Lag}} {\\mathbf W}{\\mathbf y} + {\\mathbf X}{\\mathbf \\beta} + {\\mathbf W}{\\mathbf X}{\\mathbf \\gamma} + {\\mathbf u}, \\qquad {\\mathbf u} = \\rho_{\\mathrm{Err}} {\\mathbf W} {\\mathbf u} + {\\mathbf \\varepsilon}, \\] This may be constrained to the double spatial coefficient model SAC/SARAR by setting \\({\\mathbf \\gamma} = 0\\), to the spatial Durbin (SDM) by setting \\(\\rho_{\\mathrm{Err}} = 0\\), and to the error Durbin model (SDEM) by setting \\(\\rho_{\\mathrm{Lag}} = 0\\). Imposing more conditions gives the spatial lag model (SLM) with \\({\\mathbf \\gamma} = 0\\) and \\(\\rho_{\\mathrm{Err}} = 0\\), the spatial error model (SEM) with \\({\\mathbf \\gamma} = 0\\) and \\(\\rho_{\\mathrm{Lag}} = 0\\), and the spatially lagged X model (SLX) with \\(\\rho_{\\mathrm{Lag}} = 0\\) and \\(\\rho_{\\mathrm{Err}} = 0\\). Although making predictions for new locations for which covariates are observed was raised as an issue some time ago, it has many years to make progress in reviewing the possibilities (Bivand 2002; Goulard, Laurent, and Thomas-Agnan 2017). The prediction methods for SLM, SDM, SEM, SDEM, SAC and GNM models fitted with maximum likelihood were contributed as a Google Summer of Coding project by Martin Gubri. This work, and work on similar models with missing data (Suesse 2018) is also relevant for exploring censored median house values in the Boston data set. Work on prediction also exposed the importance of the reduced form of these models, in which the spatial process in the response interacts with the regression coefficients in the SLM, SDM, SAC and GNM models. The consequence of these interactions is that a unit change in a covariate will only impact the response as the value of the regression coefficient if the spatial coefficient of the lagged response is zero. Where it is non-zero, global spillovers, impacts, come into play, and these impacts should be reported rather than the regression coefficients (LeSage and Pace 2009; Elhorst 2010; Bivand 2012; LeSage 2014; Halleck Vega and Elhorst 2015). Local impacts may be reported for SDEM and SLX models, using linear combination to calculate standard errors for the total impacts of each covariate (sums of coefficients on the covariates and their spatial lags). This can be seen from the GNM data generation process: \\[ ({\\mathbf I} - \\rho_{\\mathrm{Err}} {\\mathbf W})({\\mathbf I} - \\rho_{\\mathrm{Lag}} {\\mathbf W}){\\mathbf y} = ({\\mathbf I} - \\rho_{\\mathrm{Err}} {\\mathbf W})({\\mathbf X}{\\mathbf \\beta} + {\\mathbf W}{\\mathbf X}{\\mathbf \\gamma}) + {\\mathbf \\varepsilon}, \\] re-writing: \\[ {\\mathbf y} = ({\\mathbf I} - \\rho_{\\mathrm{Lag}} {\\mathbf W})^{-1}({\\mathbf X}{\\mathbf \\beta} + {\\mathbf W}{\\mathbf X}{\\mathbf \\gamma}) + ({\\mathbf I} - \\rho_{\\mathrm{Lag}} {\\mathbf W})^{-1}({\\mathbf I} - \\rho_{\\mathrm{Err}} {\\mathbf W})^{-1}{\\mathbf \\varepsilon}. \\] There is interaction between the \\(\\rho_{\\mathrm{Lag}}\\) and \\({\\mathbf \\beta}\\) (and \\({\\mathbf \\gamma}\\) if present) coefficients. This can be seen from the partial derivatives: \\(\\partial y_i / \\partial x_{jr} = (({\\mathbf I} - \\rho_{\\mathrm{Lag}} {\\mathbf W})^{-1} ({\\mathbf I} \\beta_r + {\\mathbf W} \\gamma_r))_{ij}\\). This dense matrix \\(S_r({\\mathbf W}) = (({\\mathbf I} - \\rho_{\\mathrm{Lag}} {\\mathbf W})^{-1} ({\\mathbf I} \\beta_r + {\\mathbf W} \\gamma_r))\\) expresses the direct impacts (effects) on its principal diagonal, and indirect impacts in off-diagonal elements. Current work in the spatialreg package is focused on refining the handling of spatially lagged covariates using a consistent Durbin= argument taking either a logical value or a formula giving the subset of covariates to add in spatially lagged form. There is a speculation that some covariates, for example some dummy variables, should not be added in spatially lagged form. This then extends to handling these included spatially lagged covariates appropriately in calculating impacts. This work applies to cross-sectional models fitted using MCMC or maximum likelihood, and will offer facilities to spatial panel models. It is worth mentioning the almost unexplored issues of functional form assumptions, for which flexible structures are useful, including spatial quantile regression presented in the McSpatial package (McMillen 2013). There are further issues with discrete response variables, covered by some functions in McSpatial, and in the spatialprobit and ProbitSpatial packages (Wilhelm and Matos 2013; Martinetti and Geniaux 2017); the MCMC implementations of the former are based on LeSage and Pace (2009). Finally, Wagner and Zeileis (2019) show how an SLM model may be used in the setting of recursive partitioning, with an implementation using spatialreg::lagsarlm() in the lagsarlmtree package. 14.2 Estimators The review of cross-sectional maximum likelihood and generalized method of moments (GMM) estimators in spatialreg and sphet for spatial econometrics style spatial regression models by Bivand and Piras (2015) is still largely valid. In the review, estimators in these R packages were compared with alternative implementations available in other programming languages elsewhere. The review did not cover Bayesian spatial econometrics style spatial regression. More has changed with respect to spatial panel estimators described in Millo and Piras (2012), but will not be covered here. 14.2.1 Maximum likelihood For models with single spatial coefficients (SEM and SDEM using errorsarlm(), SLM and SDM using lagsarlm()), the methods initially described by Ord (1975) are used. The following table shows the functions that can be used to estimate the models described above using maximum likelihood. model model name maximum likelihood estimation function SEM spatial error errorsarlm(..., Durbin=FALSE, ...) SEM spatial error spautolm(..., family=\"SAR\", ...) SDEM spatial Durbin error errorsarlm(..., Durbin=TRUE, ...) SLM spatial lag lagsarlm(..., Durbin=FALSE, ...) SDM spatial Durbin lagsarlm(..., Durbin=TRUE, ...) SAC spatial autoregressive combined sacsarlm(..., Durbin=FALSE, ...) GNM general nested sacsarlm(..., Durbin=TRUE, ...) The estimating functions errorsarlm() and lagsarlm() take similar arguments, where the first two, formula= and data= are shared by most model estimating functions. The third argument is a listw spatial weights object, while na.action= behaves as in other model estimating functions if the spatial weights can reasonably be subsetted to avoid observations with missing values. The weights= argument may be used to provide weights indicating the known degree of per-observation variability in the variance term - this is not available for lagsarlm(). args(errorsarlm) # function (formula, data = list(), listw, na.action, weights = NULL, # Durbin, etype, method = &quot;eigen&quot;, quiet = NULL, zero.policy = NULL, # interval = NULL, tol.solve = .Machine$double.eps, trs = NULL, # control = list()) # NULL args(lagsarlm) # function (formula, data = list(), listw, na.action, Durbin, type, # method = &quot;eigen&quot;, quiet = NULL, zero.policy = NULL, interval = NULL, # tol.solve = .Machine$double.eps, trs = NULL, control = list()) # NULL The Durbin= argument replaces the earlier type= and etype= arguments, and if not given is taken as FALSE. If given, it may be FALSE, TRUE in which case all spatially lagged covariates are included, or a one-sided formula specifying which spatially lagged covariates should be included. The method= argument gives the method for calculating the log determinant term in the log likelihood function, and defaults to \"eigen\", suitable for moderately sized data sets. The interval= argument gives the bounds of the domain for the line search using stats::optimize() used for finding the spatial coefficient. The tol.solve() argument, passed through to base::solve(), was needed to handle data sets with differing numerical scales among the coefficients which hindered inversion of the variance-covariance matrix; the default value in base::solve() used to be much larger. The control= argument takes a list of control values to permit more careful adjustment of the running of the estimation function. The spautolm() function also fits spatial regressions with the spatial process in the residuals, and takes a possibly poorly named family= argument, taking the values of \"SAR\" for the simultaneous autoregressive model (like errorsarlm()), \"CAR\" for the conditional autoregressive model, and \"SMA\" for the spatial moving average model. args(spautolm) # function (formula, data = list(), listw, weights, na.action, # family = &quot;SAR&quot;, method = &quot;eigen&quot;, verbose = NULL, trs = NULL, # interval = NULL, zero.policy = NULL, tol.solve = .Machine$double.eps, # llprof = NULL, control = list()) # NULL The sacsarlm() function may take second spatial weights and interval arguments if the spatial weights used to model the two spatial processes in the SAC and GNM specifications differ. By default, the same spatial weights are used. By default, stats::nlminb() is used for numerical optimization, using a heuristic to choose starting values. args(sacsarlm) # function (formula, data = list(), listw, listw2 = NULL, na.action, # Durbin, type, method = &quot;eigen&quot;, quiet = NULL, zero.policy = NULL, # tol.solve = .Machine$double.eps, llprof = NULL, interval1 = NULL, # interval2 = NULL, trs1 = NULL, trs2 = NULL, control = list()) # NULL Where larger data sets are used, a numerical Hessian approach is used to calculate the variance-covariance matrix of coefficients, rather than an analytical asymptotic approach. Apart from spautolm() which returns an \"spautolm\" object, the model fitting functions return \"Sarlm\" objects. Standard methods for fitted models are provided, such as summary(): args(getS3method(&quot;summary&quot;, &quot;Sarlm&quot;)) # function (object, correlation = FALSE, Nagelkerke = FALSE, Hausman = FALSE, # adj.se = FALSE, ...) # NULL The Nagelkerke= argument permits the return of a value approximately corresponding to a coefficient of determination, although the summary method anyway provides the value of stats::AIC() because a stats::logLik() method is provided for \"sarlm\" and \"spautolm\" objects. If the \"sarlm\" object is a SEM or SDEM, the Hausman test may be performed by setting Hausman=TRUE to see whether the regression coefficients are sufficiently like least squares coefficients, indicating absence of mis-specification from that source. As an example, we may fit SEM and SDEM to the 94 and 489 observation Boston data sets, and present the Hausman test results: eigs_489 &lt;- eigenw(lw_q_489) SDEM_489 &lt;- errorsarlm(form, data=boston_489, listw=lw_q_489, Durbin=TRUE, zero.policy=TRUE, control=list(pre_eig=eigs_489)) SEM_489 &lt;- errorsarlm(form, data=boston_489, listw=lw_q_489, zero.policy=TRUE, control=list(pre_eig=eigs_489)) cbind(data.frame(model=c(&quot;SEM&quot;, &quot;SDEM&quot;)), rbind(broom::tidy(Hausman.test(SEM_489)), broom::tidy(Hausman.test(SDEM_489))))[,1:4] # model statistic p.value parameter # 1 SEM 52.0 2.83e-06 14 # 2 SDEM 48.7 6.48e-03 27 Here we are using the control= list argument to pass through pre-computed eigenvalues for the default \"eigen\" method. Both test results for the 489 tract data set suggest that the regression coefficients do differ, perhaps that the footprints of the spatial processes do not match. Likelihood ratio tests of the spatial models against their least squares equivalents show that spatial process(es) are present, but we find that neither SEM not SDM are adequate representations. cbind(data.frame(model=c(&quot;SEM&quot;, &quot;SDEM&quot;)), rbind(broom::tidy(LR1.Sarlm(SEM_489)), broom::tidy(LR1.Sarlm(SDEM_489))))[,c(1, 4:6)] # model statistic p.value parameter # 1 SEM 198 0 1 # 2 SDEM 159 0 1 For the 94 air pollution model output zones, the Hausman tests find little difference between coefficients, but this is related to the fact that the SEM and SDEM models add little to least squares or SLX using likelihood ratio tests. eigs_94 &lt;- eigenw(lw_q_94) SDEM_94 &lt;- errorsarlm(form, data=boston_94, listw=lw_q_94, Durbin=TRUE, control=list(pre_eig=eigs_94)) SEM_94 &lt;- errorsarlm(form, data=boston_94, listw=lw_q_94, control=list(pre_eig=eigs_94)) cbind(data.frame(model=c(&quot;SEM&quot;, &quot;SDEM&quot;)), rbind(broom::tidy(Hausman.test(SEM_94)), broom::tidy(Hausman.test(SDEM_94))))[, 1:4] # model statistic p.value parameter # 1 SEM 15.66 0.335 14 # 2 SDEM 9.21 0.999 27 cbind(data.frame(model=c(&quot;SEM&quot;, &quot;SDEM&quot;)), rbind(broom::tidy(LR1.Sarlm(SEM_94)), broom::tidy(LR1.Sarlm(SDEM_94))))[,c(1, 4:6)] # model statistic p.value parameter # 1 SEM 2.593 0.107 1 # 2 SDEM 0.216 0.642 1 We can use spatialreg::LR.sarlm() to apply a likelihood ratio test between nested models, but here choose lmtest::lrtest(), which gives the same results, preferring models including spatially lagged covariates. broom::tidy(lmtest::lrtest(SEM_489, SDEM_489)) # Warning in tidy.anova(lmtest::lrtest(SEM_489, SDEM_489)): The following column # names in ANOVA output were not recognized or transformed: X.Df, LogLik # # A tibble: 2 x 5 # X.Df LogLik df statistic p.value # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 16 273. NA NA NA # 2 29 311. 13 74.4 1.23e-10 broom::tidy(lmtest::lrtest(SEM_94, SDEM_94)) # Warning in tidy.anova(lmtest::lrtest(SEM_94, SDEM_94)): The following column # names in ANOVA output were not recognized or transformed: X.Df, LogLik # # A tibble: 2 x 5 # X.Df LogLik df statistic p.value # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 16 59.7 NA NA NA # 2 29 81.3 13 43.2 0.0000421 The SLX model is fitted using least squares, and also returns a log likelihood value, letting us test whether we need a spatial process in the residuals. args(lmSLX) # function (formula, data = list(), listw, na.action, weights = NULL, # Durbin = TRUE, zero.policy = NULL) # NULL In the tract data set we obviously do: SLX_489 &lt;- lmSLX(form, data=boston_489, listw=lw_q_489, zero.policy=TRUE) broom::tidy(lmtest::lrtest(SLX_489, SDEM_489)) # Warning in modelUpdate(objects[[i - 1]], objects[[i]]): original model was of # class &quot;SlX&quot;, updated model is of class &quot;Sarlm&quot; # Warning in tidy.anova(lmtest::lrtest(SLX_489, SDEM_489)): The following column # names in ANOVA output were not recognized or transformed: X.Df, LogLik # # A tibble: 2 x 5 # X.Df LogLik df statistic p.value # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 28 231. NA NA NA # 2 29 311. 1 159. 1.55e-36 but in the output zone case we do not. SLX_94 &lt;- lmSLX(form, data=boston_94, listw=lw_q_94) broom::tidy(lmtest::lrtest(SLX_94, SDEM_94)) # Warning in modelUpdate(objects[[i - 1]], objects[[i]]): original model was of # class &quot;SlX&quot;, updated model is of class &quot;Sarlm&quot; # Warning in tidy.anova(lmtest::lrtest(SLX_94, SDEM_94)): The following column # names in ANOVA output were not recognized or transformed: X.Df, LogLik # # A tibble: 2 x 5 # X.Df LogLik df statistic p.value # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 28 81.2 NA NA NA # 2 29 81.3 1 0.216 0.642 This outcome is sustained also when we use the counts of house units by tract and output zones as weights: SLX_94w &lt;- lmSLX(form, data=boston_94, listw=lw_q_94, weights=units) SDEM_94w &lt;- errorsarlm(form, data=boston_94, listw=lw_q_94, Durbin=TRUE, weights=units, control=list(pre_eig=eigs_94)) broom::tidy(lmtest::lrtest(SLX_94w, SDEM_94w)) # Warning in modelUpdate(objects[[i - 1]], objects[[i]]): original model was of # class &quot;SlX&quot;, updated model is of class &quot;Sarlm&quot; # Warning in tidy.anova(lmtest::lrtest(SLX_94w, SDEM_94w)): The following column # names in ANOVA output were not recognized or transformed: X.Df, LogLik # # A tibble: 2 x 5 # X.Df LogLik df statistic p.value # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 28 97.5 NA NA NA # 2 29 98.0 1 0.917 0.338 14.2.2 Generalized method of moments The estimation methods used for fitting SLM and the spatially lagged response part of SARAR models are based on instrumental variables, using the spatially lagged covariates (usually including lags of lags too) as instruments for the spatially lagged response (Piras 2010). This, and the use of spatially lagged covariates in the moment conditions for models including a spatial process in the residuals, means that including the spatially lagged covariates in the initial model is challenging, and functions in the sphet package do not provide a Durbin= argument. This makes it harder to accommodate data with multiple spatial process footprints. However, as Kelejian and Piras show in their recent review (2017), these approaches have other benefits, such as being able to instrument variables suspected of suffering from endogeneity or measurement error. Let us first compare the ML and GMM estimates of the SEM regression coefficients for rescaled squared NOX values. We use the sphet::spreg() wrapper function, and fit a SEM model. Extracting the matching rows from the summary objects for these models, we can see that the values are not dissimilar, despite the difference in estimation methods. SEM1_94 &lt;- sphet::spreg(form, data=boston_94, listw=lw_q_94, model=&quot;error&quot;) res &lt;- rbind(summary(SEM_94)$Coef[&quot;I((NOX * 10)^2)&quot;,], summary(SEM1_94)$CoefTable[&quot;I((NOX * 10)^2)&quot;,]) rownames(res) &lt;- c(&quot;ML&quot;, &quot;GMM&quot;) res # Estimate Std. Error z value Pr(&gt;|z|) # ML -0.00956 0.00261 -3.66 0.000247 # GMM -0.00885 0.00267 -3.31 0.000930 Using sphet::spreg(), we can instrument the rescaled squared NOX variable, dropping it first from the formula, next creating the rescaled squared NOX variable as a column in the \"sf\" object, extracting a matrix of coordinates from the centroids of the output zones, and creating a one-sided instrument formula from a second order polynomial in the coordinates (here improperly, as they are not projected) and mean distances to downtown and radial roads. The endog= argument takes a one-sided formula for the variables to be modelled in the first stage of the model. Had we re-run the original air pollution model many times under slightly varying scenarios, we could have used an ensemble of NOX loadings to yield its distribution by output zone. Because this is not possible, we assume that the measurement error can be captured by using selected instruments. Unfortunately, the NOX regression coefficient estimate from the second stage has fallen substantially in absolute size, although the sign is unchanged. formiv &lt;- update(form, . ~ . - I((NOX*10)^2)) boston_94$NOX2 &lt;- (boston_94$NOX*10)^2 suppressWarnings(ccoords &lt;- st_coordinates(st_centroid(st_geometry(boston_94)))) iform &lt;- formula(~poly(ccoords, degree=2) + DIS + RAD) SEM1_94iv &lt;- sphet::spreg(formiv, data=boston_94, listw=lw_q_94, endog = ~NOX2, instruments=iform, model=&quot;error&quot;) summary(SEM1_94iv)$CoefTable[&quot;NOX2&quot;,] # Estimate Std. Error t-value Pr(&gt;|t|) # -0.00195 0.00454 -0.43063 0.66674 Handling measurement error in this or similar ways is one of the benefits of GMM estimation methods, although here the choice of instruments was somewhat arbitrary. 14.2.3 Markov chain Monte Carlo (draft - a comparative piece is being written for submission in about two months) The Spatial Econometrics Library is part of the extensive Matlab code repository at https://www.spatial-econometrics.com/ and documented in LeSage and Pace (2009). The Google Summer of Coding project in 2011 by Abhirup Mallik mentored by Virgilio Gómez-Rubio yielded translations of some of the model fitting functions for SEM, SDEM, SLM, SDM, SAC and GNM from the Matlab code. These have now been added to spatialreg as spBreg_err(), spBreg_lag() and spBreg_sac() with Durbin= arguments to handle the inclusion of spatially lagged covariates. As yet, heteroskedastic disturbances are not accommodated. The functions return \"mcmc\" objects as specified in the coda package, permitting the use of tools from coda for handling model output. The default burnin is 500 draws, followed by default 2000 draws returned, but these values and many others may be set through the control= list argument. Fitting the SDEM model for the output zones takes about an order of magnitude longer than using ML, but there is more work to do subsequently, and this difference scales more in the number of samples than covariates or observations. system.time(SDEM_94B &lt;- spBreg_err(form, data=boston_94, listw=lw_q_94, Durbin=TRUE)) # user system elapsed # 1.595 0.068 1.664 system.time(SDEM_489B &lt;- spBreg_err(form, data=boston_489, listw=lw_q_489, Durbin=TRUE, zero.policy=TRUE)) # user system elapsed # 2.444 0.036 2.480 Most time in the ML case using eigenvalues is taken by log determinant setup and optimization, and by dense matrix asymptotic standard errors if chosen (always chosen with default \"eigen\" log determinant method): t(errorsarlm(form, data=boston_94, listw=lw_q_94, Durbin=TRUE)$timings[,2]) # set_up eigen_set_up eigen_opt coefs eigen_hcov eigen_se # [1,] 0.004 0.006 0.008 0.004 0.002 0.002 t(errorsarlm(form, data=boston_489, listw=lw_q_489, Durbin=TRUE, zero.policy=TRUE)$timings[,2]) # set_up eigen_set_up eigen_opt coefs eigen_hcov eigen_se # [1,] 0.009 0.056 0.022 0.005 0.086 0.179 while in the MCMC case, the default use of Spatial Econometric Toolbox gridded log determinants and obviously sampling takes most time: t(attr(SDEM_94B, &quot;timings&quot;)[ , 3]) # set_up SE_classic_set_up complete_setup sampling finalise # [1,] 0.007 0.259 0.03 1.31 0.059 t(attr(SDEM_489B, &quot;timings&quot;)[ , 3]) # set_up SE_classic_set_up complete_setup sampling finalise # [1,] 0.035 0.445 0.071 1.88 0.052 However, as we will see shortly, inference from model impacts may need sampling (where the spatially lagged response is included in the model), and in the case of MCMC models, the samples have already been drawn. 14.3 Implementation details We will briefly touch on selected implementation details that applied users of spatial regression models would be wise to review. The handling of the log determinant term applies to all such users, while impacts are restricted to those employing spatial econometrics style models either including the spatially lagged response or including spatially lagged covariates. 14.3.1 Handling the log determinant term It has been known for over twenty years that the sparse matrix representation of spatial weights overcomes the difficulties of fitting models with larger numbers of observations using maximum likelihood and MCMC where the log determinant term comes into play (R. K. Pace and Barry 1997a, 1997c, 1997d, 1997b). During the development of these approaches in model fitting functions in spatialreg, use was first made of C code also used in the S-PLUS SpatialStats module (Kaluzny et al. 1998), then SparseM which used a compressed sparse row form very similar to \"nb\" and \"listw\" objects. This was followed by the use of spam and Matrix methods, both of which mainly use compressed sparse column representations. Details are provided in Bivand, Hauke and Kossowski (2013). The domain of the spatial coefficient(s) is given by the interval= argument to model fitting functions, and returned in the fitted object: SEM_94$interval # [1] -1.53 1.00 This case is trivial, because the upper bound is unity by definition, because of the use of row standardization. The interval is the inverse of the range of the eigenvalues of the weights matrix: 1/range(eigs_94) # [1] -1.53 1.00 Finding the interval within which to search for the spatial coefficient is trivial for smaller data sets, but more complex for larger ones. It is possible to use heuristics implemented in lextrW() (Griffith, Bivand, and Chun 2015): 1/c(lextrW(lw_q_94)) # lambda_n lambda_1 # -1.53 1.00 or RSpectra::eigs() after coercion to a Matrix package compressed sparse column representation: W &lt;- as(lw_q_94, &quot;CsparseMatrix&quot;) 1/Re(c(RSpectra::eigs(W, k=1, which=&quot;SR&quot;)$values, RSpectra::eigs(W, k=1, which=&quot;LR&quot;)$values)) # [1] -1.53 1.00 Why are we extracting the real part of the values returned by eigs()? Since nb_q_94 is symmetric, the row-standardized object lw_q_94 is symmetric by similarity, a result known since Ord (1975); consequently, we can take the real part without concern. Had the underlying neighbour relationships not been symmetric, we should be more careful. The baseline log determinant term as given by Ord (1975) for a coefficient value proposed in sampling or during numerical optimization; this extract matches the \"eigen\" method (with or without control=list(pre_eig=...)\"): coef &lt;- 0.5 sum(log(1 - coef * eigs_94)) # [1] -2.87 Using sparse matrix functions from Matrix, the LU decomposition can be used for asymmetric matrices; this extract matches the \"LU\" method: I &lt;- Diagonal(nrow(boston_94)) LU &lt;- lu(I - coef * W) dU &lt;- abs(diag(slot(LU, &quot;U&quot;))) sum(log(dU)) # [1] -2.87 and Cholesky decomposition for symmetric matrices, with similar.listw() used to handle asymmetric weights that are similar to symmetric. The default value od super allows the underlying Matrix code to choose between supernodal or simplicial decomposition; this extract matches the \"Matrix_J\" method: W &lt;- as(similar.listw(lw_q_94), &quot;CsparseMatrix&quot;) super &lt;- as.logical(NA) cch &lt;- Cholesky((I - coef * W), super=super) c(2 * determinant(cch, logarithm = TRUE)$modulus) # [1] -2.87 The \"Matrix\" and \"spam_update\" methods are to be preferred as they pre-compute the fill-reducing permutation of the decomposition since the weights do not change for different values of the coefficient. Maximum likelihood model fitting functions in spatialreg and splm use jacobianSetup() to populate env= environment with intermediate objects needed to find log determinants during optimization. Passing environments to objective functions is efficient because they are passed by reference rather than value. The con= argument passes through the populated control list, containing default values unless the key-value pairs were given in the function call (pre_eig= is extracted separately). The which= argument is 1 by default, but will also take 2 in SAC and GNM models. HSAR uses mcdet_setup() to set up Monte Carlo approximation terms. args(jacobianSetup) # function (method, env, con, pre_eig = NULL, trs = NULL, interval = NULL, # which = 1) # NULL For each value of coef, the do_ldet() function returns the log determinant, using the values stored in environment env=: args(do_ldet) # function (coef, env, which = 1) # NULL As yet the Bayesian models are limited to control argument ldet_method=\"SE_classic\" at present, using \"LU\" to generate a coarse grid of control argument nrho=200L log determinant values in the interval, spline interpolated to a finer grid of length control argument interpn=2000L, from which griddy Gibbs samples are drawn. It is hoped to add facilities to choose alternative methods in the future. This would offer possibilities to move beyond griddy Gibbs, but using gridded log determinant values seems reasonable at present. 14.3.2 Impacts Global impacts have been seen as crucial for reporting results from fitting models including the spatially lagged response (SLM, SDM, SAC. GNM) for over ten years (LeSage and Pace 2009). Extension to other models including spatially lagged covariates (SLX, SDEM) has followed (Elhorst 2010; Bivand 2012; Halleck Vega and Elhorst 2015). In order to infer from the impacts, linear combination may be used for SLX and SDEM models. For SLM, SDM, SAC and GNM models fitted with maximum likelihood or GMM, the variance-covariance matrix of the coefficients is available, and can be used to make random draws from a multivariate Normal distribution with mean set to coefficient values and variance to the estimated variance-covariance matrix. For these models fitted using Bayesian methods, draws are already available. In the SDEM case, the draws on the regression coefficients of the unlagged covariates represent direct impacts, and draws on the coefficients of the spatially lagged covariates represent indirect impacts, and their by-draw sums the total impacts. Impacts are calculated using model object class specific impacts() methods, here taking the method for \"sarlm\" objects as an example. In the sphet package, the impacts method for \"gstsls\" uses the spatialreg impacts() framework, as does the splm package for \"splm\" fitted model objects. impacts() methods require either a tr= - a vector of traces of the power series of the weights object typically computed with trW() or a listw= argument. If listw= is given, dense matrix methods are used. The evalues= argument is experimental, does not yet work for all model types, and takes the eigenvalues of the weights matrix. The R= argument gives the number of samples to be taken from the fitted model. The Q= permits the decomposition of impacts to components of the power series of the weights matrix (LeSage and Pace 2009). args(getS3method(&quot;impacts&quot;, &quot;Sarlm&quot;)) # function (obj, ..., tr = NULL, R = NULL, listw = NULL, evalues = NULL, # useHESS = NULL, tol = 1e-06, empirical = FALSE, Q = NULL) # NULL The summary method for the output of impacts() methods where inference from samples was requested by default uses the summary() method for \"mcmc\" objects defined in the coda package. It can instead report just matrices of standard errors, z-values and p-values by setting zstats= and short= to TRUE. args(getS3method(&quot;summary&quot;, &quot;LagImpact&quot;)) # function (object, ..., zstats = FALSE, short = FALSE, reportQ = NULL) # NULL Since sampling is not required for inference for SLX and SDEM models, linear combination is used for models fitted using maximum likelihood; results are shown here for the air pollution variable only. The literature has not yet resolved the question of how to report model output, as each covariate is now represented by three impacts. Where spatially lagged covariates are included, two coefficients are replaced by three impacts. sum_imp_94_SDEM &lt;- summary(impacts(SDEM_94)) rbind(Impacts=sum_imp_94_SDEM$mat[5,], SE=sum_imp_94_SDEM$semat[5,]) # Direct Indirect Total # Impacts -0.01276 -0.01845 -0.0312 # SE 0.00235 0.00472 0.0053 The impacts from the same model fitted by MCMC are very similar: sum_imp_94_SDEM_B &lt;- summary(impacts(SDEM_94B)) rbind(Impacts=sum_imp_94_SDEM_B$mat[5,], SE=sum_imp_94_SDEM_B$semat[5,]) # Direct Indirect Total # Impacts -0.01276 -0.01768 -0.03044 # SE 0.00266 0.00567 0.00692 as also are those from the SLX model. In the SLX and SDEM models, the direct impacts are the consequences for the response of changes in air pollution in the same observational entity, and the indirect (local) impacts are the consequences for the response of changes in air pollution in neighbouring observational entities. sum_imp_94_SLX &lt;- summary(impacts(SLX_94)) rbind(Impacts=sum_imp_94_SLX$mat[5,], SE=sum_imp_94_SLX$semat[5,]) # Direct Indirect Total # Impacts -0.0128 -0.01874 -0.03151 # SE 0.0028 0.00556 0.00611 In contrast to local indirect impacts in SLX and SDEM models, global indirect impacts are found in models including the spatially lagged response. For purposes of exposition, let us fit an SLM: SLM_489 &lt;- lagsarlm(form, data=boston_489, listw=lw_q_489, zero.policy=TRUE) Traces of the first m= matrices of the power series in the spatial weights are pre-computed to speed up inference from samples from the fitted model, and from existing MCMC samples (LeSage and Pace 2009). The traces can also be used in other contexts too, so their pre-computation may be worthwhile anyway. The type= argument is \"mult\" by default, but may also be set to \"MC\" for Monte Carlo simulation or \"moments\" using a space-saving looping algorithm. args(trW) # function (W = NULL, m = 30, p = 16, type = &quot;mult&quot;, listw = NULL, # momentsSymmetry = TRUE) # NULL W &lt;- as(lw_q_489, &quot;CsparseMatrix&quot;) tr_489 &lt;- trW(W) str(tr_489) # num [1:30] 0 90.8 29.4 42.1 26.5 ... # - attr(*, &quot;timings&quot;)= Named num [1:2] 0.151 0.167 # ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;user.self&quot; &quot;elapsed&quot; # - attr(*, &quot;type&quot;)= chr &quot;mult&quot; # - attr(*, &quot;n&quot;)= int 489 In this case, the spatial process in the response is not strong, so the global indirect impacts (here for the air pollution variable) are weak. SLM_489_imp &lt;- impacts(SLM_489, tr=tr_489, R=2000) SLM_489_imp_sum &lt;- summary(SLM_489_imp, short=TRUE, zstats=TRUE) res &lt;- rbind(Impacts=sapply(SLM_489_imp$res, &quot;[&quot;, 5), SE=SLM_489_imp_sum$semat[5,]) colnames(res) &lt;- c(&quot;Direct&quot;, &quot;Indirect&quot;, &quot;Total&quot;) res # Direct Indirect Total # Impacts -0.00593 -1.01e-05 -0.00594 # SE 0.00106 1.00e-04 0.00107 Of more interest is trying to reconstruct the direct and total impacts using dense matrix methods; the direct global impacts are the mean of the diagonal of the dense impacts matrix, and the total global impacts are the sum of all matrix elements divided by the number of observations. The direct impacts agree, but the total impacts differ slightly. coef_SLM_489 &lt;- coef(SLM_489) IrW &lt;- Diagonal(489) - coef_SLM_489[1] * W S_W &lt;- solve(IrW) S_NOX_W &lt;- S_W %*% (diag(489) * coef_SLM_489[7]) c(Direct=mean(diag(S_NOX_W)), Total=sum(S_NOX_W)/489) # Direct Total # -0.00593 -0.00594 This bare-bones approach corresponds to using the listw= argument, and as expected gives the same output. sapply(impacts(SLM_489, listw=lw_q_489), &quot;[&quot;, 5) # direct indirect total # -5.93e-03 -1.01e-05 -5.94e-03 The experimental evalues= approach which is known to be numerically exact by definition gives the same results as the matrix power series trace approach, so the slight difference may be attributed to the consequences of inverting the spatial process matrix. sapply(impacts(SLM_489, evalues=eigs_489), &quot;[&quot;, 5) # direct indirect total # -5.93e-03 -1.01e-05 -5.94e-03 Impacts are crucial to the interpretation of Durbin models including spatially lagged covariates and models including the spatially lagged response. Tools to calculate impacts and their inferential bases are now available, and should be employed, but as yet some implementation details are under development and ways of presenting results in tabular form have not reached maturity. 14.3.3 Predictions We will use the predict() method for \"sarlm\" objects to double-check impacts, here for the pupil-teacher ratio (PTRATIO). The method was re-written by Martin Gubri based on Goulard, Laurent and Thomas-Agnan (2017). The pred.type= argument specifies the prediction strategy among those presented in the article. args(getS3method(&quot;predict&quot;, &quot;sarlm&quot;)) # function (object, newdata = NULL, listw = NULL, pred.type = &quot;TS&quot;, # all.data = FALSE, zero.policy = NULL, legacy = TRUE, legacy.mixed = FALSE, # power = NULL, order = 250, tol = .Machine$double.eps^(3/5), # spChk = NULL, ...) # NULL First we’ll increment PTRATIO by one to show that, using least squares, the mean difference between predictions from the incremented new data and fitted values is equal to the regression coefficient. nd_489 &lt;- boston_489 nd_489$PTRATIO &lt;- nd_489$PTRATIO + 1 OLS_489 &lt;- lm(form, data=boston_489) fitted &lt;- predict(OLS_489) nd_fitted &lt;- predict(OLS_489, newdata=nd_489) all.equal(unname(coef(OLS_489)[12]), mean(nd_fitted - fitted)) # [1] TRUE In models including the spatially lagged response, and when the spatial coefficient in different from zero, this is not the case in general, and is why we need impacts() methods. The difference here is not great, but neither is it zero, and needs to be handled. fitted &lt;- predict(SLM_489) # This method assumes the response is known - see manual page nd_fitted &lt;- predict(SLM_489, newdata=nd_489, listw=lw_q_489, pred.type=&quot;TS&quot;, zero.policy=TRUE) all.equal(unname(coef_SLM_489[13]), mean(nd_fitted - fitted)) # [1] &quot;Mean relative difference: 0.00178&quot; In the Boston tracts data set, 17 observations of median house values, the response, are censored. Using these as an example and comparing some pred.type= variants for the SDEM model and predicting out-of-sample, we can see that there are differences, suggesting that this is a fruitful area for study. There have been a number of alternative proposals for handling missing variables (Gómez-Rubio, Bivand, and Rue 2015; Suesse 2018). Another reason for increasing attention on prediction is that it is fundamental for machine learning approaches, in which prediction for validation and test data sets drives model specification choice. The choice of training and other data sets with dependent spatial data remains an open question, and is certainly not as simple as with independent data. Here, we’ll list the predictions for the censored tract observations using three different prediction types, taking the exponent to get back to the USD median house values. nd &lt;- boston_506[is.na(boston_506$median),] t0 &lt;- exp(predict(SDEM_489, newdata=nd, listw=lw_q, pred.type=&quot;TS&quot;, zero.policy=TRUE)) suppressWarnings(t1 &lt;- exp(predict(SDEM_489, newdata=nd, listw=lw_q, pred.type=&quot;KP2&quot;, zero.policy=TRUE))) suppressWarnings(t2 &lt;- exp(predict(SDEM_489, newdata=nd, listw=lw_q, pred.type=&quot;KP5&quot;, zero.policy=TRUE))) data.frame(fit_TS=t0[,1], fit_KP2=c(t1), fit_KP5=c(t2), censored=boston_506$censored[as.integer(attr(t0, &quot;region.id&quot;))]) # fit_TS fit_KP2 fit_KP5 censored # 13 23912 29477 28147 right # 14 28126 27001 28516 right # 15 30553 36184 32476 right # 17 18518 19621 18878 right # 43 9564 6817 7561 left # 50 8371 7196 7383 left # 312 51477 53301 54173 right # 313 45921 45823 47095 right # 314 44196 44586 45361 right # 317 43427 45707 45442 right # 337 39879 42072 41127 right # 346 44708 46694 46108 right # 355 48188 49068 48911 right # 376 42881 45883 44966 right # 408 44294 44615 45670 right # 418 38211 43375 41914 right # 434 41647 41690 42398 right 14.4 Markov random field and multilevel models with spatial weights There is a large literature in spatial epidemiology using CAR and ICAR models in spatially structured random effects. These extend to multilevel models, in which the spatially structured random effects may apply at different levels of the model (Roger S. Bivand et al. 2017). In order to try out some of the variants, we need to remove the no-neighbour observations from the tract level, and from the model output zone aggregated level, in two steps as reducing the tract level induces a no-neighbour outcome at the model output zone level. boston_94a &lt;- aggregate(boston_489[,&quot;NOX_ID&quot;], list(boston_489$NOX_ID), unique) nb_q_94a &lt;- spdep::poly2nb(boston_94a) NOX_ID_no_neighs &lt;- boston_94a$NOX_ID[which(spdep::card(nb_q_94a) == 0)] boston_487 &lt;- boston_489[is.na(match(boston_489$NOX_ID, NOX_ID_no_neighs)),] boston_93 &lt;- aggregate(boston_487[, &quot;NOX_ID&quot;], list(ids = boston_487$NOX_ID), unique) row.names(boston_93) &lt;- as.character(boston_93$NOX_ID) nb_q_93 &lt;- spdep::poly2nb(boston_93, row.names=unique(as.character(boston_93$NOX_ID))) The lme4 package lets us add an IID unstructured random effect at the model output zone level: library(lme4) MLM &lt;- lmer(update(form, . ~ . + (1 | NOX_ID)), data=boston_487, REML=FALSE) copying the random effect into the \"sf\" object for mapping below. boston_93$MLM_re &lt;- ranef(MLM)[[1]][,1] Two packages, hglm and HSAR, offer SAR upper level spatially structured random effects, and require the specification of a sparse matrix mapping the upper level enities onto lower level entities, and sparse binary weights matrices: library(Matrix) suppressMessages(library(MatrixModels)) Delta &lt;- as(model.Matrix(~ -1 + as.factor(NOX_ID), data=boston_487, sparse=TRUE), &quot;dgCMatrix&quot;) M &lt;- as(spdep::nb2listw(nb_q_93, style=&quot;B&quot;), &quot;CsparseMatrix&quot;) The extension of hglm to sparse spatial setting extended its facilities (Alam, Rönnegård, and Shen 2015), and also permits the modelling of discrete responses. First we fit an IID random effect: suppressPackageStartupMessages(library(hglm)) y_hglm &lt;- log(boston_487$median) X_hglm &lt;- model.matrix(lm(form, data=boston_487)) suppressWarnings(HGLM_iid &lt;- hglm(y=y_hglm, X=X_hglm, Z=Delta)) followed by a SAR model at the upper level (corresponding to a spatial error (SEM) model), which reports the spatially structured random effect without fully converging, so coefficient standard errors are not available: suppressWarnings(HGLM_sar &lt;- hglm(y=y_hglm, X=X_hglm, Z=Delta, rand.family=SAR(D=M))) boston_93$HGLM_re &lt;- unname(HGLM_iid$ranef) boston_93$HGLM_ss &lt;- HGLM_sar$ranef[,1] The HSAR package is restricted to the Gaussian response case, and fits an upper level SEM using MCMC; if W= is a lower level weights matrix, it will also fit a lower level SLM (Dong and Harris 2015; Dong et al. 2015): library(HSAR) suppressWarnings(HSAR &lt;- hsar(form, data=boston_487, W=NULL, M=M, Delta=Delta, burnin=500, Nsim=2500, thinning=1)) boston_93$HSAR_ss &lt;- HSAR$Mus[1,] The R2BayesX package provides flexible support for structured additive regression models, including spatial multilevel models. The models include an IID unstructured random effect at the upper level using the \"re\" specification (Umlauf et al. 2015); we choose the \"MCMC\"method: suppressPackageStartupMessages(library(R2BayesX)) BX_iid &lt;- bayesx(update(form, . ~ . + sx(NOX_ID, bs=&quot;re&quot;)), family=&quot;gaussian&quot;, data=boston_487, method=&quot;MCMC&quot;, iterations=12000, burnin=2000, step=2, seed=123) # Warning in run.bayesx(file.path(res$bayesx.prg$file.dir, prg.name = # res$bayesx.prg$prg.name), : an error occurred during runtime of BayesX, please # check the BayesX logfile! boston_93$BX_re &lt;- BX_iid$effects[&quot;sx(NOX_ID):re&quot;][[1]]$Mean and the \"mrf\" (Markov random field) spatially structured random effect specification based on a graph derived from converting a suitable \"nb\" object for the upper level. The \"region.id\" attribute of the \"nb\" object needs to contain values corresponding the the indexing variable. RBX_gra &lt;- nb2gra(nb_q_93) BX_mrf &lt;- bayesx(update(form, . ~ . + sx(NOX_ID, bs=&quot;mrf&quot;, map=RBX_gra)), family=&quot;gaussian&quot;, data=boston_487, method=&quot;MCMC&quot;, iterations=12000, burnin=2000,step=2, seed=123) # Warning in run.bayesx(file.path(res$bayesx.prg$file.dir, prg.name = # res$bayesx.prg$prg.name), : an error occurred during runtime of BayesX, please # check the BayesX logfile! boston_93$BX_ss &lt;- BX_mrf$effects[&quot;sx(NOX_ID):mrf&quot;][[1]]$Mean In a very similar way, mgcv::gam() can take an \"mrf\" term using a suitable \"nb\" object for the upper level. In this case the \"nb\" object needs to have the contents of the \"region.id\" attribute copied as the names of the neighbour list components, and the indexing variable needs to be a factor (Wood 2017) (the \"REML\" method of bayesx() gives the same result here): library(mgcv) names(nb_q_93) &lt;- attr(nb_q_93, &quot;region.id&quot;) boston_487$NOX_ID &lt;- as.factor(boston_487$NOX_ID) GAM_MRF &lt;- gam(update(form, . ~ . + s(NOX_ID, bs=&quot;mrf&quot;, xt=list(nb=nb_q_93))), data=boston_487, method=&quot;REML&quot;) boston_93$GAM_ss &lt;- aggregate(predict(GAM_MRF, type=&quot;terms&quot;, se=FALSE)[,14], list(boston_487$NOX_ID), mean)$x In the cases of hglm(), bayesx() and gam(), we could also model discrete responses without further major difficulty, and bayesx() and gam() also facilitate the generalization of functional form fitting for included covariates. res &lt;- rbind(iid_lmer=summary(MLM)$coefficients[6, 1:2], iid_hglm=summary(HGLM_iid)$FixCoefMat[6, 1:2], iid_BX=BX_iid$fixed.effects[6, 1:2], sar_hsar=c(HSAR$Mbetas[1, 6], HSAR$SDbetas[1, 6]), mrf_BX=BX_mrf$fixed.effects[6, 1:2], mrf_GAM=c(summary(GAM_MRF)$p.coeff[6], summary(GAM_MRF)$se[6])) Unfortunately, the coefficient estimates for the air pollution variable for these multilevel models are not helpful. All remain negative, but the inclusion of the model output zone level effects, be they IID or spatially structured, suggest that it is hard to disentangle the influence of the scale of observation from that of covariates observed at that scale. suppressPackageStartupMessages(library(ggplot2)) df_res &lt;- as.data.frame(res) names(df_res) &lt;- c(&quot;mean&quot;, &quot;sd&quot;) limits &lt;- aes(ymax = mean + qnorm(0.975)*sd, ymin=mean + qnorm(0.025)*sd) df_res$model &lt;- row.names(df_res) p &lt;- ggplot(df_res, aes(y=mean, x=model)) + geom_point() + geom_errorbar(limits) + geom_hline(yintercept = 0, col=&quot;#EB811B&quot;) + coord_flip() p + ggtitle(&quot;NOX coefficients and error bars&quot;) + theme(plot.background = element_rect(fill = &quot;transparent&quot;,colour = NA), legend.background = element_rect(colour = NA, fill = &quot;transparent&quot;)) Figure 14.1: Polish municipality types 2015 This map shows that the model output zone level IID random effects are very similar across the three model fitting functions reported. library(tmap) # FIXME: uncommented BX_re tm_shape(boston_93) + tm_fill(c(&quot;MLM_re&quot;, &quot;HGLM_re&quot; #, &quot;BX_re&quot; ), midpoint=0, title=&quot;IID&quot;) + tm_facets(free.scales=FALSE) + tm_borders(lwd=0.3, alpha=0.4) + tm_layout(panel.labels=c(&quot;MLM&quot;, &quot;HGLM&quot;, &quot;BX&quot;)) Figure 14.2: IID random effects The spatially structured SAR and MRF random effects (MRF term in the gam() case) are also very similar, with the MRF somewhat less smoothed than the SAR values. # FIXME: uncommented BX_ss tm_shape(boston_93) + tm_fill(c(&quot;HGLM_ss&quot;, &quot;HSAR_ss&quot;, # &quot;BX_ss&quot;, &quot;GAM_ss&quot;), midpoint=0, title=&quot;SS&quot;) + tm_facets(free.scales=FALSE) + tm_borders(lwd=0.3, alpha=0.4) + tm_layout(panel.labels=c(&quot;HGLM SAR&quot;, &quot;HSAR SAR&quot;, &quot;BX MRF&quot;, &quot;GAM MRF&quot;)) Figure 14.3: Spatially structured random effects Although there is still a great need for more thorough comparative studies of model fitting functions for spatial regression, there has been much progress over recent years. References "],["sp-and-raster.html", "Chapter 15 sp and raster 15.1 links and differences between sf and sp 15.2 migration packages 15.3 raster, stars and sf", " Chapter 15 sp and raster 15.1 links and differences between sf and sp difference in data structures; how to convert; limitations of sf to sp conversion 15.2 migration packages link to sf wiki: https://github.com/r-spatial/sf/wiki/Migrating 15.3 raster, stars and sf map algebra; ABM; SDM; refer to Robert’s book on http://rspatial.org/ "],["all-r-code-in-this-book.html", "All R code in this book", " All R code in this book set.seed(1014) options(digits = 3) knitr::opts_chunk$set( comment = &quot;#&quot;, collapse = knitr::is_latex_output(), cache = TRUE, # out.width = &quot;70%&quot;, fig.align = &#39;center&#39; # fig.width = 6, # fig.asp = 0.618, # 1 / phi # fig.show = &quot;hold&quot; ) options(dplyr.print_min = 6, dplyr.print_max = 6) options(stars.crs = 17) mapview::mapviewOptions(fgb = FALSE) # for units: (?) Sys.setenv(UDUNITS2_XML_PATH=&quot;&quot;) if (knitr::is_latex_output()) options(width = 66) #options(width = 72) knitr::write_bib(c( &quot;abind&quot;, &quot;classInt&quot;, &quot;colorspace&quot;, &quot;gdalcubes&quot;, &quot;gstat&quot;, &quot;ggplot2&quot;, &quot;ggspatial&quot;, &quot;lwgeom&quot;, &quot;osmar&quot;, &quot;raster&quot;, &quot;rgee&quot;, &quot;ecmwfr&quot;, &quot;rnaturalearth&quot;, &quot;RColorBrewer&quot;, &quot;raster&quot;, &quot;rstac&quot;, &quot;sf&quot;, &quot;sp&quot;, &quot;spacetime&quot;, &quot;spatstat&quot;, #--&gt;&gt; generates too many authors!! FIXME: update before submit! &quot;spdep&quot;, &quot;splm&quot;, &quot;stars&quot;, &quot;stcos&quot;, &quot;stplanr&quot;, &quot;terra&quot;, &quot;tidyverse&quot;, &quot;tsibble&quot;, &quot;viridis&quot;, &quot;units&quot;, &quot;xts&quot;, &quot;s2&quot;, &quot;tmap&quot;, &quot;mapview&quot; ), &quot;packages.bib&quot;, width = 60) The first part of this book introduces concepts of spatial data science, and uses R only to generate text output or figures. The R code used for this is not shown, as it would distract from the message. The online version of this book contains the R sections, which can be unfolded on demand and copied into the clipboard for execution and experimenting. The second part of this book explains how the concepts introduced in part I are dealt with using R, and deals with basic handling and plotting of spatial and spatiotemporal data. Part III is dedicated to statistical modelling of spatial data. This work is licensed under the [Attribution-NonCommercial-NoDerivatives 4.0](https://creativecommons.org/licenses/by-nc-nd/4.0/legalcode) International License. set.seed(1014) options(digits = 3) knitr::opts_chunk$set( comment = &quot;#&quot;, collapse = knitr::is_latex_output(), cache = TRUE, # out.width = &quot;70%&quot;, fig.align = &#39;center&#39; # fig.width = 6, # fig.asp = 0.618, # 1 / phi # fig.show = &quot;hold&quot; ) options(dplyr.print_min = 6, dplyr.print_max = 6) options(stars.crs = 17) mapview::mapviewOptions(fgb = FALSE) # for units: (?) Sys.setenv(UDUNITS2_XML_PATH=&quot;&quot;) if (knitr::is_latex_output()) options(width = 66) #options(width = 72) Text introducing Part I library(tidyverse) library(sf) system.file(&quot;gpkg/nc.gpkg&quot;, package=&quot;sf&quot;) %&gt;% read_sf() -&gt; nc nc.32119 &lt;- st_transform(nc, 32119) nc.32119 %&gt;% select(BIR74) %&gt;% plot(graticule = TRUE, axes = TRUE) nc %&gt;% select(AREA, BIR74, SID74) %&gt;% print(n = 3) year_labels = c(&quot;SID74&quot; = &quot;1974 - 1978&quot;, &quot;SID79&quot; = &quot;1979 - 1984&quot;) nc.32119 %&gt;% select(SID74, SID79) %&gt;% gather(VAR, SID, -geom) -&gt; nc2 ggplot() + geom_sf(data = nc2, aes(fill = SID)) + facet_wrap(~VAR, ncol = 1, labeller = labeller(VAR = year_labels)) + scale_y_continuous(breaks = 34:36) + scale_fill_gradientn(colors = sf.colors(20)) + theme(panel.grid.major = element_line(color = &quot;white&quot;)) suppressPackageStartupMessages(library(mapview)) nc.32119 %&gt;% mapview(zcol = &quot;BIR74&quot;, legend = TRUE, col.regions = sf.colors) library(stars) par(mfrow = c(2, 2)) par(mar = rep(1, 4)) tif &lt;- system.file(&quot;tif/L7_ETMs.tif&quot;, package = &quot;stars&quot;) x &lt;- read_stars(tif)[,,,1] image(x, main = &quot;(a)&quot;) image(x[,1:10,1:10], text_values = TRUE, border = &#39;grey&#39;, main = &quot;(b)&quot;) image(x, main = &quot;(c)&quot;) set.seed(131) pts = st_sample(st_as_sfc(st_bbox(x)), 3) plot(pts, add = TRUE, pch = 3, col = &#39;blue&#39;) image(x, main = &quot;(d)&quot;) plot(st_buffer(pts, 500), add = TRUE, pch = 3, border = &#39;blue&#39;, col = NA, lwd = 2) st_extract(x, pts) aggregate(x, st_buffer(pts, 500), FUN = mean) %&gt;% st_as_sf() plot(st_rasterize(nc[&quot;BIR74&quot;], dx = 0.1), col =sf.colors(), breaks = &quot;equal&quot;) x = 1:5 y = 1:4 d = st_dimensions(x = x, y = y, .raster = c(&quot;x&quot;, &quot;y&quot;)) m = matrix(runif(20),5,4) r1 = st_as_stars(r = m, dimensions = d) r = attr(d, &quot;raster&quot;) r$affine = c(0.2, -0.2) attr(d, &quot;raster&quot;) = r r2 = st_as_stars(r = m, dimensions = d) r = attr(d, &quot;raster&quot;) r$affine = c(0.1, -0.3) attr(d, &quot;raster&quot;) = r r3 = st_as_stars(r = m, dimensions = d) x = c(1, 2, 3.5, 5, 6) y = c(1, 1.5, 3, 3.5) d = st_dimensions(x = x, y = y, .raster = c(&quot;x&quot;, &quot;y&quot;)) r4 = st_as_stars(r = m, dimensions = d) grd = st_make_grid(cellsize = c(10,10), offset = c(-130,10), n= c(8,5), crs=st_crs(4326)) r5 = st_transform(grd, &quot;+proj=laea +lon_0=-70 +lat_0=35&quot;) par(mfrow = c(2,3), mar = c(0.1, 1, 1.1, 1)) r1 = st_make_grid(cellsize = c(1,1), n = c(5,4), offset = c(0,0)) plot(r1, main = &quot;regular&quot;) plot(st_geometry(st_as_sf(r2)), main = &quot;rotated&quot;) plot(st_geometry(st_as_sf(r3)), main = &quot;sheared&quot;) plot(st_geometry(st_as_sf(r4, as_points = FALSE)), main = &quot;rectilinear&quot;) plot(st_geometry((r5)), main = &quot;curvilinear&quot;) knitr::include_graphics(&quot;images/sf_deps.png&quot;) set.seed(1014) options(digits = 3) knitr::opts_chunk$set( comment = &quot;#&quot;, collapse = knitr::is_latex_output(), cache = TRUE, # out.width = &quot;70%&quot;, fig.align = &#39;center&#39; # fig.width = 6, # fig.asp = 0.618, # 1 / phi # fig.show = &quot;hold&quot; ) options(dplyr.print_min = 6, dplyr.print_max = 6) options(stars.crs = 17) mapview::mapviewOptions(fgb = FALSE) # for units: (?) Sys.setenv(UDUNITS2_XML_PATH=&quot;&quot;) if (knitr::is_latex_output()) options(width = 66) #options(width = 72) par(mar = rep(0,4)) plot(3, 4, xlim = c(-6,6), ylim = c(-6,6), asp = 1) axis(1, pos = 0, at = 0:6) axis(2, pos = 0, at = -6:6) xd = seq(-5, 5, by = .1) lines(xd, sqrt(25 - xd^2), col = &#39;grey&#39;) lines(xd, -sqrt(25 - xd^2), col = &#39;grey&#39;) arrows(0, 0, 3, 4, col = &#39;red&#39;, length = .15, angle = 20) text(1.5, 2.7, label = &quot;r&quot;, col = &#39;red&#39;) xd = seq(3/5, 1, by = .1) lines(xd, sqrt(1 - xd^2), col = &#39;red&#39;) text(1.2, 0.5, label = parse(text = &quot;phi&quot;), col = &#39;red&#39;) lines(c(3,3), c(0,4), lty = 2, col = &#39;blue&#39;) lines(c(0,3), c(4,4), lty = 2, col = &#39;blue&#39;) text(3.3, 0.3, label = &quot;x&quot;, col = &#39;blue&#39;) text(0.3, 4.3, label = &quot;y&quot;, col = &#39;blue&#39;) suppressPackageStartupMessages(library(sf)) e = cbind(-90:90,0) # equator f1 = rbind(cbind(0, -90:90)) # 0/antimerid. f2 = rbind(cbind(90, -90:90), cbind(270, 90:-90))# +/- 90 eq = st_sfc(st_linestring(e), st_linestring(f1), st_linestring(f2), crs=4326) geoc = st_transform(eq, &quot;+proj=geocent&quot;) cc = rbind(geoc[[1]], NA, geoc[[2]], NA, geoc[[3]]) from3d = function(x, offset, maxz, minz) { x = x[,c(2,3,1)] + offset # move to y right, x up, z backw x[,2] = x[,2] - maxz # shift y to left d = maxz z = x[,3] - minz + offset x[,1] = x[,1] * (d/z) x[,2] = x[,2] * (d/z) x[,1:2] } maxz = max(cc[,3], na.rm = TRUE) minz = min(cc[,3], na.rm = TRUE) offset = 3e7 circ = from3d(cc, offset, maxz, minz) mx = max(cc, na.rm = TRUE) * 1.1 x = rbind(c(0, 0, 0), c(mx, 0, 0)) y = rbind(c(0, 0, 0), c(0, mx, 0)) z = rbind(c(0, 0, 0), c(0, 0, mx)) ll = rbind(x, NA, y, NA, z) l0 = from3d(ll, offset, maxz, minz) mx = max(cc, na.rm = TRUE) * 1.2 x = rbind(c(0, 0, 0), c(mx, 0, 0)) y = rbind(c(0, 0, 0), c(0, mx, 0)) z = rbind(c(0, 0, 0), c(0, 0, mx)) ll = rbind(x, NA, y, NA, z) l = from3d(ll, offset, maxz, minz) par(mfrow = c(1, 2)) par(mar=rep(0,4)) plot.new() plot.window(xlim = c(min(circ[,1],na.rm = TRUE), 3607103*1.02), ylim = c(min(circ[,2],na.rm = TRUE), 2873898*1.1), asp = 1) lines(circ) lines(l0) text(l[c(2,5,8),], labels = c(&quot;x&quot;, &quot;y&quot;, &quot;z&quot;), col = &#39;red&#39;) # add POINT(60 47) p = st_as_sfc(&quot;POINT(60 47)&quot;, crs = 4326) %&gt;% st_transform(&quot;+proj=geocent&quot;) p = p[[1]] pts = rbind(c(0,0,0), c(p[1],0,0), c(p[1],p[2],0), c(p[1],p[2],p[2])) ptsl = from3d(pts, offset, maxz, minz) lines(ptsl, col = &#39;blue&#39;, lty = 2, lwd = 2) points(ptsl[4,1], ptsl[4,2], col = &#39;blue&#39;, cex = 1, pch = 16) plot.new() plot.window(xlim = c(min(circ[,1],na.rm = TRUE), 3607103*1.02), ylim = c(min(circ[,2],na.rm = TRUE), 2873898*1.1), asp = 1) lines(circ) p = st_as_sfc(&quot;POINT(60 47)&quot;, crs = 4326) %&gt;% st_transform(&quot;+proj=geocent&quot;) p = p[[1]] pts = rbind(c(0,0,0), c(p[1],p[2],p[3])) pt = from3d(pts, offset, maxz, minz) lines(pt) points(pt[2,1], pt[2,2], col = &#39;blue&#39;, cex = 1, pch = 16) p0 = st_as_sfc(&quot;POINT(60 0)&quot;, crs = 4326) %&gt;% st_transform(&quot;+proj=geocent&quot;) p0 = p0[[1]] pts = rbind(c(0,0,0), c(p0[1],p0[2],p0[3])) pt = from3d(pts, offset, maxz, minz) lines(pt) p0 = st_as_sfc(&quot;POINT(0 0)&quot;, crs = 4326) %&gt;% st_transform(&quot;+proj=geocent&quot;) p0 = p0[[1]] pts = rbind(c(0,0,0), c(p0[1],p0[2],p0[3])) pt = from3d(pts, offset, maxz, minz) lines(pt) p0 = st_as_sfc(&quot;POINT(0 90)&quot;, crs = 4326) %&gt;% st_transform(&quot;+proj=geocent&quot;) p0 = p0[[1]] pts = rbind(c(0,0,0), c(p0[1],p0[2],p0[3])) pt = from3d(pts, offset, maxz, minz) lines(pt, lty = 2) p0 = st_as_sfc(&quot;POINT(90 0)&quot;, crs = 4326) %&gt;% st_transform(&quot;+proj=geocent&quot;) p0 = p0[[1]] pts = rbind(c(0,0,0), c(p0[1],p0[2],p0[3])) pt = from3d(pts, offset, maxz, minz) lines(pt, lty = 2) f1 = rbind(cbind(0:60, 0)) arc = st_sfc(st_linestring(f1), crs=4326) geoc = st_transform(arc, &quot;+proj=geocent&quot;) cc = rbind(geoc[[1]]) circ = from3d(cc, offset, maxz, minz) lines(circ, col = &#39;red&#39;, lwd = 2, lty = 2) f1 = rbind(cbind(60, 0:47)) arc = st_sfc(st_linestring(f1), crs=4326) geoc = st_transform(arc, &quot;+proj=geocent&quot;) cc = rbind(geoc[[1]]) circ = from3d(cc, offset, maxz, minz) lines(circ, col = &#39;blue&#39;, lwd = 2, lty = 2) text(pt[1,1]+100000, pt[1,2]+50000, labels = expression(phi), col = &#39;blue&#39;) # lat text(pt[1,1]+20000, pt[1,2]-50000, labels = expression(lambda), col = &#39;red&#39;) # lng p = st_as_sfc(&quot;POINT(60 47)&quot;, crs = 4326) p[[1]] p = st_as_sfc(&quot;POINT(60 47)&quot;, crs = 4326) %&gt;% st_transform(&quot;+proj=geocent&quot;) p[[1]] par(mar = rep(0,4)) x = 4 y = 5/8 * sqrt(48) plot(x, y, xlim = c(-6,6), ylim = c(-8,8), asp = 1) axis(1, pos = 0, at = 0:9) axis(2, pos = 0, at = -5:5) xd = seq(-8, 8, by = .1) lines(xd, 5/8 * sqrt(64 - xd^2), col = &#39;grey&#39;) lines(xd, 5/8 * -sqrt(64 - xd^2), col = &#39;grey&#39;) arrows(0, 0, x, y, col = &#39;red&#39;, length = .15, angle = 20) b = (x * 25) / (-y * 64) a = y - x * b abline(a, b, col = &#39;grey&#39;) b = -1/b x0 = x - y / b arrows(x0, 0, x, y, col = &#39;blue&#39;, length = .15, angle = 20) text(1.2, 0.5, label = parse(text = &quot;psi&quot;), col = &#39;red&#39;) text(3, 0.5, label = parse(text = &quot;phi&quot;), col = &#39;blue&#39;) pts = st_sfc(st_point(c(13.4050, 52.5200)), st_point(c(2.3522, 48.8566)), crs = &#39;EPSG:4326&#39;) sf_use_s2(FALSE) d1 = c(gc_ellipse = st_distance(pts)[1,2]) sf_use_s2(TRUE) # or, without using s2, use st_distance(st_transform(pts, &quot;+proj=longlat +ellps=sphere&quot;)) d2 = c(gc_sphere = st_distance(pts)[1,2]) p = st_transform(pts, &quot;+proj=geocent&quot;) d3 = c(straight_ellipse = units::set_units(sqrt(sum(apply(do.call(cbind, p), 1, diff)^2)), m)) p2 = st_transform(pts, &quot;+proj=longlat +ellps=sphere&quot;) %&gt;% st_transform(&quot;+proj=geocent&quot;) d4 = c(straight_sphere = units::set_units(sqrt(sum(apply(do.call(cbind, p2), 1, diff)^2)), m)) res = c(d1,d3,d2,d4) # print as km, re-add names: res %&gt;% units::set_units(km) %&gt;% setNames(names(res)) %&gt;% print(digits = 5) library(stars) library(rnaturalearth) countries110 = st_as_sf(countries110) uk = countries110[countries110$admin %in% c(&quot;United Kingdom&quot;),] %&gt;% st_geometry() r = read_stars(&quot;data/uk_os_OSTN15_NTv2_OSGBtoETRS.tif&quot;) # r = read_stars(&quot;/vsicurl/https://cdn.proj.org/uk_os_OSTN15_NTv2_OSGBtoETRS.tif&quot;) hook = function() { plot(uk, border = &quot;orange&quot;, col = NA, add = TRUE) } plot(r[,,,1:2], axes = TRUE, hook = hook, key.pos = 4) h = read_stars(&quot;data/uk_os_OSGM15_GB.tif&quot;) # h = read_stars(&quot;/vsicurl/https://cdn.proj.org/uk_os_OSGM15_GB.tif&quot;) plot(h, axes = TRUE, reset = FALSE) plot(uk, border = &quot;orange&quot;, col = NA, add = TRUE) set.seed(1014) options(digits = 3) knitr::opts_chunk$set( comment = &quot;#&quot;, collapse = knitr::is_latex_output(), cache = TRUE, # out.width = &quot;70%&quot;, fig.align = &#39;center&#39; # fig.width = 6, # fig.asp = 0.618, # 1 / phi # fig.show = &quot;hold&quot; ) options(dplyr.print_min = 6, dplyr.print_max = 6) options(stars.crs = 17) mapview::mapviewOptions(fgb = FALSE) # for units: (?) Sys.setenv(UDUNITS2_XML_PATH=&quot;&quot;) if (knitr::is_latex_output()) options(width = 66) #options(width = 72) library(sf) par(mfrow = c(2,4)) par(mar = c(1,1,1.2,1)) # 1 p = st_point(0:1) plot(p, pch = 16) title(&quot;point&quot;) box(col = &#39;grey&#39;) # 2 mp = st_multipoint(rbind(c(1,1), c(2, 2), c(4, 1), c(2, 3), c(1,4))) plot(mp, pch = 16) title(&quot;multipoint&quot;) box(col = &#39;grey&#39;) # 3 ls = st_linestring(rbind(c(1,1), c(5,5), c(5, 6), c(4, 6), c(3, 4), c(2, 3))) plot(ls, lwd = 2) title(&quot;linestring&quot;) box(col = &#39;grey&#39;) # 4 mls = st_multilinestring(list( rbind(c(1,1), c(5,5), c(5, 6), c(4, 6), c(3, 4), c(2, 3)), rbind(c(3,0), c(4,1), c(2,1)))) plot(mls, lwd = 2) title(&quot;multilinestring&quot;) box(col = &#39;grey&#39;) # 5 polygon po = st_polygon(list(rbind(c(2,1), c(3,1), c(5,2), c(6,3), c(5,3), c(4,4), c(3,4), c(1,3), c(2,1)), rbind(c(2,2), c(3,3), c(4,3), c(4,2), c(2,2)))) plot(po, border = &#39;black&#39;, col = &#39;#ff8888&#39;, lwd = 2) title(&quot;polygon&quot;) box(col = &#39;grey&#39;) # 6 multipolygon mpo = st_multipolygon(list( list(rbind(c(2,1), c(3,1), c(5,2), c(6,3), c(5,3), c(4,4), c(3,4), c(1,3), c(2,1)), rbind(c(2,2), c(3,3), c(4,3), c(4,2), c(2,2))), list(rbind(c(3,7), c(4,7), c(5,8), c(3,9), c(2,8), c(3,7))))) plot(mpo, border = &#39;black&#39;, col = &#39;#ff8888&#39;, lwd = 2) title(&quot;multipolygon&quot;) box(col = &#39;grey&#39;) # 7 geometrycollection gc = st_geometrycollection(list(po, ls + c(0,5), st_point(c(2,5)), st_point(c(5,4)))) plot(gc, border = &#39;black&#39;, col = &#39;#ff6666&#39;, pch = 16, lwd = 2) title(&quot;geometrycollection&quot;) box(col = &#39;grey&#39;) p mp ls mls po mpo gc (ls = st_linestring(rbind(c(0,0), c(1,1), c(2,2), c(0,2), c(1,1), c(2,0)))) c(is_simple = st_is_simple(ls)) st_point(c(1,3,2)) st_point(c(1,3,2), dim = &quot;XYM&quot;) st_linestring(rbind(c(3,1,2,4), c(4,4,2,2))) (e = st_intersection(st_point(c(0,0)), st_point(c(1,1)))) st_point() st_linestring(matrix(1,1,3)[0,], dim = &quot;XYM&quot;) library(sf) polygon = po = st_polygon(list(rbind(c(0,0), c(1,0), c(1,1), c(0,1), c(0,0)))) p0 = st_polygon(list(rbind(c(-1,-1), c(2,-1), c(2,2), c(-1,2), c(-1,-1)))) line = li = st_linestring(rbind(c(.5, -.5), c(.5, 0.5))) s = st_sfc(po, li) par(mfrow = c(3,3)) par(mar = c(1,1,1,1)) # &quot;1020F1102&quot; # 1: 1 plot(s, col = c(NA, &#39;darkgreen&#39;), border = &#39;blue&#39;, main = expression(paste(&quot;I(pol)&quot;,intersect(),&quot;I(line) = 1&quot;))) lines(rbind(c(.5,0), c(.5,.495)), col = &#39;red&#39;, lwd = 2) points(0.5, 0.5, pch = 1) # 2: 0 plot(s, col = c(NA, &#39;darkgreen&#39;), border = &#39;blue&#39;, main = expression(paste(&quot;I(pol)&quot;,intersect(),&quot;B(line) = 0&quot;))) points(0.5, 0.5, col = &#39;red&#39;, pch = 16) # 3: 2 plot(s, col = c(NA, &#39;darkgreen&#39;), border = &#39;blue&#39;, main = expression(paste(&quot;I(pol)&quot;,intersect(),&quot;E(line) = 2&quot;))) plot(po, col = &#39;#ff8888&#39;, add = TRUE) plot(s, col = c(NA, &#39;darkgreen&#39;), border = &#39;blue&#39;, add = TRUE) # 4: 0 plot(s, col = c(NA, &#39;darkgreen&#39;), border = &#39;blue&#39;, main = expression(paste(&quot;B(pol)&quot;,intersect(),&quot;I(line) = 0&quot;))) points(.5, 0, col = &#39;red&#39;, pch = 16) # 5: F plot(s, col = c(NA, &#39;darkgreen&#39;), border = &#39;blue&#39;, main = expression(paste(&quot;B(pol)&quot;,intersect(),&quot;B(line) = F&quot;))) # 6: 1 plot(s, col = c(NA, &#39;darkgreen&#39;), border = &#39;blue&#39;, main = expression(paste(&quot;B(pol)&quot;,intersect(),&quot;E(line) = 1&quot;))) plot(po, border = &#39;red&#39;, col = NA, add = TRUE, lwd = 2) # 7: 1 plot(s, col = c(NA, &#39;darkgreen&#39;), border = &#39;blue&#39;, main = expression(paste(&quot;E(pol)&quot;,intersect(),&quot;I(line) = 1&quot;))) lines(rbind(c(.5, -.5), c(.5, 0)), col = &#39;red&#39;, lwd = 2) # 8: 0 plot(s, col = c(NA, &#39;darkgreen&#39;), border = &#39;blue&#39;, main = expression(paste(&quot;E(pol)&quot;,intersect(),&quot;B(line) = 0&quot;))) points(.5, -.5, col = &#39;red&#39;, pch = 16) # 9: 2 plot(s, col = c(NA, &#39;darkgreen&#39;), border = &#39;blue&#39;, main = expression(paste(&quot;E(pol)&quot;,intersect(),&quot;E(line) = 2&quot;))) plot(p0 / po, col = &#39;#ff8888&#39;, add = TRUE) plot(s, col = c(NA, &#39;darkgreen&#39;), border = &#39;blue&#39;, add = TRUE) st_relate(polygon, line) par(mar = rep(0,4), mfrow = c(1, 2)) plot(st_geometry(nc)[1], col = NA, border = &#39;black&#39;) plot(st_convex_hull(st_geometry(nc)[1]), add = TRUE, col = NA, border = &#39;red&#39;) box() set.seed(131) mp = st_multipoint(matrix(runif(20), 10)) plot(mp) plot(st_voronoi(mp), add = TRUE, col = NA, border = &#39;red&#39;) box() par(mar = rep(.1, 4), mfrow = c(1, 2)) sq = function(pt, sz = 1) st_polygon(list(rbind(c(pt - sz), c(pt[1] + sz, pt[2] - sz), c(pt + sz), c(pt[1] - sz, pt[2] + sz), c(pt - sz)))) x = st_sf(box = 1:3, st_sfc(sq(c(0,0)), sq(c(1.7, -0.5)), sq(c(0.5, 1)))) plot(st_geometry(x), col = NA, border = sf.colors(3, categorical=TRUE), lwd = 3) plot(st_intersection(st_geometry(x)), col = sf.colors(7, categorical=TRUE, alpha = .5)) par(mar = rep(.1, 4), mfrow = c(1, 2)) xg = st_geometry(x) plot(st_difference(xg), col = sf.colors(3, alpha = .5, categorical=TRUE)) plot(st_difference(xg[3:1]), col = sf.colors(3, alpha = .5, categorical=TRUE)) library(stars) ls = st_sf(a = 1:2, st_sfc(st_linestring(rbind(c(0.1, 0), c(1, .9))))) grd = st_as_stars(st_bbox(ls), nx = 10, ny = 10, xlim = c(0, 1.0), ylim = c(0, 1), values = -1) r = st_rasterize(ls, grd, options = &quot;ALL_TOUCHED=TRUE&quot;) r[r == -1] = NA plot(st_geometry(st_as_sf(grd)), border = &#39;orange&#39;, col = NA, reset = FALSE, key.pos=NULL) plot(r, axes = TRUE, add = TRUE) # ALL_TOUCHED=FALSE; plot(ls, add = TRUE, col = &quot;red&quot;, lwd = 2) set.seed(1014) options(digits = 3) knitr::opts_chunk$set( comment = &quot;#&quot;, collapse = knitr::is_latex_output(), cache = TRUE, # out.width = &quot;70%&quot;, fig.align = &#39;center&#39; # fig.width = 6, # fig.asp = 0.618, # 1 / phi # fig.show = &quot;hold&quot; ) options(dplyr.print_min = 6, dplyr.print_max = 6) options(stars.crs = 17) mapview::mapviewOptions(fgb = FALSE) # for units: (?) Sys.setenv(UDUNITS2_XML_PATH=&quot;&quot;) if (knitr::is_latex_output()) options(width = 66) #options(width = 72) library(sf) suppressPackageStartupMessages(library(maps)) # maps: par(mfrow = c(2,2)) par(mar = c(1,1.2,1,1)) m = st_as_sf(map(fill=TRUE, plot=FALSE)) a = m[m$ID == &quot;Antarctica&quot;, ] st_bbox(a) library(s2) s2_bounds_cap(a) s2_bounds_rect(a) st_bbox(m[m$ID == &quot;Fiji&quot;,]) s2_bounds_rect(m[m$ID == &quot;Fiji&quot;,]) library(sf) suppressPackageStartupMessages(library(maps)) # maps: par(mfrow = c(2,2)) par(mar = c(1,1.2,1,1)) m = st_as_sf(map(fill=TRUE, plot=FALSE)) m = m[m$ID == &quot;Antarctica&quot;, ] plot(st_geometry(m), asp = 2) title(&quot;a (not valid)&quot;) # ne: library(rnaturalearth) ne = ne_countries(returnclass = &quot;sf&quot;) ne = ne[ne$region_un == &quot;Antarctica&quot;, &quot;region_un&quot;] plot(st_geometry(ne), asp = 2) title(&quot;b (valid)&quot;) # 3031 m %&gt;% st_geometry() %&gt;% st_transform(3031) %&gt;% plot() title(&quot;c (valid)&quot;) ne %&gt;% st_geometry() %&gt;% st_transform(3031) %&gt;% plot() title(&quot;d (not valid)&quot;) set.seed(1014) options(digits = 3) knitr::opts_chunk$set( comment = &quot;#&quot;, collapse = knitr::is_latex_output(), cache = TRUE, # out.width = &quot;70%&quot;, fig.align = &#39;center&#39; # fig.width = 6, # fig.asp = 0.618, # 1 / phi # fig.show = &quot;hold&quot; ) options(dplyr.print_min = 6, dplyr.print_max = 6) options(stars.crs = 17) mapview::mapviewOptions(fgb = FALSE) # for units: (?) Sys.setenv(UDUNITS2_XML_PATH=&quot;&quot;) if (knitr::is_latex_output()) options(width = 66) #options(width = 72) library(sf) library(dplyr) system.file(&quot;gpkg/nc.gpkg&quot;, package=&quot;sf&quot;) %&gt;% read_sf() %&gt;% st_transform(32119) %&gt;% select(BIR74, SID74, NAME) %&gt;% st_centroid() -&gt; x nc &lt;- read_sf(system.file(&quot;gpkg/nc.gpkg&quot;, package=&quot;sf&quot;)) #plot(nc[&quot;SID74&quot;], axes = TRUE) sf_use_s2(TRUE) # encode quadrant by two logicals: nc$lng = st_coordinates(st_centroid(st_geometry(nc)))[,1] &gt; -79 nc$lat = st_coordinates(st_centroid(st_geometry(nc)))[,2] &gt; 35.5 nc.grp = aggregate(nc[&quot;SID74&quot;], list(nc$lng, nc$lat), sum) plot(nc.grp[&quot;SID74&quot;], axes = TRUE) nc &lt;- st_transform(nc, 2264) gr = st_sf( label = apply(expand.grid(1:10, LETTERS[10:1])[,2:1], 1, paste0, collapse = &quot; &quot;), geom = st_make_grid(nc)) plot(st_geometry(nc), reset = FALSE, border = &#39;grey&#39;) plot(st_geometry(gr), add = TRUE) a = aggregate(nc[&quot;SID74&quot;], gr, sum) c(sid74_sum_counties = sum(nc$SID74), sid74_sum_rectangles = sum(a$SID74, na.rm = TRUE)) set.seed(1014) options(digits = 3) knitr::opts_chunk$set( comment = &quot;#&quot;, collapse = knitr::is_latex_output(), cache = TRUE, # out.width = &quot;70%&quot;, fig.align = &#39;center&#39; # fig.width = 6, # fig.asp = 0.618, # 1 / phi # fig.show = &quot;hold&quot; ) options(dplyr.print_min = 6, dplyr.print_max = 6) options(stars.crs = 17) mapview::mapviewOptions(fgb = FALSE) # for units: (?) Sys.setenv(UDUNITS2_XML_PATH=&quot;&quot;) if (knitr::is_latex_output()) options(width = 66) #options(width = 72) # (C) 2019, Edzer Pebesma, CC-BY-SA set.seed(1331) suppressPackageStartupMessages(library(stars)) library(colorspace) tif = system.file(&quot;tif/L7_ETMs.tif&quot;, package = &quot;stars&quot;) r = read_stars(tif) nrow = 5 ncol = 8 m = r[[1]][1:nrow,1:ncol,1] dim(m) = c(x = nrow, y = ncol) # named dim s = st_as_stars(m) # s attr(s, &quot;dimensions&quot;)[[1]]$delta = 3 attr(s, &quot;dimensions&quot;)[[2]]$delta = -.5 attr(attr(s, &quot;dimensions&quot;), &quot;raster&quot;)$affine = c(-1.2, 0.0) plt = function(x, yoffset = 0, add, li = TRUE) { attr(x, &quot;dimensions&quot;)[[2]]$offset = attr(x, &quot;dimensions&quot;)[[2]]$offset + yoffset l = st_as_sf(x, as_points = FALSE) pal = sf.colors(10) if (li) pal = lighten(pal, 0.3 + rnorm(1, 0, 0.1)) if (! add) plot(l, axes = FALSE, breaks = &quot;equal&quot;, pal = pal, reset = FALSE, border = grey(.75), key.pos = NULL, main = NULL, xlab = &quot;time&quot;) else plot(l, axes = TRUE, breaks = &quot;equal&quot;, pal = pal, add = TRUE, border = grey(.75)) u = st_union(l) # print(u) plot(st_geometry(u), add = TRUE, col = NA, border = &#39;black&#39;, lwd = 2.5) } pl = function(s, x, y, add = TRUE, randomize = FALSE) { attr(s, &quot;dimensions&quot;)[[1]]$offset = x attr(s, &quot;dimensions&quot;)[[2]]$offset = y m = r[[1]][y + 1:nrow,x + 1:ncol,1] if (randomize) m = m[sample(y + 1:nrow),x + 1:ncol] dim(m) = c(x = nrow, y = ncol) # named dim s[[1]] = m plt(s, 0, add) plt(s, 1, TRUE) plt(s, 2, TRUE) plt(s, 3, TRUE) plt(s, 4, TRUE) plt(s, 5, TRUE) plt(s, 6, TRUE) plt(s, 7, TRUE) plt(s, 8, TRUE, FALSE) } plot.new() par(mar = rep(0.5,4)) plot.window(xlim = c(-12,15), ylim = c(-5,10), asp=1) pl(s, 0, 0) # box() text(-10, 0, &quot;time&quot;, srt = -90, col = &#39;black&#39;) text(-5, 6.5, &quot;latitude&quot;, srt = 25, col = &#39;black&#39;) text( 5, 8.5, &quot;longitude&quot;, srt = 0, col = &#39;black&#39;) # (C) 2021, Jonathan Bahlmann, CC-BY-SA # https://github.com/Open-EO/openeo.org/tree/datacube_doc/documentation/1.0/datacubes/.scripts # based on work by Edzer Pebesma, 2019, here: https://gist.github.com/edzer/5f1b0faa3e93073784e01d5a4bb60eca # plotting runs via a dummy stars object with x, y dimensions (no bands) # to not be overly dependent on an input image, time steps and bands # are displayed by replacing the matrix contained in the dummy stars object # every time something is plotted # packages, read input ---- set.seed(1331) library(stars) suppressPackageStartupMessages(library(colorspace)) suppressPackageStartupMessages(library(scales)) # make color palettes ---- blues &lt;- sequential_hcl(n = 20, h1 = 211, c1 = 80, l1 = 40, l2 = 100, p1 = 2) greens &lt;- sequential_hcl(n = 20, h1 = 134, c1 = 80, l1 = 40, l2 = 100, p1 = 2) reds &lt;- sequential_hcl(n = 20, h1 = 360, c1 = 80, l1 = 40, l2 = 100, p1 = 2) purples &lt;- sequential_hcl(n = 20, h1 = 299, c1 = 80, l1 = 40, l2 = 100, p1 = 2) greys &lt;- sequential_hcl(n = 20, h1 = 0, c1 = 0, l1 = 40, l2 = 100, p1 = 2) # matrices from raster ---- # make input matrices from an actual raster image input &lt;- read_stars(&quot;data/iceland_delta_cutout_2.tif&quot;) # this raster needs approx 6x7 format # if the input raster is changed, every image where a pixel value is written as text needs to be checked and corrected accordingly input &lt;- input[,,,1:4] warped &lt;- st_warp(input, crs = st_crs(input), cellsize = 200) # warp to approx. 6x7 pixel # these are only needed for resampling warped_highres &lt;- st_warp(warped, crs = st_crs(warped), cellsize = 100) # with different input, cellsize must be adapted # this is a bit of a trick, because 3:4 is different format than 6:7 # when downsampling, the raster of origin isn&#39;t so important anyway warped_lowres &lt;- st_warp(warped_highres[,1:11,,], crs = st_crs(warped), cellsize = 390) # plot(warped_lowres) # image(warped[,,,1], text_values = TRUE) t1 &lt;- floor(matrix(runif(42, -30, 150), ncol = 7)) # create timesteps 2 and 3 randomly t2 &lt;- floor(matrix(runif(42, -250, 50), ncol = 7)) # create dummy stars object ---- make_dummy_stars &lt;- function(x, y, d1, d2, aff) { m = warped_highres[[1]][1:x,1:y,1] # underlying raster doesn&#39;t matter because it&#39;s just dummy construct dim(m) = c(x = x, y = y) # named dim dummy = st_as_stars(m) attr(dummy, &quot;dimensions&quot;)[[1]]$delta = d1 attr(dummy, &quot;dimensions&quot;)[[2]]$delta = d2 attr(attr(dummy, &quot;dimensions&quot;), &quot;raster&quot;)$affine = c(aff, 0.0) return(dummy) } s &lt;- make_dummy_stars(6, 7, 2.5, -.5714286, -1.14) # mainly used, perspective f &lt;- make_dummy_stars(6, 7, 1, 1, 0) # flat highres &lt;- make_dummy_stars(12, 14, 1.25, -.2857143, -.57) # for resampling lowres &lt;- make_dummy_stars(3, 4, 5, -1, -2) # for resampling # matrices from image ---- make_matrix &lt;- function(image, band, n = 42, ncol = 7, t = 0) { # this is based on an input image with &gt;= 4 input bands # n is meant to cut off NAs, ncol is y, t is random matrix for time difference return(matrix(image[,,,band][[1]][1:n], ncol = ncol) - t) # before: b3 &lt;- matrix(warped[,,,1][[1]][1:42], ncol = 7) - t2 } # now use function: b1 &lt;- make_matrix(warped, 1) b2 &lt;- make_matrix(warped, 1, t = t1) b3 &lt;- make_matrix(warped, 1, t = t2) g1 &lt;- make_matrix(warped, 2) g2 &lt;- make_matrix(warped, 2, t = t1) g3 &lt;- make_matrix(warped, 2, t = t2) r1 &lt;- make_matrix(warped, 3) r2 &lt;- make_matrix(warped, 3, t = t1) r3 &lt;- make_matrix(warped, 3, t = t2) n1 &lt;- make_matrix(warped, 4) n2 &lt;- make_matrix(warped, 4, t = t1) n3 &lt;- make_matrix(warped, 4, t = t2) # plot functions ---- plt = function(x, yoffset = 0, add, li = TRUE, pal, print_geom = TRUE, border = .75, breaks = &quot;equal&quot;) { # pal is color palette attr(x, &quot;dimensions&quot;)[[2]]$offset = attr(x, &quot;dimensions&quot;)[[2]]$offset + yoffset l = st_as_sf(x, as_points = FALSE) if (li) pal = lighten(pal, 0.2) # + rnorm(1, 0, 0.1)) if (! add) plot(l, axes = FALSE, breaks = breaks, pal = pal, reset = FALSE, border = grey(border), key.pos = NULL, main = NULL, xlab = &quot;time&quot;) else plot(l, axes = TRUE, breaks = breaks, pal = pal, add = TRUE, border = grey(border)) u = st_union(l) # print(u) if(print_geom) { plot(st_geometry(u), add = TRUE, col = NA, border = &#39;black&#39;, lwd = 2.5) } else { # not print geometry } } pl_stack = function(s, x, y, add = TRUE, nrM, imgY = 7, inner = 1) { # nrM is the timestep {1, 2, 3}, cause this function # prints all 4 bands at once attr(s, &quot;dimensions&quot;)[[1]]$offset = x attr(s, &quot;dimensions&quot;)[[2]]$offset = y # m = r[[1]][y + 1:nrow,x + 1:ncol,1] m &lt;- eval(parse(text=paste0(&quot;n&quot;, nrM))) s[[1]] = m[,c(imgY:1)] # turn around to have same orientation as flat plot plt(s, 0, TRUE, pal = purples) m &lt;- eval(parse(text=paste0(&quot;r&quot;, nrM))) s[[1]] = m[,c(imgY:1)] plt(s, 1*inner, TRUE, pal = reds) m &lt;- eval(parse(text=paste0(&quot;g&quot;, nrM))) s[[1]] = m[,c(imgY:1)] plt(s, 2*inner, TRUE, pal = greens) m &lt;- eval(parse(text=paste0(&quot;b&quot;, nrM))) s[[1]] = m[,c(imgY:1)] plt(s, 3*inner, TRUE, pal = blues) # li FALSE deleted } # flat plot function # prints any dummy stars with any single matrix to position pl = function(s, x, y, add = TRUE, randomize = FALSE, pal, m, print_geom = TRUE, border = .75, breaks = &quot;equal&quot;) { # m is matrix to replace image with # m &lt;- t(m) attr(s, &quot;dimensions&quot;)[[1]]$offset = x attr(s, &quot;dimensions&quot;)[[2]]$offset = y # print(m) s[[1]] = m plt(s, 0, add = TRUE, pal = pal, print_geom = print_geom, border = border, breaks = breaks) #plot(s, text_values = TRUE) } print_segments &lt;- function(x, y, seg, by = 1, lwd = 4, col = &quot;black&quot;) { seg = seg * by seg[,1] &lt;- seg[,1] + x seg[,3] &lt;- seg[,3] + x seg[,2] &lt;- seg[,2] + y seg[,4] &lt;- seg[,4] + y segments(seg[,1], seg[,2], seg[,3], seg[,4], lwd = lwd, col = col) } # time series ---- # from: cube1_ts_6x7_bigger.png offset = 26 plot.new() #par(mar = c(3, 2, 7, 2)) par(mar = c(0, 0, 0, 0)) #plot.window(xlim = c(10, 50), ylim = c(-3, 10), asp = 1) plot.window(xlim = c(-15, 75), ylim = c(-3, 10), asp = 1) pl_stack(s, 0, 0, nrM = 3) pl_stack(s, offset, 0, nrM = 2) pl_stack(s, 2 * offset, 0, nrM = 1) # po &lt;- matrix(c(0,-8,7,0,15,3.5, 0,1,1,5,5,14), ncol = 2) heads &lt;- matrix(c(3.5, 3.5 + offset, 3.5 + 2*offset, 14,14,14), ncol = 2) points(heads, pch = 16) # 4 or 16 segments(c(-8, 7, 0, 15), c(-1,-1,3,3), 3.5, 14) # first stack pyramid segments(c(-8, 7, 0, 15) + offset, c(-1,-1,3,3), 3.5 + offset, 14) # second stack pyramid segments(c(-8, 7, 0, 15) + 2*offset, c(-1,-1,3,3), 3.5 + 2*offset, 14) # third stack pyramid arrows(-13, 14, 72, 14, angle = 20, lwd = 2) # timeline text(7.5, 3.8, &quot;x&quot;, col = &quot;black&quot;) text(-10, -2.5, &quot;bands&quot;, srt = 90, col = &quot;black&quot;) text(-4.5, 1.8, &quot;y&quot;, srt = 27.5, col = &quot;black&quot;) y = 15.8 text(69, y, &quot;time&quot;, col = &quot;black&quot;) text(3.5, y, &quot;2020-10-01&quot;, col = &quot;black&quot;) text(3.5 + offset, y, &quot;2020-10-13&quot;, col = &quot;black&quot;) text(3.5 + 2*offset, y, &quot;2020-10-25&quot;, col = &quot;black&quot;) # flat ---- xlabels &lt;- seq(attr(warped, &quot;dimensions&quot;)[[1]]$offset + attr(warped, &quot;dimensions&quot;)[[1]]$delta / 2, length.out = attr(warped, &quot;dimensions&quot;)[[1]]$to, by = attr(warped, &quot;dimensions&quot;)[[1]]$delta) ylabels &lt;- seq(attr(warped, &quot;dimensions&quot;)[[2]]$offset + attr(warped, &quot;dimensions&quot;)[[2]]$delta / 2, length.out = attr(warped, &quot;dimensions&quot;)[[2]]$to, by = attr(warped, &quot;dimensions&quot;)[[2]]$delta) print_labels &lt;- function(x, y, off, lab, horizontal, cex = 1) { if(horizontal) { # x for(i in 0:(length(lab)-1)) { text(x + i*off, y, lab[i+1], cex = cex, srt = 90) } } else { # y lab &lt;- lab[length(lab):0] for(i in 0:(length(lab)-1)) { text(x, y + i*off, lab[i+1], cex = cex) } } } # before: width=1000, xlim(-2, 33), date labels x=31 plot.new() # par(mar = c(0,0,0,0)) par(mar = c(3,0,0,0)) plot.window(xlim = c(-2, 40), ylim = c(0, 25), asp = 1) pl(f, 7, 0, pal = blues, m = b1) pl(f, 7, 10, pal = blues, m = b2) pl(f, 7, 20, pal = blues, m = b3) pl(f, 14, 0, pal = greens, m = g1) pl(f, 14, 10, pal = greens, m = g2) pl(f, 14, 20, pal = greens, m = g3) pl(f, 21, 0, pal = reds, m = r1) pl(f, 21, 10, pal = reds, m = r2) pl(f, 21, 20, pal = reds, m = r3) pl(f, 28, 0, pal = purples, m = n1) pl(f, 28, 10, pal = purples, m = n2) pl(f, 28, 20, pal = purples, m = n3) print_labels(28.5, -2, 1, xlabels, horizontal = TRUE, cex = 0.7) print_labels(36, 0.5, 1, ylabels, horizontal = FALSE, cex = 0.7) # arrows(6, 27, 6, 0, angle = 20, lwd = 2) # text(5, 14, &quot;time&quot;, srt = 90, col = &quot;black&quot;) text(10, 28, &quot;blue&quot;, col = &quot;black&quot;) text(17, 28, &quot;green&quot;, col = &quot;black&quot;) text(24, 28, &quot;red&quot;, col = &quot;black&quot;) text(31, 28, &quot;nir&quot;, col = &quot;black&quot;) text(3, 23.5, &quot;2020-10-01&quot;, col = &quot;black&quot;) text(3, 13.5, &quot;2020-10-13&quot;, col = &quot;black&quot;) text(3, 3.5, &quot;2020-10-25&quot;, col = &quot;black&quot;) # filter ---- # mask &lt;- matrix(c(rep(NA, 26), 1,NA,1,NA,1,1,1, rep(NA, 9)), ncol = 7) mask &lt;- matrix(c(NA,NA,NA,NA,NA,NA, NA,NA,NA,NA,NA,NA, NA,NA,NA, 1, 1, 1, NA,NA, 1, 1, 1,NA, NA,NA,NA, 1, 1,NA, NA,NA,NA,NA,NA,NA, NA,NA,NA,NA,NA,NA), ncol = 7) print_grid &lt;- function(x, y) { pl(f, 0+x, 0+y, pal = blues, m = b1) pl(f, 0+x, 10+y, pal = blues, m = b2) pl(f, 0+x, 20+y, pal = blues, m = b3) pl(f, 7+x, 0+y, pal = greens, m = g1) pl(f, 7+x, 10+y, pal = greens, m = g2) pl(f, 7+x, 20+y, pal = greens, m = g3) pl(f, 14+x, 0+y, pal = reds, m = r1) pl(f, 14+x, 10+y, pal = reds, m = r2) pl(f, 14+x, 20+y, pal = reds, m = r3) pl(f, 21+x, 0+y, pal = purples, m = n1) pl(f, 21+x, 10+y, pal = purples, m = n2) pl(f, 21+x, 20+y, pal = purples, m = n3) } print_alpha_grid &lt;- function(x,y, alp = 0.2, geom = FALSE) { pl(f, 0+x, 0+y, pal = alpha(blues, alp), print_geom = geom, m = b1, border = 1) pl(f, 0+x, 10+y, pal = alpha(blues, alp), print_geom = geom, m = b2, border = 1) pl(f, 0+x, 20+y, pal = alpha(blues, alp), print_geom = geom, m = b3, border = 1) pl(f, 7+x, 0+y, pal = alpha(greens, alp), print_geom = geom, m = g1, border = 1) pl(f, 7+x, 10+y, pal = alpha(greens, alp), print_geom = geom, m = g2, border = 1) pl(f, 7+x, 20+y, pal = alpha(greens, alp), print_geom = geom, m = g3, border = 1) pl(f, 14+x, 0+y, pal = alpha(reds, alp), print_geom = geom, m = r1, border = 1) pl(f, 14+x, 10+y, pal = alpha(reds, alp), print_geom = geom, m = r2, border = 1) pl(f, 14+x, 20+y, pal = alpha(reds, alp), print_geom = geom, m = r3, border = 1) pl(f, 21+x, 0+y, pal = alpha(purples, alp), print_geom = geom, m = n1, border = 1) pl(f, 21+x, 10+y, pal = alpha(purples, alp), print_geom = geom, m = n2, border = 1) invisible(pl(f, 21+x, 20+y, pal = alpha(purples, alp), print_geom = geom, m = n3, border = 1)) } print_grid_filter &lt;- function(x, y) { pl(f, 0+x, 0+y, pal = blues, m = matrix(b1[mask == TRUE], ncol = 7)) pl(f, 0+x, 10+y, pal = blues, m = matrix(b2[mask == TRUE], ncol = 7)) pl(f, 0+x, 20+y, pal = blues, m = matrix(b3[mask == TRUE], ncol = 7)) pl(f, 7+x, 0+y, pal = greens, m = matrix(g1[mask == TRUE], ncol = 7)) pl(f, 7+x, 10+y, pal = greens, m = matrix(g2[mask == TRUE], ncol = 7)) pl(f, 7+x, 20+y, pal = greens, m = matrix(g3[mask == TRUE], ncol = 7)) pl(f, 14+x, 0+y, pal = reds, m = matrix(r1[mask == TRUE], ncol = 7)) pl(f, 14+x, 10+y, pal = reds, m = matrix(r2[mask == TRUE], ncol = 7)) pl(f, 14+x, 20+y, pal = reds, m = matrix(r3[mask == TRUE], ncol = 7)) pl(f, 21+x, 0+y, pal = purples, m = matrix(n1[mask == TRUE], ncol = 7)) pl(f, 21+x, 10+y, pal = purples, m = matrix(n2[mask == TRUE], ncol = 7)) pl(f, 21+x, 20+y, pal = purples, m = matrix(n3[mask == TRUE], ncol = 7)) } print_grid_time_filter &lt;- function(x, y) { # 3x1, 28x7 pl(f, 0+x, 10+y, pal = blues, m = b3) pl(f, 7+x, 10+y, pal = greens, m = g3) pl(f, 14+x, 10+y, pal = reds, m = r3) pl(f, 21+x, 10+y, pal = purples, m = n3) } print_grid_bands_filter &lt;- function(x, y, pal = greys) { # 1x3 6x27 pl(f, 0+x, 0+y, pal = pal, m = n1) pl(f, 0+x, 10+y, pal = pal, m = n2) pl(f, 0+x, 20+y, pal = pal, m = n3) } # build exactly like reduce plot.new() par(mar = c(3,3,3,3)) x = 120 y = 100 down = 0 plot.window(xlim = c(0, x), ylim = c(0, y), asp = 1) print_grid(x/2-28/2,y-27) print_alpha_grid((x/3-28)/2, 0-down) # alpha grid print_grid_time_filter((x/3-28)/2, -10-down) # select 3rd print_alpha_grid(x/3+((x/3-6)/2) -10.5, 0-down) # alpha grid print_grid_bands_filter(x/3+((x/3-12.4)), 0-down, pal = purples) print_alpha_grid(2*(x/3)+((x/3-28)/2), 0-down) # alpha grid print_grid_filter(2*(x/3)+((x/3-28)/2), 0-down) text(3, 13.5-down, &quot;time&quot;, srt = 90, col = &quot;black&quot;) text(43, 13.5-down, &quot;time&quot;, srt = 90, col = &quot;black&quot;) text(83, 13.5-down, &quot;time&quot;, srt = 90, col = &quot;black&quot;) text(20, 30, &quot;bands&quot;, col = &quot;black&quot;) text(60, 30, &quot;bands&quot;, col = &quot;black&quot;) text(100, 30, &quot;bands&quot;, col = &quot;black&quot;) arrows(x/2-28/2,y-30, x/6,32, angle = 20, lwd = 2) arrows(x/2,y-30, x/2,32, angle = 20, lwd = 2) arrows(x/2+28/2,y-30, 100, 32, angle = 20, lwd = 2) # points(seq(1,120,10), seq(1,120,10)) text(28.5,49, &quot;filter temporally&quot;, srt = 55.5, col = &quot;black&quot;, cex = 0.8) text(57,49, &quot;filter bands&quot;, srt = 90, col = &quot;black&quot;, cex = 0.8) text(91.5,49, &quot;filter spatially&quot;, srt = -55.5, col = &quot;black&quot;, cex = 0.8) print_labels(x = x/2-28/2 + 3, y = y+4, off = 7, lab = c(&quot;blue&quot;, &quot;green&quot;, &quot;red&quot;, &quot;nir&quot;), horizontal = TRUE, cex = 0.6) print_labels(x = x/2-28/2 - 9, y = y-23, off = 10, lab = c(&quot;2020-10-01&quot;, &quot;2020-10-13&quot;, &quot;2020-10-25&quot;), horizontal = FALSE, cex = 0.6) print_labels(x = x/2-28/2 + 21.5, y = y-30, off = 1, lab = xlabels, horizontal = TRUE, cex = 0.3) print_labels(x = x/2-28/2 + 30, y = y-26.5, off = 1, lab = ylabels, horizontal = FALSE, cex = 0.3) # apply ---- print_text = function(s, x, y, m) { # m &lt;- t(m) # transverse for correct order # print(m) r &lt;- rep(seq(0.5, 5.5, 1), 7) r &lt;- r + x # consider offsets u &lt;- c(rep(0.5, 6), rep(1.5, 6), rep(2.5, 6), rep(3.5, 6), rep(4.5, 6), rep(5.5, 6), rep(6.5, 6)) u &lt;- u + y # offset tab &lt;- matrix(c(r,u), nrow = 42, byrow = FALSE) # make point table for (i in 1:42) { #text(tab[i, 1], tab[i, 2], labels = paste0(&quot;&quot;, m[i]), cex = 1.1) text(tab[i, 1], tab[i, 2], labels = paste0(&quot;&quot;, m[i]), cex = 0.8) } } abs_brks &lt;- seq(-500,500, 50) abs_pal &lt;- sequential_hcl(n = 20, h1 = 211, c1 = 80, l1 = 30, l2 = 100, p1 = 1.2) #png(&quot;exp_apply_unary.png&quot;, width = 2400, height = 1000, pointsize = 24) #plot.new() #par(mar = c(2,2,2,2)) #x = 30 #y = 7.5 #plot.window(xlim = c(0, x), ylim = c(0, y), asp = 1) #pl(f, 3, 2, pal = abs_pal, m = b3 - 200, breaks = abs_brks) #pl(f, 1.5, .5, pal = abs_pal, m = b2 - 200, breaks = abs_brks) #pl(f, 0, -1, pal = abs_pal, m = b1 - 200, breaks = abs_brks) #print_text(s, 0, -1, m = b1 - 200) #pl(f, 23, 2, pal = abs_pal, m = abs(b3 - 200), breaks = abs_brks) #pl(f, 21.5, 0.5, pal = abs_pal, m = abs(b2 - 200), breaks = abs_brks) #pl(f, 20, -1, pal = abs_pal, m = abs(b1 - 200), breaks = abs_brks) #print_text(s, 20, -1, m = abs(b1 - 200)) #arrows(11, 4, 17.5, 4, lwd = 3) #text(14.3, 3.5, &quot;absolute()&quot;, cex = 1.4) #dev.off() vNeumann_seg &lt;- matrix(c(c(0,0,1,1,2,2,1,1,0,0,-1,-1), c(0,-1,-1,0,0,1,1,2,2,1,1,0), c(0,1,1,2,2,1,1,0,0,-1,-1,0), c(-1,-1,0,0,1,1,2,2,1,1,0,0)), ncol = 4) apply_filter &lt;- function(input, pad = TRUE, padValue = 1) { ras &lt;- raster::focal(raster::raster(input), w = matrix(c(0,0.2,0, 0.2,0.2,0.2, 0,0.2,0), ncol = 3), pad = pad, padValue = padValue) ras &lt;- raster::as.matrix(ras) ras[ras == &quot;NaN&quot;] &lt;- -999 return(floor(ras)) } brks &lt;- seq(0,1000, 50) plot.new() par(mar = c(0,2,0,0)) x = 30 y = 7.5 plot.window(xlim = c(0, x), ylim = c(0, y), asp = 1) pl(f, 3, 2, pal = blues, m = b3, breaks = brks) pl(f, 1.5, .5, pal = blues, m = b2, breaks = brks) pl(f, 0, -1, pal = blues, m = b1, breaks = brks) print_text(s, 0, -1, m = b1) # print text on left first stack print_segments(2, 3, seg = vNeumann_seg, lwd = 3) pl(f, 23, 2, pal = blues, m = apply_filter(b3), breaks = brks) pl(f, 21.5, 0.5, pal = blues, m = apply_filter(b2), breaks = brks) pl(f, 20, -1, pal = blues, m = apply_filter(b1), breaks = brks) print_text(s, 20, -1, m = apply_filter(b1)) # set pad = FALSE for -99 print_segments(22, 3, seg = vNeumann_seg, lwd = 3) arrows(11, 4, 17.5, 4, lwd = 3) text(14.3, 3.5, &quot;apply_kernel()&quot;, cex = 1.4) print_segments(13.8, 1, seg = vNeumann_seg, lwd = 3) cex = .8 text(14.3, 1.5, &quot;0.2&quot;, cex = cex) text(13.3, 1.5, &quot;0.2&quot;, cex = cex) text(15.3, 1.5, &quot;0.2&quot;, cex = cex) text(14.3, 2.5, &quot;0.2&quot;, cex = cex) text(14.3, .5, &quot;0.2&quot;, cex = cex) time_arrow_seg &lt;- matrix(c(c(-1.0, 0.3, 1.5, 2.7, 3.9, 5.1), c(-1.0, 0.3, 1.5, 2.7, 3.9, 5.1), c(-0.5, 0.7, 1.9, 3.1, 4.3, 5.6), c(-0.5, 0.7, 1.9, 3.1, 4.3, 5.6)), ncol = 4) time_arrow_flag_seg &lt;- matrix(c(c(-1.0, 1.5, 2.7, 3.9), c(-1.0, 1.5, 2.7, 3.9), c(0.7, 1.9, 3.1, 5.6), c(0.7, 1.9, 3.1, 5.6)), ncol = 4) b11 &lt;- b2 - t1 b12 &lt;- b1 - t2 + t1 # png(&quot;exp_apply_ts.png&quot;, width = 2400, height = 1000, pointsize = 24) plot.new() par(mar = c(2,2,2,2)) x = 30 y = 10 # 7.5 plot.window(xlim = c(0, x), ylim = c(0, y), asp = 1) pl(f, 4.8, 3.8, pal = blues, m = b3, breaks = brks) print_text(s, 4.8, 3.8, m = b3) pl(f, 3.6, 2.6, pal = blues, m = b11, breaks = brks) print_text(s, 3.6, 2.6, m = b11) pl(f, 2.4, 1.4, pal = blues, m = b12, breaks = brks) print_text(s, 2.4, 1.4, m = b12) pl(f, 1.2, .2, pal = blues, m = b2, breaks = brks) print_text(s, 1.2, .2, m = b2) pl(f, 0, -1, pal = blues, m = b1, breaks = brks) print_text(s, 0, -1, m = b1) # print text on left first stack pl(f, 24.8, 3.8, pal = alpha(greys, 0.1), m = matrix(rep(&quot;NA&quot;, 42), ncol = 7)) pl(f, 23.6, 2.6, pal = blues, m = (b12 + b11 + b3) / 3, breaks = brks) print_text(s, 23.6, 2.6, m = floor((b12 + b11 + b3) / 3)) pl(f, 22.4, 1.4, pal = blues, m = (b2 + b12 + b11) / 3, breaks = brks) print_text(s, 22.4, 1.4, m = floor((b2 + b12 + b11) / 3)) pl(f, 21.2, .2, pal = blues, m = (b1 + b2 + b12) / 3, breaks = brks) print_text(s, 21.2, .2, m = floor((b1 + b2 + b12) / 3)) pl(f, 20, -1, pal = alpha(greys, 0.1), m = matrix(rep(&quot;NA&quot;, 42), ncol = 7)) print_segments(5.7, 1.7, seg = time_arrow_seg, col = &quot;forestgreen&quot;) arrows(12.5, 9, 20, 9, lwd = 2) cex = .9 text(16.3, 8.3, &quot;apply_dimension(dimension = &#39;t&#39;)&quot;, cex = cex) print_segments(9.7, 1.7, time_arrow_seg, col = &quot;forestgreen&quot;) # draw ma explanation text(-0.5 + 10, -0.5 + 2, &quot;496&quot;, cex = cex) text(.7 + 10, .7 + 2, &quot;363&quot;, cex = cex) text(1.9 + 10, 1.9 + 2, &quot;658&quot;, cex = cex) text(3.1 + 10, 3.1 + 2, &quot;230&quot;, cex = cex) text(4.3 + 10, 4.3 + 2, &quot;525&quot;, cex = cex) t_formula &lt;- expression(&quot;t&quot;[n]*&quot; = (t&quot;[n-1]*&quot; + t&quot;[n]*&quot; + t&quot;[n+1]*&quot;) / 3&quot;) # text(13.8, 3, t_formula, srt = 45, cex = 1.2) text(14.4, 3.6, &quot;calculate moving average&quot;, srt = 45, cex = cex) arrows(15, 5.7, 18, 5.7, lwd = 2) print_segments(15.4, 1.7, seg = time_arrow_seg, col = &quot;forestgreen&quot;) # draw ma explanation text(-0.5 + 15.7, -0.5 + 2, &quot;NA&quot;, cex = cex) text(.7 + 15.7, .7 + 2, &quot;505&quot;, cex = cex) text(1.9 + 15.7, 1.9 + 2, &quot;417&quot;, cex = cex) text(3.1 + 15.7, 3.1 + 2, &quot;471&quot;, cex = cex) text(4.3 + 15.7, 4.3 + 2, &quot;NA&quot;, cex = cex) print_segments(25.7, 1.7, seg = time_arrow_seg, col = &quot;forestgreen&quot;) # reduce ---- # calc mean over time timeB &lt;- (b1 + b2 + b3) / 3 timeG &lt;- (g1 + g2 + g3) / 3 timeR &lt;- (r1 + r2 + r3) / 3 timeN &lt;- (n1 + n2 + n3) / 3 print_grid_time &lt;- function(x, y) { # 3x1, 28x7 pl(f, 0+x, 10+y, pal = blues, m = timeB) pl(f, 7+x, 10+y, pal = greens, m = timeG) pl(f, 14+x, 10+y, pal = reds, m = timeR) pl(f, 21+x, 10+y, pal = purples, m = timeN) } # calc ndvi ndvi1 &lt;- (n1 - r1) / (n1 + r1) ndvi2 &lt;- (n2 - r2) / (n2 + r2) ndvi3 &lt;- (n3 - r3) / (n3 + r3) print_grid_bands &lt;- function(x, y, pal = greys) { # 1x3 6x27 pl(f, 0+x, 0+y, pal = pal, m = ndvi1) pl(f, 0+x, 10+y, pal = pal, m = ndvi2) pl(f, 0+x, 20+y, pal = pal, m = ndvi3) } plte = function(s, x, y, add = TRUE, randomize = FALSE, pal, m) { attr(s, &quot;dimensions&quot;)[[1]]$offset = x attr(s, &quot;dimensions&quot;)[[2]]$offset = y # m = r[[1]][y + 1:nrow,x + 1:ncol,1] # dim(m) = c(x = nrow, y = ncol) # named dim # s[[1]] = m # me &lt;- floor(mean(s[[1]])) me &lt;- floor(mean(m)) if (me[1] &gt; 100) { # in case non-artificial grids with very high me &lt;- m / 10 # numbers are used, make them smaller me &lt;- floor(mean(me)) } text(x,y,me,cex = 0.8) } print_grid_spat &lt;- function(x, y) { x = x + 3 y = y + 3.5 plte(s, 0+x, 0+y, pal = blues, m = b1) plte(s, 0+x, 10+y, pal = blues, m = b2) plte(s, 0+x, 20+y, pal = blues, m = b3) plte(s, 7+x, 0+y, pal = greens, m = g1) plte(s, 7+x, 10+y, pal = greens, m = g2) plte(s, 7+x, 20+y, pal = greens, m = g3) plte(s, 14+x, 0+y, pal = reds, m = r1) plte(s, 14+x, 10+y, pal = reds, m = r2) plte(s, 14+x, 20+y, pal = reds, m = r3) plte(s, 21+x, 0+y, pal = purples, m = n1) plte(s, 21+x, 10+y, pal = purples, m = n2) plte(s, 21+x, 20+y, pal = purples, m = n3) } # png(&quot;exp_reduce.png&quot;, width = 1200, height = 1000, pointsize = 32) plot.new() #par(mar = c(3,3,3,3)) par(mar = c(3,0,2,0)) x = 120 y = 100 plot.window(xlim = c(0, x), ylim = c(0, y), asp = 1) print_grid(x/2-28/2,y-27) # print_alpha_grid((x/3-28)/2, 0) # alpha grid print_grid_time((x/3-28)/2, 0) # off = 5.5 # print_alpha_grid(x/3+((x/3-6)/2) -10.5, 0) # alpha grid print_grid_bands(x/3+((x/3-6)/2), 0) print_alpha_grid(2*(x/3)+((x/3-28)/2), 0, alp = 0, geom = TRUE) # alpha grid print_grid_spat(2*(x/3)+((x/3-28)/2), 0) text(3, 13.5, &quot;time&quot;, srt = 90, col = &quot;black&quot;) #segments(3.6, 8, 3.7, 19, col = &quot;red&quot;, lwd=3) segments(3.4, 8, 3.4, 19, col = &quot;red&quot;, lwd=3) text(43, 13.5, &quot;time&quot;, srt = 90, col = &quot;black&quot;) text(83, 13.5, &quot;time&quot;, srt = 90, col = &quot;black&quot;) text(20, 30, &quot;bands&quot;, col = &quot;black&quot;) text(60, 30, &quot;bands&quot;, col = &quot;black&quot;) segments(53,29.8, 67,29.8, col = &quot;red&quot;, lwd=3) text(100, 30, &quot;bands&quot;, col = &quot;black&quot;) text(30, 7, &quot;x&quot;, col = &quot;black&quot;) text(36, 13, &quot;y&quot;, col = &quot;black&quot;) text(60, -3, &quot;x&quot;, col = &quot;black&quot;) text(66, 3, &quot;y&quot;, col = &quot;black&quot;) text(110, -3, &quot;x&quot;, col = &quot;black&quot;) text(116, 3, &quot;y&quot;, col = &quot;black&quot;) segments(108,-2.4, 112,-3.2, col = &quot;red&quot;, lwd=3) segments(114,3.2, 118,2.4, col = &quot;red&quot;, lwd=3) text(60, y+4, &quot;bands&quot;, col = &quot;black&quot;) # dim names on main text(43, y-14, &quot;time&quot;, srt = 90, col = &quot;black&quot;) text(x/2-28/2 + 24, y-30, &quot;x&quot;, col = &quot;black&quot;) text(x/2-28/2 + 30, y-24, &quot;y&quot;, col = &quot;black&quot;) arrows(x/2-28/2,y-30, x/6,32, angle = 20, lwd = 2) arrows(x/2,y-30, x/2,32, angle = 20, lwd = 2) arrows(x/2+28/2,y-30, 100, 32, angle = 20, lwd = 2) # points(seq(1,120,10), seq(1,120,10)) text(28.5,49, &quot;reduce temporally&quot;, srt = 55.5, col = &quot;black&quot;, cex = 0.8) text(57,49, &quot;reduce bands&quot;, srt = 90, col = &quot;black&quot;, cex = 0.8) text(91.5,49, &quot;reduce spatially&quot;, srt = -55.5, col = &quot;black&quot;, cex = 0.8) # aggregate ---- mask_agg &lt;- matrix(c(NA,NA,NA,NA,NA,NA, NA, 1, 1,NA,NA,NA, 1, 1,NA,NA, 1,NA, 1,NA,NA, 1, 1,NA, 1,NA,NA, 1, 1,NA, 1,NA,NA,NA, 1,NA, NA,NA,NA,NA,NA,NA), ncol = 7) pl_stack_agg = function(s, x, y, add = TRUE, nrM, imgY = 7, inner = 1) { # pl_stack that masks the added matrices # nrM is the timestep {1, 2, 3}, cause this function # prints all 4 bands at once attr(s, &quot;dimensions&quot;)[[1]]$offset = x attr(s, &quot;dimensions&quot;)[[2]]$offset = y # m = r[[1]][y + 1:nrow,x + 1:ncol,1] m &lt;- eval(parse(text=paste0(&quot;n&quot;, nrM))) m &lt;- matrix(m[mask_agg == TRUE], ncol = 7) s[[1]] = m[,c(imgY:1)] # turn around to have same orientation as flat plot plt(s, 0, TRUE, pal = purples) m &lt;- eval(parse(text=paste0(&quot;r&quot;, nrM))) m &lt;- matrix(m[mask_agg == TRUE], ncol = 7) s[[1]] = m[,c(imgY:1)] plt(s, 1*inner, TRUE, pal = reds) m &lt;- eval(parse(text=paste0(&quot;g&quot;, nrM))) m &lt;- matrix(m[mask_agg == TRUE], ncol = 7) s[[1]] = m[,c(imgY:1)] plt(s, 2*inner, TRUE, pal = greens) m &lt;- eval(parse(text=paste0(&quot;b&quot;, nrM))) m &lt;- matrix(m[mask_agg == TRUE], ncol = 7) s[[1]] = m[,c(imgY:1)] plt(s, 3*inner, TRUE, pal = blues) # li FALSE } polygon_1 &lt;- matrix(c(c(0.0, 5.1, 4.9,-2.3), c(0.0, 2.4, 3.1, 1.8), c(5.1, 4.9,-2.3, 0.0), c(2.4, 3.1, 1.8, 0.0)), ncol = 4) a &lt;- make_dummy_stars(6, 7, 5, -1.14, -2.28) print_vector_content &lt;- function(x, y, cex = 0.8) { vec &lt;- floor(rnorm(8, 250, 100)) text( 0 + x,12 + y, vec[1], cex = cex) text( 0 + x, 8 + y, vec[2], cex = cex) text( 0 + x, 4 + y, vec[3], cex = cex) text( 0 + x, 0 + y, vec[4], cex = cex) text( 12 + x,12 + y, vec[5], cex = cex) text( 12 + x, 8 + y, vec[6], cex = cex) text( 12 + x, 4 + y, vec[7], cex = cex) text( 12 + x, 0 + y, vec[8], cex = cex) } print_ts &lt;- function(off2, yoff2) { pl_stack(s, 0 + off2, yoff2, nrM = 3) # input 2 pl_stack(s, off + off2, yoff2, nrM = 2) pl_stack(s, 2 * off + off2, yoff2, nrM = 1) arrows(-13 + off2, 14 + yoff2, 72 + off2, 14 + yoff2, angle = 20, lwd = 2) # timeline heads &lt;- matrix(c(3.5+off2, 3.5 + off + off2, 3.5 + 2*off + off2, 14+yoff2,14+yoff2,14+yoff2), ncol = 2) points(heads, pch = 16) # 4 or 16 segments(c(-8, 7, 0, 15)+off2, c(-1,-1,3,3)+yoff2, 3.5+off2, 14+yoff2) # first stack pyramid segments(c(-8, 7, 0, 15) + off + off2, c(-1,-1,3,3)+yoff2, 3.5 + off + off2, 14+yoff2) # second stack pyramid segments(c(-8, 7, 0, 15) + 2*off + off2, c(-1,-1,3,3)+yoff2, 3.5 + 2*off + off2, 14+yoff2) # third stack pyramid text(7.5+off2, 4.3+yoff2, &quot;x&quot;, col = &quot;black&quot;, cex = secText) text(-9.5+off2, -2.5+yoff2, &quot;bands&quot;, srt = 90, col = &quot;black&quot;, cex = secText) text(-4.5+off2, 2+yoff2, &quot;y&quot;, srt = 27.5, col = &quot;black&quot;, cex = secText) text(69+off2, 15.5+yoff2+1, &quot;time&quot;, col = &quot;black&quot;) text(3.5+off2, 15.5+yoff2, &quot;2020-10-01&quot;, col = &quot;black&quot;) text(3.5 + off + off2, 15.5+yoff2, &quot;2020-10-13&quot;, col = &quot;black&quot;) text(3.5 + 2*off + off2, 15.5+yoff2, &quot;2020-10-25&quot;, col = &quot;black&quot;) } secText = 0.8 # secondary text size (dimension naming) off = 26 # image stacks are always 26 apart x = 72 # png X y = 48 # png Y yoff = 30 plot.new() par(mar = c(5,3,3,3)) plot.window(xlim = c(-1, x), ylim = c(0, y), asp = 1) print_ts(5, yoff) col = &#39;#ff5555&#39; print_segments(10.57, yoff-.43, seg = polygon_1, col = col) x = 3 segments(c(2, 0, -1.3)+x, c(-.2, 0, 1)+yoff, c(0, -1.3, 1)+x, c(0, 1, 2)+yoff, lwd = 4, col = col) print_segments(10.57+off, yoff-.43, seg = polygon_1, col = col) x = 3 + off segments(c(2, 0, -1.3)+x, c(-.2, 0, 1)+yoff, c(0, -1.3, 1)+x, c(0, 1, 2)+yoff, lwd = 4, col = col) print_segments(10.57+2*off, yoff-.43, seg = polygon_1, col = col) x = 3 + 2 * off segments(c(2, 0, -1.3)+x, c(-.2, 0, 1)+yoff, c(0, -1.3, 1)+x, c(0, 1, 2)+yoff, lwd = 4, col = col) # old 75 28 poly 86.25, 27.15 pl_stack_agg(a, 5, 5, nrM = 1, inner = 3) # print masked enlargement print_segments(16.25, 7.15, seg = polygon_1, by = 2, col = col) x &lt;- 1 y &lt;- 8 segments(c(4, 0, -2.6)+x, c(-.4, 0, 2)+y, c(0, -2.6, 2)+x, c(0, 2, 4)+y, lwd = 4, col = col) # line in large segments(-3, 25, -7, 9, lwd = 3, col = &#39;grey&#39;) segments(21, 29, 27.5, 14, lwd = 3, col = &#39;grey&#39;) text(10, 20, &quot;1. Group by geometry&quot;, cex = 1.3) vecM &lt;- matrix(rep(1,8), ncol = 2) text(57, 21, &quot;2. Reduce to vector cube&quot;, cex = 1.3) b &lt;- make_dummy_stars(2, 4, 12, 4, 0) pl(b, 48, -5, m = vecM, pal = alpha(&quot;white&quot;, 0.9), border = 0) print_vector_content(54, -3) pl(b, 46.5, -3.5, m = vecM, pal = alpha(&quot;white&quot;, 0.9), border = 0) print_vector_content(52.5, -1.5) pl(b, 45, -2, m = vecM, pal = alpha(&quot;white&quot;, 0.9), border = 0) print_vector_content(51, 0) text(51.5, 15, &quot;Line_1&quot;, col = col) text(63, 15, &quot;Polygon_1&quot;, col = col) text(57, 17.5, &quot;Geometries&quot;, cex = 1.1) text(42, 12, &quot;blue&quot;) text(42, 8, &quot;green&quot;) text(42, 4, &quot;red&quot;) text(42, 0, &quot;nir&quot;) text(38, 6, &quot;Bands&quot;, srt = 90, cex = 1.1) # arrows(13.5, -2, 13.5, -6, angle = 20, lwd = 3) text(72, 15.5, &quot;time&quot;, srt = 315, cex = 1.1) arrows(69.5, 15, 72.5, 12, angle = 20, lwd = 2) # print_segments(30, 35, seg = arrow_seg) set.seed(1331) library(stars) library(colorspace) tif = system.file(&quot;tif/L7_ETMs.tif&quot;, package = &quot;stars&quot;) r = read_stars(tif) nrow = 5 ncol = 8 #m = matrix(runif(nrow * ncol), nrow = nrow, ncol = ncol) m = r[[1]][1:nrow,1:ncol,1] dim(m) = c(x = nrow, y = ncol) # named dim s = st_as_stars(m) # s attr(s, &quot;dimensions&quot;)[[1]]$delta = 3 attr(s, &quot;dimensions&quot;)[[2]]$delta = -.5 attr(attr(s, &quot;dimensions&quot;), &quot;raster&quot;)$affine = c(-1.2, 0.0) plt = function(x, yoffset = 0, add, li = TRUE) { attr(x, &quot;dimensions&quot;)[[2]]$offset = attr(x, &quot;dimensions&quot;)[[2]]$offset + yoffset l = st_as_sf(x, as_points = FALSE) pal = sf.colors(10) if (li) pal = lighten(pal, 0.3 + rnorm(1, 0, 0.1)) if (! add) plot(l, axes = FALSE, breaks = &quot;equal&quot;, pal = pal, reset = FALSE, border = grey(.75), key.pos = NULL, main = NULL, xlab = &quot;time&quot;) else plot(l, axes = TRUE, breaks = &quot;equal&quot;, pal = pal, add = TRUE, border = grey(.75)) u = st_union(l) plot(st_geometry(u), add = TRUE, col = NA, border = &#39;black&#39;, lwd = 2.5) } pl = function(s, x, y, add = TRUE, randomize = FALSE) { attr(s, &quot;dimensions&quot;)[[1]]$offset = x attr(s, &quot;dimensions&quot;)[[2]]$offset = y m = r[[1]][y + 1:nrow,x + 1:ncol,1] if (randomize) m = m[sample(y + 1:nrow),x + 1:ncol] dim(m) = c(x = nrow, y = ncol) # named dim s[[1]] = m plt(s, 0, add) plt(s, 1, TRUE) plt(s, 2, TRUE) plt(s, 3, TRUE) plt(s, 4, TRUE) plt(s, 5, TRUE) plt(s, 6, TRUE) plt(s, 7, TRUE) plt(s, 8, TRUE, FALSE) } # point vector data cube: plot.new() par(mar = c(5, 0, 5, 0)) plot.window(xlim = c(-10, 16), ylim = c(-2,12), asp = 1) library(spacetime) data(air) de = st_geometry(st_normalize(st_as_sf(DE))) # pl(s, 0, 0, TRUE, randomize = TRUE) de = de * 6 + c(-7, 9) plot(de, add = TRUE, border = grey(.5)) text(-10, 0, &quot;time&quot;, srt = -90, col = &#39;black&#39;) text(-5, 7.5, &quot;sensor location&quot;, srt = 25, col = &#39;black&#39;) text( 7, 10.5, &quot;air quality parameter&quot;, srt = 0, col = &#39;black&#39;) text( 1.5, 8.5, expression(PM[10]), col = &#39;black&#39;, cex = .75) text( 4.5, 8.5, expression(NO[x]), col = &#39;black&#39;, cex = .75) text( 8, 8.5, expression(SO[4]), col = &#39;black&#39;, cex = .75) text( 11, 8.5, expression(O[3]), col = &#39;black&#39;, cex = .75) text( 14, 8.5, expression(CO), col = &#39;black&#39;, cex = .75) # location points: p = st_coordinates(s[,1]) p[,1] = p[,1]-1.4 p[,2] = p[,2] + 8.2 points(p, col = grey(.7), pch = 16) # centroids: set.seed(131) cent = st_coordinates(st_sample(de, 8)) points(cent, col = grey(.7), pch = 16) cent = cent[rev(order(cent[,1])),] seg = cbind(p, cent[1:8,]) segments(seg[,1], seg[,2], seg[,3], seg[,4], col = &#39;grey&#39;) set.seed(1014) options(digits = 3) knitr::opts_chunk$set( comment = &quot;#&quot;, collapse = knitr::is_latex_output(), cache = TRUE, # out.width = &quot;70%&quot;, fig.align = &#39;center&#39; # fig.width = 6, # fig.asp = 0.618, # 1 / phi # fig.show = &quot;hold&quot; ) options(dplyr.print_min = 6, dplyr.print_max = 6) options(stars.crs = 17) mapview::mapviewOptions(fgb = FALSE) # for units: (?) Sys.setenv(UDUNITS2_XML_PATH=&quot;&quot;) if (knitr::is_latex_output()) options(width = 66) #options(width = 72) library(sf) p1 = st_point(c(7.35, 52.42)) p2 = st_point(c(7.22, 52.18)) p3 = st_point(c(7.44, 52.19)) sfc = st_sfc(list(p1, p2, p3), crs = &#39;OGC:CRS84&#39;) st_sf(elev = c(33.2, 52.1, 81.2), marker = c(&quot;Id01&quot;, &quot;Id02&quot;, &quot;Id03&quot;), geom = sfc) knitr::include_graphics(&quot;images/sf_obj.png&quot;) library(sf) (file = system.file(&quot;gpkg/nc.gpkg&quot;, package=&quot;sf&quot;)) nc = st_read(file) st_layers(file) (file = tempfile(fileext = &quot;.gpkg&quot;)) st_write(nc, file, layer = &quot;layer_nc&quot;) nc[2:5, 3:7] nc5 = nc[1:5, ] nc7 = nc[1:7, ] (i = st_intersects(nc5, nc7)) plot(st_geometry(nc7)) plot(st_geometry(nc5), add = TRUE, border = &quot;brown&quot;) cc = st_coordinates(st_centroid(nc7)) text(cc, labels=1:nrow(nc7), col=&quot;blue&quot;) as.matrix(i) lengths(i) lengths(t(i)) methods(class = &quot;sgbp&quot;) library(tidyverse) nc %&gt;% as_tibble() %&gt;% select(BIR74) %&gt;% head(3) sf_use_s2(TRUE) orange &lt;- nc %&gt;% filter(NAME == &quot;Orange&quot;) wd = st_is_within_distance(nc, orange, units::set_units(50, km)) o50 &lt;- nc %&gt;% filter(lengths(wd) &gt; 0) nrow(o50) og = st_geometry(orange) plot(st_geometry(o50), lwd = 2) plot(og, col = &#39;orange&#39;, add = TRUE) plot(st_buffer(og, units::set_units(50, km)), add = TRUE, col = NA, border = &#39;brown&#39;) plot(st_geometry(nc), add = TRUE, border = &#39;grey&#39;) # example of largest = TRUE: nc &lt;- st_transform(read_sf(system.file(&quot;shape/nc.shp&quot;, package=&quot;sf&quot;)), 2264) gr = st_sf( label = apply(expand.grid(1:10, LETTERS[10:1])[,2:1], 1, paste0, collapse = &quot; &quot;), geom = st_make_grid(nc)) gr$col = sf.colors(10, categorical = TRUE, alpha = .3) # cut, to check, NA&#39;s work out: gr = gr[-(1:30),] suppressWarnings(nc_j &lt;- st_join(nc, gr, largest = TRUE)) # the two datasets: opar = par(mfrow = c(2,1), mar = rep(0,4)) plot(st_geometry(nc_j)) plot(st_geometry(gr), add = TRUE, col = gr$col) text(st_coordinates(st_centroid(st_geometry(gr))), labels = gr$label) # the joined dataset: plot(st_geometry(nc_j), border = &#39;black&#39;, col = nc_j$col) text(st_coordinates(st_centroid(st_geometry(nc_j))), labels = nc_j$label, cex = .8) plot(st_geometry(gr), border = &#39;green&#39;, add = TRUE) par(opar) tif = system.file(&quot;tif/L7_ETMs.tif&quot;, package = &quot;stars&quot;) library(stars) (r = read_stars(tif)) length(r) class(r[[1]]) dim(r[[1]]) st_dimensions(r) st_bbox(r) tf = tempfile(fileext = &quot;.tif&quot;) write_stars(r, tf) st_drivers(&quot;raster&quot;) r[,1:100, seq(1, 250, 5), 4] %&gt;% dim() r[,1:100, seq(1, 250, 5), 4, drop = TRUE] %&gt;% dim() library(dplyr, warn.conflicts = FALSE) filter(r, x &gt; 289000, x &lt; 290000) slice(r, band, 3) b = st_bbox(r) %&gt;% st_as_sfc() %&gt;% st_centroid() %&gt;% st_buffer(units::set_units(500, m)) r[b] plot(r[b][,,,1], reset = FALSE) plot(b, border = &#39;brown&#39;, lwd = 2, col = NA, add = TRUE) r[b] %&gt;% st_normalize() %&gt;% st_dimensions() r[b, crop = FALSE] st_crop(r, b) aperm(r, c(3, 1, 2)) (rs = split(r)) merge(rs) st_redimension(r, c(349, 352, 3, 2)) set.seed(115517) pts = st_bbox(r) %&gt;% st_as_sfc() %&gt;% st_sample(20) (e = st_extract(r, pts)) plot(r[,,,1], reset = FALSE) col = rep(&quot;green&quot;, 20) col[c(8, 14, 15, 18, 19)] = &quot;red&quot; st_as_sf(e) %&gt;% st_coordinates() %&gt;% text(labels = 1:20, col = col) rs = split(r) trn = st_extract(rs, pts) trn$cls = rep(&quot;land&quot;, 20) trn$cls[c(8, 14, 15, 18, 19)] = &quot;water&quot; model = MASS::lda(cls ~ ., st_set_geometry(trn, NULL)) pr = predict(rs, model) plot(pr[1], key.pos = 4, key.width = lcm(3.5), key.length = lcm(2)) plot(r) par(mfrow = c(1, 2)) plot(r, rgb = c(3,2,1), reset = FALSE, main = &quot;RGB&quot;) # rgb plot(r, rgb = c(4,3,2), main = &quot;False color (NIR-R-G)&quot;) # false color log(r) r + 2 * log(r) r2 = r r2[r &lt; 50] = NA r2 r2[is.na(r2)] = 0 r2 st_apply(r, c(&quot;x&quot;, &quot;y&quot;), mean) ndvi = function(b1, b2, b3, b4, b5, b6) (b4 - b3)/(b4 + b3) st_apply(r, c(&quot;x&quot;, &quot;y&quot;), ndvi) ndvi2 = function(x) (x[4]-x[3])/(x[4]+x[3]) as.data.frame(st_apply(r, c(&quot;band&quot;), mean)) st_apply(r, c(&quot;band&quot;), quantile, c(.25, .5, .75)) st_apply(r, c(&quot;x&quot;, &quot;y&quot;), quantile, c(.25, .5, .75)) options(&quot;rgdal_show_exportToProj4_warnings&quot;=&quot;none&quot;) # led to: # Warning in sp::proj4string(obj): CRS object has comment, which is # lost in output library(spacetime) data(air) # this loads several datasets in .GlobalEnv dim(air) d = st_dimensions(station = st_as_sfc(stations), time = dates) (aq = st_as_stars(list(PM10 = air), dimensions = d)) image(aperm(log(aq), 2:1), main = &quot;NA pattern (white) in PM10 station time series&quot;) plot(st_as_sf(st_apply(aq, 1, mean, na.rm = TRUE)), reset = FALSE, pch = 16, ylim = st_bbox(DE)[c(2,4)]) plot(DE, add=TRUE) (a = aggregate(aq, st_as_sf(DE_NUTS1), mean, na.rm = TRUE)) library(tidyverse) a %&gt;% filter(time &gt;= &quot;2008-01-01&quot;, time &lt; &quot;2008-01-07&quot;) %&gt;% plot(key.pos = 4) suppressPackageStartupMessages(library(xts)) plot(as.xts(a)[,4], main = DE_NUTS1$NAME_1[4]) library(spDataLarge) plot(st_geometry(bristol_zones), axes = TRUE, graticule = TRUE) plot(st_geometry(bristol_zones)[33], col = &#39;red&#39;, add = TRUE) head(bristol_od) nrow(bristol_zones)^2 # all combinations nrow(bristol_od) # non-zero combinations # create O-D-mode array: bristol_tidy &lt;- bristol_od %&gt;% select(-all) %&gt;% gather(&quot;mode&quot;, &quot;n&quot;, -o, -d) head(bristol_tidy) od = bristol_tidy %&gt;% pull(&quot;o&quot;) %&gt;% unique() nod = length(od) mode = bristol_tidy %&gt;% pull(&quot;mode&quot;) %&gt;% unique() nmode = length(mode) a = array(0L, c(nod, nod, nmode), dimnames = list(o = od, d = od, mode = mode)) dim(a) a[as.matrix(bristol_tidy[c(&quot;o&quot;, &quot;d&quot;, &quot;mode&quot;)])] = bristol_tidy$n order = match(od, bristol_zones$geo_code) # it happens this equals 1:102 zones = st_geometry(bristol_zones)[order] library(stars) (d = st_dimensions(o = zones, d = zones, mode = mode)) (odm = st_as_stars(list(N = a), dimensions = d)) plot(odm[,,33] + 1, logz = TRUE) d = st_apply(odm, 2, sum) which.max(d[[1]]) st_apply(odm, 1:2, sum) st_apply(odm, c(1,3), sum) st_apply(odm, c(2,3), sum) o = st_apply(odm, 1, sum) d = st_apply(odm, 2, sum) x = (c(o, d, along = list(od = c(&quot;origin&quot;, &quot;destination&quot;)))) plot(x, logz = TRUE) library(units) a = set_units(st_area(st_as_sf(o)), km^2) o$sum_km = o$sum / a d$sum_km = d$sum / a od = c(o[&quot;sum_km&quot;], d[&quot;sum_km&quot;], along = list(od = c(&quot;origin&quot;, &quot;destination&quot;))) plot(od, logz = TRUE) sf_use_s2(TRUE) (file = system.file(&quot;gpkg/nc.gpkg&quot;, package=&quot;sf&quot;)) read_sf(file) %&gt;% st_geometry() %&gt;% st_as_stars() %&gt;% plot() library(dplyr) read_sf(file) %&gt;% mutate(name = as.factor(NAME)) %&gt;% select(SID74, SID79, name) %&gt;% st_rasterize() read_sf(file) %&gt;% st_cast(&quot;MULTILINESTRING&quot;) %&gt;% select(CNTY_ID) %&gt;% st_rasterize() %&gt;% plot() x = st_crs(&quot;OGC:CRS84&quot;) x$proj4string sf_proj_search_paths() paste0(tail(sf_proj_search_paths(), 1), .Platform$file.sep, &quot;proj.db&quot;) sf_extSoftVersion()[&quot;PROJ&quot;] sf_proj_network() sf_proj_network(TRUE) list.files(sf_proj_search_paths()[1], full.names = TRUE) (p = sf_proj_pipelines(&quot;EPSG:4326&quot;, &quot;EPSG:22525&quot;)) sf_proj_network(FALSE) sf_proj_pipelines(&quot;EPSG:4326&quot;, &quot;EPSG:22525&quot;) names(p) p$accuracy p[is.na(p$accuracy),] tif = system.file(&quot;tif/L7_ETMs.tif&quot;, package = &quot;stars&quot;) read_stars(tif) %&gt;% st_transform(4326) read_stars(tif) %&gt;% st_warp(crs = st_crs(4326)) %&gt;% st_dimensions() r = read_stars(tif) grd = st_bbox(r) %&gt;% st_as_sfc() %&gt;% st_transform(4326) %&gt;% st_bbox() %&gt;% st_as_stars(nx = dim(r)[&quot;x&quot;], ny = dim(r)[&quot;y&quot;]) st_warp(r, grd) set.seed(1014) options(digits = 3) knitr::opts_chunk$set( comment = &quot;#&quot;, collapse = knitr::is_latex_output(), cache = TRUE, # out.width = &quot;70%&quot;, fig.align = &#39;center&#39; # fig.width = 6, # fig.asp = 0.618, # 1 / phi # fig.show = &quot;hold&quot; ) options(dplyr.print_min = 6, dplyr.print_max = 6) options(stars.crs = 17) mapview::mapviewOptions(fgb = FALSE) # for units: (?) Sys.setenv(UDUNITS2_XML_PATH=&quot;&quot;) if (knitr::is_latex_output()) options(width = 66) #options(width = 72) library(sf) (file = system.file(&quot;gpkg/nc.gpkg&quot;, package=&quot;sf&quot;)) bb = &quot;POLYGON ((-81.7 36.2, -80.4 36.2, -80.4 36.5, -81.7 36.5, -81.7 36.2))&quot; nc.1 = st_read(file, wkt_filter = bb) q = paste(&quot;select BIR74,SID74,geom from &#39;nc.gpkg&#39; where BIR74 &gt; 1500&quot;) nc.2 = st_read(file, query = q) q = paste(&quot;select BIR74,SID74,geom from &#39;nc.gpkg&#39; LIMIT 10 OFFSET 50&quot;) nc.2 = st_read(file, query = q) has_PG &lt;- any(&quot;PostgreSQL&quot; %in% st_drivers()$name) nc = read_sf(file) write_sf(nc, &quot;PG:dbname=postgis&quot;, &quot;nc&quot;) pg &lt;- DBI::dbConnect( RPostgres::Postgres(), host = &quot;localhost&quot;, dbname = &quot;postgis&quot;) st_read(pg, query = &quot;select BIR74,wkb_geometry from nc limit 3&quot;) q = &quot;SELECT BIR74,wkb_geometry FROM nc WHERE \\ ST_Intersects(wkb_geometry, &#39;SRID=4267;POINT (-81.49826 36.4314)&#39;);&quot; st_read(pg, query = q) library(dplyr, warn.conflicts = FALSE) nc_db = tbl(pg, &quot;nc&quot;) nc_db %&gt;% filter(ST_Intersects(wkb_geometry, &#39;SRID=4267;POINT (-81.49826 36.4314)&#39;)) %&gt;% collect() nc_db %&gt;% filter(ST_Area(wkb_geometry) &gt; 0.1) %&gt;% head(3) download.file( &quot;https://openstreetmap.org/api/0.6/map?bbox=7.595,51.969,7.598,51.970&quot; &quot;data/ms.osm&quot;, method = &quot;auto&quot;) o = read_sf(&quot;data/ms.osm&quot;, &quot;lines&quot;) p = read_sf(&quot;data/ms.osm&quot;, &quot;multipolygons&quot;) bb = st_bbox(c(xmin=7.595, ymin = 51.969, xmax = 7.598, ymax = 51.970), crs = 4326) plot(st_as_sfc(bb), axes = TRUE, lwd = 2, lty = 2, cex.axis = .5) plot(o[,1], lwd = 2, add = TRUE) plot(st_geometry(p), border = NA, col = &#39;#88888888&#39;, add = TRUE) install.packages(&quot;starsdata&quot;, repos = &quot;http://pebesma.staff.ifgi.de&quot;, type = &quot;source&quot;) f = &quot;sentinel/S2A_MSIL1C_20180220T105051_N0206_R051_T32ULE_20180221T134037.zip&quot; granule = system.file(file = f, package = &quot;starsdata&quot;) file.size(granule) base_name = strsplit(basename(granule), &quot;.zip&quot;)[[1]] s2 = paste0(&quot;SENTINEL2_L1C:/vsizip/&quot;, granule, &quot;/&quot;, base_name, &quot;.SAFE/MTD_MSIL1C.xml:10m:EPSG_32632&quot;) (p = read_stars(s2, proxy = TRUE)) object.size(p) p2 = p * 2 plot(p) (ds = floor(sqrt(prod(dim(p)) / prod(dev.size(&quot;px&quot;))))) methods(class = &quot;stars_proxy&quot;) set.seed(1014) options(digits = 3) knitr::opts_chunk$set( comment = &quot;#&quot;, collapse = knitr::is_latex_output(), cache = TRUE, # out.width = &quot;70%&quot;, fig.align = &#39;center&#39; # fig.width = 6, # fig.asp = 0.618, # 1 / phi # fig.show = &quot;hold&quot; ) options(dplyr.print_min = 6, dplyr.print_max = 6) options(stars.crs = 17) mapview::mapviewOptions(fgb = FALSE) # for units: (?) Sys.setenv(UDUNITS2_XML_PATH=&quot;&quot;) if (knitr::is_latex_output()) options(width = 66) #options(width = 72) library(sf) library(rnaturalearth) #sf_use_s2(FALSE) # FIXME: w &lt;- ne_countries(scale = &quot;medium&quot;, returnclass = &quot;sf&quot;) suppressWarnings(st_crs(w) &lt;- st_crs(4326)) layout(matrix(1:2, 1, 2), c(2,1)) par(mar = rep(0, 4)) plot(st_geometry(w)) # sphere: library(s2) g = as_s2_geography(TRUE) # Earth co = s2_data_countries() oc = s2_difference(g, s2_union_agg(co)) # oceans b = s2_buffer_cells(as_s2_geography(&quot;POINT(-30 -10)&quot;), 9800000) # visible half i = s2_intersection(b, oc) # visible ocean co = s2_intersection(b, co) plot(st_transform(st_as_sfc(i), &quot;+proj=ortho +lat_0=-10 +lon_0=-30&quot;), col = &#39;lightblue&#39;) plot(st_transform(st_as_sfc(co), &quot;+proj=ortho +lat_0=-10 +lon_0=-30&quot;), col = NA, add = TRUE) library(sf) library(rnaturalearth) w &lt;- ne_countries(scale = &quot;medium&quot;, returnclass = &quot;sf&quot;) plot(st_geometry(w)) st_is_longlat(w) DE = st_geometry(ne_countries(country = &quot;germany&quot;, returnclass = &quot;sf&quot;)) DE.eqc = st_transform(DE, &quot;+proj=eqc +lat_ts=51.14 +lon_0=90w&quot;) print(mean(st_bbox(DE)[c(&quot;ymin&quot;, &quot;ymax&quot;)]), digits = 4) par(mfrow = c(1, 2)) plot(DE, axes = TRUE) plot(DE.eqc, axes = TRUE) library(classInt) # set.seed(1) needed ? r = rnorm(100) (cI &lt;- classIntervals(r)) cI$brks sf_use_s2(TRUE) library(sf) nc = read_sf(system.file(&quot;gpkg/nc.gpkg&quot;, package=&quot;sf&quot;)) plot(nc[&quot;BIR74&quot;], reset = FALSE, key.pos = 4) plot(st_buffer(nc[1,1], units::set_units(10, km)), col = &#39;NA&#39;, border = &#39;red&#39;, lwd = 2, add = TRUE) library(stars) r = read_stars(system.file(&quot;tif/L7_ETMs.tif&quot;, package = &quot;stars&quot;)) circ = st_bbox(r) %&gt;% st_as_sfc() %&gt;% st_sample(5) %&gt;% st_buffer(300) hook = function() plot(circ, col = NA, border = &#39;yellow&#39;, add = TRUE) plot(r, hook = hook, key.pos = 4) suppressPackageStartupMessages(library(tidyverse)) nc.32119 = st_transform(nc, 32119) year_labels = c(&quot;SID74&quot; = &quot;1974 - 1978&quot;, &quot;SID79&quot; = &quot;1979 - 1984&quot;) nc.32119 %&gt;% select(SID74, SID79) %&gt;% gather(VAR, SID, -geom) -&gt; nc2 ggplot() + geom_sf(data = nc2, aes(fill = SID)) + facet_wrap(~VAR, ncol = 1, labeller = labeller(VAR = year_labels)) + scale_y_continuous(breaks = 34:36) + scale_fill_gradientn(colors = sf.colors(20)) + theme(panel.grid.major = element_line(color = &quot;white&quot;)) library(ggplot2) library(stars) r = read_stars(system.file(&quot;tif/L7_ETMs.tif&quot;, package = &quot;stars&quot;)) ggplot() + geom_stars(data = r) + facet_wrap(~band) + coord_equal() + theme_void() + scale_x_discrete(expand=c(0,0)) + scale_y_discrete(expand=c(0,0)) + scale_fill_viridis_c() library(tmap) tm_shape(nc.32119) + tm_polygons(c(&quot;SID74&quot;, &quot;SID79&quot;)) tm_shape(nc2) + tm_polygons(&quot;SID&quot;) + tm_facets(by = &quot;VAR&quot;) tm_shape(r) + tm_raster(&quot;L7_ETMs.tif&quot;) tmap_mode(&quot;view&quot;) tmap_mode(&quot;plot&quot;) set.seed(1014) options(digits = 3) knitr::opts_chunk$set( comment = &quot;#&quot;, collapse = knitr::is_latex_output(), cache = TRUE, # out.width = &quot;70%&quot;, fig.align = &#39;center&#39; # fig.width = 6, # fig.asp = 0.618, # 1 / phi # fig.show = &quot;hold&quot; ) options(dplyr.print_min = 6, dplyr.print_max = 6) options(stars.crs = 17) mapview::mapviewOptions(fgb = FALSE) # for units: (?) Sys.setenv(UDUNITS2_XML_PATH=&quot;&quot;) if (knitr::is_latex_output()) options(width = 66) #options(width = 72) set.seed(1014) options(digits = 3) knitr::opts_chunk$set( comment = &quot;#&quot;, collapse = knitr::is_latex_output(), cache = TRUE, # out.width = &quot;70%&quot;, fig.align = &#39;center&#39; # fig.width = 6, # fig.asp = 0.618, # 1 / phi # fig.show = &quot;hold&quot; ) options(dplyr.print_min = 6, dplyr.print_max = 6) options(stars.crs = 17) mapview::mapviewOptions(fgb = FALSE) # for units: (?) Sys.setenv(UDUNITS2_XML_PATH=&quot;&quot;) if (knitr::is_latex_output()) options(width = 66) #options(width = 72) set.seed(1014) options(digits = 3) knitr::opts_chunk$set( comment = &quot;#&quot;, collapse = knitr::is_latex_output(), cache = TRUE, # out.width = &quot;70%&quot;, fig.align = &#39;center&#39; # fig.width = 6, # fig.asp = 0.618, # 1 / phi # fig.show = &quot;hold&quot; ) options(dplyr.print_min = 6, dplyr.print_max = 6) options(stars.crs = 17) mapview::mapviewOptions(fgb = FALSE) # for units: (?) Sys.setenv(UDUNITS2_XML_PATH=&quot;&quot;) if (knitr::is_latex_output()) options(width = 66) #options(width = 72) # try(load(&quot;data/ch16.rda&quot;)) files = list.files(&quot;aq&quot;, pattern = &quot;*.csv&quot;, full.names = TRUE) r = lapply(files[-1], function(f) read.csv(f)) Sys.setenv(TZ = &quot;UTC&quot;) # make sure times are not interpreted as DST r = lapply(r, function(f) { f$t = as.POSIXct(f$DatetimeBegin) f[order(f$t), ] } ) r = r[sapply(r, nrow) &gt; 1000] names(r) = sapply(r, function(f) unique(f$AirQualityStationEoICode)) length(r) == length(unique(names(r))) library(xts) r = lapply(r, function(f) xts(f$Concentration, f$t)) aq = do.call(cbind, r) # remove stations with more than 75% missing values: sel = apply(aq, 2, function(x) sum(is.na(x)) &lt; 0.75 * 365 * 24) aqsel = aq[, sel] # stations are in columns library(tidyverse) read.csv(&quot;aq/AirBase_v8_stations.csv&quot;, sep = &quot;\\t&quot;, stringsAsFactors = FALSE) %&gt;% as_tibble %&gt;% filter(country_iso_code == &quot;DE&quot;, station_type_of_area == &quot;rural&quot;, type_of_station == &quot;Background&quot;) -&gt; a2 library(sf) library(stars) a2.sf = st_as_sf(a2, coords = c(&quot;station_longitude_deg&quot;, &quot;station_latitude_deg&quot;), crs = 4326) sel = colnames(aqsel) %in% a2$station_european_code aqsel = aqsel[, sel] tb = tibble(NO2 = apply(aqsel, 2, mean, na.rm = TRUE), station_european_code = colnames(aqsel)) crs = 32632 right_join(a2.sf, tb) %&gt;% st_transform(crs) -&gt; no2.sf # load German boundaries data(air, package = &quot;spacetime&quot;) de &lt;- st_transform(st_as_sf(DE_NUTS1), crs) ggplot() + geom_sf(data = de) + geom_sf(data = no2.sf, mapping = aes(col = NO2)) library(gstat) v = variogram(NO2~1, no2.sf) plot(v, plot.numbers = TRUE) library(gstat) v0 = variogram(NO2~1, no2.sf, cutoff = 100000, width = 10000) plot(v0, plot.numbers = TRUE) v.m = fit.variogram(v, vgm(1, &quot;Exp&quot;, 50000, 1)) plot(v, v.m, plot.numbers = TRUE) # build a grid over Germany: st_bbox(de) %&gt;% st_as_stars(dx = 10000) %&gt;% st_set_crs(crs) %&gt;% st_crop(de) -&gt; grd grd k = krige(NO2~1, no2.sf, grd, v.m) ggplot() + geom_stars(data = k, aes(fill = var1.pred, x = x, y = y)) + geom_sf(data = st_cast(de, &quot;MULTILINESTRING&quot;)) + geom_sf(data = no2.sf) a = aggregate(no2.sf[&quot;NO2&quot;], by = de, FUN = mean) b = krige(NO2~1, no2.sf, de, v.m) b$sample = a$NO2 b$kriging = b$var1.pred b %&gt;% select(sample, kriging) %&gt;% gather(var, NO2, -geometry) -&gt; b2 ggplot() + geom_sf(data = b2, mapping = aes(fill = NO2)) + facet_wrap(~var) + scale_fill_gradientn(colors = sf.colors(20)) SE = function(x) sqrt(var(x)/length(x)) a = aggregate(no2.sf[&quot;NO2&quot;], de, SE) b$sample = a$NO2 b$kriging = sqrt(b$var1.var) b %&gt;% select(sample, kriging) %&gt;% gather(var, NO2, -geometry) -&gt; b2 ggplot() + geom_sf(data = b2, mapping = aes(fill = NO2)) + facet_wrap(~var) + scale_fill_gradientn(colors = sf.colors(20)) library(viridis) s = krige(NO2~1, no2.sf, grd, v.m, nmax = 30, nsim = 10) g = ggplot() + coord_equal() + scale_fill_viridis() + theme_void() + scale_x_discrete(expand=c(0,0)) + scale_y_discrete(expand=c(0,0)) g + geom_stars(data = s[,,,1:6]) + facet_wrap(~sample) v = vroom::vroom(&quot;aq/pop/Zensus_Bevoelkerung_100m-Gitter.csv&quot;) v %&gt;% filter(Einwohner &gt; 0) %&gt;% select(-Gitter_ID_100m) %&gt;% st_as_sf(coords = c(&quot;x_mp_100m&quot;, &quot;y_mp_100m&quot;), crs = 3035) %&gt;% st_transform(st_crs(grd)) -&gt; b a = aggregate(b, st_as_sf(grd, na.rm = FALSE), sum) v = vroom::vroom(&quot;aq/pop/Zensus_Bevoelkerung_100m-Gitter.csv&quot;) v1 &lt;- v[v$Einwohner &gt; 0,-1] rm(v); gc(full=TRUE) v2 &lt;- st_as_sf(v1, coords = c(&quot;x_mp_100m&quot;, &quot;y_mp_100m&quot;), crs = 3035) rm(v1); gc() b &lt;- st_transform(v2, st_crs(grd)) rm(v2); gc() a = aggregate(b, st_as_sf(grd, na.rm = FALSE), sum) gc() grd$ID = 1:prod(dim(grd)) # so we can find out later which grid cell we have ii = st_intersects(grd[&quot;ID&quot;], st_cast(st_union(de), &quot;MULTILINESTRING&quot;)) grd_sf = st_as_sf(grd[&quot;ID&quot;], na.rm = FALSE)[lengths(ii) &gt; 0,] iii = st_intersection(grd_sf, st_union(de)) grd$area = st_area(grd)[[1]] + units::set_units(grd$values, m^2) # NA&#39;s grd$area[iii$ID] = st_area(iii) grd$pop_dens = a$Einwohner / grd$area sum(grd$pop_dens * grd$area, na.rm = TRUE) # verify sum(b$Einwohner) g + geom_stars(data = grd, aes(fill = sqrt(pop_dens), x = x, y = y)) (a = aggregate(grd[&quot;pop_dens&quot;], no2.sf, mean)) no2.sf$pop_dens = st_as_sf(a)[[1]] summary(lm(NO2~sqrt(pop_dens), no2.sf)) plot(NO2~sqrt(pop_dens), no2.sf) abline(lm(NO2~sqrt(pop_dens), no2.sf)) no2.sf = no2.sf[!is.na(no2.sf$pop_dens),] vr = variogram(NO2~sqrt(pop_dens), no2.sf) vr.m = fit.variogram(vr, vgm(1, &quot;Exp&quot;, 50000, 1)) plot(vr, vr.m, plot.numbers = TRUE) kr = krige(NO2~sqrt(pop_dens), no2.sf, grd[&quot;pop_dens&quot;], vr.m) k$kr1 = k$var1.pred k$kr2 = kr$var1.pred st_redimension(k[c(&quot;kr1&quot;, &quot;kr2&quot;)], along = list(what = c(&quot;kriging&quot;, &quot;residual kriging&quot;))) %&gt;% setNames(&quot;NO2&quot;) -&gt; km g + geom_stars(data = km, aes(fill = NO2, x = x, y = y)) + geom_sf(data = st_cast(de, &quot;MULTILINESTRING&quot;)) + geom_sf(data = no2.sf) + facet_wrap(~what) library(gstat) demo(cokriging) demo(cosimulation) aqx = aq[ , colnames(aq) %in% a2$station_european_code] sfc = st_geometry(a2.sf)[match(colnames(aqx), a2.sf$station_european_code)] st_as_stars(NO2 = as.matrix(aqx)) %&gt;% st_set_dimensions(names = c(&quot;time&quot;, &quot;station&quot;)) %&gt;% st_set_dimensions(&quot;time&quot;, index(aqx)) %&gt;% st_set_dimensions(&quot;station&quot;, sfc) -&gt; no2.st v.st = variogramST(NO2~1, no2.st[,1:(24*31)], tlags = 0:48, cores = getOption(&quot;mc.cores&quot;, 2)) v1 = plot(v.st) v2 = plot(v.st, map = FALSE) print(v1, split = c(1,1,2,1), more = TRUE) print(v2, split = c(2,1,2,1), more = FALSE) # product-sum prodSumModel &lt;- vgmST(&quot;productSum&quot;, space=vgm(150, &quot;Exp&quot;, 200, 0), time= vgm(20, &quot;Sph&quot;, 40, 0), k=2) StAni = estiStAni(v.st, c(0,200000)) (fitProdSumModel &lt;- fit.StVariogram(v.st, prodSumModel, fit.method = 7, stAni = StAni, method = &quot;L-BFGS-B&quot;, control = list(parscale = c(1,10,1,1,0.1,1,10)), lower = rep(0.0001, 7))) plot(v.st, fitProdSumModel, wireframe=FALSE, all=TRUE, scales=list(arrows=FALSE), zlim=c(0,150)) plot(v.st, model=fitProdSumModel, wireframe=TRUE, all=TRUE, scales=list(arrows=FALSE), zlim=c(0,185)) set.seed(123) pt = st_sample(de, 2) t = st_get_dimension_values(no2.st, 1) st_as_stars(list(pts = matrix(1, length(t), length(pt)))) %&gt;% st_set_dimensions(names = c(&quot;time&quot;, &quot;station&quot;)) %&gt;% st_set_dimensions(&quot;time&quot;, t) %&gt;% st_set_dimensions(&quot;station&quot;, pt) -&gt; new_pt no2.st &lt;- st_transform(no2.st, crs) new_ts &lt;- krigeST(NO2~1, data = no2.st[&quot;NO2&quot;], newdata = new_pt, nmax = 50, stAni = StAni, modelList = fitProdSumModel, progress = FALSE) plot(as.xts(new_ts[2])) t4 = t[(1:4 - 0.5) * (3*24*30)] d = dim(grd) st_as_stars(pts = array(1, c(d[1], d[2], time=length(t4)))) %&gt;% st_set_dimensions(&quot;time&quot;, t4) %&gt;% st_set_dimensions(&quot;x&quot;, st_get_dimension_values(grd, &quot;x&quot;)) %&gt;% st_set_dimensions(&quot;y&quot;, st_get_dimension_values(grd, &quot;y&quot;)) %&gt;% st_set_crs(crs) -&gt; grd.st new_int &lt;- krigeST(NO2~1, data = no2.st[&quot;NO2&quot;], newdata = grd.st, nmax = 200, stAni = StAni, modelList = fitProdSumModel, progress = FALSE) names(new_int)[2] = &quot;NO2&quot; g + geom_stars(data = new_int, aes(fill = NO2, x = x, y = y)) + facet_wrap(~time) + geom_sf(data = st_cast(de, &quot;MULTILINESTRING&quot;)) + geom_sf(data = no2.sf, col = &#39;grey&#39;, cex = .5) set.seed(1014) options(digits = 3) knitr::opts_chunk$set( comment = &quot;#&quot;, collapse = knitr::is_latex_output(), cache = TRUE, # out.width = &quot;70%&quot;, fig.align = &#39;center&#39; # fig.width = 6, # fig.asp = 0.618, # 1 / phi # fig.show = &quot;hold&quot; ) options(dplyr.print_min = 6, dplyr.print_max = 6) options(stars.crs = 17) mapview::mapviewOptions(fgb = FALSE) # for units: (?) Sys.setenv(UDUNITS2_XML_PATH=&quot;&quot;) if (knitr::is_latex_output()) options(width = 66) #options(width = 72) library(sf) data(pol_pres15, package=&quot;spDataLarge&quot;) head(pol_pres15[, c(1, 4, 6)]) library(tmap) tm_shape(pol_pres15) + tm_fill(&quot;types&quot;) library(spdep) args(poly2nb) system.time(nb_q &lt;- poly2nb(pol_pres15, queen=TRUE)) nb_q system.time(nb_q_legacy &lt;- poly2nb(pol_pres15, queen=TRUE, small_n=2500)) all.equal(nb_q, nb_q_legacy, check.attributes=FALSE) n.comp.nb(nb_q)$nc tf &lt;- tempfile(fileext=&quot;.gal&quot;) write.nb.gal(nb_q, tf) coords &lt;- st_centroid(st_geometry(pol_pres15), of_largest_polygon=TRUE) suppressMessages(nb_tri &lt;- tri2nb(coords)) nb_tri summary(unlist(nbdists(nb_tri, coords))) n.comp.nb(nb_tri)$nc nb_soi &lt;- graph2nb(soi.graph(nb_tri, coords)) nb_soi n_comp &lt;- n.comp.nb(nb_soi) n_comp$nc table(n_comp$comp.id) opar &lt;- par(mar=c(0,0,0,0)+0.5) plot(st_geometry(pol_pres15), border=&quot;grey&quot;, lwd=0.5) plot(nb_soi, coords=st_coordinates(coords), add=TRUE, points=FALSE, lwd=0.5) plot(diffnb(nb_tri, nb_soi), coords=st_coordinates(coords), col=&quot;orange&quot;, add=TRUE, points=FALSE, lwd=0.5) par(opar) k1 &lt;- knn2nb(knearneigh(coords)) k1dists &lt;- unlist(nbdists(k1, coords)) summary(k1dists) system.time(nb_d18 &lt;- dnearneigh(coords, 0, 18000)) system.time(nb_d18a &lt;- dnearneigh(coords, 0, 18000, use_kd_tree=FALSE)) all.equal(nb_d18, nb_d18a, check.attributes=FALSE) nb_d18 n_comp &lt;- n.comp.nb(nb_d18) n_comp$nc table(n_comp$comp.id) nb_d183 &lt;- dnearneigh(coords, 0, 18300) nb_d183 n_comp &lt;- n.comp.nb(nb_d183) n_comp$nc nb_d16 &lt;- dnearneigh(coords, 0, 16000) nb_d16 knn_k6 &lt;- knearneigh(coords, k=6) knn2nb(knn_k6) nb_k6s &lt;- knn2nb(knn_k6, sym=TRUE) nb_k6s n_comp &lt;- n.comp.nb(nb_k6s) n_comp$nc args(nb2listw) args(spweights.constants) lw_q_B &lt;- nb2listw(nb_q, style=&quot;B&quot;) unlist(spweights.constants(lw_q_B)) lw_q_W &lt;- nb2listw(nb_q, style=&quot;W&quot;) unlist(spweights.constants(lw_q_W))[c(1,6:8)] gwts &lt;- lapply(nbdists(nb_d183, coords), function(x) 1/(x/1000)) lw_d183_idw_B &lt;- nb2listw(nb_d183, glist=gwts, style=&quot;B&quot;) unlist(spweights.constants(lw_d183_idw_B))[c(1,6:8)] try(lw_d16_B &lt;- nb2listw(nb_d16, style=&quot;B&quot;)) lw_d16_B &lt;- nb2listw(nb_d16, style=&quot;B&quot;, zero.policy=TRUE) unlist(spweights.constants(lw_d16_B, zero.policy=TRUE))[c(1,6:8)] set.seed(1) x &lt;- rnorm(nrow(pol_pres15)) mt &lt;- moran.test(x, lw_q_B, randomisation=FALSE, alternative=&quot;two.sided&quot;) glance_htest &lt;- function(ht) c(ht$estimate, &quot;Std deviate&quot;=unname(ht$statistic), &quot;p.value&quot;=unname(ht$p.value)) glance_htest(mt) beta &lt;- 0.15e-02 t &lt;- st_coordinates(coords)[,1]/1000 x_t &lt;- x + beta*t mt &lt;- moran.test(x_t, lw_q_B, randomisation=FALSE, alternative=&quot;two.sided&quot;) glance_htest(mt) lmt &lt;- lm.morantest(lm(x_t ~ t), lw_q_B, alternative=&quot;two.sided&quot;) glance_htest(lmt) args(joincount.test) table(pol_pres15$types) joincount.multi(pol_pres15$types, listw=lw_q_B) joincount.multi(pol_pres15$types, listw=lw_d183_idw_B) args(moran.test) mt &lt;- moran.test(pol_pres15$I_turnout, listw=lw_q_B, randomisation=FALSE) glance_htest(mt) ols &lt;- lm(I_turnout ~ 1, pol_pres15) lmt &lt;- lm.morantest(ols, listw=lw_q_B) glance_htest(lmt) mtr &lt;- moran.test(pol_pres15$I_turnout, listw=lw_q_B) glance_htest(mtr) set.seed(1) mmc &lt;- moran.mc(pol_pres15$I_turnout, listw=lw_q_B, nsim=999, return_boot = TRUE) c(&quot;Permutation bootstrap&quot;=var(mmc$t), &quot;Analytical randomisation&quot;=unname(mtr$estimate[3])) infl_W &lt;- moran.plot(pol_pres15$I_turnout, listw=lw_q_W, labels=pol_pres15$TERYT, cex=1, pch=&quot;.&quot;, xlab=&quot;I round turnout&quot;, ylab=&quot;lagged turnout&quot;) pol_pres15$hat_value &lt;- infl_W$hat tm_shape(pol_pres15) + tm_fill(&quot;hat_value&quot;) locm &lt;- localmoran(pol_pres15$I_turnout, listw=lw_q_W, alternative=&quot;two.sided&quot;) all.equal(sum(locm[,1])/Szero(lw_q_W), unname(moran.test(pol_pres15$I_turnout, lw_q_W)$estimate[1])) pvs &lt;- cbind(&quot;none&quot;=locm[,5], &quot;bonferroni&quot;=p.adjust(locm[,5], &quot;bonferroni&quot;), &quot;fdr&quot;=p.adjust(locm[,5],&quot;fdr&quot;), &quot;BY&quot;=p.adjust(locm[,5], &quot;BY&quot;)) apply(pvs, 2, function(x) sum(x &lt; 0.05)) library(parallel) set.coresOption(ifelse(detectCores() == 1, 1, detectCores()-1L)) system.time(locm_p &lt;- localmoran_perm(pol_pres15$I_turnout, listw=lw_q_W, nsim=499, alternative=&quot;two.sided&quot;, iseed=1)) pv &lt;- locm_p[,5] pvsp &lt;- cbind(&quot;none&quot;=pv, &quot;bonferroni&quot;=p.adjust(pv, &quot;bonferroni&quot;), &quot;fdr&quot;=p.adjust(pv, &quot;fdr&quot;), &quot;BY&quot;=p.adjust(pv, &quot;BY&quot;)) apply(pvsp, 2, function(x) sum(x &lt;= 0.05)) brks &lt;- qnorm(c(0, 0.00001, 0.0001, 0.001, 0.01, 0.025, 0.5, 0.975, 0.99, 0.999, 0.9999, 0.99999, 1)) (tab &lt;- table(cut(locm_p[,4], brks))) sum(tab[c(1:5, 8:12)]) sum(tab[c(1, 12)]) pol_pres15$locm_Z &lt;- locm[,4] pol_pres15$locm_p_Z &lt;- locm_p[,4] tm_shape(pol_pres15) + tm_fill(c(&quot;locm_Z&quot;, &quot;locm_p_Z&quot;), breaks=brks, midpoint=0, title=&quot;Standard deviates of\\nLocal Moran&#39;s I&quot;) + tm_facets(free.scales=FALSE) + tm_layout(panel.labels=c(&quot;Analytical&quot;, &quot;Conditional permutation&quot;)) quadr &lt;- interaction(cut(infl_W$x, c(-Inf, mean(infl_W$x), Inf), labels=c(&quot;Low X&quot;, &quot;High X&quot;)), cut(infl_W$wx, c(-Inf, mean(infl_W$wx), Inf), labels=c(&quot;Low WX&quot;, &quot;High WX&quot;)), sep=&quot; : &quot;) a &lt;- table(quadr) pol_pres15$hs_an_q &lt;- quadr is.na(pol_pres15$hs_an_q) &lt;- !(pol_pres15$locm_Z &lt; brks[6] | pol_pres15$locm_Z &gt; brks[8]) b &lt;- table(pol_pres15$hs_an_q) pol_pres15$hs_cp_q &lt;- quadr is.na(pol_pres15$hs_cp_q) &lt;- !(pol_pres15$locm_p_Z &lt; brks[2] | pol_pres15$locm_p_Z &gt; brks[12]) c &lt;- table(pol_pres15$hs_cp_q) t(rbind(&quot;Moran plot quadrants&quot;=a, &quot;Unadjusted analytical&quot;=b, &quot;Bonferroni cond. perm.&quot;=c)) tm_shape(pol_pres15) + tm_fill(c(&quot;hs_an_q&quot;, &quot;hs_cp_q&quot;), colorNA=&quot;grey95&quot;, textNA=&quot;Not significant&quot;, title=&quot;Turnout hotspot status\\nLocal Moran&#39;s I&quot;) + tm_facets(free.scales=FALSE) + tm_layout(panel.labels=c(&quot;Unadjusted analytical&quot;, &quot;Cond. perm. with Bonferroni&quot;)) system.time(locG &lt;- localG(pol_pres15$I_turnout, lw_q_W)) system.time(locG_p &lt;- localG_perm(pol_pres15$I_turnout, lw_q_W, nsim=499, iseed=1)) pv &lt;- 2 * pnorm(abs(c(locG)), lower.tail = FALSE) pvsp &lt;- cbind(pv, p.adjust(pv, &quot;bonferroni&quot;), p.adjust(pv, &quot;fdr&quot;), p.adjust(pv, &quot;BY&quot;)) colnames(pvsp) &lt;- c(&quot;none&quot;, &quot;bonferroni&quot;, &quot;fdr&quot;, &quot;BY&quot;) apply(pvsp, 2, function(x) sum(x &lt; 0.05)) library(ggplot2) p1 &lt;- ggplot(data.frame(Zi=locm[,4], Zi_perm=locm_p[,4])) + geom_point(aes(x=Zi, y=Zi_perm), alpha=0.2) + xlab(&quot;Analytical&quot;) + ylab(&quot;Conditional permutation&quot;) + coord_fixed() + ggtitle(&quot;Local Moran&#39;s I&quot;) p2 &lt;- ggplot(data.frame(Zi=c(locG), Zi_perm=c(locG_p))) + geom_point(aes(x=Zi, y=Zi_perm), alpha=0.2) + xlab(&quot;Analytical&quot;) + ylab(&quot;Conditional permutation&quot;) + coord_fixed() + ggtitle(&quot;Local G&quot;) gridExtra::grid.arrange(p1, p2, nrow=1) pol_pres15$locG_Z &lt;- c(locG) pol_pres15$hs_G &lt;- cut(c(locG), c(-Inf, brks[2], brks[12], Inf), labels=c(&quot;Low&quot;, &quot;Not significant&quot;, &quot;High&quot;)) table(pol_pres15$hs_G) m1 &lt;- tm_shape(pol_pres15) + tm_fill(c(&quot;locG_Z&quot;), midpoint=0, title=&quot;Standard\\ndeviate&quot;) m2 &lt;- tm_shape(pol_pres15) + tm_fill(c(&quot;hs_G&quot;), title=&quot;Bonferroni\\nhotspot status&quot;) tmap_arrange(m1, m2, nrow=1) library(rgeoda) system.time(Geoda_w &lt;- queen_weights(pol_pres15)) summary(Geoda_w) system.time(lisa &lt;- local_moran(Geoda_w, pol_pres15[&quot;I_turnout&quot;], cpu_threads=ifelse(parallel::detectCores() == 1, 1, parallel::detectCores()-1L), permutations=499, seed=1)) all.equal(card(nb_q), lisa_num_nbrs(lisa), check.attributes=FALSE) all.equal(lisa_values(lisa), localmoran(pol_pres15$I_turnout, listw=lw_q_W, mlvar=FALSE)[,1], check.attributes=FALSE) set.seed(1014) options(digits = 3) knitr::opts_chunk$set( comment = &quot;#&quot;, collapse = knitr::is_latex_output(), cache = TRUE, # out.width = &quot;70%&quot;, fig.align = &#39;center&#39; # fig.width = 6, # fig.asp = 0.618, # 1 / phi # fig.show = &quot;hold&quot; ) options(dplyr.print_min = 6, dplyr.print_max = 6) options(stars.crs = 17) mapview::mapviewOptions(fgb = FALSE) # for units: (?) Sys.setenv(UDUNITS2_XML_PATH=&quot;&quot;) if (knitr::is_latex_output()) options(width = 66) #options(width = 72) library(sf) library(spatialreg) boston_506 &lt;- st_read(system.file(&quot;shapes/boston_tracts.shp&quot;, package=&quot;spData&quot;)[1]) nb_q &lt;- spdep::poly2nb(boston_506) lw_q &lt;- spdep::nb2listw(nb_q, style=&quot;W&quot;) table(boston_506$censored) summary(boston_506$median) boston_489 &lt;- boston_506[!is.na(boston_506$median),] nb_q_489 &lt;- spdep::poly2nb(boston_489) lw_q_489 &lt;- spdep::nb2listw(nb_q_489, style=&quot;W&quot;, zero.policy=TRUE) agg_96 &lt;- list(as.character(boston_506$NOX_ID)) boston_96 &lt;- aggregate(boston_506[, &quot;NOX_ID&quot;], by=agg_96, unique) nb_q_96 &lt;- spdep::poly2nb(boston_96) lw_q_96 &lt;- spdep::nb2listw(nb_q_96) boston_96$NOX &lt;- aggregate(boston_506$NOX, agg_96, mean)$x boston_96$CHAS &lt;- aggregate(as.integer(boston_506$CHAS)-1, agg_96, max)$x nms &lt;- names(boston_506) ccounts &lt;- 23:31 for (nm in nms[c(22, ccounts, 36)]) { boston_96[[nm]] &lt;- aggregate(boston_506[[nm]], agg_96, sum)$x } br2 &lt;- c(3.50, 6.25, 8.75, 12.50, 17.50, 22.50, 30.00, 42.50, 60.00)*1000 counts &lt;- as.data.frame(boston_96)[, nms[ccounts]] f &lt;- function(x) matrixStats::weightedMedian(x=br2, w=x, interpolate=TRUE) boston_96$median &lt;- apply(counts, 1, f) is.na(boston_96$median) &lt;- boston_96$median &gt; 50000 summary(boston_96$median) POP &lt;- boston_506$POP f &lt;- function(x) matrixStats::weightedMean(x[,1], x[,2]) for (nm in nms[c(9:11, 14:19, 21, 33)]) { s0 &lt;- split(data.frame(boston_506[[nm]], POP), agg_96) boston_96[[nm]] &lt;- sapply(s0, f) } boston_94 &lt;- boston_96[!is.na(boston_96$median),] nb_q_94 &lt;- spdep::subset.nb(nb_q_96, !is.na(boston_96$median)) lw_q_94 &lt;- spdep::nb2listw(nb_q_94, style=&quot;W&quot;) form &lt;- formula(log(median) ~ CRIM + ZN + INDUS + CHAS + I((NOX*10)^2) + I(RM^2) + AGE + log(DIS) + log(RAD) + TAX + PTRATIO + I(BB/100) + log(I(LSTAT/100))) args(errorsarlm) args(lagsarlm) args(spautolm) args(sacsarlm) args(getS3method(&quot;summary&quot;, &quot;Sarlm&quot;)) eigs_489 &lt;- eigenw(lw_q_489) SDEM_489 &lt;- errorsarlm(form, data=boston_489, listw=lw_q_489, Durbin=TRUE, zero.policy=TRUE, control=list(pre_eig=eigs_489)) SEM_489 &lt;- errorsarlm(form, data=boston_489, listw=lw_q_489, zero.policy=TRUE, control=list(pre_eig=eigs_489)) cbind(data.frame(model=c(&quot;SEM&quot;, &quot;SDEM&quot;)), rbind(broom::tidy(Hausman.test(SEM_489)), broom::tidy(Hausman.test(SDEM_489))))[,1:4] cbind(data.frame(model=c(&quot;SEM&quot;, &quot;SDEM&quot;)), rbind(broom::tidy(LR1.Sarlm(SEM_489)), broom::tidy(LR1.Sarlm(SDEM_489))))[,c(1, 4:6)] eigs_94 &lt;- eigenw(lw_q_94) SDEM_94 &lt;- errorsarlm(form, data=boston_94, listw=lw_q_94, Durbin=TRUE, control=list(pre_eig=eigs_94)) SEM_94 &lt;- errorsarlm(form, data=boston_94, listw=lw_q_94, control=list(pre_eig=eigs_94)) cbind(data.frame(model=c(&quot;SEM&quot;, &quot;SDEM&quot;)), rbind(broom::tidy(Hausman.test(SEM_94)), broom::tidy(Hausman.test(SDEM_94))))[, 1:4] cbind(data.frame(model=c(&quot;SEM&quot;, &quot;SDEM&quot;)), rbind(broom::tidy(LR1.Sarlm(SEM_94)), broom::tidy(LR1.Sarlm(SDEM_94))))[,c(1, 4:6)] broom::tidy(lmtest::lrtest(SEM_489, SDEM_489)) broom::tidy(lmtest::lrtest(SEM_94, SDEM_94)) args(lmSLX) SLX_489 &lt;- lmSLX(form, data=boston_489, listw=lw_q_489, zero.policy=TRUE) broom::tidy(lmtest::lrtest(SLX_489, SDEM_489)) SLX_94 &lt;- lmSLX(form, data=boston_94, listw=lw_q_94) broom::tidy(lmtest::lrtest(SLX_94, SDEM_94)) SLX_94w &lt;- lmSLX(form, data=boston_94, listw=lw_q_94, weights=units) SDEM_94w &lt;- errorsarlm(form, data=boston_94, listw=lw_q_94, Durbin=TRUE, weights=units, control=list(pre_eig=eigs_94)) broom::tidy(lmtest::lrtest(SLX_94w, SDEM_94w)) SEM1_94 &lt;- sphet::spreg(form, data=boston_94, listw=lw_q_94, model=&quot;error&quot;) res &lt;- rbind(summary(SEM_94)$Coef[&quot;I((NOX * 10)^2)&quot;,], summary(SEM1_94)$CoefTable[&quot;I((NOX * 10)^2)&quot;,]) rownames(res) &lt;- c(&quot;ML&quot;, &quot;GMM&quot;) res formiv &lt;- update(form, . ~ . - I((NOX*10)^2)) boston_94$NOX2 &lt;- (boston_94$NOX*10)^2 suppressWarnings(ccoords &lt;- st_coordinates(st_centroid(st_geometry(boston_94)))) iform &lt;- formula(~poly(ccoords, degree=2) + DIS + RAD) SEM1_94iv &lt;- sphet::spreg(formiv, data=boston_94, listw=lw_q_94, endog = ~NOX2, instruments=iform, model=&quot;error&quot;) summary(SEM1_94iv)$CoefTable[&quot;NOX2&quot;,] system.time(SDEM_94B &lt;- spBreg_err(form, data=boston_94, listw=lw_q_94, Durbin=TRUE)) system.time(SDEM_489B &lt;- spBreg_err(form, data=boston_489, listw=lw_q_489, Durbin=TRUE, zero.policy=TRUE)) t(errorsarlm(form, data=boston_94, listw=lw_q_94, Durbin=TRUE)$timings[,2]) t(errorsarlm(form, data=boston_489, listw=lw_q_489, Durbin=TRUE, zero.policy=TRUE)$timings[,2]) t(attr(SDEM_94B, &quot;timings&quot;)[ , 3]) t(attr(SDEM_489B, &quot;timings&quot;)[ , 3]) SEM_94$interval 1/range(eigs_94) 1/c(lextrW(lw_q_94)) W &lt;- as(lw_q_94, &quot;CsparseMatrix&quot;) 1/Re(c(RSpectra::eigs(W, k=1, which=&quot;SR&quot;)$values, RSpectra::eigs(W, k=1, which=&quot;LR&quot;)$values)) coef &lt;- 0.5 sum(log(1 - coef * eigs_94)) I &lt;- Diagonal(nrow(boston_94)) LU &lt;- lu(I - coef * W) dU &lt;- abs(diag(slot(LU, &quot;U&quot;))) sum(log(dU)) W &lt;- as(similar.listw(lw_q_94), &quot;CsparseMatrix&quot;) super &lt;- as.logical(NA) cch &lt;- Cholesky((I - coef * W), super=super) c(2 * determinant(cch, logarithm = TRUE)$modulus) args(jacobianSetup) args(do_ldet) args(getS3method(&quot;impacts&quot;, &quot;Sarlm&quot;)) args(getS3method(&quot;summary&quot;, &quot;LagImpact&quot;)) sum_imp_94_SDEM &lt;- summary(impacts(SDEM_94)) rbind(Impacts=sum_imp_94_SDEM$mat[5,], SE=sum_imp_94_SDEM$semat[5,]) sum_imp_94_SDEM_B &lt;- summary(impacts(SDEM_94B)) rbind(Impacts=sum_imp_94_SDEM_B$mat[5,], SE=sum_imp_94_SDEM_B$semat[5,]) sum_imp_94_SLX &lt;- summary(impacts(SLX_94)) rbind(Impacts=sum_imp_94_SLX$mat[5,], SE=sum_imp_94_SLX$semat[5,]) SLM_489 &lt;- lagsarlm(form, data=boston_489, listw=lw_q_489, zero.policy=TRUE) args(trW) W &lt;- as(lw_q_489, &quot;CsparseMatrix&quot;) tr_489 &lt;- trW(W) str(tr_489) SLM_489_imp &lt;- impacts(SLM_489, tr=tr_489, R=2000) SLM_489_imp_sum &lt;- summary(SLM_489_imp, short=TRUE, zstats=TRUE) res &lt;- rbind(Impacts=sapply(SLM_489_imp$res, &quot;[&quot;, 5), SE=SLM_489_imp_sum$semat[5,]) colnames(res) &lt;- c(&quot;Direct&quot;, &quot;Indirect&quot;, &quot;Total&quot;) res coef_SLM_489 &lt;- coef(SLM_489) IrW &lt;- Diagonal(489) - coef_SLM_489[1] * W S_W &lt;- solve(IrW) S_NOX_W &lt;- S_W %*% (diag(489) * coef_SLM_489[7]) c(Direct=mean(diag(S_NOX_W)), Total=sum(S_NOX_W)/489) sapply(impacts(SLM_489, listw=lw_q_489), &quot;[&quot;, 5) sapply(impacts(SLM_489, evalues=eigs_489), &quot;[&quot;, 5) args(getS3method(&quot;predict&quot;, &quot;sarlm&quot;)) nd_489 &lt;- boston_489 nd_489$PTRATIO &lt;- nd_489$PTRATIO + 1 OLS_489 &lt;- lm(form, data=boston_489) fitted &lt;- predict(OLS_489) nd_fitted &lt;- predict(OLS_489, newdata=nd_489) all.equal(unname(coef(OLS_489)[12]), mean(nd_fitted - fitted)) fitted &lt;- predict(SLM_489) nd_fitted &lt;- predict(SLM_489, newdata=nd_489, listw=lw_q_489, pred.type=&quot;TS&quot;, zero.policy=TRUE) all.equal(unname(coef_SLM_489[13]), mean(nd_fitted - fitted)) nd &lt;- boston_506[is.na(boston_506$median),] t0 &lt;- exp(predict(SDEM_489, newdata=nd, listw=lw_q, pred.type=&quot;TS&quot;, zero.policy=TRUE)) suppressWarnings(t1 &lt;- exp(predict(SDEM_489, newdata=nd, listw=lw_q, pred.type=&quot;KP2&quot;, zero.policy=TRUE))) suppressWarnings(t2 &lt;- exp(predict(SDEM_489, newdata=nd, listw=lw_q, pred.type=&quot;KP5&quot;, zero.policy=TRUE))) data.frame(fit_TS=t0[,1], fit_KP2=c(t1), fit_KP5=c(t2), censored=boston_506$censored[as.integer(attr(t0, &quot;region.id&quot;))]) boston_94a &lt;- aggregate(boston_489[,&quot;NOX_ID&quot;], list(boston_489$NOX_ID), unique) nb_q_94a &lt;- spdep::poly2nb(boston_94a) NOX_ID_no_neighs &lt;- boston_94a$NOX_ID[which(spdep::card(nb_q_94a) == 0)] boston_487 &lt;- boston_489[is.na(match(boston_489$NOX_ID, NOX_ID_no_neighs)),] boston_93 &lt;- aggregate(boston_487[, &quot;NOX_ID&quot;], list(ids = boston_487$NOX_ID), unique) row.names(boston_93) &lt;- as.character(boston_93$NOX_ID) nb_q_93 &lt;- spdep::poly2nb(boston_93, row.names=unique(as.character(boston_93$NOX_ID))) library(lme4) MLM &lt;- lmer(update(form, . ~ . + (1 | NOX_ID)), data=boston_487, REML=FALSE) boston_93$MLM_re &lt;- ranef(MLM)[[1]][,1] library(Matrix) suppressMessages(library(MatrixModels)) Delta &lt;- as(model.Matrix(~ -1 + as.factor(NOX_ID), data=boston_487, sparse=TRUE), &quot;dgCMatrix&quot;) M &lt;- as(spdep::nb2listw(nb_q_93, style=&quot;B&quot;), &quot;CsparseMatrix&quot;) suppressPackageStartupMessages(library(hglm)) y_hglm &lt;- log(boston_487$median) X_hglm &lt;- model.matrix(lm(form, data=boston_487)) suppressWarnings(HGLM_iid &lt;- hglm(y=y_hglm, X=X_hglm, Z=Delta)) suppressWarnings(HGLM_sar &lt;- hglm(y=y_hglm, X=X_hglm, Z=Delta, rand.family=SAR(D=M))) boston_93$HGLM_re &lt;- unname(HGLM_iid$ranef) boston_93$HGLM_ss &lt;- HGLM_sar$ranef[,1] library(HSAR) suppressWarnings(HSAR &lt;- hsar(form, data=boston_487, W=NULL, M=M, Delta=Delta, burnin=500, Nsim=2500, thinning=1)) boston_93$HSAR_ss &lt;- HSAR$Mus[1,] suppressPackageStartupMessages(library(R2BayesX)) BX_iid &lt;- bayesx(update(form, . ~ . + sx(NOX_ID, bs=&quot;re&quot;)), family=&quot;gaussian&quot;, data=boston_487, method=&quot;MCMC&quot;, iterations=12000, burnin=2000, step=2, seed=123) boston_93$BX_re &lt;- BX_iid$effects[&quot;sx(NOX_ID):re&quot;][[1]]$Mean RBX_gra &lt;- nb2gra(nb_q_93) BX_mrf &lt;- bayesx(update(form, . ~ . + sx(NOX_ID, bs=&quot;mrf&quot;, map=RBX_gra)), family=&quot;gaussian&quot;, data=boston_487, method=&quot;MCMC&quot;, iterations=12000, burnin=2000,step=2, seed=123) boston_93$BX_ss &lt;- BX_mrf$effects[&quot;sx(NOX_ID):mrf&quot;][[1]]$Mean library(mgcv) names(nb_q_93) &lt;- attr(nb_q_93, &quot;region.id&quot;) boston_487$NOX_ID &lt;- as.factor(boston_487$NOX_ID) GAM_MRF &lt;- gam(update(form, . ~ . + s(NOX_ID, bs=&quot;mrf&quot;, xt=list(nb=nb_q_93))), data=boston_487, method=&quot;REML&quot;) boston_93$GAM_ss &lt;- aggregate(predict(GAM_MRF, type=&quot;terms&quot;, se=FALSE)[,14], list(boston_487$NOX_ID), mean)$x res &lt;- rbind(iid_lmer=summary(MLM)$coefficients[6, 1:2], iid_hglm=summary(HGLM_iid)$FixCoefMat[6, 1:2], iid_BX=BX_iid$fixed.effects[6, 1:2], sar_hsar=c(HSAR$Mbetas[1, 6], HSAR$SDbetas[1, 6]), mrf_BX=BX_mrf$fixed.effects[6, 1:2], mrf_GAM=c(summary(GAM_MRF)$p.coeff[6], summary(GAM_MRF)$se[6])) suppressPackageStartupMessages(library(ggplot2)) df_res &lt;- as.data.frame(res) names(df_res) &lt;- c(&quot;mean&quot;, &quot;sd&quot;) limits &lt;- aes(ymax = mean + qnorm(0.975)*sd, ymin=mean + qnorm(0.025)*sd) df_res$model &lt;- row.names(df_res) p &lt;- ggplot(df_res, aes(y=mean, x=model)) + geom_point() + geom_errorbar(limits) + geom_hline(yintercept = 0, col=&quot;#EB811B&quot;) + coord_flip() p + ggtitle(&quot;NOX coefficients and error bars&quot;) + theme(plot.background = element_rect(fill = &quot;transparent&quot;,colour = NA), legend.background = element_rect(colour = NA, fill = &quot;transparent&quot;)) library(tmap) # FIXME: uncommented BX_re tm_shape(boston_93) + tm_fill(c(&quot;MLM_re&quot;, &quot;HGLM_re&quot; #, &quot;BX_re&quot; ), midpoint=0, title=&quot;IID&quot;) + tm_facets(free.scales=FALSE) + tm_borders(lwd=0.3, alpha=0.4) + tm_layout(panel.labels=c(&quot;MLM&quot;, &quot;HGLM&quot;, &quot;BX&quot;)) # FIXME: uncommented BX_ss tm_shape(boston_93) + tm_fill(c(&quot;HGLM_ss&quot;, &quot;HSAR_ss&quot;, # &quot;BX_ss&quot;, &quot;GAM_ss&quot;), midpoint=0, title=&quot;SS&quot;) + tm_facets(free.scales=FALSE) + tm_borders(lwd=0.3, alpha=0.4) + tm_layout(panel.labels=c(&quot;HGLM SAR&quot;, &quot;HSAR SAR&quot;, &quot;BX MRF&quot;, &quot;GAM MRF&quot;)) set.seed(1014) options(digits = 3) knitr::opts_chunk$set( comment = &quot;#&quot;, collapse = knitr::is_latex_output(), cache = TRUE, # out.width = &quot;70%&quot;, fig.align = &#39;center&#39; # fig.width = 6, # fig.asp = 0.618, # 1 / phi # fig.show = &quot;hold&quot; ) options(dplyr.print_min = 6, dplyr.print_max = 6) options(stars.crs = 17) mapview::mapviewOptions(fgb = FALSE) # for units: (?) Sys.setenv(UDUNITS2_XML_PATH=&quot;&quot;) if (knitr::is_latex_output()) options(width = 66) #options(width = 72) set.seed(1014) options(digits = 3) knitr::opts_chunk$set( comment = &quot;#&quot;, collapse = knitr::is_latex_output(), cache = TRUE, # out.width = &quot;70%&quot;, fig.align = &#39;center&#39; # fig.width = 6, # fig.asp = 0.618, # 1 / phi # fig.show = &quot;hold&quot; ) options(dplyr.print_min = 6, dplyr.print_max = 6) options(stars.crs = 17) mapview::mapviewOptions(fgb = FALSE) # for units: (?) Sys.setenv(UDUNITS2_XML_PATH=&quot;&quot;) if (knitr::is_latex_output()) options(width = 66) #options(width = 72) # All R code in this book {-} set.seed(1014) options(digits = 3) knitr::opts_chunk$set( comment = &quot;#&quot;, collapse = knitr::is_latex_output(), cache = TRUE, # out.width = &quot;70%&quot;, fig.align = &#39;center&#39; # fig.width = 6, # fig.asp = 0.618, # 1 / phi # fig.show = &quot;hold&quot; ) options(dplyr.print_min = 6, dplyr.print_max = 6) options(stars.crs = 17) mapview::mapviewOptions(fgb = FALSE) # for units: (?) Sys.setenv(UDUNITS2_XML_PATH=&quot;&quot;) if (knitr::is_latex_output()) options(width = 66) #options(width = 72) a %&gt;% b() %&gt;% c() %&gt;% d(n = 10) d(c(b(a)), n = 10) tmp1 &lt;- b(a) tmp2 &lt;- c(tmp1) tmp3 &lt;- d(tmp2, n = 10) typeof(1:10) length(1:10) typeof(1.0) length(1.0) typeof(c(&quot;foo&quot;, &quot;bar&quot;)) length(c(&quot;foo&quot;, &quot;bar&quot;)) typeof(c(TRUE, FALSE)) i = integer(0) typeof(i) i length(i) a = c(1,2,3) a[2] a[[2]] a[2:3] a[2:3] = c(5,6) a a[[3]] = 10 a l &lt;- list(3, TRUE, &quot;foo&quot;) typeof(l) length(l) l[1] l[[1]] l[1:2] = list(4, FALSE) l l[[3]] = &quot;bar&quot; l l = list(first = 3, second = TRUE, third = &quot;foo&quot;) l l$second l$second = FALSE l 3 == NULL # not FALSE! NULL == NULL # not even TRUE! is.null(NULL) l = l[c(1,3)] # remove second, implicitly l l$second = NULL l a = 1:3 attr(a, &quot;some_meta_data&quot;) = &quot;foo&quot; a attr(a, &quot;some_meta_data&quot;) attr(a, &quot;some_meta_data&quot;) = &quot;bar&quot; attr(a, &quot;some_meta_data&quot;) attributes(a) attributes(a) = list(some_meta_data = &quot;foo&quot;) attributes(a) class(1:3) class(c(TRUE, FALSE)) class(c(&quot;TRUE&quot;, &quot;FALSE&quot;)) a = 1:3 class(a) = &quot;foo&quot; a class(a) attributes(a) print.foo = function(x, ...) print(paste(&quot;an object of class foo with length&quot;, length(x))) print(a) unclass(a) library(sf) p = st_polygon(list(rbind(c(0,0), c(1,0), c(1,1), c(0,0)))) p unclass(p) a = 1:8 class(a) attr(a, &quot;dim&quot;) = c(2,4) # or: dim(a) = c(2,4) class(a) a attr(a, &quot;dim&quot;) = c(2,2,2) # or: dim(a) = c(2,2,2) class(a) a a = c(first = 3, second = 4, last = 5) a[&quot;second&quot;] attributes(a) a = matrix(1:4, 2, 2) dimnames(a) = list(rows = c(&quot;row1&quot;, &quot;row2&quot;), cols = c(&quot;col1&quot;, &quot;col2&quot;)) a attributes(a) df = data.frame(a = 1:3, b = c(TRUE, FALSE, TRUE)) attributes(df) f = function(x) { a = create_obj(x) # call some other function attributes(a) = list(class = &quot;foo&quot;, meta = 33) a } f = function(x) { a = create_obj(x) # call some other function structure(a, class = &quot;foo&quot;, meta = 33) } set.seed(1014) options(digits = 3) knitr::opts_chunk$set( comment = &quot;#&quot;, collapse = knitr::is_latex_output(), cache = TRUE, # out.width = &quot;70%&quot;, fig.align = &#39;center&#39; # fig.width = 6, # fig.asp = 0.618, # 1 / phi # fig.show = &quot;hold&quot; ) options(dplyr.print_min = 6, dplyr.print_max = 6) options(stars.crs = 17) mapview::mapviewOptions(fgb = FALSE) # for units: (?) Sys.setenv(UDUNITS2_XML_PATH=&quot;&quot;) if (knitr::is_latex_output()) options(width = 66) #options(width = 72) "],["r-basics.html", "R basics Pipes 15.4 Data structures", " R basics This chapter provides some minimal set of R basics that may make it easier to read this book. A more comprehensive book on R basics is given in (Wickham 2014a), chapter 2. Pipes The %&gt;% (pipe) symbols should be read as then: we read a %&gt;% b() %&gt;% c() %&gt;% d(n = 10) as with a do b then c then d, and that is just alternative syntax for d(c(b(a)), n = 10) or tmp1 &lt;- b(a) tmp2 &lt;- c(tmp1) tmp3 &lt;- d(tmp2, n = 10) To many, the pipe-form is easier to read because execution order follows reading order (from left to right). Like nested function calls, it avoids the need to choose names for intermediate results. 15.4 Data structures As pointed out by (Chambers 2016), everything that exists in R is an object. This includes objects that make things happen, such as language objects or functions, but also the more basic “things”, such as data objects. Some basic R data structures will now be discussed. 15.4.1 Homogeneous vectors Data objects contain data, and possibly metadata. Data is always in the form of a vector, which can have different type. We can find the type by typeof, and vector length by length. Vectors are created by c, which combines individual elements: typeof(1:10) # [1] &quot;integer&quot; length(1:10) # [1] 10 typeof(1.0) # [1] &quot;double&quot; length(1.0) # [1] 1 typeof(c(&quot;foo&quot;, &quot;bar&quot;)) # [1] &quot;character&quot; length(c(&quot;foo&quot;, &quot;bar&quot;)) # [1] 2 typeof(c(TRUE, FALSE)) # [1] &quot;logical&quot; Vectors of this kind can only have a single type. Note that vectors can have length zero, e.g. in, i = integer(0) typeof(i) # [1] &quot;integer&quot; i # integer(0) length(i) # [1] 0 We can retrieve (or in assignments: replace) elements in a vector using [ or [[: a = c(1,2,3) a[2] # [1] 2 a[[2]] # [1] 2 a[2:3] # [1] 2 3 a[2:3] = c(5,6) a # [1] 1 5 6 a[[3]] = 10 a # [1] 1 5 10 where the difference is that [ can operate on an index range (or multiple indexes), and [[ operates on a single vector value. 15.4.2 Heterogeneous vectors: list An additional vector type is the list, which can combine any types in its elements: l &lt;- list(3, TRUE, &quot;foo&quot;) typeof(l) # [1] &quot;list&quot; length(l) # [1] 3 For lists, there is a further distinction between [ and [[: the single [ returns always a list, and [[ returns the contents of a list element: l[1] # [[1]] # [1] 3 l[[1]] # [1] 3 For replacement, one case use [ when providing a list, and [[ when providing a new value: l[1:2] = list(4, FALSE) l # [[1]] # [1] 4 # # [[2]] # [1] FALSE # # [[3]] # [1] &quot;foo&quot; l[[3]] = &quot;bar&quot; l # [[1]] # [1] 4 # # [[2]] # [1] FALSE # # [[3]] # [1] &quot;bar&quot; In case list elements are named, as in l = list(first = 3, second = TRUE, third = &quot;foo&quot;) l # $first # [1] 3 # # $second # [1] TRUE # # $third # [1] &quot;foo&quot; we can use names as in l[[\"second\"]] and this can be abbreviated to l$second # [1] TRUE l$second = FALSE l # $first # [1] 3 # # $second # [1] FALSE # # $third # [1] &quot;foo&quot; This is convenient, but also requires name look-up in the names attribute (see below). 15.4.2.1 NULL and removing list elements NULL is the null value in R; it is special in the sense that it doesn’t work in simple comparisons: 3 == NULL # not FALSE! # logical(0) NULL == NULL # not even TRUE! # logical(0) but has to be treated specially, using is.null: is.null(NULL) # [1] TRUE When we want to remove one or more list elements, we can do so by creating a new list that does not contain the elements that needed removal, as in l = l[c(1,3)] # remove second, implicitly l # $first # [1] 3 # # $third # [1] &quot;foo&quot; but we can also assign NULL to the element we want to eliminate: l$second = NULL l # $first # [1] 3 # # $third # [1] &quot;foo&quot; 15.4.3 Attributes We can glue arbitrary metadata objects to data objects, as in a = 1:3 attr(a, &quot;some_meta_data&quot;) = &quot;foo&quot; a # [1] 1 2 3 # attr(,&quot;some_meta_data&quot;) # [1] &quot;foo&quot; and this can be retrieved, or replaced by attr(a, &quot;some_meta_data&quot;) # [1] &quot;foo&quot; attr(a, &quot;some_meta_data&quot;) = &quot;bar&quot; attr(a, &quot;some_meta_data&quot;) # [1] &quot;bar&quot; In essence, the attribute of an object is a named list, and we can get or set the complete list by attributes(a) # $some_meta_data # [1] &quot;bar&quot; attributes(a) = list(some_meta_data = &quot;foo&quot;) attributes(a) # $some_meta_data # [1] &quot;foo&quot; A number of attributes are treated specially by R, see e.g. ?attributes. 15.4.3.1 object class and class attribute Every object in R “has a class”, meaning that class(obj) returns a character vector with the class of obj. Some objects have an implicit class, e.g. vectors class(1:3) # [1] &quot;integer&quot; class(c(TRUE, FALSE)) # [1] &quot;logical&quot; class(c(&quot;TRUE&quot;, &quot;FALSE&quot;)) # [1] &quot;character&quot; but we can also set the class explicit, either by using attr or by using class in the left-hand side of an expression: a = 1:3 class(a) = &quot;foo&quot; a # [1] 1 2 3 # attr(,&quot;class&quot;) # [1] &quot;foo&quot; class(a) # [1] &quot;foo&quot; attributes(a) # $class # [1] &quot;foo&quot; in which case the newly set class overrides the earlier implicit class. This way, we can add methods for class foo, e.g. by print.foo = function(x, ...) print(paste(&quot;an object of class foo with length&quot;, length(x))) print(a) # [1] &quot;an object of class foo with length 3&quot; Providing such methods are generally intended to create more usable software, but at the same time they may make the objects more opaque. It is sometimes useful to see what an object “is made of” by printing it after the class attribute is removed, as in unclass(a) # [1] 1 2 3 As a more elaborate example, consider the case where a polygon is made using package sf: library(sf) p = st_polygon(list(rbind(c(0,0), c(1,0), c(1,1), c(0,0)))) p # POLYGON ((0 0, 1 0, 1 1, 0 0)) which prints the well-known-text form; to understand what the data structure is like, we can use unclass(p) # [[1]] # [,1] [,2] # [1,] 0 0 # [2,] 1 0 # [3,] 1 1 # [4,] 0 0 15.4.3.2 the dim attribute The dim attribute sets the matrix or array dimensions: a = 1:8 class(a) # [1] &quot;integer&quot; attr(a, &quot;dim&quot;) = c(2,4) # or: dim(a) = c(2,4) class(a) # [1] &quot;matrix&quot; &quot;array&quot; a # [,1] [,2] [,3] [,4] # [1,] 1 3 5 7 # [2,] 2 4 6 8 attr(a, &quot;dim&quot;) = c(2,2,2) # or: dim(a) = c(2,2,2) class(a) # [1] &quot;array&quot; a # , , 1 # # [,1] [,2] # [1,] 1 3 # [2,] 2 4 # # , , 2 # # [,1] [,2] # [1,] 5 7 # [2,] 6 8 15.4.4 various names attributes Named vectors carry their names in a names attribute. We saw examples for lists above, an example for a numeric vector is: a = c(first = 3, second = 4, last = 5) a[&quot;second&quot;] # second # 4 attributes(a) # $names # [1] &quot;first&quot; &quot;second&quot; &quot;last&quot; More name attributes are e.g. dimnames of matrices or arrays, which not only names dimensions, but also the labels associated with each of the dimensions: a = matrix(1:4, 2, 2) dimnames(a) = list(rows = c(&quot;row1&quot;, &quot;row2&quot;), cols = c(&quot;col1&quot;, &quot;col2&quot;)) a # cols # rows col1 col2 # row1 1 3 # row2 2 4 attributes(a) # $dim # [1] 2 2 # # $dimnames # $dimnames$rows # [1] &quot;row1&quot; &quot;row2&quot; # # $dimnames$cols # [1] &quot;col1&quot; &quot;col2&quot; Data.frame objects have rows and columns, and each have names: df = data.frame(a = 1:3, b = c(TRUE, FALSE, TRUE)) attributes(df) # $names # [1] &quot;a&quot; &quot;b&quot; # # $class # [1] &quot;data.frame&quot; # # $row.names # [1] 1 2 3 15.4.5 using structure When programming, the pattern of adding or modifying attributes before returning an object is extremely common, an example being: f = function(x) { a = create_obj(x) # call some other function attributes(a) = list(class = &quot;foo&quot;, meta = 33) a } The last two statements can be contracted in f = function(x) { a = create_obj(x) # call some other function structure(a, class = &quot;foo&quot;, meta = 33) } where function structure adds, replaces, or (in case of value NULL) removes attributes from the object in its first argument. References "],["references.html", "References", " References "]]
